<!DOCTYPE html>
<html  lang="zh-CN" >
<head>
  <meta charset="UTF-8">
  <meta name="viewport"
    content="width=device-width, initial-scale=1.0, minimum-scale=1.0">
  <meta http-equiv="X-UA-Compatible" content="ie=edge">
  <meta name="theme-color" content="#fff" id="theme-color">
  <meta name="description" content="网络安全 Android ML CTF">
  <link rel="icon" href="/images/logo.png">
  <title>More about Auto-Encoder</title>
  
  
  <meta property="og:title" content="More about Auto-Encoder">
  
  
  <meta property="og:url" content="https://ch3nye.top/More-about-Auto-Encoder/index.html">
  
  
  <meta property="og:img" content="/images/logo.png">
  
  
  <meta property="og:img" content="网络安全 Android ML CTF">
  
  
  <meta property="og:type" content="article">
  <meta property="og:article:published_time" content="2020-12-11">
  <meta property="og:article:modified_time" content="2020-12-11">
  <meta property="og:article:author" content="Ch3nYe">
  
  
  <meta property="og:article:tag" content="ML">
  
  <meta property="og:article:tag" content="note">
  
  <meta property="og:article:tag" content="Next-Step-of-ML">
  
  
  
  
  <script>
    // control reverse button
    var reverseDarkList = {
      dark: 'light',
      light: 'dark'
    };
    var themeColor = {
      dark: '#1c1c1e',
      light: '#fff'
    }
    // get the data of css prefers-color-scheme
    var getCssMediaQuery = function() {
      return window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'light';
    };
    // reverse current darkmode setting function
    var reverseDarkModeSetting = function() {
      var setting = localStorage.getItem('user-color-scheme');
      if(reverseDarkList[setting]) {
        setting = reverseDarkList[setting];
      } else if(setting === null) {
        setting = reverseDarkList[getCssMediaQuery()];
      } else {
        return;
      }
      localStorage.setItem('user-color-scheme', setting);
      return setting;
    };
    // apply current darkmode setting
    var setDarkmode = function(mode) {
      var setting = mode || localStorage.getItem('user-color-scheme');
      if(setting === getCssMediaQuery()) {
        document.documentElement.removeAttribute('data-user-color-scheme');
        localStorage.removeItem('user-color-scheme');
        document.getElementById('theme-color').content = themeColor[setting];
      } else if(reverseDarkList[setting]) {
        document.documentElement.setAttribute('data-user-color-scheme', setting);
        document.getElementById('theme-color').content = themeColor[setting];
      } else {
        document.documentElement.removeAttribute('data-user-color-scheme');
        localStorage.removeItem('user-color-scheme');
        document.getElementById('theme-color').content = themeColor[getCssMediaQuery()];
      }
    };
    setDarkmode();
  </script>
  <script>
    function loadScript(url, cb) {
      var script = document.createElement('script');
      script.src = url;
      if (cb) script.onload = cb;
      script.async = true;
      document.body.appendChild(script);
    }
  </script>
  
  <link rel="preload" href="//at.alicdn.com/t/font_1946621_f7g5jnuftcf.css" as="style" >
  <link rel="preload" href="//at.alicdn.com/t/font_1952792_89b4ac4k4up.css" as="style" >
  
  <link rel="preload" href="//cdn.jsdelivr.net/npm/fslightbox@3.1.0/index.min.js" as="script">
  
  
  <link rel="preload" href="/js/lib/lozad.min.js" as="script">
  
  
  
  
  
  <link rel="prefetch" href="//cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.css" as="script">
  
  
  
  <link rel="prefetch" href="//cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.js" as="script">
  
  
  
  
<link rel="stylesheet" href="/css/main.css">

  
  
<link rel="stylesheet" href="//at.alicdn.com/t/font_1946621_f7g5jnuftcf.css">

  
  
<link rel="stylesheet" href="//at.alicdn.com/t/font_1952792_89b4ac4k4up.css">

  
  
  
  
  
  <link href="/js/lib/prism/prism-tomorrow.min.css" rel="stylesheet" data-prism="prism-tomorrow">
  
  
  
<link rel="stylesheet" href="/js/lib/prism/prism-line-numbers.min.css">

  
  
  
<meta name="generator" content="Hexo 5.3.0"></head>


<body>
  <div class="wrapper">
    
    <nav class="navbar">
  <div class="navbar-logo">
    <span class="navbar-logo-main">
      
      <img class="navbar-logo-img" src="/images/logo.png" alt="logo">
      
      <span class="navbar-logo-dsc">ch3nye's blog</span>
    </span>
  </div>
  <div class="navbar-menu">
    
    <a href="/" class="navbar-menu-item">
    
    首页
    
    </a>
    
    <a href="/archives" class="navbar-menu-item">
    
    归档
    
    </a>
    
    <a href="/tags" class="navbar-menu-item">
    
    标签
    
    </a>
    
    <a href="/categories" class="navbar-menu-item">
    
    分类
    
    </a>
    
    <a href="/about" class="navbar-menu-item">
    
    关于
    
    </a>
    
    <a href="/links" class="navbar-menu-item">
    
    友链
    
    </a>
    
    <a class="navbar-menu-item darknavbar" id="dark"><i class="iconfont icon-weather"></i></a>
    <a class="navbar-menu-item searchnavbar" id="search"><i class="iconfont icon-search" style="font-size: 1.2rem; font-weight: 400;"></i></a>
  </div>
</nav>
    
    <div id="local-search" style="display: none;">
      <input class="navbar-menu-item" id="search-input" placeholder="请输入搜索内容...">
      <div id="search-content"></div>
    </div>
    
    <div class="section-wrap">
      <div class="container">
        <div class="columns">
          <main class="main-column">
<article class="card card-content">
  <header>
    <h1 class="post-title">
      More about Auto-Encoder
    </h1>
  </header>
  <div class="post-meta post-show-meta">
    <time datetime="2020-12-10T16:00:00.000Z">
      <i class="iconfont icon-calendar" style="margin-right: 2px;"></i>
      <span>2020-12-11</span>
    </time>
    
    <span class="dot"></span>
    
    <a href="/categories/李宏毅机器学习笔记/" class="post-meta-link">李宏毅机器学习笔记</a>
    
    
    
    <span class="dot"></span>
    <span>3.6k 字</span>
    
  </div>
  
  <div class="post-meta post-show-meta" style="margin-top: -10px;">
    <div style="display: flex; align-items: center;">
      <i class="iconfont icon-biaoqian" style="margin-right: 2px; font-size: 1.15rem;"></i>
      
      
        <a href="/tags/ML/" class="post-meta-link">ML</a>
      
      
      <span class="dot"></span>
      
        <a href="/tags/note/" class="post-meta-link">note</a>
      
      
      <span class="dot"></span>
      
        <a href="/tags/Next-Step-of-ML/" class="post-meta-link">Next-Step-of-ML</a>
      
    </div>
  </div>
  
  </header>
  <div id="section" class="post-content">
    <h1 id="more-about-auto-encoder"><a class="markdownIt-Anchor" href="#more-about-auto-encoder"></a> More about Auto-Encoder</h1>
<p>这节课要讲的是在以前的课程上没有讲的， 近年来的热门的Auto-Encoder 技术。</p>
<h2 id="review"><a class="markdownIt-Anchor" href="#review"></a> Review</h2>
<p><img src="/images/image-20201210112818688.png" alt="image-20201210112818688" / srcset="/images/LoadingImage.gif" data-src="/images/image-20201210112818688.png" class="lozad post-image"></p>
<p>Auto-Encoder 就是有一个Encoder 的network ，有个Decoder 的network ，中间会产出一个vector ，你可以叫它Embedding，Latent Representation，Latent Code等。Decoder 会根据vector 产出一个输出，我们希望这个输出和原始的输入越接近越好，以此作为loss 更新模型参数。</p>
<p>今天要讲的内容分两个部分：</p>
<ul>
<li>为什么一定要Minimize reconstruction error ，有没有其他的做法呢</li>
<li>怎么让Encoder 输出的vector 更容易被解读</li>
</ul>
<p>接下来我们要先讲第一个问题，这里是通过引入Discriminator ，来替代minimize Reconstruction Error 。详情且看下面的描述。</p>
<h2 id="what-is-good-embedding"><a class="markdownIt-Anchor" href="#what-is-good-embedding"></a> What is good embedding?</h2>
<p>我们回想一下为什么要做embedding 呢，我们是希望通过这个embedding 来代表原来的输入。举例来说，我们看到耳机就能想到三玖，而不会想到一花。🤣</p>
<p><img src="/images/image-20201210113946970.png" alt="image-20201210113946970" / srcset="/images/LoadingImage.gif" data-src="/images/image-20201210113946970.png" class="lozad post-image"></p>
<h2 id="beyond-reconstruction"><a class="markdownIt-Anchor" href="#beyond-reconstruction"></a> <strong>Beyond Reconstruction</strong></h2>
<p>Encoder 吃进一个输入，就产生一个对应的代表该输入的embedding：</p>
<p><img src="/images/image-20201210114322581.png" alt="image-20201210114322581" / srcset="/images/LoadingImage.gif" data-src="/images/image-20201210114322581.png" class="lozad post-image"></p>
<p>那我们怎么知道这个产生的embedding 的代表性好不好呢，我们训一个Discriminator ，input 是一个张图片和embedding ，output 就是这一对输入是不是对应的，你可以想象这是一个二分类器。如上图左上方所示。</p>
<p>这个Discriminator 的训练资料就是很多（图片+embedding）的二元组，这些二元组有的是对应的有的是故意用不对应的组合，并且打上标签。如上图右下角所示。</p>
<p>然后就要定一个loss function <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>L</mi><mi>D</mi></msub></mrow><annotation encoding="application/x-tex">L_D</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault">L</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.32833099999999993em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:0.02778em;">D</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>，对于这个Discriminator 最简单的就是用Binary cross entropy ，没什么好说的。</p>
<p>接下来就是训练这个Discriminator 去minimize loss function ，去找一组最好的Discriminator 的参数 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>ϕ</mi></mrow><annotation encoding="application/x-tex">\phi</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8888799999999999em;vertical-align:-0.19444em;"></span><span class="mord mathdefault">ϕ</span></span></span></span> 。找到的最好的 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>ϕ</mi></mrow><annotation encoding="application/x-tex">\phi</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8888799999999999em;vertical-align:-0.19444em;"></span><span class="mord mathdefault">ϕ</span></span></span></span> 达到的最低的loss 我们把此时的损失函数值叫做 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msup><msub><mi>L</mi><mi>D</mi></msub><mo>∗</mo></msup></mrow><annotation encoding="application/x-tex">{L_D}^*</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.911926em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord"><span class="mord"><span class="mord mathdefault">L</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.32833099999999993em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:0.02778em;">D</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.761926em;"><span style="top:-3.1362300000000003em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mbin mtight">∗</span></span></span></span></span></span></span></span></span></span></span> 。</p>
<p>如果说这个 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msup><msub><mi>L</mi><mi>D</mi></msub><mo>∗</mo></msup></mrow><annotation encoding="application/x-tex">{L_D}^*</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.911926em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord"><span class="mord"><span class="mord mathdefault">L</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.32833099999999993em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:0.02778em;">D</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.761926em;"><span style="top:-3.1362300000000003em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mbin mtight">∗</span></span></span></span></span></span></span></span></span></span></span> 很小，那意味着现在这个Encoder 是很好的，Discriminator 很容易就能知道embedding 之间是不同的，可以轻易的分辨出各个embedding 应该对应的原始输入；如果说这个 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msup><msub><mi>L</mi><mi>D</mi></msub><mo>∗</mo></msup></mrow><annotation encoding="application/x-tex">{L_D}^*</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.911926em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord"><span class="mord"><span class="mord mathdefault">L</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.32833099999999993em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:0.02778em;">D</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.761926em;"><span style="top:-3.1362300000000003em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mbin mtight">∗</span></span></span></span></span></span></span></span></span></span></span> 很大，那就意味着现在这个Encoder 就是比较差的，这个Encoder 得到的embedding 都比较相似，Discriminator 难以分辨这些embedding 之间的区别。</p>
<p>如果形象点用颜色来说，好的Encoder 得到的embedding 应该会是如上图颜色各异的，而差的Encoder 得到的embedding 可能就是下面这样颜色相近的：</p>
<p><img src="/images/image-20201210120313813.png" alt="image-20201210120313813" / srcset="/images/LoadingImage.gif" data-src="/images/image-20201210120313813.png" class="lozad post-image"></p>
<p>接下来我们要做的事，就是根据 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msup><msub><mi>L</mi><mi>D</mi></msub><mo>∗</mo></msup></mrow><annotation encoding="application/x-tex">{L_D}^*</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.911926em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord"><span class="mord"><span class="mord mathdefault">L</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.32833099999999993em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:0.02778em;">D</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.761926em;"><span style="top:-3.1362300000000003em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mbin mtight">∗</span></span></span></span></span></span></span></span></span></span></span> 调整Encoder 的参数 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>θ</mi></mrow><annotation encoding="application/x-tex">\theta</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.69444em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.02778em;">θ</span></span></span></span> ，使得Encoder 得到的embedding 在通过Discriminator 时候能被很好的分辨，得到较低的 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msup><msub><mi>L</mi><mi>D</mi></msub><mo>∗</mo></msup></mrow><annotation encoding="application/x-tex">{L_D}^*</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.911926em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord"><span class="mord"><span class="mord mathdefault">L</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.32833099999999993em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:0.02778em;">D</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.761926em;"><span style="top:-3.1362300000000003em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mbin mtight">∗</span></span></span></span></span></span></span></span></span></span></span> 。</p>
<p><img src="/images/image-20201210120556079.png" alt="image-20201210120556079" / srcset="/images/LoadingImage.gif" data-src="/images/image-20201210120556079.png" class="lozad post-image"></p>
<p>上述的这个方法，被用在Deep InfoMax(DIM) 这篇paper 中。这个Discriminator 可以做不同的设计，比如用不同的loss 计算方法等，结果会不太一样。</p>
<p>在训练的时候，我们一起train Encoder 和Discriminator ，这件事就好像我们在train 一般的Auto-Encoder 时，一起train Encoder 和Decoder 去minimize Reconstruction Error 一样。</p>
<p>这就是引入Discriminator ，来替代minimize Reconstruction Error 的做法。</p>
<h2 id="typical-auto-encoder-is-a-special-case"><a class="markdownIt-Anchor" href="#typical-auto-encoder-is-a-special-case"></a> Typical auto-encoder is a special case</h2>
<p>经典的auto-encoder 可以看作上述的方法的一个特例。怎么说呢</p>
<p><img src="/images/image-20201210121556480.png" alt="image-20201210121556480" / srcset="/images/LoadingImage.gif" data-src="/images/image-20201210121556480.png" class="lozad post-image"></p>
<p>你可以把Discriminator 看作它根据vector 做了一个Decoder 做的事情，然后把输出和原始输入算一个Reconstruction Error 分数。</p>
<h2 id="sequential-data"><a class="markdownIt-Anchor" href="#sequential-data"></a> Sequential Data</h2>
<p>如果你的数据是序列性的，比如说文章，这时你就可以做更多的变化。</p>
<p><img src="/images/image-20201210122806382.png" alt="image-20201210122806382" / srcset="/images/LoadingImage.gif" data-src="/images/image-20201210122806382.png" class="lozad post-image"></p>
<h3 id="skip-thought"><a class="markdownIt-Anchor" href="#skip-thought"></a> Skip thought</h3>
<blockquote>
<p><a target="_blank" rel="noopener" href="https://papers.nips.cc/paper/5950-skip-thought-vectors.pdf">https://papers.nips.cc/paper/5950-skip-thought-vectors.pdf</a></p>
</blockquote>
<p>如上图所示，Skip thought 就是根据当前输入的的句子预测上下文。这件事和训 WordEmbedding 是很像的，WordEmbedding 是说把文章中出现在同样或者相似上下文的单词语义应该是一样的，所以用相同或者相似的Embedding 来表示。Skip thought 的思想也是这样，只是扩展到句子，如果两个句子的上下文是相同或者相似的，那这两个句子应该是语义相近的。比如说，A提问这个东西有多贵，B回答10块，A提问这个东西要多少钱，B回答10块。Skip thought 就会知道有多贵和要多少钱的语义是相似的。</p>
<p>Skip thought 不仅要训练Encoder（做embedding） 还要训练Decoder （做预测上下文），让机器产生预测结果，这是比较耗时的。</p>
<h3 id="quick-thought"><a class="markdownIt-Anchor" href="#quick-thought"></a> Quick thought</h3>
<blockquote>
<p><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1803.02893.pdf">https://arxiv.org/pdf/1803.02893.pdf</a></p>
</blockquote>
<p>这是Skip thought 的升级版，速度比较快。我就只learn encoder 不去learn decode 了。</p>
<p>我们现在把每个句子都同过encoder 得到各自的embedding ，每个句子要跟它的下个句子的embedding 越近越好。如果是其他的句子，那它们的embedding 就要和当前句子的embedding 越远越好。</p>
<p>真正实作的时候就是，有个classifier 它吃当前句子embedding 、下个句子embedding 和一些随机sample 的句子的embedding ，然后给出哪个句子是当前句子的下一句。如上图所示。</p>
<p>classifer 和encoder 是共同训练的。文章中的classifier 是很简单的，它就是算当前句子embedding 和其他句子embedding 之间的内积，内积越大就越相信这个句子是当前句子的下一个。</p>
<h3 id="contrastive-predictive-coding-cpc"><a class="markdownIt-Anchor" href="#contrastive-predictive-coding-cpc"></a> Contrastive Predictive Coding (CPC)</h3>
<p><img src="/images/image-20201210124600407.png" alt="image-20201210124600407" / srcset="/images/LoadingImage.gif" data-src="/images/image-20201210124600407.png" class="lozad post-image"></p>
<blockquote>
<p><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1807.03748.pdf">https://arxiv.org/pdf/1807.03748.pdf</a></p>
</blockquote>
<p>看图就大概能明白它在做什么，输入是一段声音，这段声音分成小段，通过encoder 得到对应的embedding ，希望得到的embedding 能去预测接下来的同一个encoder 会output 的embedding 。就和上面Quick thought 的概念有点像。</p>
<h2 id="more-interpretable-embedding"><a class="markdownIt-Anchor" href="#more-interpretable-embedding"></a> More interpretable embedding</h2>
<p>接下来我们进入下一个主题，怎么让embedding 更具解释性。</p>
<p><img src="/images/image-20201210154838644.png" alt="image-20201210154838644" / srcset="/images/LoadingImage.gif" data-src="/images/image-20201210154838644.png" class="lozad post-image"></p>
<h3 id="feature-disentangle"><a class="markdownIt-Anchor" href="#feature-disentangle"></a> Feature Disentangle</h3>
<p>Disentangle 这个词的中文意思是&quot;解开&quot;，大概就是下图中一团东西缠在一起想要解开的那个解开的意思。</p>
<p><img src="/images/image-20201210155841879.png" alt="image-20201210155841879" / srcset="/images/LoadingImage.gif" data-src="/images/image-20201210155841879.png" class="lozad post-image"></p>
<p>我们encoding 的对象包含各式各样的信息，比如说一段声音信号，包含语者的语义、语者的语调、环境噪音等等的信息，文字也是一样，有语义的信息、文法的信息等，图片也是，内容的信息、图片风格的信息等。</p>
<p>以语音信息为例，encoder 得到的embedding 可能包括语者的语义信息、语调信息、环境噪声等很多信息，但是我们不知道这个向量中那些维度对应那些信息。现在，我们希望decoder 可以告诉我们那些维度是语义信息、哪些维度是语者的信息等等。</p>
<p><img src="/images/image-20201210160407780.png" alt="image-20201210160407780" / srcset="/images/LoadingImage.gif" data-src="/images/image-20201210160407780.png" class="lozad post-image"></p>
<p>这个想法，可以通过两个思路来做。第一种，如上图上半部分所示，我们希望encoder 输出的embedding 有一部分代表语音信息，一部分代表语者信息。第二种，做个变形如上图下半部分所示，我们搞两个encoder 一个提取出语义信息的embedding 一个提取出语者信息的embedding ，把两个embedding 拼在一起放入Decoder 才能还原输入。</p>
<p>这里是做了简化，假设只有两个信息，实际上有更多的。</p>
<p>那这样的话能做到的什么呢？</p>
<h3 id="feature-disentangle-voice-conversion"><a class="markdownIt-Anchor" href="#feature-disentangle-voice-conversion"></a> Feature Disentangle-Voice Conversion</h3>
<p>一看就知道李老师也是老变声器了。🤣</p>
<p><img src="/images/image-20201210161246132.png" alt="image-20201210161246132" / srcset="/images/LoadingImage.gif" data-src="/images/image-20201210161246132.png" class="lozad post-image"></p>
<p>上图很好理解了，我们如上图所示做训练，我们用语音信息训练这个auto-encoder 模型，让encoder output 的embedding 可以把语者和语义信息分开来。然后：</p>
<p><img src="/images/image-20201210161745063.png" alt="image-20201210161745063" / srcset="/images/LoadingImage.gif" data-src="/images/image-20201210161745063.png" class="lozad post-image"></p>
<p>我们做如上图所示的embedding 拼接，丢给decoder 它就能输出男生说&quot;How are you?&quot;的音频。😮</p>
<p>你可能会说这有什么用？这当然有用，比如用新垣结衣的声音劝你读博士，你可能就会满口答应下来。🤣</p>
<p><img src="/images/image-20201210162000658.png" alt="image-20201210162000658" / srcset="/images/LoadingImage.gif" data-src="/images/image-20201210162000658.png" class="lozad post-image"></p>
<p>那要怎么做才能让encoder 把不同的信息分开到不同的维度上呢？下面介绍几种做法。</p>
<h3 id="feature-disentangle-adversarial-training"><a class="markdownIt-Anchor" href="#feature-disentangle-adversarial-training"></a> Feature Disentangle-Adversarial Training</h3>
<p>用GAN的思想</p>
<p><img src="/images/image-20201210194924926.png" alt="image-20201210194924926" / srcset="/images/LoadingImage.gif" data-src="/images/image-20201210194924926.png" class="lozad post-image"></p>
<p>如上图所示，做法是这样的，我们训练一个语者辨识的classifier ，Encoder 想办法去骗过speaker classifier 。比如说把embedding 的前100维给speaker classifier 做辨识，当Encoder 尝试骗过speaker classifier 的时候，Encoder 就可能会把有关语者的信息藏在embedding 后面的维度中。以此，来做到把语者和语义信息分开到不同的维度上。</p>
<p>所以，从GAN的角度来说，speaker classifier 就是Discriminator ，Encoder 就是Generator，speaker classifier 和Encoder 是迭代train的，就是你先train speaker classifier 再train Encoder ，如此交替往复。</p>
<h3 id="feature-disentangle-designed-network-architecture"><a class="markdownIt-Anchor" href="#feature-disentangle-designed-network-architecture"></a> Feature Disentangle-Designed Network Architecture</h3>
<p><img src="/images/image-20201210201419636.png" alt="image-20201210201419636" / srcset="/images/LoadingImage.gif" data-src="/images/image-20201210201419636.png" class="lozad post-image"></p>
<p>我们还可以直接修改Encoder 的架构，如上图所示，直接让不同的Encoder 输出对应的信息，滤掉不需要的信息。举例来说，有一种神经网络的layer 叫instance normalization ，这种layer 我们就不展开讲了，它能做到的就是移除global information ，global information 就是所有样本都具有的信息。那当我们把同一个人说的话都输入网络，网络种instance normalization layer 就可能会滤掉语者信息。</p>
<p>但是这样是不够的，这样就算我们保证了Encoder1 只包含了语义信息，我们也不能保证Encoder2 只包含语者信息。所以我们要在Decoder 上加一个adaptive instance normalization layer ，如下图所示：</p>
<p><img src="/images/image-20201210202421151.png" alt="image-20201210202421151" / srcset="/images/LoadingImage.gif" data-src="/images/image-20201210202421151.png" class="lozad post-image"></p>
<p>Encoder1 的embedding 直接input 给Deocder ，Encoder2 的embedding input 到AdaIN 这个layer ，AdaIN 会调整输出的global 的information ，也就是说如果Encoder2 在其输出的embedding 中放了语义信息，这个embedding 就会很大程度上改变Deocder 的输出。用这种方法，使Encoder2 输出的embedding 尽量不包含语义信息。</p>
<p>用这种改变Encoder 架构的方法来实现，将不同信息分离到不同维度上。</p>
<p>这里老师演示了他的学生Ju-chieh Chou 用Adversarial Training 得到的结果，可以参考：</p>
<blockquote>
<p><a target="_blank" rel="noopener" href="https://jjery2243542.github.io/voice_conversion_demo/">https://jjery2243542.github.io/voice_conversion_demo/</a></p>
</blockquote>
<h2 id="discrete-representation"><a class="markdownIt-Anchor" href="#discrete-representation"></a> Discrete Representation</h2>
<p>接下来我们要讲的是，过去我们在训Auto-Encoder 的时候得到的向量都是连续值，这个向量具体表示什么，你可能自己也不是很清楚。现在我们考虑Encoder 能不能输出离散的embedding ，这样我们就更容易解读这个embedding 的含义，也更容易做分群，比如说Encoder 输出是1的图片是一类，输出是2的图片是一类。所以，Encoder 如果能输出Discrete 的embedding ，那解读起来会更容易。</p>
<p><img src="/images/image-20201211115801104.png" alt="image-20201211115801104" / srcset="/images/LoadingImage.gif" data-src="/images/image-20201211115801104.png" class="lozad post-image"></p>
<blockquote>
<p><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1611.01144.pdf">https://arxiv.org/pdf/1611.01144.pdf</a></p>
</blockquote>
<p>举例来说，我们就让embedding 是One-hot embedding，如上图所示，我们就在Encoder 输出embedding 后面加点东西，它做到的事情就是把整个embedding 中最大的一维设为1，其他都是设0就好了。</p>
<p>如果你不想要one-hot vector ，那也可以让embedding 转为binary vector ，如上图下半部分所示，某个维度的值大于0.5就设1，否则就设0。</p>
<p>你可能会说那这个东西没法微分啊，但是实际上还是有一些技巧可以做的，这里就不展开了，可以自行查阅论文。</p>
<h3 id="vector-quantized-variational-auto-encoder-vqvae"><a class="markdownIt-Anchor" href="#vector-quantized-variational-auto-encoder-vqvae"></a> Vector Quantized Variational Auto-encoder (VQVAE)</h3>
<p>上述的做法在文献上有个非常知名的做法叫做VQVAE</p>
<p><img src="/images/image-20201211120347473.png" alt="image-20201211120347473" / srcset="/images/LoadingImage.gif" data-src="/images/image-20201211120347473.png" class="lozad post-image"></p>
<blockquote>
<p><a target="_blank" rel="noopener" href="https://arxiv.org/abs/1711.00937">https://arxiv.org/abs/1711.00937</a></p>
<p><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1901.08810.pdf">https://arxiv.org/pdf/1901.08810.pdf</a></p>
</blockquote>
<p>VQVAE 的做法是这样的，有一个Codebook 其中包含多个vector，这里假设只有5个好了，这些vector 是从数据中学出来的。输入一张图片给Encoder 输出一个数值上是连续的embedding ，那这个embedding 和Codebook 中的vector 算相似度，取相似度最高的作为Encoder 的输入，然后取minimize reconstruction error，结束。</p>
<p>你会说，你算完相似度然后取最相似的vector 这个步骤相当于是在做Discrete，但是这不是没法微分吗，实际上是可以做的，有一些技巧，不展开了，自行读文献吧。</p>
<p>还有一个重要的事情是，如果你用VQVAE 或者其他的Discrete embedding的方法，那你就能做到让Deocder 得到的Discrete embedding 只包含语义信息而不包含语者信息和环境噪声等，也就是说只有有关文字的信息才会被存下来。原因是这样的，你想想看这些Discrete embedding 就是容易存Discrete 的信息，而声音信息、环境噪声都是连续的，但文字信息是一个一个的token，是离散的，所以说文字信息被保留下来，其他信息被滤掉了。</p>
<h3 id="sequence-as-embedding"><a class="markdownIt-Anchor" href="#sequence-as-embedding"></a> Sequence as Embedding</h3>
<p>我们上面说让embedding 变成离散的会更容解读，那我们甚至可以让中间表示不再是一个向量，让它用word sequence 表示：</p>
<p><img src="/images/image-20201211122057281.png" alt="image-20201211122057281" / srcset="/images/LoadingImage.gif" data-src="/images/image-20201211122057281.png" class="lozad post-image"></p>
<blockquote>
<p><a target="_blank" rel="noopener" href="https://arxiv.org/abs/1810.02851">https://arxiv.org/abs/1810.02851</a></p>
</blockquote>
<p>假如说我们的Encoder 的输入对象是document，我们可以learn 一个seq2seq2seq 的model ，Encoder 做文章压缩，得到中间的word sequence ，Decoder 根据中间词序列做文章还原。这样的话我们直觉上会觉得，中间的word sequence 就是对文章的summary 。但是事实上如果我们直接这么train 下去中间的结果是不可读的。因为，Encoder 和Decoder 都是机器，他们会自己的暗语，说一些只有他们才能懂的word sequence 。比如说，台湾大学，中结果可能不是&quot;台大&quot;，而是是&quot;湾学&quot;，反正只要Deocder 能正确解回台湾大学就可以了。</p>
<p>那怎么让Encoder 输出的sequence 让人能看懂呢，我们就用GAN 的技术。</p>
<p><img src="/images/image-20201211123024043.png" alt="image-20201211123024043" / srcset="/images/LoadingImage.gif" data-src="/images/image-20201211123024043.png" class="lozad post-image"></p>
<p>我们再那一个Discriminator 来，它来判断一个句子是不是人写的，让Encoder 努力学习骗过Discriminator 。这样就能让中间的word sequence 变得人类可读。</p>
<p>你可能又说，这里又不能微分啊，中间的latent representation 是一个word sequence ，也是Discrete 的，Encoder Decoder 整个network 合起来不能微分啊。确实不能微分，所以实作的时候是用reinforcement learning 硬train Encoder 和Deocder 的。</p>
<p>下面是一些实验结果：</p>
<p><img src="/images/image-20201211123204980.png" alt="image-20201211123204980" / srcset="/images/LoadingImage.gif" data-src="/images/image-20201211123204980.png" class="lozad post-image"></p>
<p><img src="/images/image-20201211123221056.png" alt="image-20201211123221056" / srcset="/images/LoadingImage.gif" data-src="/images/image-20201211123221056.png" class="lozad post-image"></p>
<h3 id="tree-as-embedding"><a class="markdownIt-Anchor" href="#tree-as-embedding"></a> Tree as Embedding</h3>
<p><img src="/images/image-20201211123735983.png" alt="image-20201211123735983" / srcset="/images/LoadingImage.gif" data-src="/images/image-20201211123735983.png" class="lozad post-image"></p>
<blockquote>
<p><a target="_blank" rel="noopener" href="https://arxiv.org/abs/1806.07832">https://arxiv.org/abs/1806.07832</a></p>
<p><a target="_blank" rel="noopener" href="https://arxiv.org/abs/1904.03746">https://arxiv.org/abs/1904.03746</a></p>
</blockquote>
<p>另外还有一些比较新的研究成果，供大家参考。</p>
<h2 id="conclusion"><a class="markdownIt-Anchor" href="#conclusion"></a> Conclusion</h2>
<p><img src="/images/image-20201211123820607.png" alt="image-20201211123820607" / srcset="/images/LoadingImage.gif" data-src="/images/image-20201211123820607.png" class="lozad post-image"></p>
<p>总结一下就是讲了：</p>
<ul>
<li>除了reconstruction error 以外有没有别的做法
<ul>
<li>Using Discriminator</li>
<li>Sequential Data</li>
</ul>
</li>
<li>有没有比较好的解释embedding 的方法
<ul>
<li>Feature Disentangle</li>
<li>Discrete and Structured</li>
</ul>
</li>
</ul>

  </div>
  <div>
  
  <div class="post-note note-info copyright" style="margin-top: 42px">
    <p><span style="font-weight: bold;">作者：</span><a target="_blank" rel="nofollow noopener noreferrer" href="https://ch3nye.top/about">Ch3nYe</a></p>
    <p><span style="font-weight: bold;">文章链接：</span><a target="_blank" rel="nofollow noopener noreferrer" href="https://ch3nye.top/More-about-Auto-Encoder/">https://ch3nye.top/More-about-Auto-Encoder/</a></p>
    <p><span style="font-weight: bold;">版权声明：</span>本博客所有文章除特别声明外，均采用<a target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh">CC BY-NC-SA 4.0 协议</a>。转载请注明出处！</p>
  </div>
  
  </div>
</article>
<div class="nav">
  
  <div class="nav-item-prev">
    <a href="/Network-Compression/" class="nav-link">
      <i class="iconfont icon-left nav-prev-icon"></i>
      <div>
        <div class="nav-label">上一篇</div>
        
        <div class="nav-title">Network Compression </div>
        
      </div>
    </a>
  </div>
  
  
  <div class="nav-item-next">
    <a href="/Meta-Learning-Metric-based/" class="nav-link">
      <div>
        <div class="nav-label">下一篇</div>
        
        <div class="nav-title">Meta Learning-Metric-based </div>
        
      </div>
      <i class="iconfont icon-right nav-next-icon"></i>
    </a>
  </div>
  
</div>

<div class="card card-content comment-card" style="margin-top: 16px;">
  <div class="comment-card-title">评论</div>
  
  <div id="gitalk-container"></div>
  
<link rel="stylesheet" href="//cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.css">

  
<script src="//cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.js"></script>

  <script>
    var gitalk = new Gitalk({
      clientID: '049b30eb10ea05082ef1',
      clientSecret: 'c33ad95041c69907b3136b895008320f000987db',
      repo: 'Ch3nYe.github.io',
      owner: 'Ch3nYe',
      admin: 'Ch3nYe',
      id: 'location.href',
      distractionFreeMode: 'false',
      language: 'navigator.language || navigator.userLanguage',
    })

    gitalk.render('gitalk-container')
  </script>

</div>

<div class="card card-content toc-card" id="mobiletoc">
  <div class="toc-header"><i class="iconfont icon-menu" style="padding-right: 2px;"></i>目录</div>
<ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#more-about-auto-encoder"><span class="toc-text"> More about Auto-Encoder</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#review"><span class="toc-text"> Review</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#what-is-good-embedding"><span class="toc-text"> What is good embedding?</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#beyond-reconstruction"><span class="toc-text"> Beyond Reconstruction</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#typical-auto-encoder-is-a-special-case"><span class="toc-text"> Typical auto-encoder is a special case</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#sequential-data"><span class="toc-text"> Sequential Data</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#skip-thought"><span class="toc-text"> Skip thought</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#quick-thought"><span class="toc-text"> Quick thought</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#contrastive-predictive-coding-cpc"><span class="toc-text"> Contrastive Predictive Coding (CPC)</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#more-interpretable-embedding"><span class="toc-text"> More interpretable embedding</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#feature-disentangle"><span class="toc-text"> Feature Disentangle</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#feature-disentangle-voice-conversion"><span class="toc-text"> Feature Disentangle-Voice Conversion</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#feature-disentangle-adversarial-training"><span class="toc-text"> Feature Disentangle-Adversarial Training</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#feature-disentangle-designed-network-architecture"><span class="toc-text"> Feature Disentangle-Designed Network Architecture</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#discrete-representation"><span class="toc-text"> Discrete Representation</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#vector-quantized-variational-auto-encoder-vqvae"><span class="toc-text"> Vector Quantized Variational Auto-encoder (VQVAE)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#sequence-as-embedding"><span class="toc-text"> Sequence as Embedding</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#tree-as-embedding"><span class="toc-text"> Tree as Embedding</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#conclusion"><span class="toc-text"> Conclusion</span></a></li></ol></li></ol>
</div></main>
          <aside class="left-column">
            
            <div class="card card-author">
              
<img src="/images/logo.png" class="author-img">

<p class="author-name">Ch3nYe</p>
<p class="author-description">如果有文章有任何错误请留言，谢谢🙏</p>
<div class="author-message">
  <a class="author-posts-count" href="/archives">
    <span>33</span>
    <span>文章</span>
  </a>
  <a class="author-categories-count" href="/categories">
    <span>5</span>
    <span>分类</span>
  </a>
  <a class="author-tags-count" href="/tags">
    <span>42</span>
    <span>标签</span>
  </a>
</div>

<div class="author-card-society">
  
    <div class="author-card-society-icon">
      <a target="_blank" rel="noopener" href="https://github.com/ch3nye/">
        <i class="iconfont icon-github society-icon"></i>
      </a>
    </div>
  
    <div class="author-card-society-icon">
      <a href="mailto:sud0su@163.com">
        <i class="iconfont icon-mail society-icon"></i>
      </a>
    </div>
  
</div>

            </div>
            
            <div class="sticky-tablet">
  
  
  <article class="display-when-two-columns spacer">
    <div class="card card-content toc-card">
      <div class="toc-header"><i class="iconfont icon-menu" style="padding-right: 2px;"></i>目录</div>
<ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#more-about-auto-encoder"><span class="toc-text"> More about Auto-Encoder</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#review"><span class="toc-text"> Review</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#what-is-good-embedding"><span class="toc-text"> What is good embedding?</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#beyond-reconstruction"><span class="toc-text"> Beyond Reconstruction</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#typical-auto-encoder-is-a-special-case"><span class="toc-text"> Typical auto-encoder is a special case</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#sequential-data"><span class="toc-text"> Sequential Data</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#skip-thought"><span class="toc-text"> Skip thought</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#quick-thought"><span class="toc-text"> Quick thought</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#contrastive-predictive-coding-cpc"><span class="toc-text"> Contrastive Predictive Coding (CPC)</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#more-interpretable-embedding"><span class="toc-text"> More interpretable embedding</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#feature-disentangle"><span class="toc-text"> Feature Disentangle</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#feature-disentangle-voice-conversion"><span class="toc-text"> Feature Disentangle-Voice Conversion</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#feature-disentangle-adversarial-training"><span class="toc-text"> Feature Disentangle-Adversarial Training</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#feature-disentangle-designed-network-architecture"><span class="toc-text"> Feature Disentangle-Designed Network Architecture</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#discrete-representation"><span class="toc-text"> Discrete Representation</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#vector-quantized-variational-auto-encoder-vqvae"><span class="toc-text"> Vector Quantized Variational Auto-encoder (VQVAE)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#sequence-as-embedding"><span class="toc-text"> Sequence as Embedding</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#tree-as-embedding"><span class="toc-text"> Tree as Embedding</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#conclusion"><span class="toc-text"> Conclusion</span></a></li></ol></li></ol>
    </div>
  </article>
  
  
  <article class="card card-content categories-widget">
    <div class="categories-card">
  <div class="categories-header"><i class="iconfont icon-fenlei" style="padding-right: 2px;"></i>分类</div>
  <div class="categories-list">
    
      <a href="/categories/备忘">
        <div class="categories-list-item">
          备忘
          <span class="categories-list-item-badge">1</span>
        </div>
      </a>
    
      <a href="/categories/李宏毅机器学习笔记">
        <div class="categories-list-item">
          李宏毅机器学习笔记
          <span class="categories-list-item-badge">15</span>
        </div>
      </a>
    
      <a href="/categories/论文阅读">
        <div class="categories-list-item">
          论文阅读
          <span class="categories-list-item-badge">8</span>
        </div>
      </a>
    
      <a href="/categories/笔记">
        <div class="categories-list-item">
          笔记
          <span class="categories-list-item-badge">6</span>
        </div>
      </a>
    
      <a href="/categories/实战">
        <div class="categories-list-item">
          实战
          <span class="categories-list-item-badge">3</span>
        </div>
      </a>
    
  </div>
</div>
  </article>
  
  <article class="card card-content tags-widget">
    <div class="tags-card">
  <div class="tags-header"><i class="iconfont icon-biaoqian" style="padding-right: 2px;"></i>热门标签</div>
  <div class="tags-list">
    
    <a href="\tags\note" title="note"><div class="tags-list-item">note</div></a>
    
    <a href="\tags\ML" title="ML"><div class="tags-list-item">ML</div></a>
    
    <a href="\tags\Next-Step-of-ML" title="Next-Step-of-ML"><div class="tags-list-item">Next-Step-of-ML</div></a>
    
    <a href="\tags\笔记" title="笔记"><div class="tags-list-item">笔记</div></a>
    
    <a href="\tags\Android" title="Android"><div class="tags-list-item">Android</div></a>
    
    <a href="\tags\论文" title="论文"><div class="tags-list-item">论文</div></a>
    
    <a href="\tags\总结" title="总结"><div class="tags-list-item">总结</div></a>
    
    <a href="\tags\Deep-Learning" title="Deep-Learning"><div class="tags-list-item">Deep-Learning</div></a>
    
    <a href="\tags\实战" title="实战"><div class="tags-list-item">实战</div></a>
    
    <a href="\tags\crack" title="crack"><div class="tags-list-item">crack</div></a>
    
    <a href="\tags\二进制" title="二进制"><div class="tags-list-item">二进制</div></a>
    
    <a href="\tags\学习" title="学习"><div class="tags-list-item">学习</div></a>
    
    <a href="\tags\静态分析" title="静态分析"><div class="tags-list-item">静态分析</div></a>
    
    <a href="\tags\Reverse-Engineering" title="Reverse-Engineering"><div class="tags-list-item">Reverse-Engineering</div></a>
    
    <a href="\tags\Binary" title="Binary"><div class="tags-list-item">Binary</div></a>
    
    <a href="\tags\HTTPS" title="HTTPS"><div class="tags-list-item">HTTPS</div></a>
    
  </div>
</div>
  </article>
  
  
</div>
          </aside>
          <aside class="right-column">
            <div class="sticky-widescreen">
  
  
  <article class="card card-content toc-card">
    <div class="toc-header"><i class="iconfont icon-menu" style="padding-right: 2px;"></i>目录</div>
<ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#more-about-auto-encoder"><span class="toc-text"> More about Auto-Encoder</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#review"><span class="toc-text"> Review</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#what-is-good-embedding"><span class="toc-text"> What is good embedding?</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#beyond-reconstruction"><span class="toc-text"> Beyond Reconstruction</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#typical-auto-encoder-is-a-special-case"><span class="toc-text"> Typical auto-encoder is a special case</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#sequential-data"><span class="toc-text"> Sequential Data</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#skip-thought"><span class="toc-text"> Skip thought</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#quick-thought"><span class="toc-text"> Quick thought</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#contrastive-predictive-coding-cpc"><span class="toc-text"> Contrastive Predictive Coding (CPC)</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#more-interpretable-embedding"><span class="toc-text"> More interpretable embedding</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#feature-disentangle"><span class="toc-text"> Feature Disentangle</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#feature-disentangle-voice-conversion"><span class="toc-text"> Feature Disentangle-Voice Conversion</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#feature-disentangle-adversarial-training"><span class="toc-text"> Feature Disentangle-Adversarial Training</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#feature-disentangle-designed-network-architecture"><span class="toc-text"> Feature Disentangle-Designed Network Architecture</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#discrete-representation"><span class="toc-text"> Discrete Representation</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#vector-quantized-variational-auto-encoder-vqvae"><span class="toc-text"> Vector Quantized Variational Auto-encoder (VQVAE)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#sequence-as-embedding"><span class="toc-text"> Sequence as Embedding</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#tree-as-embedding"><span class="toc-text"> Tree as Embedding</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#conclusion"><span class="toc-text"> Conclusion</span></a></li></ol></li></ol>
  </article>
  
  
  <article class="card card-content">
    <div class="recent-posts-card">
  <div class="recent-posts-header"><i class="iconfont icon-wenzhang_huaban" style="padding-right: 2px;"></i>最近文章</div>
  <div class="recent-posts-list">
    
      <div class="recent-posts-item">
        <div class="recent-posts-item-title">2021-09-05</div>
        <a href="/Glibc内存管理笔记/"><div class="recent-posts-item-content">Glibc内存管理笔记</div></a>
      </div>
    
      <div class="recent-posts-item">
        <div class="recent-posts-item-title">2021-08-25</div>
        <a href="/Graph-Neural-Network/"><div class="recent-posts-item-content">Graph Neural Network</div></a>
      </div>
    
      <div class="recent-posts-item">
        <div class="recent-posts-item-title">2021-08-21</div>
        <a href="/Note-PalmTree-CCS2021/"><div class="recent-posts-item-content">Note 《PalmTree Learning an Assembly Language Model for Instruction Embedding》</div></a>
      </div>
    
      <div class="recent-posts-item">
        <div class="recent-posts-item-title">2021-07-06</div>
        <a href="/Note-NFRE/"><div class="recent-posts-item-content">Note《A Lightweight Framework for Function Name Reassignment Based on Large-Scale Stripped Binaries》</div></a>
      </div>
    
  </div>
</div>
  </article>
  
  
</div>
          </aside>
        </div>
      </div>
    </div>
  </div>
  
  <footer class="footer">
  <div class="footer-container">
    <div>
      <div class="footer-dsc">
        <span>Copyright ©
          
          2020 -
          
          2021
        </span>
        &nbsp;
        <a href="/" class="footer-link">ch3nye's blog </a>
      </div>
    </div>

    
    <div class="footer-dsc">
      
      Powered by
      <a href="https://hexo.io/" class="footer-link" target="_blank" rel="nofollow noopener noreferrer">&nbsp;Hexo </a>
      
      
      <span>&nbsp;|&nbsp;</span>
      
      
      Theme -
      <a href="https://github.com/theme-kaze" class="footer-link" target="_blank"
        rel="nofollow noopener noreferrer">&nbsp;Kaze</a>
      
    </div>
    
    
    
    
      <div class="footer-dsc">
        
        本站总访问量<span id="busuanzi_value_site_pv"></span>次
        
        
        <span>&nbsp;|&nbsp;</span>
        
        
        本站总访客数<span id="busuanzi_value_site_uv"></span>次
        
      </div>
      
    
</footer>
  <a role="button" id="scrollbutton" class="basebutton"  aria-label="回到顶部">
  <i class="iconfont icon-arrowleft button-icon"></i>
</a>
<a role="button" id="menubutton" class="basebutton">
  <i class="iconfont icon-menu button-icon"></i>
</a>
<a role="button" id="popbutton" class="basebutton" aria-label="控制中心">
  <i class="iconfont icon-expand button-icon"></i>
</a>
<a role="button" id="darkbutton" class="basebutton darkwidget" aria-label="夜色模式">
  <i class="iconfont icon-weather button-icon"></i>
</a>
<a role="button" id="searchbutton" class="basebutton searchwidget" aria-label="搜索">
  <i class="iconfont icon-search button-icon"></i>
</a>

  
  
  

  
  
    
<link rel="stylesheet" href="//cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.css">

  

  
  
  <script>
  var addImgLayout = function () {
    var img = document.querySelectorAll('.post-content img');
    var i;
    for (i = 0; i < img.length; i++) {
      var wrapper = document.createElement('a');
      wrapper.setAttribute('data-fslightbox', 'gallery');
      wrapper.setAttribute('href', img[i].getAttribute('data-src'));
      wrapper.setAttribute('aria-label', 'illustration');
      wrapper.style.cssText = 'width: 100%; display: flex; justify-content: center;';
      img[i].before(wrapper);
      wrapper.append(img[i]);
    }
    refreshFsLightbox();
  }
</script>
<script>loadScript("//cdn.jsdelivr.net/npm/fslightbox@3.1.0/index.min.js", addImgLayout)</script>
  
  
  
<script src="/js/main.js"></script>

  
  <script>loadScript("/js/lib/busuanzi.min.js")</script>
  
  
  <script>
    var addLazyload = function () {
      var observer = lozad('.lozad', {
        load: function (el) {
          el.srcset = el.getAttribute('data-src');
        },
        loaded: function (el) {
          el.classList.add('loaded');
        }
      });
      observer.observe();
    }
  </script>
  <script>loadScript("/js/lib/lozad.min.js", addLazyload)</script>
  
  
  <script>
    var googleAnalytics = function() {
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-WHGL11014T');
    }
  </script>
  <script>loadScript("https://www.googletagmanager.com/gtag/js?id=" + "G-WHGL11014T", googleAnalytics)</script>
  
</body>

</html>