<!DOCTYPE html>
<html lang="zh-CN">
  <head>
  <meta charset="UTF-8">
  <meta 
    name="viewport"
    content="width=device-width, initial-scale=1.0, minimum-scale=1.0">
  <meta 
    http-equiv="X-UA-Compatible" 
    content="ie=edge">
  <meta 
    name="theme-color" 
    content="#fff" 
    id="theme-color">
  <meta 
    name="description" 
    content="网络安全 Android ML CTF">
  <link 
    rel="icon" 
    href="/images/logo.png">
  <title>ELMO, BERT, GPT</title>
  
    
      <meta 
        property="og:title" 
        content="ELMO, BERT, GPT">
    
    
      <meta 
        property="og:url" 
        content="https://ch3nye.top/ELMO-BERT-GPT/index.html">
    
    
      <meta 
        property="og:img" 
        content="/images/image-20210116112027277.png">
    
    
      <meta 
        property="og:img" 
        content="网络安全 Android ML CTF">
    
    
      <meta 
        property="og:type" 
        content="article">
      <meta 
        property="og:article:published_time" 
        content="2021-01-16">
      <meta 
        property="og:article:modified_time" 
        content="2021-01-16">
      <meta 
        property="og:article:author" 
        content="Ch3nYe">
      
        
          <meta 
            property="og:article:tag" 
            content="ML">
        
          <meta 
            property="og:article:tag" 
            content="note">
        
          <meta 
            property="og:article:tag" 
            content="Next-Step-of-ML">
        
      
    
  
  <script>
    function loadScript(url, cb) {
      var script = document.createElement('script');
      script.src = url;
      if (cb) script.onload = cb;
      script.async = true;
      document.body.appendChild(script);
    }
    function loadCSS(href, data, attr) {
      var sheet = document.createElement('link');
      sheet.ref = 'stylesheet';
      sheet.href = href;
      sheet.dataset[data] = attr;
      document.head.appendChild(sheet);
    }
    function changeCSS(cssFile, data, attr) {
      var oldlink = document.querySelector(data);
      var newlink = document.createElement("link");
      newlink.setAttribute("rel", "stylesheet");
      newlink.setAttribute("href", cssFile);
      newlink.dataset.prism = attr;
      document.head.replaceChild(newlink, oldlink);
    }
  </script>
  
    
      
      
      
      
        
        
        
        <script>
          function prismThemeChange() {
            if(document.getElementById('theme-color').dataset.mode === 'dark') {
              if(document.querySelector('[data-prism]')) {
                changeCSS('/js/lib/prism/prism-tomorrow.min.css', '[data-prism]', 'prism-tomorrow');
              } else {
                loadCSS('/js/lib/prism/prism-tomorrow.min.css', 'prism', 'prism-tomorrow');
              }
            } else {
              if(document.querySelector('[data-prism]')) {
                changeCSS('/js/lib/prism/prism.min.css', '[data-prism]', 'prism');
              } else {
                loadCSS('/js/lib/prism/prism.min.css', 'prism', 'prism');
              }
            }
          }
          prismThemeChange()
        </script>
      
      
        
        <link rel="stylesheet" href="/js/lib/prism/prism-line-numbers.min.css">
      
    
  
  <script>
    // control reverse button
    var reverseDarkList = {
      dark: 'light',
      light: 'dark'
    };
    var themeColor = {
      dark: '#1c1c1e',
      light: '#fff'
    }
    // get the data of css prefers-color-scheme
    var getCssMediaQuery = function() {
      return window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'light';
    };
    // reverse current darkmode setting function
    var reverseDarkModeSetting = function() {
      var setting = localStorage.getItem('user-color-scheme');
      if(reverseDarkList[setting]) {
        setting = reverseDarkList[setting];
      } else if(setting === null) {
        setting = reverseDarkList[getCssMediaQuery()];
      } else {
        return;
      }
      localStorage.setItem('user-color-scheme', setting);
      return setting;
    };
    // apply current darkmode setting
  </script>
  
    <script>
      var setDarkmode = function(mode) {
      var setting = mode || localStorage.getItem('user-color-scheme');
      if(setting === getCssMediaQuery()) {
        document.documentElement.removeAttribute('data-user-color-scheme');
        localStorage.removeItem('user-color-scheme');
        document.getElementById('theme-color').content = themeColor[setting];
        document.getElementById('theme-color').dataset.mode = setting;
        prismThemeChange();
      } else if(reverseDarkList[setting]) {
        document.documentElement.setAttribute('data-user-color-scheme', setting);
        document.getElementById('theme-color').content = themeColor[setting];
        document.getElementById('theme-color').dataset.mode = setting;
        prismThemeChange();
      } else {
        document.documentElement.removeAttribute('data-user-color-scheme');
        localStorage.removeItem('user-color-scheme');
        document.getElementById('theme-color').content = themeColor[getCssMediaQuery()];
        document.getElementById('theme-color').dataset.mode = getCssMediaQuery();
        prismThemeChange();
      }
    };
    setDarkmode();
    </script>
  
  
  <link rel="preload" href="//at.alicdn.com/t/font_1946621_i1kgafibvw.css" as="style" >
  <link rel="preload" href="//at.alicdn.com/t/font_1952792_89b4ac4k4up.css" as="style" >
  
  
    <link rel="preload" href="/js/lib/lightbox/baguetteBox.min.js" as="script">
    <link rel="preload" href="/js/lib/lightbox/baguetteBox.min.css" as="style" >
  
  
    <link rel="preload" href="/js/lib/lozad.min.js" as="script">
  
  
  
  
    
    <link rel="prefetch" href="//cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.css" as="script">
  
  
    
    <link rel="prefetch" href="//cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.js" as="script">
  
  
  
  <link rel="stylesheet" href="/css/main.css">
  
  <link rel="stylesheet" href="//at.alicdn.com/t/font_1946621_i1kgafibvw.css">
  
  <link rel="stylesheet" href="//at.alicdn.com/t/font_1952792_89b4ac4k4up.css">
  
    <link rel="stylesheet" href="/js/lib/lightbox/baguetteBox.min.css">
  
<meta name="generator" content="Hexo 5.3.0"></head>

  <body>
    <div class="wrapper">
       
      <nav class="navbar">
  <div class="navbar-logo">
    <span class="navbar-logo-main">
      
        <img 
          class="navbar-logo-img"
          width="32"
          height="32"
          src="/images/logo.png" 
          alt="blog logo">
      
      <span class="navbar-logo-dsc">ch3nye's blog</span>
    </span>
  </div>
  <div class="navbar-menu">
    
      <a 
        href="/" 
        class="navbar-menu-item">
        
          首页
        
      </a>
    
      <a 
        href="/archives" 
        class="navbar-menu-item">
        
          归档
        
      </a>
    
      <a 
        href="/tags" 
        class="navbar-menu-item">
        
          标签
        
      </a>
    
      <a 
        href="/categories" 
        class="navbar-menu-item">
        
          分类
        
      </a>
    
      <a 
        href="/about" 
        class="navbar-menu-item">
        
          关于
        
      </a>
    
      <a 
        href="/links" 
        class="navbar-menu-item">
        
          友链
        
      </a>
    
      <a 
        href="/shuoshuo" 
        class="navbar-menu-item">
        
          说说
        
      </a>
    
    <a 
      class="navbar-menu-item darknavbar" 
      id="dark">
      <i class="iconfont icon-weather"></i>
    </a>
    <a 
      class="navbar-menu-item searchnavbar" 
      id="search">
      <i 
        class="iconfont icon-search" 
        style="font-size: 1.2rem; font-weight: 400;">
      </i>
    </a>
  </div>
</nav> 
      
      <div 
        id="local-search" 
        style="display: none">
        <input
          class="navbar-menu-item"
          id="search-input"
          placeholder="请输入搜索内容..." />
        <div id="search-content"></div>
      </div>
      
      <div class="section-wrap">
        <div class="container">
          <div class="columns">
            <main class="main-column">
  <div class="image-wrapper">
    <img 
      src="/images/image-20210116112027277.png" 
      data-src="/images/image-20210116112027277.png"
      srcset="/images/LoadingImage.gif"
      class="image lozad"
      alt="ELMO, BERT, GPT thumbnail">
  </div>

<article class="card card-content">
  <header>
    <h1 class="post-title">
      ELMO, BERT, GPT
    </h1>
  </header>
  <div class="post-meta post-show-meta">
    <time datetime="2021-01-15T16:00:00.000Z">
      <i 
        class="iconfont icon-calendar" 
        style="margin-right: 2px;">
      </i>
      <span>2021-01-16</span>
    </time>
    
      <span class="dot"></span>
      
        <a 
          href="/categories/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/" 
          class="post-meta-link">
          李宏毅机器学习笔记
        </a>
      
    
    
      <span class="dot"></span>
      <span>4.2k 字</span>
    
  </div>
  
    <div 
      class="post-meta post-show-meta" 
      style="margin-top: -10px;">
      <div style="display: flex; align-items: center;">
        <i 
          class="iconfont icon-biaoqian" 
          style="margin-right: 2px; font-size: 1.15rem;">
        </i>
        
          
          <a 
            href="/tags/ML/" 
            class="post-meta-link">
            ML
          </a>
        
          
            <span class="dot"></span>
          
          <a 
            href="/tags/note/" 
            class="post-meta-link">
            note
          </a>
        
          
            <span class="dot"></span>
          
          <a 
            href="/tags/Next-Step-of-ML/" 
            class="post-meta-link">
            Next-Step-of-ML
          </a>
        
      </div>
    </div>
  
  </header>
  <div 
    id="section" 
    class="post-content">
    <p><img src="/images/image-20210116112027277.png" alt="image-20210116112027277" / srcset="/images/LoadingImage.gif" data-src="/images/image-20210116112027277.png" class="lozad post-image"></p>
<h1 id="elmo-bert-gpt"><a class="markdownIt-Anchor" href="#elmo-bert-gpt"></a> ELMO, BERT, GPT</h1>
<p>这节课要讲的是关于如何让机器看懂人类的文字，也就是自然语言模型。今天要将的是到今天为止（2019年6月），这个方向上最较新的模型包括BERT 还有和BERT 很相似的也很知名的模型ELMO 和GPT 。</p>
<h2 id="representation-of-word"><a class="markdownIt-Anchor" href="#representation-of-word"></a> Representation of Word</h2>
<p>先回顾一下，在计算机中怎么表示一个词。</p>
<p><img src="/images/image-20210115190210303.png" alt="image-20210115190210303" / srcset="/images/LoadingImage.gif" data-src="/images/image-20210115190210303.png" class="lozad post-image"></p>
<p><strong>1-of-N Encoding</strong></p>
<p>最早的词表示方法。它就是one-hot encoding 没什么好说的，就是说如果词典中有N个词，就用N维向量表示每个词，向量中只有一个位置是1，其余位置都是0.</p>
<p>但是这样的方式就失去的词意，比如说dog cat bird 都是动物，这些词的表示在数学上要是接近的。</p>
<p><strong>Word Class</strong></p>
<p>根据词的类型划分，但是这种方法还是太粗糙了，比如说dog cat 都是哺乳动物，bird是鸟类应该再细分，而dog cat 又不是同一科又可以再分，且cat 和car都是四个腿或许这两个词应该近一些。于是就有了Word Embedding</p>
<p><strong>Word Embedding</strong></p>
<p>有点像是soft 的word class，把词嵌入到高维空间中，具有相同属性，或者相似的词之间，距离近一些。Word Embedding的技术过去讲过，参考：<a target="_blank" rel="noopener" href="https://www.youtube.com/watch?v=X7PH3NuYW0Q">https://www.youtube.com/watch?v=X7PH3NuYW0Q</a></p>
<p>接下来进入没讲过的知识</p>
<h2 id="一词多义"><a class="markdownIt-Anchor" href="#一词多义"></a> 一词多义</h2>
<p><img src="/images/image-20210115192328778.png" alt="image-20210115192328778" / srcset="/images/LoadingImage.gif" data-src="/images/image-20210115192328778.png" class="lozad post-image"></p>
<blockquote>
<p><a target="_blank" rel="noopener" href="https://arxiv.org/abs/1902.06006">https://arxiv.org/abs/1902.06006</a></p>
</blockquote>
<p>如上图所示，上面五个bank有三种意思。也就是说同一个词往往在不同的上下文中具有不一样的意思，就是说我们希望一个词可以有多个embedding 。以前要处理这件事，我们是让机器去查词典，看这个词有几种解释就给它设置几个embedding，然后通过语料库train 出这些向量。但是不同的词典对同一个词有不同的解释，比如说上图第五个blood bank（血库），有的词典认为这个bank就是银行的意思，有的词典认为这是区别于银行的第三种意思。事实上，有很多这种微妙的词汇，让人难以判断它应该设置几个embedding 。</p>
<h2 id="contextualized-word-embedding"><a class="markdownIt-Anchor" href="#contextualized-word-embedding"></a> Contextualized Word Embedding</h2>
<p><img src="/images/image-20210115193525234.png" alt="image-20210115193525234" / srcset="/images/LoadingImage.gif" data-src="/images/image-20210115193525234.png" class="lozad post-image"></p>
<p>我们希望机器能做到：</p>
<ul>
<li>
<p>每个token（不同上下文中的词）都有一个embedding （无论它们在词典解释中是否属于同一类）</p>
</li>
<li>
<p>word token 的embedding 依赖于上下文</p>
</li>
</ul>
<p>如上图所示，三个bank是不同的token，它们的上下文是不一样的，未来将有不同的embedding （包括灰色的方块，每个token都会输出一个embedding）。这个技术叫做Contextualized Word Embedding 。</p>
<p>那怎么做到Contextualized Word Embedding 呢，有一个实现的技术叫做：Embeddings from Language Model (ELMO)</p>
<h2 id="embeddings-from-language-model-elmo"><a class="markdownIt-Anchor" href="#embeddings-from-language-model-elmo"></a> Embeddings from Language Model (ELMO)</h2>
<p><img src="/images/image-20210115194845861.png" alt="image-20210115194845861" / srcset="/images/LoadingImage.gif" data-src="/images/image-20210115194845861.png" class="lozad post-image"></p>
<blockquote>
<p><a target="_blank" rel="noopener" href="https://arxiv.org/abs/1802.05365">https://arxiv.org/abs/1802.05365</a></p>
</blockquote>
<p>ELMO是一个RNN-based Language Model，训练的方法就是找一大堆的句子，也不需要做标注，然后做上图所示的训练，RNN忘记了的话去看以前的讲解吧。</p>
<p>RNN-based Language Model 的训练过程就是不断学习预测下一个单词是什么。举例来说，你要训练模型输出“潮水退了就知道谁没穿裤子”，你教model，如果看到一个开始符号&lt;BOS&gt; ，就输出潮水，再给它潮水，就输出退了，再给它退了，就输出就…学完以后你就有Contextualized Word Embedding ，我们可以把RNN 的hidden layer 拿出来作为Embedding 。</p>
<p>为什么说这个hidden layer 做Embedding 就是Contextualized 呢，因为RNN中每个输出都是结合前面所有的输入做出的。</p>
<p>你可能会疑惑上图不是只看了单词的前文吗，怎么说是上下文呢？</p>
<p>事实上，我们是如上图所示做了正反双向的训练，最终的word embedding 是把正向的RNN 得到的token embedding 和反向RNN 得到的token embedding 接起来作为最终的Contextualized Word Embedding 。</p>
<p>现在出现了新的问题，通常来说RNN都是Deep的，如下图所示：</p>
<p><img src="/images/image-20210115204219520.png" alt="image-20210115204219520" / srcset="/images/LoadingImage.gif" data-src="/images/image-20210115204219520.png" class="lozad post-image"></p>
<p>中间的word embedding通常是多个，怎么解决呢？ELMO的解决方法很简单就是：我全部都要</p>
<p><img src="/images/image-20210115204416850.png" alt="image-20210115204416850" / srcset="/images/LoadingImage.gif" data-src="/images/image-20210115204416850.png" class="lozad post-image"></p>
<p><img src="/images/image-20210115204428454.png" alt="image-20210115204428454" / srcset="/images/LoadingImage.gif" data-src="/images/image-20210115204428454.png" class="lozad post-image"></p>
<p>如上图所示，ELMO会将每个词输出多个embedding，这里我们假设LSTM叠两层。ELMO会用做weighted sum，weight是根据你做的下游任务训练出来的，下游任务就是说你用EMLO做SRL（Sematic Role Labeling 语义角色标注）、Coref（Coreference resolution 共指解析）、SNLI（Stanford Natural Language Inference 自然语言推理）、SQuAD（Stanford Question Answering Dataset） 还是SST-5（5分类情感分析数据集）。</p>
<p>具体来说，你要先train好EMLO，得到每个token 对应的多个embedding，然后决定你要做什么task，然后在下游task 的model 中学习weight 的值。</p>
<p>原始ELMO的paper 中给出了上图实验结果，Token是说没有做Contextualized Embedding之前的原始向量，LSTM-1、LSTM-2是EMLO的两层得到的embedding，然后根据下游5个task 学出来的weight 的比重情况。我们可以看出Coref 和SQuAD 这两个任务比较看重LSTM-1抽出的embedding，而其他task 都比较平均的看了三个输入。</p>
<h2 id="bidirectional-encoder-representations-from-transformers-bert"><a class="markdownIt-Anchor" href="#bidirectional-encoder-representations-from-transformers-bert"></a> Bidirectional Encoder Representations from Transformers (BERT)</h2>
<p><strong>BERT = Encoder of Transformer</strong></p>
<p>我们在transformer 一节中讲过Transformer 了，BERT 也有提及。</p>
<p>如果后面讲的东西你没有听的很懂的话，你就记住BERT的用法就是吃一个句子，输出句中每个词的embedding 。</p>
<p><img src="/images/image-20210115205718800.png" alt="image-20210115205718800" / srcset="/images/LoadingImage.gif" data-src="/images/image-20210115205718800.png" class="lozad post-image"></p>
<p>这里老师做了一个提醒，由于中文的词太多，所以用词作为sequence的基本单位，可能导致输入向量过长（one-hot），所以也许使用字作为基本单位更好。</p>
<h3 id="training-of-bert"><a class="markdownIt-Anchor" href="#training-of-bert"></a> Training of BERT</h3>
<p>paper上提及的BERT的训练方法有两种，<strong>Masked LM</strong> 和<strong>Next Sentence Prediction</strong> 。</p>
<p><strong>Masked LM</strong></p>
<p><img src="/images/image-20210115210456001.png" alt="image-20210115210456001" / srcset="/images/LoadingImage.gif" data-src="/images/image-20210115210456001.png" class="lozad post-image"></p>
<p>基本思路：盖住输入的句子中15%的词，让BERT把词填回来。具体方法是，挖空的位置用符号[MASK]替换，然后输入到BERT，BERT给出输出向量，将挖空地方的输出向量，输入到一个简单的线性模型中，这个模型会分辨这个向量是那个词。所以说，BERT必须好好做embedding ，只有BERT抽出的向量能很好地表示被挖空的词，这个简单的线性模型才能找到到底是哪个词被Masked。</p>
<p><strong>Next Sentence Prediction</strong></p>
<p><img src="/images/image-20210115211034797.png" alt="image-20210115211034797" / srcset="/images/LoadingImage.gif" data-src="/images/image-20210115211034797.png" class="lozad post-image"></p>
<p>基本思路：给BERT两个句子，然后判断这两个句子是不是应该接在一起。具体做法是，[SEP]符号告诉BERT交接的地方在哪里，[CLS]这个符号通常放在句子开头，它通过BERT得到的embedding 输入到简单的线性分类器中，分类器判断当前这个两个句子是不是应该接在一起。</p>
<p>你可能会疑惑[CLS]难道不应该放在句末，让BERT看完整个句子再做判断吗？</p>
<p>你仔细想想看，BERT里面一般用的是Transformer的Encoder，也就是说它做的是self-attention，我们之前说self-attention layer 不受位置的影响，它会看完整个句子，所以一个token放在句子的开头或者结尾是没有差别的。</p>
<p>最后提一下上述两个方法中，<strong>Linear classifier 是和BERT一起训练的</strong>。<strong>上述两个方法在文献上是同时使用的</strong>，让BERT的输出去解这两个任务的时候会得到最好的训练效果。</p>
<h3 id="how-to-use-bert"><a class="markdownIt-Anchor" href="#how-to-use-bert"></a> How to use BERT</h3>
<p>你当然可以把BERT当作一个抽embedding 的工具，抽出embedding 以后去做别的task，但是在文献中并不是这样，文献中是把BERT和down stream task 一起做训练😮。举了四个栗子：</p>
<p><img src="/images/image-20210115212020349.png" alt="image-20210115212020349" / srcset="/images/LoadingImage.gif" data-src="/images/image-20210115212020349.png" class="lozad post-image"></p>
<p>如上图，输入一个sentence 输出一个class ，有代表性的任务有情感分析，文章分类等。</p>
<p>具体做法，以句子情感分析为例，你找一堆带有情感标签的句子，丢给BERT（BERT有trained model），再句子开头设一个判断情感的符号[CLS]，把这个符号通过BERT的输出丢给一个线性分类器做情感分类。线性分类器是随机初始化参数用你的训练资料train 的，这个过程你也可以堆BRET进行fine-tune ，或者就fix 住BRET的参数，都行。</p>
<p><img src="/images/image-20210115212741387.png" alt="image-20210115212741387" / srcset="/images/LoadingImage.gif" data-src="/images/image-20210115212741387.png" class="lozad post-image"></p>
<p>如上图，输入一个句子，输出每个单词的类别。有代表性的栗子，Slot filling。</p>
<p>和上一个栗子类似，这里你就去train 每个线性分类器就好了，BERT也可以去fine-tune 。</p>
<p><img src="/images/image-20210115213314301.png" alt="image-20210115213314301" / srcset="/images/LoadingImage.gif" data-src="/images/image-20210115213314301.png" class="lozad post-image"></p>
<p>如上图，输入两个句子，输出自然语言推理的结果。举栗来说，给出一个前提一个假设，机器根据这个前提判断这个假设是T/F/unknown.</p>
<p>做法如上图所示，你在开头设一个提问的符号，在把两个句子用[SEP]接起来，上面接一个线性分类器（3分类），同上的训练方法。</p>
<p><img src="/images/image-20210115214817287.png" alt="image-20210115214817287" / srcset="/images/LoadingImage.gif" data-src="/images/image-20210115214817287.png" class="lozad post-image"></p>
<p>那BERT解QA问题，而且明确是Extraction-based Question Answering ，这种QA问题是说，文中一定能找到问题的答案。</p>
<p>解这种QA问题的model 如上图所示，模型输入Document D 有N个单词，Query Q有M个单词，模型输出答案在文中的起始位置和结束位置：s和e。举例来说，上图中第一个问题的答案是gravity，是Document 中第17个单词；第三个问题的答案是within a cloud，是Document 中第77到第79个单词。</p>
<p>怎么用BERT解这个问题呢？</p>
<p><img src="/images/image-20210115215102884.png" alt="image-20210115215102884" / srcset="/images/LoadingImage.gif" data-src="/images/image-20210115215102884.png" class="lozad post-image"></p>
<p>输入如上图所示，Document 中每个词都会有一个向量表示，然后你再去learn 两个向量，上图红色和蓝色条，这两个向量的维度和BERT的输出向量相同，红色向量和Document 中的词汇做点积得到一堆数值，把这些数值做softmax 最大值的位置就是s，同样的蓝色的向量做相同的运算，得到e：</p>
<p><img src="/images/image-20210115215506034.png" alt="image-20210115215506034" / srcset="/images/LoadingImage.gif" data-src="/images/image-20210115215506034.png" class="lozad post-image"></p>
<p>如果e落在s的前面，有可能就是无法回答的问题。</p>
<p>这个训练方法，你需要label很多data，每个问题的答案都需要给出在文中的位置。两个向量是白手起家learn出来的，BERT可能需要fine-tune 。</p>
<h3 id="bert-屠榜"><a class="markdownIt-Anchor" href="#bert-屠榜"></a> BERT 屠榜</h3>
<p>现在BERT在NLP任务中几乎是屠榜的，几乎每个NLP任务都已经被BERT洗过了。</p>
<p><img src="/images/image-20210115215908684.png" alt="image-20210115215908684" / srcset="/images/LoadingImage.gif" data-src="/images/image-20210115215908684.png" class="lozad post-image"></p>
<h3 id="enhanced-representation-through-knowledge-integration-ernie"><a class="markdownIt-Anchor" href="#enhanced-representation-through-knowledge-integration-ernie"></a> Enhanced Representation through Knowledge Integration (ERNIE)</h3>
<p><img src="/images/image-20210116085745533.png" alt="image-20210116085745533" / srcset="/images/LoadingImage.gif" data-src="/images/image-20210116085745533.png" class="lozad post-image"></p>
<blockquote>
<p><a target="_blank" rel="noopener" href="https://arxiv.org/abs/1904.09223">https://arxiv.org/abs/1904.09223</a></p>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/59436589">https://zhuanlan.zhihu.com/p/59436589</a></p>
</blockquote>
<p>ERNIE是转为中文设计的NLP预训练模型，by 百度。细节可以看一下文章。</p>
<h3 id="what-does-bert-learn"><a class="markdownIt-Anchor" href="#what-does-bert-learn"></a> What does BERT learn?</h3>
<p><img src="/images/image-20210116090313445.png" alt="image-20210116090313445" / srcset="/images/LoadingImage.gif" data-src="/images/image-20210116090313445.png" class="lozad post-image"></p>
<blockquote>
<p><a target="_blank" rel="noopener" href="https://arxiv.org/abs/1905.05950">https://arxiv.org/abs/1905.05950</a></p>
<p><a target="_blank" rel="noopener" href="https://openreview.net/pdf?id=SJzSgnRcKX">https://openreview.net/pdf?id=SJzSgnRcKX</a></p>
</blockquote>
<p>思考一下BERT每一层都在做什么，列出上述两个reference 给大家做参考。假如说我们的BERT有24层，down stream task有POS、Consts等等，我们来分析一下上述实验结果。这个实验把BERT的每一层的Contextualized Embedding 抽出来做weighted sum，就和EMLO的做法一样，ELMO就是把每层的Contextualized Embedding 抽出来做weighted sum，然后通过下游任务learn 出weight。</p>
<p>这里也一样，假设我们是单纯用BERT做embedding ，用得到的词向量做下游任务。那BERT有24层，所以每个词都会抽出24个vector ，把这些vector 做weighted sum，weight 是根据下游任务learn出来的，看最后learn出的weight 的情况，就可以知道这个任务更需要那些层的vector 。</p>
<p>上图中右侧蓝色的多个柱状图，代表通过不同任务learn 出的BERT各层的weight ，POS是做词性标注任务，会更依赖11-14层，Coref是做分析代词指代，会更依赖BERT高层的向量，而SRL语义角色标注就比较平均地依赖各层抽出的信息。</p>
<h3 id="multilingual-bert"><a class="markdownIt-Anchor" href="#multilingual-bert"></a> Multilingual BERT</h3>
<p><img src="/images/image-20210116101548611.png" alt="image-20210116101548611" / srcset="/images/LoadingImage.gif" data-src="/images/image-20210116101548611.png" class="lozad post-image"></p>
<blockquote>
<p><a target="_blank" rel="noopener" href="https://arxiv.org/abs/1904.09077">https://arxiv.org/abs/1904.09077</a></p>
</blockquote>
<p>Multilingual BERT 用104种语言去训练。google 到wiki百科上爬了104种语言的百科给BERT学习，虽然BERT没看过这些语言之间的翻译，但是它看过104种语言的百科以后，它似乎自动学会了不同语言之间的对应关系。</p>
<p>所以，如果你现在要用这个预训练好的BERT去做文章分类，你只要给他英文文章分类的label data set，它学完之后，竟然可以直接去做中文文章的分类 amazing😮。更多细节你可以参考上述reference</p>
<h2 id="generative-pre-training-gpt"><a class="markdownIt-Anchor" href="#generative-pre-training-gpt"></a> Generative Pre-Training (GPT)</h2>
<p><img src="/images/image-20210116101945716.png" alt="image-20210116101945716" / srcset="/images/LoadingImage.gif" data-src="/images/image-20210116101945716.png" class="lozad post-image"></p>
<blockquote>
<p><a target="_blank" rel="noopener" href="https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf">https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf</a></p>
<p>Source of image: <a target="_blank" rel="noopener" href="https://huaban.com/pins/1714071707/">https://huaban.com/pins/1714071707/</a></p>
</blockquote>
<p>从上图你就可以看出，GPT非常巨大，其实它的卖点就是大。</p>
<p>GPT目前有三个版本，GPT就是最开始版本，它的大小是平平无奇的，只是提出了一种NLP预训练模型架构，GPT-2是GPT的加大版本，GPT-3是上一个版本的加大版本。也就是中杯、大杯和超大杯。尤其是GPT-3在2020年引起了较大轰动，有人说它能为人工智能的热潮续命5年，关于GPT-3的文章已经有很多了，感兴趣可以自行查阅，本节主要探究GPT的架构。</p>
<p>为什么用独角兽的形象，后面会解释。</p>
<p>我们上面说BERT是Transformer 的Encoder ，GPT其实是Transformer 的Decoder 。</p>
<p><img src="/images/image-20210116103103686.png" alt="image-20210116103103686" / srcset="/images/LoadingImage.gif" data-src="/images/image-20210116103103686.png" class="lozad post-image"></p>
<p>GPT和一般的Language Model 做的事情一样，就是你给他一些词汇，它预测接下来的词汇。举例来说，如上图所示，把“潮水的”q拿出来做self-attention，然后做softmax 产生 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mover accent="true"><mi>α</mi><mo>^</mo></mover></mrow><annotation encoding="application/x-tex">\hat{\alpha}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.69444em;vertical-align:0em;"></span><span class="mord accent"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.69444em;"><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.0037em;">α</span></span></span><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="accent-body" style="left:-0.22222em;">^</span></span></span></span></span></span></span></span></span> ，再分别和v做相乘求和得到b，self-attention 可以有很多层（b是vector，上面还可以再接self-attention layer），通过很多层以后要预测“退了”这个词汇。</p>
<p><img src="/images/image-20210116104709265.png" alt="image-20210116104709265" / srcset="/images/LoadingImage.gif" data-src="/images/image-20210116104709265.png" class="lozad post-image"></p>
<p>预测出“退了”以后，把“退了”拿下来，做同样的计算，预测“就”这个词汇，如此往复。</p>
<h3 id="result-of-gpt"><a class="markdownIt-Anchor" href="#result-of-gpt"></a> Result of GPT</h3>
<p><img src="/images/image-20210116105305047.png" alt="image-20210116105305047" / srcset="/images/LoadingImage.gif" data-src="/images/image-20210116105305047.png" class="lozad post-image"></p>
<p><strong>GPT-2是一个巨大的预训练模型</strong>，它可以在没有更多训练资料的情况下做以下任务：</p>
<ul>
<li><strong>Reading Comprehension</strong></li>
</ul>
<p>BERT也可以做Reading Comprehension，但是BERT需要新的训练资料train 线性分类器，对BERT本身进行微调。而GPT可以在没有训练资料的情况下做这个任务。</p>
<p>如上图所示，你就给GPT-2一段文章，给出一个问题，再写一个A:，他就会尝试做出回答。右侧是GPT-2在CoQA上的结果，最大的GPT-2可以和DrQA达到相同的效果，不要忘了GPT-2在这个任务上是zero-shot learning ，从来没有人教过它做QA 。</p>
<ul>
<li><strong>Summarization</strong></li>
</ul>
<p>给出一段文章加一个too long don’t read 的缩写&quot;TL;DR:&quot; 就会尝试总结这段文字。</p>
<ul>
<li><strong>Translation</strong></li>
</ul>
<p>以上图所示的形式给出 一段英文=对应的法语，这样的栗子，然后机器就知道要给出第三句英文的法语翻译。</p>
<p>其实后两个任务效果其实不是很好，Summarization就像是随机生成的句子一样。</p>
<h3 id="visualization"><a class="markdownIt-Anchor" href="#visualization"></a> Visualization</h3>
<p><img src="/images/image-20210116110330889.png" alt="image-20210116110330889" / srcset="/images/LoadingImage.gif" data-src="/images/image-20210116110330889.png" class="lozad post-image"></p>
<blockquote>
<p><a target="_blank" rel="noopener" href="https://arxiv.org/abs/1904.02679">https://arxiv.org/abs/1904.02679</a></p>
</blockquote>
<p>有人分析了一下GTP-2的attention做的事情是什么。</p>
<p>上图右侧的两列，GPT-2中左列词汇是下一层的结果，右列是前一层需要被attention的对象，我们可以观察到，She 是通过nurse attention 出来的，He是通过doctor attention 出来的，所以机器学到了某些词汇是和性别有关系的（虽然它大概不知道性别是什么）。</p>
<p>上图左侧，是对不同层的不同head 做一下分析，你会发现一个现象，很多不同的词汇都要attend 到第一个词汇。一个可能的原因是，如果机器不知道应该attend 到哪里，或者说不需要attend 的时候就attend 在第一个词汇。如果真是这样的话，以后我们未来在做这种model 的时候可以设一个特别的词汇，当机器不知道要attend 到哪里的时候就attend 到这个特殊词汇上。这就是Visualization 可以告诉我们的事情。</p>
<h3 id="gpt-2-write-novel"><a class="markdownIt-Anchor" href="#gpt-2-write-novel"></a> GPT-2 write novel</h3>
<p><img src="/images/image-20210116111213257.png" alt="image-20210116111213257" / srcset="/images/LoadingImage.gif" data-src="/images/image-20210116111213257.png" class="lozad post-image"></p>
<p>GPT-2是OpenAI 做的，上图是OpenAI 用GPT-2 做了一个续写故事的栗子，他们给机器看第一段，后面都是机器脑部出来的。机器生成的段落中提到了独角兽和安第斯山，所以现在都拿独角兽和安第斯山来隐喻GPT：</p>
<p><img src="/images/image-20210116111457443.png" alt="image-20210116111457443" / srcset="/images/LoadingImage.gif" data-src="/images/image-20210116111457443.png" class="lozad post-image"></p>
<p>OpenAI 担心GPT-2最大的模型过于强大，可能会被用来产生假新闻这种坏事上，所以只发布了GPT-2的小模型。BERT是你能得到模型，GPT-2却是你得不到的，于是上图就很形象的表达了众多平民AI Master 的心情。</p>
<p>有人用GPT-2的公开模型做了一个在线demo，大家可以去试试：</p>
<p><a target="_blank" rel="noopener" href="https://app.inferkit.com/demo">https://app.inferkit.com/demo</a></p>
<p>参考文章：</p>
<blockquote>
<p>Attention isn’t all you need！BERT的力量之源远不止注意力：<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/58430637">https://zhuanlan.zhihu.com/p/58430637</a></p>
</blockquote>

  </div>
  <div>
    
      <div 
        class="post-note note-info copyright" 
        style="margin-top: 42px">
        <p>
          <span style="font-weight: bold;">作者：</span><a 
            target="_blank" 
            rel="nofollow noopener noreferrer" 
            href="/about">
            Ch3nYe
          </a>
        </p>
        <p>
          <span style="font-weight: bold;">文章链接：</span><a 
            target="_blank" 
            rel="nofollow noopener noreferrer" 
            href="https://ch3nye.top/ELMO-BERT-GPT/">
            https://ch3nye.top/ELMO-BERT-GPT/
          </a>
        </p>
        <p><span style="font-weight: bold;">版权声明：</span>本博客所有文章除特别声明外，均采用<a target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh">CC BY-NC-SA 4.0 协议</a>。转载请注明出处！</p>
      </div>
    
  </div>
</article>
<div class="nav">
  
    <div class="nav-item-prev">
      <a 
        href="/Note-FANS-Fuzzing-Android-Native-System-Services/" 
        class="nav-link">
        <i class="iconfont icon-left nav-prev-icon"></i>
        <div>
          <div class="nav-label">上一篇</div>
          
            <div class="nav-title">Note《FANS Fuzzing Android Native System Services via Automated Interface Analysis》 </div>
          
        </div>
      </a>
    </div>
  
  
    <div class="nav-item-next">
      <a 
        href="/Note-FIRMSCOPE-Automatic-Uncovering-of-Privilege-Escala/" 
        class="nav-link">
        <div>
          <div class="nav-label">下一篇</div>
          
            <div class="nav-title">Note《FIRMSCOPE Automatic Uncovering of Privilege-Escala Vulnerabilities in Pre-Installed Apps in Android Firmware》 </div>
          
        </div>
        <i class="iconfont icon-right nav-next-icon"></i>
      </a>
    </div>
  
</div>

  <div 
    class="card card-content comment-card" 
    style="margin-top: 16px;">
    <div class="comment-card-title">评论</div>
    
  <div id="gitalk-container"></div>
  
<link rel="stylesheet" href="//cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.css">

  
<script src="//cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.js"></script>

  
<script src="/js/lib/md5.min.js"></script>

  <script>
    var gitalk = new Gitalk({
      clientID: '049b30eb10ea05082ef1',
      clientSecret: 'c33ad95041c69907b3136b895008320f000987db',
      repo: 'Ch3nYe.github.io',
      owner: 'Ch3nYe',
      admin: "Ch3nYe",
      id: md5(location.href),
      distractionFreeMode: false,
      language: 'navigator.language || navigator.userLanguage',
      labels: ["Gitalk"],
      perPage: 10
    })

    gitalk.render('gitalk-container')
  </script>

  </div>

<div 
  class="card card-content toc-card" 
  id="mobiletoc">
  <div class="toc-header">
  <i 
    class="iconfont icon-menu" 
    style="padding-right: 2px;">
  </i>目录
</div>
<ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#elmo-bert-gpt"><span class="toc-text"> ELMO, BERT, GPT</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#representation-of-word"><span class="toc-text"> Representation of Word</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%B8%80%E8%AF%8D%E5%A4%9A%E4%B9%89"><span class="toc-text"> 一词多义</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#contextualized-word-embedding"><span class="toc-text"> Contextualized Word Embedding</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#embeddings-from-language-model-elmo"><span class="toc-text"> Embeddings from Language Model (ELMO)</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#bidirectional-encoder-representations-from-transformers-bert"><span class="toc-text"> Bidirectional Encoder Representations from Transformers (BERT)</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#training-of-bert"><span class="toc-text"> Training of BERT</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#how-to-use-bert"><span class="toc-text"> How to use BERT</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#bert-%E5%B1%A0%E6%A6%9C"><span class="toc-text"> BERT 屠榜</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#enhanced-representation-through-knowledge-integration-ernie"><span class="toc-text"> Enhanced Representation through Knowledge Integration (ERNIE)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#what-does-bert-learn"><span class="toc-text"> What does BERT learn?</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#multilingual-bert"><span class="toc-text"> Multilingual BERT</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#generative-pre-training-gpt"><span class="toc-text"> Generative Pre-Training (GPT)</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#result-of-gpt"><span class="toc-text"> Result of GPT</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#visualization"><span class="toc-text"> Visualization</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#gpt-2-write-novel"><span class="toc-text"> GPT-2 write novel</span></a></li></ol></li></ol></li></ol>
</div></main>
            <aside class="left-column">
              
              <div class="card card-author">
                
  <img 
    src="/images/logo.png" 
    class="author-img"
    width="88"
    height="88"
    alt="author avatar">

<p class="author-name">Ch3nYe</p>
<p class="author-description">如果有文章有任何错误请留言，谢谢🙏</p>
<div class="author-message">
  <a 
    class="author-posts-count" 
    href="/archives">
    <span>34</span>
    <span>文章</span>
  </a>
  <a 
    class="author-categories-count" 
    href="/categories">
    <span>6</span>
    <span>分类</span>
  </a>
  <a 
    class="author-tags-count" 
    href="/tags">
    <span>44</span>
    <span>标签</span>
  </a>
</div>

  <div class="author-card-society">
    
      <div class="author-card-society-icon">
        <a target="_blank" rel="noopener" href="https://github.com/ch3nye/">
          <i class="iconfont icon-github society-icon"></i>
        </a>
      </div>
    
      <div class="author-card-society-icon">
        <a href="mailto:sud0su@163.com">
          <i class="iconfont icon-mail society-icon"></i>
        </a>
      </div>
    
  </div>

              </div>
               <div class="sticky-tablet">
  
  
    <article class="display-when-two-columns spacer">
      <div class="card card-content toc-card">
        <div class="toc-header">
  <i 
    class="iconfont icon-menu" 
    style="padding-right: 2px;">
  </i>目录
</div>
<ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#elmo-bert-gpt"><span class="toc-text"> ELMO, BERT, GPT</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#representation-of-word"><span class="toc-text"> Representation of Word</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%B8%80%E8%AF%8D%E5%A4%9A%E4%B9%89"><span class="toc-text"> 一词多义</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#contextualized-word-embedding"><span class="toc-text"> Contextualized Word Embedding</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#embeddings-from-language-model-elmo"><span class="toc-text"> Embeddings from Language Model (ELMO)</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#bidirectional-encoder-representations-from-transformers-bert"><span class="toc-text"> Bidirectional Encoder Representations from Transformers (BERT)</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#training-of-bert"><span class="toc-text"> Training of BERT</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#how-to-use-bert"><span class="toc-text"> How to use BERT</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#bert-%E5%B1%A0%E6%A6%9C"><span class="toc-text"> BERT 屠榜</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#enhanced-representation-through-knowledge-integration-ernie"><span class="toc-text"> Enhanced Representation through Knowledge Integration (ERNIE)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#what-does-bert-learn"><span class="toc-text"> What does BERT learn?</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#multilingual-bert"><span class="toc-text"> Multilingual BERT</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#generative-pre-training-gpt"><span class="toc-text"> Generative Pre-Training (GPT)</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#result-of-gpt"><span class="toc-text"> Result of GPT</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#visualization"><span class="toc-text"> Visualization</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#gpt-2-write-novel"><span class="toc-text"> GPT-2 write novel</span></a></li></ol></li></ol></li></ol>
      </div>
    </article>
  
  
  <article class="card card-content categories-widget">
    <div class="categories-card">
  <div class="categories-header">
    <i 
      class="iconfont icon-fenlei" 
      style="padding-right: 2px;">
    </i>分类
  </div>
  <div class="categories-list">
    
      <a href="/categories/%E5%A4%87%E5%BF%98/">
        <div class="categories-list-item">
          备忘
          <span class="categories-list-item-badge">1</span>
        </div>
      </a>
    
      <a href="/categories/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/">
        <div class="categories-list-item">
          李宏毅机器学习笔记
          <span class="categories-list-item-badge">15</span>
        </div>
      </a>
    
      <a href="/categories/%E7%AC%94%E8%AE%B0/">
        <div class="categories-list-item">
          笔记
          <span class="categories-list-item-badge">6</span>
        </div>
      </a>
    
      <a href="/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/">
        <div class="categories-list-item">
          论文阅读
          <span class="categories-list-item-badge">8</span>
        </div>
      </a>
    
      <a href="/categories/%E5%AE%9E%E6%88%98/">
        <div class="categories-list-item">
          实战
          <span class="categories-list-item-badge">3</span>
        </div>
      </a>
    
      <a href="/categories/%E7%94%9F%E6%B4%BB/">
        <div class="categories-list-item">
          生活
          <span class="categories-list-item-badge">1</span>
        </div>
      </a>
    
  </div>
</div>
  </article>
  
  <article class="card card-content tags-widget">
    <div class="tags-card">
  <div class="tags-header">
    <i 
      class="iconfont icon-biaoqian" 
      style="padding-right: 2px;">
    </i>热门标签
  </div>
  <div class="tags-list">
    
      <a 
        href="/tags/note/" 
        title="note">
        <div class="tags-list-item">note</div>
      </a>
    
      <a 
        href="/tags/ML/" 
        title="ML">
        <div class="tags-list-item">ML</div>
      </a>
    
      <a 
        href="/tags/Next-Step-of-ML/" 
        title="Next-Step-of-ML">
        <div class="tags-list-item">Next-Step-of-ML</div>
      </a>
    
      <a 
        href="/tags/%E7%AC%94%E8%AE%B0/" 
        title="笔记">
        <div class="tags-list-item">笔记</div>
      </a>
    
      <a 
        href="/tags/Android/" 
        title="Android">
        <div class="tags-list-item">Android</div>
      </a>
    
      <a 
        href="/tags/%E8%AE%BA%E6%96%87/" 
        title="论文">
        <div class="tags-list-item">论文</div>
      </a>
    
      <a 
        href="/tags/%E6%80%BB%E7%BB%93/" 
        title="总结">
        <div class="tags-list-item">总结</div>
      </a>
    
      <a 
        href="/tags/%E5%AE%9E%E6%88%98/" 
        title="实战">
        <div class="tags-list-item">实战</div>
      </a>
    
      <a 
        href="/tags/crack/" 
        title="crack">
        <div class="tags-list-item">crack</div>
      </a>
    
      <a 
        href="/tags/%E4%BA%8C%E8%BF%9B%E5%88%B6/" 
        title="二进制">
        <div class="tags-list-item">二进制</div>
      </a>
    
      <a 
        href="/tags/Deep-Learning/" 
        title="Deep-Learning">
        <div class="tags-list-item">Deep-Learning</div>
      </a>
    
      <a 
        href="/tags/%E5%AD%A6%E4%B9%A0/" 
        title="学习">
        <div class="tags-list-item">学习</div>
      </a>
    
      <a 
        href="/tags/HTTPS/" 
        title="HTTPS">
        <div class="tags-list-item">HTTPS</div>
      </a>
    
      <a 
        href="/tags/%E9%9D%99%E6%80%81%E5%88%86%E6%9E%90/" 
        title="静态分析">
        <div class="tags-list-item">静态分析</div>
      </a>
    
      <a 
        href="/tags/Reverse-Engineering/" 
        title="Reverse-Engineering">
        <div class="tags-list-item">Reverse-Engineering</div>
      </a>
    
      <a 
        href="/tags/Binary/" 
        title="Binary">
        <div class="tags-list-item">Binary</div>
      </a>
    
  </div>
</div>
  </article>
  
  
</div>
            </aside>
            <aside class="right-column">
              <div class="sticky-widescreen">
  
  
    <article class="card card-content toc-card">
      <div class="toc-header">
  <i 
    class="iconfont icon-menu" 
    style="padding-right: 2px;">
  </i>目录
</div>
<ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#elmo-bert-gpt"><span class="toc-text"> ELMO, BERT, GPT</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#representation-of-word"><span class="toc-text"> Representation of Word</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%B8%80%E8%AF%8D%E5%A4%9A%E4%B9%89"><span class="toc-text"> 一词多义</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#contextualized-word-embedding"><span class="toc-text"> Contextualized Word Embedding</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#embeddings-from-language-model-elmo"><span class="toc-text"> Embeddings from Language Model (ELMO)</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#bidirectional-encoder-representations-from-transformers-bert"><span class="toc-text"> Bidirectional Encoder Representations from Transformers (BERT)</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#training-of-bert"><span class="toc-text"> Training of BERT</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#how-to-use-bert"><span class="toc-text"> How to use BERT</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#bert-%E5%B1%A0%E6%A6%9C"><span class="toc-text"> BERT 屠榜</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#enhanced-representation-through-knowledge-integration-ernie"><span class="toc-text"> Enhanced Representation through Knowledge Integration (ERNIE)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#what-does-bert-learn"><span class="toc-text"> What does BERT learn?</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#multilingual-bert"><span class="toc-text"> Multilingual BERT</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#generative-pre-training-gpt"><span class="toc-text"> Generative Pre-Training (GPT)</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#result-of-gpt"><span class="toc-text"> Result of GPT</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#visualization"><span class="toc-text"> Visualization</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#gpt-2-write-novel"><span class="toc-text"> GPT-2 write novel</span></a></li></ol></li></ol></li></ol>
    </article>
  
  
  <article class="card card-content">
    <div class="recent-posts-card">
  <div class="recent-posts-header">
    <i 
      class="iconfont icon-wenzhang_huaban" 
      style="padding-right: 2px;">
    </i>最近文章
  </div>
  <div class="recent-posts-list">
    
      <div class="recent-posts-item">
        <div class="recent-posts-item-title">2021-09-13</div>
        <a href="/USTC%E5%AE%BF%E8%88%8D%E8%A3%85%E4%BF%AE%E7%90%86%E8%AE%BA%E4%B8%8E%E5%BA%94%E7%94%A8/"><div class="recent-posts-item-content">USTC宿舍装修理论与应用</div></a>
      </div>
    
      <div class="recent-posts-item">
        <div class="recent-posts-item-title">2021-09-05</div>
        <a href="/Glibc%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86%E7%AC%94%E8%AE%B0/"><div class="recent-posts-item-content">Glibc内存管理笔记</div></a>
      </div>
    
      <div class="recent-posts-item">
        <div class="recent-posts-item-title">2021-08-25</div>
        <a href="/Graph-Neural-Network/"><div class="recent-posts-item-content">Graph Neural Network</div></a>
      </div>
    
      <div class="recent-posts-item">
        <div class="recent-posts-item-title">2021-08-21</div>
        <a href="/Note-PalmTree-CCS2021/"><div class="recent-posts-item-content">Note 《PalmTree Learning an Assembly Language Model for Instruction Embedding》</div></a>
      </div>
    
  </div>
</div>
  </article>
  
  
</div>
            </aside>
          </div>
        </div>
      </div>
    </div>
     
    <footer class="footer">
  <div class="footer-container">
    <div>
      <div class="footer-dsc">
        <span>
          Copyright ©
          
            2020 -
          
          2021
        </span>
        &nbsp;
        <a 
          href="/" 
          class="footer-link">
          ch3nye's blog
        </a>
      </div>
    </div>

    
      <div class="footer-dsc">
        
          Powered by
          <a 
            href="https://hexo.io/" 
            class="footer-link" 
            target="_blank" 
            rel="nofollow noopener noreferrer">
            &nbsp;Hexo
          </a>
        
        
          <span>&nbsp;|&nbsp;</span>
        
        
          Theme -
          <a 
            href="https://github.com/theme-kaze" 
            class="footer-link" 
            target="_blank"
            rel="nofollow noopener noreferrer">
            &nbsp;Kaze
          </a>
        
      </div>
    
    
    
    
      <div class="footer-dsc">
        
          本站总访问量<span id="busuanzi_value_site_pv"></span>次
        
        
          <span>&nbsp;|&nbsp;</span>
        
        
          本站总访客数<span id="busuanzi_value_site_uv"></span>次
        
      </div>
      
    
</footer> 
    
  <a 
    role="button" 
    id="scrollbutton" 
    class="basebutton" 
    aria-label="回到顶部">
    <i class="iconfont icon-arrowleft button-icon"></i>
  </a>

<a 
  role="button" 
  id="menubutton"
  aria-label="menu button"
  class="basebutton">
  <i class="iconfont icon-menu button-icon"></i>
</a>
<a 
  role="button" 
  id="popbutton" 
  class="basebutton" 
  aria-label="控制中心">
  <i class="iconfont icon-expand button-icon"></i>
</a>
<a 
  role="button" 
  id="darkbutton" 
  class="basebutton darkwidget" 
  aria-label="夜色模式">
  <i class="iconfont icon-weather button-icon"></i>
</a>
<a 
  role="button" 
  id="searchbutton" 
  class="basebutton searchwidget" 
  aria-label="搜索">
  <i class="iconfont icon-search button-icon"></i>
</a> 
     
     
      

  
  
    
<link rel="stylesheet" href="//cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.css">

  
 
     
     
      <script>
  var addImgLayout = function () {
    var img = document.querySelectorAll('.post-content img')
    var i
    for (i = 0; i < img.length; i++) {
      var wrapper = document.createElement('a')
      wrapper.setAttribute('href', img[i].getAttribute('data-src'))
      wrapper.setAttribute('aria-label', 'illustration')
      wrapper.style.cssText =
        'width: 100%; display: flex; justify-content: center;'
      if (img[i].alt) wrapper.dataset.caption = img[i].alt
      wrapper.dataset.nolink = true
      img[i].before(wrapper)
      wrapper.append(img[i])
      var divWrap = document.createElement('div')
      divWrap.classList.add('gallery')
      wrapper.before(divWrap)
      divWrap.append(wrapper)
    }
    baguetteBox.run('.gallery')
  }
</script>
<script>
  loadScript(
    "/js/lib/lightbox/baguetteBox.min.js",
    addImgLayout
  )
</script>
 
     
     
    <script src="/js/main.js"></script> 
    
      <script> 
        loadScript('/js/lib/busuanzi.min.js') 
      </script>
     
    
      <script>
        var addLazyload = function () {
          var observer = lozad('.lozad', {
            load: function (el) {
              el.srcset = el.getAttribute('data-src')
            },
            loaded: function (el) {
              el.classList.add('loaded')
            },
          })
          observer.observe()
        }
      </script>
      <script>
        loadScript('/js/lib/lozad.min.js', addLazyload)
      </script>
     
    
      <script>
        var googleAnalytics = function () {
          window.dataLayer = window.dataLayer || []
          function gtag() {
            dataLayer.push(arguments)
          }
          gtag('js', new Date())
          gtag('config', 'G-WHGL11014T')
        }
    </script>
      <script>
        loadScript(
          'https://www.googletagmanager.com/gtag/js?id=' +
            'G-WHGL11014T',
          googleAnalytics
        )
      </script>
    
    
      <script>
        setTimeout(() => {localSearch("search.json")}, 0)
      </script>
    
  </body>
</html>
