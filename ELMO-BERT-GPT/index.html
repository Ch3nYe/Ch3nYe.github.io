<!DOCTYPE html>
<html  lang="zh-CN" >
<head>
  <meta charset="UTF-8">
  <meta name="viewport"
    content="width=device-width, initial-scale=1.0, minimum-scale=1.0">
  <meta http-equiv="X-UA-Compatible" content="ie=edge">
  <meta name="theme-color" content="#fff" id="theme-color">
  <meta name="description" content="网络安全 Android ML CTF">
  <link rel="icon" href="/images/logo.png">
  <title>ELMO, BERT, GPT</title>
  
  
  <meta property="og:title" content="ELMO, BERT, GPT">
  
  
  <meta property="og:url" content="https://ch3nye.top/ELMO-BERT-GPT/index.html">
  
  
  <meta property="og:img" content="/images/image-20210116112027277.png">
  
  
  <meta property="og:img" content="网络安全 Android ML CTF">
  
  
  <meta property="og:type" content="article">
  <meta property="og:article:published_time" content="2021-01-16">
  <meta property="og:article:modified_time" content="2021-01-16">
  <meta property="og:article:author" content="Ch3nYe">
  
  
  <meta property="og:article:tag" content="ML">
  
  <meta property="og:article:tag" content="note">
  
  <meta property="og:article:tag" content="Next-Step-of-ML">
  
  
  
  
  <script>
    // control reverse button
    var reverseDarkList = {
      dark: 'light',
      light: 'dark'
    };
    var themeColor = {
      dark: '#1c1c1e',
      light: '#fff'
    }
    // get the data of css prefers-color-scheme
    var getCssMediaQuery = function() {
      return window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'light';
    };
    // reverse current darkmode setting function
    var reverseDarkModeSetting = function() {
      var setting = localStorage.getItem('user-color-scheme');
      if(reverseDarkList[setting]) {
        setting = reverseDarkList[setting];
      } else if(setting === null) {
        setting = reverseDarkList[getCssMediaQuery()];
      } else {
        return;
      }
      localStorage.setItem('user-color-scheme', setting);
      return setting;
    };
    // apply current darkmode setting
    var setDarkmode = function(mode) {
      var setting = mode || localStorage.getItem('user-color-scheme');
      if(setting === getCssMediaQuery()) {
        document.documentElement.removeAttribute('data-user-color-scheme');
        localStorage.removeItem('user-color-scheme');
        document.getElementById('theme-color').content = themeColor[setting];
      } else if(reverseDarkList[setting]) {
        document.documentElement.setAttribute('data-user-color-scheme', setting);
        document.getElementById('theme-color').content = themeColor[setting];
      } else {
        document.documentElement.removeAttribute('data-user-color-scheme');
        localStorage.removeItem('user-color-scheme');
        document.getElementById('theme-color').content = themeColor[getCssMediaQuery()];
      }
    };
    setDarkmode();
  </script>
  <script>
    function loadScript(url, cb) {
      var script = document.createElement('script');
      script.src = url;
      if (cb) script.onload = cb;
      script.async = true;
      document.body.appendChild(script);
    }
  </script>
  
  <link rel="preload" href="//at.alicdn.com/t/font_1946621_f7g5jnuftcf.css" as="style" >
  <link rel="preload" href="//at.alicdn.com/t/font_1952792_89b4ac4k4up.css" as="style" >
  
  <link rel="preload" href="//cdn.jsdelivr.net/npm/fslightbox@3.1.0/index.min.js" as="script">
  
  
  <link rel="preload" href="/js/lib/lozad.min.js" as="script">
  
  
  
  
  
  <link rel="prefetch" href="//cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.css" as="script">
  
  
  
  <link rel="prefetch" href="//cdn-city.livere.com/js/embed.dist.js" as="script">
  
  
  
  
<link rel="stylesheet" href="/css/main.css">

  
  
<link rel="stylesheet" href="//at.alicdn.com/t/font_1946621_f7g5jnuftcf.css">

  
  
<link rel="stylesheet" href="//at.alicdn.com/t/font_1952792_89b4ac4k4up.css">

  
  
  
  
  
  <link href="/js/lib/prism/prism-tomorrow.min.css" rel="stylesheet" data-prism="prism-tomorrow">
  
  
  
<link rel="stylesheet" href="/js/lib/prism/prism-line-numbers.min.css">

  
  
  
<meta name="generator" content="Hexo 5.3.0"></head>


<body>
  <div class="wrapper">
    
    <nav class="navbar">
  <div class="navbar-logo">
    <span class="navbar-logo-main">
      
      <img class="navbar-logo-img" src="/images/logo.png" alt="logo">
      
      <span class="navbar-logo-dsc">ch3nye's blog</span>
    </span>
  </div>
  <div class="navbar-menu">
    
    <a href="/" class="navbar-menu-item">
    
    首页
    
    </a>
    
    <a href="/archives" class="navbar-menu-item">
    
    归档
    
    </a>
    
    <a href="/tags" class="navbar-menu-item">
    
    标签
    
    </a>
    
    <a href="/categories" class="navbar-menu-item">
    
    分类
    
    </a>
    
    <a href="/about" class="navbar-menu-item">
    
    关于
    
    </a>
    
    <a href="/links" class="navbar-menu-item">
    
    友链
    
    </a>
    
    <a class="navbar-menu-item darknavbar" id="dark"><i class="iconfont icon-weather"></i></a>
    <a class="navbar-menu-item searchnavbar" id="search"><i class="iconfont icon-search" style="font-size: 1.2rem; font-weight: 400;"></i></a>
  </div>
</nav>
    
    <div id="local-search" style="display: none;">
      <input class="navbar-menu-item" id="search-input" placeholder="请输入搜索内容...">
      <div id="search-content"></div>
    </div>
    
    <div class="section-wrap">
      <div class="container">
        <div class="columns">
          <main class="main-column">
<div class="image-wrapper">
  <img src="/images/image-20210116112027277.png" data-src="/images/image-20210116112027277.png"
    srcset="/images/LoadingImage.gif"
    class="image lozad"
    alt="thumbnail"
  >
</div>

<article class="card card-content">
  <header>
    <h1 class="post-title">
      ELMO, BERT, GPT
    </h1>
  </header>
  <div class="post-meta post-show-meta">
    <time datetime="2021-01-15T16:00:00.000Z">
      <i class="iconfont icon-calendar" style="margin-right: 2px;"></i>
      <span>2021-01-16</span>
    </time>
    
    <span class="dot"></span>
    
    <a href="/categories/李宏毅机器学习笔记/" class="post-meta-link">李宏毅机器学习笔记</a>
    
    
    
    <span class="dot"></span>
    <span>4.2k 字</span>
    
  </div>
  
  <div class="post-meta post-show-meta" style="margin-top: -10px;">
    <div style="display: flex; align-items: center;">
      <i class="iconfont icon-biaoqian" style="margin-right: 2px; font-size: 1.15rem;"></i>
      
      
        <a href="/tags/ML/" class="post-meta-link">ML</a>
      
      
      <span class="dot"></span>
      
        <a href="/tags/note/" class="post-meta-link">note</a>
      
      
      <span class="dot"></span>
      
        <a href="/tags/Next-Step-of-ML/" class="post-meta-link">Next-Step-of-ML</a>
      
    </div>
  </div>
  
  </header>
  <div id="section" class="post-content">
    <p><img src="/images/image-20210116112027277.png" alt="image-20210116112027277" / srcset="/images/LoadingImage.gif" data-src="/images/image-20210116112027277.png" class="lozad post-image"></p>
<h1 id="elmo-bert-gpt"><a class="markdownIt-Anchor" href="#elmo-bert-gpt"></a> ELMO, BERT, GPT</h1>
<p>这节课要讲的是关于如何让机器看懂人类的文字，也就是自然语言模型。今天要将的是到今天为止（2019年6月），这个方向上最较新的模型包括BERT 还有和BERT 很相似的也很知名的模型ELMO 和GPT 。</p>
<h2 id="representation-of-word"><a class="markdownIt-Anchor" href="#representation-of-word"></a> Representation of Word</h2>
<p>先回顾一下，在计算机中怎么表示一个词。</p>
<p><img src="/images/image-20210115190210303.png" alt="image-20210115190210303" / srcset="/images/LoadingImage.gif" data-src="/images/image-20210115190210303.png" class="lozad post-image"></p>
<p><strong>1-of-N Encoding</strong></p>
<p>最早的词表示方法。它就是one-hot encoding 没什么好说的，就是说如果词典中有N个词，就用N维向量表示每个词，向量中只有一个位置是1，其余位置都是0.</p>
<p>但是这样的方式就失去的词意，比如说dog cat bird 都是动物，这些词的表示在数学上要是接近的。</p>
<p><strong>Word Class</strong></p>
<p>根据词的类型划分，但是这种方法还是太粗糙了，比如说dog cat 都是哺乳动物，bird是鸟类应该再细分，而dog cat 又不是同一科又可以再分，且cat 和car都是四个腿或许这两个词应该近一些。于是就有了Word Embedding</p>
<p><strong>Word Embedding</strong></p>
<p>有点像是soft 的word class，把词嵌入到高维空间中，具有相同属性，或者相似的词之间，距离近一些。Word Embedding的技术过去讲过，参考：<a target="_blank" rel="noopener" href="https://www.youtube.com/watch?v=X7PH3NuYW0Q">https://www.youtube.com/watch?v=X7PH3NuYW0Q</a></p>
<p>接下来进入没讲过的知识</p>
<h2 id="一词多义"><a class="markdownIt-Anchor" href="#一词多义"></a> 一词多义</h2>
<p><img src="/images/image-20210115192328778.png" alt="image-20210115192328778" / srcset="/images/LoadingImage.gif" data-src="/images/image-20210115192328778.png" class="lozad post-image"></p>
<blockquote>
<p><a target="_blank" rel="noopener" href="https://arxiv.org/abs/1902.06006">https://arxiv.org/abs/1902.06006</a></p>
</blockquote>
<p>如上图所示，上面五个bank有三种意思。也就是说同一个词往往在不同的上下文中具有不一样的意思，就是说我们希望一个词可以有多个embedding 。以前要处理这件事，我们是让机器去查词典，看这个词有几种解释就给它设置几个embedding，然后通过语料库train 出这些向量。但是不同的词典对同一个词有不同的解释，比如说上图第五个blood bank（血库），有的词典认为这个bank就是银行的意思，有的词典认为这是区别于银行的第三种意思。事实上，有很多这种微妙的词汇，让人难以判断它应该设置几个embedding 。</p>
<h2 id="contextualized-word-embedding"><a class="markdownIt-Anchor" href="#contextualized-word-embedding"></a> Contextualized Word Embedding</h2>
<p><img src="/images/image-20210115193525234.png" alt="image-20210115193525234" / srcset="/images/LoadingImage.gif" data-src="/images/image-20210115193525234.png" class="lozad post-image"></p>
<p>我们希望机器能做到：</p>
<ul>
<li>
<p>每个token（不同上下文中的词）都有一个embedding （无论它们在词典解释中是否属于同一类）</p>
</li>
<li>
<p>word token 的embedding 依赖于上下文</p>
</li>
</ul>
<p>如上图所示，三个bank是不同的token，它们的上下文是不一样的，未来将有不同的embedding （包括灰色的方块，每个token都会输出一个embedding）。这个技术叫做Contextualized Word Embedding 。</p>
<p>那怎么做到Contextualized Word Embedding 呢，有一个实现的技术叫做：Embeddings from Language Model (ELMO)</p>
<h2 id="embeddings-from-language-model-elmo"><a class="markdownIt-Anchor" href="#embeddings-from-language-model-elmo"></a> Embeddings from Language Model (ELMO)</h2>
<p><img src="/images/image-20210115194845861.png" alt="image-20210115194845861" / srcset="/images/LoadingImage.gif" data-src="/images/image-20210115194845861.png" class="lozad post-image"></p>
<blockquote>
<p><a target="_blank" rel="noopener" href="https://arxiv.org/abs/1802.05365">https://arxiv.org/abs/1802.05365</a></p>
</blockquote>
<p>ELMO是一个RNN-based Language Model，训练的方法就是找一大堆的句子，也不需要做标注，然后做上图所示的训练，RNN忘记了的话去看以前的讲解吧。</p>
<p>RNN-based Language Model 的训练过程就是不断学习预测下一个单词是什么。举例来说，你要训练模型输出“潮水退了就知道谁没穿裤子”，你教model，如果看到一个开始符号&lt;BOS&gt; ，就输出潮水，再给它潮水，就输出退了，再给它退了，就输出就…学完以后你就有Contextualized Word Embedding ，我们可以把RNN 的hidden layer 拿出来作为Embedding 。</p>
<p>为什么说这个hidden layer 做Embedding 就是Contextualized 呢，因为RNN中每个输出都是结合前面所有的输入做出的。</p>
<p>你可能会疑惑上图不是只看了单词的前文吗，怎么说是上下文呢？</p>
<p>事实上，我们是如上图所示做了正反双向的训练，最终的word embedding 是把正向的RNN 得到的token embedding 和反向RNN 得到的token embedding 接起来作为最终的Contextualized Word Embedding 。</p>
<p>现在出现了新的问题，通常来说RNN都是Deep的，如下图所示：</p>
<p><img src="/images/image-20210115204219520.png" alt="image-20210115204219520" / srcset="/images/LoadingImage.gif" data-src="/images/image-20210115204219520.png" class="lozad post-image"></p>
<p>中间的word embedding通常是多个，怎么解决呢？ELMO的解决方法很简单就是：我全部都要</p>
<p><img src="/images/image-20210115204416850.png" alt="image-20210115204416850" / srcset="/images/LoadingImage.gif" data-src="/images/image-20210115204416850.png" class="lozad post-image"></p>
<p><img src="/images/image-20210115204428454.png" alt="image-20210115204428454" / srcset="/images/LoadingImage.gif" data-src="/images/image-20210115204428454.png" class="lozad post-image"></p>
<p>如上图所示，ELMO会将每个词输出多个embedding，这里我们假设LSTM叠两层。ELMO会用做weighted sum，weight是根据你做的下游任务训练出来的，下游任务就是说你用EMLO做SRL（Sematic Role Labeling 语义角色标注）、Coref（Coreference resolution 共指解析）、SNLI（Stanford Natural Language Inference 自然语言推理）、SQuAD（Stanford Question Answering Dataset） 还是SST-5（5分类情感分析数据集）。</p>
<p>具体来说，你要先train好EMLO，得到每个token 对应的多个embedding，然后决定你要做什么task，然后在下游task 的model 中学习weight 的值。</p>
<p>原始ELMO的paper 中给出了上图实验结果，Token是说没有做Contextualized Embedding之前的原始向量，LSTM-1、LSTM-2是EMLO的两层得到的embedding，然后根据下游5个task 学出来的weight 的比重情况。我们可以看出Coref 和SQuAD 这两个任务比较看重LSTM-1抽出的embedding，而其他task 都比较平均的看了三个输入。</p>
<h2 id="bidirectional-encoder-representations-from-transformers-bert"><a class="markdownIt-Anchor" href="#bidirectional-encoder-representations-from-transformers-bert"></a> Bidirectional Encoder Representations from Transformers (BERT)</h2>
<p><strong>BERT = Encoder of Transformer</strong></p>
<p>我们在transformer 一节中讲过Transformer 了，BERT 也有提及。</p>
<p>如果后面讲的东西你没有听的很懂的话，你就记住BERT的用法就是吃一个句子，输出句中每个词的embedding 。</p>
<p><img src="/images/image-20210115205718800.png" alt="image-20210115205718800" / srcset="/images/LoadingImage.gif" data-src="/images/image-20210115205718800.png" class="lozad post-image"></p>
<p>这里老师做了一个提醒，由于中文的词太多，所以用词作为sequence的基本单位，可能导致输入向量过长（one-hot），所以也许使用字作为基本单位更好。</p>
<h3 id="training-of-bert"><a class="markdownIt-Anchor" href="#training-of-bert"></a> Training of BERT</h3>
<p>paper上提及的BERT的训练方法有两种，<strong>Masked LM</strong> 和<strong>Next Sentence Prediction</strong> 。</p>
<p><strong>Masked LM</strong></p>
<p><img src="/images/image-20210115210456001.png" alt="image-20210115210456001" / srcset="/images/LoadingImage.gif" data-src="/images/image-20210115210456001.png" class="lozad post-image"></p>
<p>基本思路：盖住输入的句子中15%的词，让BERT把词填回来。具体方法是，挖空的位置用符号[MASK]替换，然后输入到BERT，BERT给出输出向量，将挖空地方的输出向量，输入到一个简单的线性模型中，这个模型会分辨这个向量是那个词。所以说，BERT必须好好做embedding ，只有BERT抽出的向量能很好地表示被挖空的词，这个简单的线性模型才能找到到底是哪个词被Masked。</p>
<p><strong>Next Sentence Prediction</strong></p>
<p><img src="/images/image-20210115211034797.png" alt="image-20210115211034797" / srcset="/images/LoadingImage.gif" data-src="/images/image-20210115211034797.png" class="lozad post-image"></p>
<p>基本思路：给BERT两个句子，然后判断这两个句子是不是应该接在一起。具体做法是，[SEP]符号告诉BERT交接的地方在哪里，[CLS]这个符号通常放在句子开头，它通过BERT得到的embedding 输入到简单的线性分类器中，分类器判断当前这个两个句子是不是应该接在一起。</p>
<p>你可能会疑惑[CLS]难道不应该放在句末，让BERT看完整个句子再做判断吗？</p>
<p>你仔细想想看，BERT里面一般用的是Transformer的Encoder，也就是说它做的是self-attention，我们之前说self-attention layer 不受位置的影响，它会看完整个句子，所以一个token放在句子的开头或者结尾是没有差别的。</p>
<p>最后提一下上述两个方法中，<strong>Linear classifier 是和BERT一起训练的</strong>。<strong>上述两个方法在文献上是同时使用的</strong>，让BERT的输出去解这两个任务的时候会得到最好的训练效果。</p>
<h3 id="how-to-use-bert"><a class="markdownIt-Anchor" href="#how-to-use-bert"></a> How to use BERT</h3>
<p>你当然可以把BERT当作一个抽embedding 的工具，抽出embedding 以后去做别的task，但是在文献中并不是这样，文献中是把BERT和down stream task 一起做训练😮。举了四个栗子：</p>
<p><img src="/images/image-20210115212020349.png" alt="image-20210115212020349" / srcset="/images/LoadingImage.gif" data-src="/images/image-20210115212020349.png" class="lozad post-image"></p>
<p>如上图，输入一个sentence 输出一个class ，有代表性的任务有情感分析，文章分类等。</p>
<p>具体做法，以句子情感分析为例，你找一堆带有情感标签的句子，丢给BERT（BERT有trained model），再句子开头设一个判断情感的符号[CLS]，把这个符号通过BERT的输出丢给一个线性分类器做情感分类。线性分类器是随机初始化参数用你的训练资料train 的，这个过程你也可以堆BRET进行fine-tune ，或者就fix 住BRET的参数，都行。</p>
<p><img src="/images/image-20210115212741387.png" alt="image-20210115212741387" / srcset="/images/LoadingImage.gif" data-src="/images/image-20210115212741387.png" class="lozad post-image"></p>
<p>如上图，输入一个句子，输出每个单词的类别。有代表性的栗子，Slot filling。</p>
<p>和上一个栗子类似，这里你就去train 每个线性分类器就好了，BERT也可以去fine-tune 。</p>
<p><img src="/images/image-20210115213314301.png" alt="image-20210115213314301" / srcset="/images/LoadingImage.gif" data-src="/images/image-20210115213314301.png" class="lozad post-image"></p>
<p>如上图，输入两个句子，输出自然语言推理的结果。举栗来说，给出一个前提一个假设，机器根据这个前提判断这个假设是T/F/unknown.</p>
<p>做法如上图所示，你在开头设一个提问的符号，在把两个句子用[SEP]接起来，上面接一个线性分类器（3分类），同上的训练方法。</p>
<p><img src="/images/image-20210115214817287.png" alt="image-20210115214817287" / srcset="/images/LoadingImage.gif" data-src="/images/image-20210115214817287.png" class="lozad post-image"></p>
<p>那BERT解QA问题，而且明确是Extraction-based Question Answering ，这种QA问题是说，文中一定能找到问题的答案。</p>
<p>解这种QA问题的model 如上图所示，模型输入Document D 有N个单词，Query Q有M个单词，模型输出答案在文中的起始位置和结束位置：s和e。举例来说，上图中第一个问题的答案是gravity，是Document 中第17个单词；第三个问题的答案是within a cloud，是Document 中第77到第79个单词。</p>
<p>怎么用BERT解这个问题呢？</p>
<p><img src="/images/image-20210115215102884.png" alt="image-20210115215102884" / srcset="/images/LoadingImage.gif" data-src="/images/image-20210115215102884.png" class="lozad post-image"></p>
<p>输入如上图所示，Document 中每个词都会有一个向量表示，然后你再去learn 两个向量，上图红色和蓝色条，这两个向量的维度和BERT的输出向量相同，红色向量和Document 中的词汇做点积得到一堆数值，把这些数值做softmax 最大值的位置就是s，同样的蓝色的向量做相同的运算，得到e：</p>
<p><img src="/images/image-20210115215506034.png" alt="image-20210115215506034" / srcset="/images/LoadingImage.gif" data-src="/images/image-20210115215506034.png" class="lozad post-image"></p>
<p>如果e落在s的前面，有可能就是无法回答的问题。</p>
<p>这个训练方法，你需要label很多data，每个问题的答案都需要给出在文中的位置。两个向量是白手起家learn出来的，BERT可能需要fine-tune 。</p>
<h3 id="bert-屠榜"><a class="markdownIt-Anchor" href="#bert-屠榜"></a> BERT 屠榜</h3>
<p>现在BERT在NLP任务中几乎是屠榜的，几乎每个NLP任务都已经被BERT洗过了。</p>
<p><img src="/images/image-20210115215908684.png" alt="image-20210115215908684" / srcset="/images/LoadingImage.gif" data-src="/images/image-20210115215908684.png" class="lozad post-image"></p>
<h3 id="enhanced-representation-through-knowledge-integration-ernie"><a class="markdownIt-Anchor" href="#enhanced-representation-through-knowledge-integration-ernie"></a> Enhanced Representation through Knowledge Integration (ERNIE)</h3>
<p><img src="/images/image-20210116085745533.png" alt="image-20210116085745533" / srcset="/images/LoadingImage.gif" data-src="/images/image-20210116085745533.png" class="lozad post-image"></p>
<blockquote>
<p><a target="_blank" rel="noopener" href="https://arxiv.org/abs/1904.09223">https://arxiv.org/abs/1904.09223</a></p>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/59436589">https://zhuanlan.zhihu.com/p/59436589</a></p>
</blockquote>
<p>ERNIE是转为中文设计的NLP预训练模型，by 百度。细节可以看一下文章。</p>
<h3 id="what-does-bert-learn"><a class="markdownIt-Anchor" href="#what-does-bert-learn"></a> What does BERT learn?</h3>
<p><img src="/images/image-20210116090313445.png" alt="image-20210116090313445" / srcset="/images/LoadingImage.gif" data-src="/images/image-20210116090313445.png" class="lozad post-image"></p>
<blockquote>
<p><a target="_blank" rel="noopener" href="https://arxiv.org/abs/1905.05950">https://arxiv.org/abs/1905.05950</a></p>
<p><a target="_blank" rel="noopener" href="https://openreview.net/pdf?id=SJzSgnRcKX">https://openreview.net/pdf?id=SJzSgnRcKX</a></p>
</blockquote>
<p>思考一下BERT每一层都在做什么，列出上述两个reference 给大家做参考。假如说我们的BERT有24层，down stream task有POS、Consts等等，我们来分析一下上述实验结果。这个实验把BERT的每一层的Contextualized Embedding 抽出来做weighted sum，就和EMLO的做法一样，ELMO就是把每层的Contextualized Embedding 抽出来做weighted sum，然后通过下游任务learn 出weight。</p>
<p>这里也一样，假设我们是单纯用BERT做embedding ，用得到的词向量做下游任务。那BERT有24层，所以每个词都会抽出24个vector ，把这些vector 做weighted sum，weight 是根据下游任务learn出来的，看最后learn出的weight 的情况，就可以知道这个任务更需要那些层的vector 。</p>
<p>上图中右侧蓝色的多个柱状图，代表通过不同任务learn 出的BERT各层的weight ，POS是做词性标注任务，会更依赖11-14层，Coref是做分析代词指代，会更依赖BERT高层的向量，而SRL语义角色标注就比较平均地依赖各层抽出的信息。</p>
<h3 id="multilingual-bert"><a class="markdownIt-Anchor" href="#multilingual-bert"></a> Multilingual BERT</h3>
<p><img src="/images/image-20210116101548611.png" alt="image-20210116101548611" / srcset="/images/LoadingImage.gif" data-src="/images/image-20210116101548611.png" class="lozad post-image"></p>
<blockquote>
<p><a target="_blank" rel="noopener" href="https://arxiv.org/abs/1904.09077">https://arxiv.org/abs/1904.09077</a></p>
</blockquote>
<p>Multilingual BERT 用104种语言去训练。google 到wiki百科上爬了104种语言的百科给BERT学习，虽然BERT没看过这些语言之间的翻译，但是它看过104种语言的百科以后，它似乎自动学会了不同语言之间的对应关系。</p>
<p>所以，如果你现在要用这个预训练好的BERT去做文章分类，你只要给他英文文章分类的label data set，它学完之后，竟然可以直接去做中文文章的分类 amazing😮。更多细节你可以参考上述reference</p>
<h2 id="generative-pre-training-gpt"><a class="markdownIt-Anchor" href="#generative-pre-training-gpt"></a> Generative Pre-Training (GPT)</h2>
<p><img src="/images/image-20210116101945716.png" alt="image-20210116101945716" / srcset="/images/LoadingImage.gif" data-src="/images/image-20210116101945716.png" class="lozad post-image"></p>
<blockquote>
<p><a target="_blank" rel="noopener" href="https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf">https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf</a></p>
<p>Source of image: <a target="_blank" rel="noopener" href="https://huaban.com/pins/1714071707/">https://huaban.com/pins/1714071707/</a></p>
</blockquote>
<p>从上图你就可以看出，GPT非常巨大，其实它的卖点就是大。</p>
<p>GPT目前有三个版本，GPT就是最开始版本，它的大小是平平无奇的，只是提出了一种NLP预训练模型架构，GPT-2是GPT的加大版本，GPT-3是上一个版本的加大版本。也就是中杯、大杯和超大杯。尤其是GPT-3在2020年引起了较大轰动，有人说它能为人工智能的热潮续命5年，关于GPT-3的文章已经有很多了，感兴趣可以自行查阅，本节主要探究GPT的架构。</p>
<p>为什么用独角兽的形象，后面会解释。</p>
<p>我们上面说BERT是Transformer 的Encoder ，GPT其实是Transformer 的Decoder 。</p>
<p><img src="/images/image-20210116103103686.png" alt="image-20210116103103686" / srcset="/images/LoadingImage.gif" data-src="/images/image-20210116103103686.png" class="lozad post-image"></p>
<p>GPT和一般的Language Model 做的事情一样，就是你给他一些词汇，它预测接下来的词汇。举例来说，如上图所示，把“潮水的”q拿出来做self-attention，然后做softmax 产生 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mover accent="true"><mi>α</mi><mo>^</mo></mover></mrow><annotation encoding="application/x-tex">\hat{\alpha}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.69444em;vertical-align:0em;"></span><span class="mord accent"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.69444em;"><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.0037em;">α</span></span></span><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="accent-body" style="left:-0.22222em;">^</span></span></span></span></span></span></span></span></span> ，再分别和v做相乘求和得到b，self-attention 可以有很多层（b是vector，上面还可以再接self-attention layer），通过很多层以后要预测“退了”这个词汇。</p>
<p><img src="/images/image-20210116104709265.png" alt="image-20210116104709265" / srcset="/images/LoadingImage.gif" data-src="/images/image-20210116104709265.png" class="lozad post-image"></p>
<p>预测出“退了”以后，把“退了”拿下来，做同样的计算，预测“就”这个词汇，如此往复。</p>
<h3 id="result-of-gpt"><a class="markdownIt-Anchor" href="#result-of-gpt"></a> Result of GPT</h3>
<p><img src="/images/image-20210116105305047.png" alt="image-20210116105305047" / srcset="/images/LoadingImage.gif" data-src="/images/image-20210116105305047.png" class="lozad post-image"></p>
<p><strong>GPT-2是一个巨大的预训练模型</strong>，它可以在没有更多训练资料的情况下做以下任务：</p>
<ul>
<li><strong>Reading Comprehension</strong></li>
</ul>
<p>BERT也可以做Reading Comprehension，但是BERT需要新的训练资料train 线性分类器，对BERT本身进行微调。而GPT可以在没有训练资料的情况下做这个任务。</p>
<p>如上图所示，你就给GPT-2一段文章，给出一个问题，再写一个A:，他就会尝试做出回答。右侧是GPT-2在CoQA上的结果，最大的GPT-2可以和DrQA达到相同的效果，不要忘了GPT-2在这个任务上是zero-shot learning ，从来没有人教过它做QA 。</p>
<ul>
<li><strong>Summarization</strong></li>
</ul>
<p>给出一段文章加一个too long don’t read 的缩写&quot;TL;DR:&quot; 就会尝试总结这段文字。</p>
<ul>
<li><strong>Translation</strong></li>
</ul>
<p>以上图所示的形式给出 一段英文=对应的法语，这样的栗子，然后机器就知道要给出第三句英文的法语翻译。</p>
<p>其实后两个任务效果其实不是很好，Summarization就像是随机生成的句子一样。</p>
<h3 id="visualization"><a class="markdownIt-Anchor" href="#visualization"></a> Visualization</h3>
<p><img src="/images/image-20210116110330889.png" alt="image-20210116110330889" / srcset="/images/LoadingImage.gif" data-src="/images/image-20210116110330889.png" class="lozad post-image"></p>
<blockquote>
<p><a target="_blank" rel="noopener" href="https://arxiv.org/abs/1904.02679">https://arxiv.org/abs/1904.02679</a></p>
</blockquote>
<p>有人分析了一下GTP-2的attention做的事情是什么。</p>
<p>上图右侧的两列，GPT-2中左列词汇是下一层的结果，右列是前一层需要被attention的对象，我们可以观察到，She 是通过nurse attention 出来的，He是通过doctor attention 出来的，所以机器学到了某些词汇是和性别有关系的（虽然它大概不知道性别是什么）。</p>
<p>上图左侧，是对不同层的不同head 做一下分析，你会发现一个现象，很多不同的词汇都要attend 到第一个词汇。一个可能的原因是，如果机器不知道应该attend 到哪里，或者说不需要attend 的时候就attend 在第一个词汇。如果真是这样的话，以后我们未来在做这种model 的时候可以设一个特别的词汇，当机器不知道要attend 到哪里的时候就attend 到这个特殊词汇上。这就是Visualization 可以告诉我们的事情。</p>
<h3 id="gpt-2-write-novel"><a class="markdownIt-Anchor" href="#gpt-2-write-novel"></a> GPT-2 write novel</h3>
<p><img src="/images/image-20210116111213257.png" alt="image-20210116111213257" / srcset="/images/LoadingImage.gif" data-src="/images/image-20210116111213257.png" class="lozad post-image"></p>
<p>GPT-2是OpenAI 做的，上图是OpenAI 用GPT-2 做了一个续写故事的栗子，他们给机器看第一段，后面都是机器脑部出来的。机器生成的段落中提到了独角兽和安第斯山，所以现在都拿独角兽和安第斯山来隐喻GPT：</p>
<p><img src="/images/image-20210116111457443.png" alt="image-20210116111457443" / srcset="/images/LoadingImage.gif" data-src="/images/image-20210116111457443.png" class="lozad post-image"></p>
<p>OpenAI 担心GPT-2最大的模型过于强大，可能会被用来产生假新闻这种坏事上，所以只发布了GPT-2的小模型。BERT是你能得到模型，GPT-2却是你得不到的，于是上图就很形象的表达了众多平民AI Master 的心情。</p>
<p>有人用GPT-2的公开模型做了一个在线demo，大家可以去试试：</p>
<p><a target="_blank" rel="noopener" href="https://app.inferkit.com/demo">https://app.inferkit.com/demo</a></p>
<p>参考文章：</p>
<blockquote>
<p>Attention isn’t all you need！BERT的力量之源远不止注意力：<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/58430637">https://zhuanlan.zhihu.com/p/58430637</a></p>
</blockquote>

  </div>
  <div>
  
  <div class="post-note note-info copyright" style="margin-top: 42px">
    <p><span style="font-weight: bold;">作者：</span><a target="_blank" rel="nofollow noopener noreferrer" href="https://ch3nye.top/about">Ch3nYe</a></p>
    <p><span style="font-weight: bold;">文章链接：</span><a target="_blank" rel="nofollow noopener noreferrer" href="https://ch3nye.top/ELMO-BERT-GPT/">https://ch3nye.top/ELMO-BERT-GPT/</a></p>
    <p><span style="font-weight: bold;">版权声明：</span>本博客所有文章除特别声明外，均采用<a target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh">CC BY-NC-SA 4.0 协议</a>。转载请注明出处！</p>
  </div>
  
  </div>
</article>
<div class="nav">
  
  <div class="nav-item-prev">
    <a href="/Note-FANS-Fuzzing-Android-Native-System-Services/" class="nav-link">
      <i class="iconfont icon-left nav-prev-icon"></i>
      <div>
        <div class="nav-label">上一篇</div>
        
        <div class="nav-title">Note《FANS Fuzzing Android Native System Services via Automated Interface Analysis》 </div>
        
      </div>
    </a>
  </div>
  
  
  <div class="nav-item-next">
    <a href="/Note-FIRMSCOPE-Automatic-Uncovering-of-Privilege-Escala/" class="nav-link">
      <div>
        <div class="nav-label">下一篇</div>
        
        <div class="nav-title">Note《FIRMSCOPE Automatic Uncovering of Privilege-Escala Vulnerabilities in Pre-Installed Apps in Android Firmware》 </div>
        
      </div>
      <i class="iconfont icon-right nav-next-icon"></i>
    </a>
  </div>
  
</div>

<div class="card card-content comment-card" style="margin-top: 16px;">
  <div class="comment-card-title">评论</div>
  
  <div id="lv-container" data-id="city" data-uid="MTAyMC80NjQ0Ny8yMjk1OA==">
    <script>
      (function (d, s) {
        var j, e = d.getElementsByTagName(s)[0];
        if (typeof LivereTower === 'function') {
          return;
        }
        j = d.createElement(s);
        j.src = 'https://cdn-city.livere.com/js/embed.dist.js';
        j.defer = true;
        e.parentNode.insertBefore(j, e);
      })(document, 'script');
    </script>
  </div>

</div>

<div class="card card-content toc-card" id="mobiletoc">
  <div class="toc-header"><i class="iconfont icon-menu" style="padding-right: 2px;"></i>目录</div>
<ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#elmo-bert-gpt"><span class="toc-text"> ELMO, BERT, GPT</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#representation-of-word"><span class="toc-text"> Representation of Word</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%B8%80%E8%AF%8D%E5%A4%9A%E4%B9%89"><span class="toc-text"> 一词多义</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#contextualized-word-embedding"><span class="toc-text"> Contextualized Word Embedding</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#embeddings-from-language-model-elmo"><span class="toc-text"> Embeddings from Language Model (ELMO)</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#bidirectional-encoder-representations-from-transformers-bert"><span class="toc-text"> Bidirectional Encoder Representations from Transformers (BERT)</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#training-of-bert"><span class="toc-text"> Training of BERT</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#how-to-use-bert"><span class="toc-text"> How to use BERT</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#bert-%E5%B1%A0%E6%A6%9C"><span class="toc-text"> BERT 屠榜</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#enhanced-representation-through-knowledge-integration-ernie"><span class="toc-text"> Enhanced Representation through Knowledge Integration (ERNIE)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#what-does-bert-learn"><span class="toc-text"> What does BERT learn?</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#multilingual-bert"><span class="toc-text"> Multilingual BERT</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#generative-pre-training-gpt"><span class="toc-text"> Generative Pre-Training (GPT)</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#result-of-gpt"><span class="toc-text"> Result of GPT</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#visualization"><span class="toc-text"> Visualization</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#gpt-2-write-novel"><span class="toc-text"> GPT-2 write novel</span></a></li></ol></li></ol></li></ol>
</div></main>
          <aside class="left-column">
            
            <div class="card card-author">
              
<img src="/images/logo.png" class="author-img">

<p class="author-name">Ch3nYe</p>
<p class="author-description">新年快乐🎉🎉🎉</p>
<div class="author-message">
  <a class="author-posts-count" href="/archives">
    <span>24</span>
    <span>文章</span>
  </a>
  <a class="author-categories-count" href="/categories">
    <span>5</span>
    <span>分类</span>
  </a>
  <a class="author-tags-count" href="/tags">
    <span>26</span>
    <span>标签</span>
  </a>
</div>

<div class="author-card-society">
  
    <div class="author-card-society-icon">
      <a target="_blank" rel="noopener" href="https://github.com/ch3nye/">
        <i class="iconfont icon-github society-icon"></i>
      </a>
    </div>
  
    <div class="author-card-society-icon">
      <a href="mailto:sud0su@163.com">
        <i class="iconfont icon-mail society-icon"></i>
      </a>
    </div>
  
</div>

            </div>
            
            <div class="sticky-tablet">
  
  
  <article class="display-when-two-columns spacer">
    <div class="card card-content toc-card">
      <div class="toc-header"><i class="iconfont icon-menu" style="padding-right: 2px;"></i>目录</div>
<ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#elmo-bert-gpt"><span class="toc-text"> ELMO, BERT, GPT</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#representation-of-word"><span class="toc-text"> Representation of Word</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%B8%80%E8%AF%8D%E5%A4%9A%E4%B9%89"><span class="toc-text"> 一词多义</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#contextualized-word-embedding"><span class="toc-text"> Contextualized Word Embedding</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#embeddings-from-language-model-elmo"><span class="toc-text"> Embeddings from Language Model (ELMO)</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#bidirectional-encoder-representations-from-transformers-bert"><span class="toc-text"> Bidirectional Encoder Representations from Transformers (BERT)</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#training-of-bert"><span class="toc-text"> Training of BERT</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#how-to-use-bert"><span class="toc-text"> How to use BERT</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#bert-%E5%B1%A0%E6%A6%9C"><span class="toc-text"> BERT 屠榜</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#enhanced-representation-through-knowledge-integration-ernie"><span class="toc-text"> Enhanced Representation through Knowledge Integration (ERNIE)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#what-does-bert-learn"><span class="toc-text"> What does BERT learn?</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#multilingual-bert"><span class="toc-text"> Multilingual BERT</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#generative-pre-training-gpt"><span class="toc-text"> Generative Pre-Training (GPT)</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#result-of-gpt"><span class="toc-text"> Result of GPT</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#visualization"><span class="toc-text"> Visualization</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#gpt-2-write-novel"><span class="toc-text"> GPT-2 write novel</span></a></li></ol></li></ol></li></ol>
    </div>
  </article>
  
  
  <article class="card card-content categories-widget">
    <div class="categories-card">
  <div class="categories-header"><i class="iconfont icon-fenlei" style="padding-right: 2px;"></i>分类</div>
  <div class="categories-list">
    
      <a href="/categories/备忘">
        <div class="categories-list-item">
          备忘
          <span class="categories-list-item-badge">1</span>
        </div>
      </a>
    
      <a href="/categories/李宏毅机器学习笔记">
        <div class="categories-list-item">
          李宏毅机器学习笔记
          <span class="categories-list-item-badge">14</span>
        </div>
      </a>
    
      <a href="/categories/论文阅读">
        <div class="categories-list-item">
          论文阅读
          <span class="categories-list-item-badge">4</span>
        </div>
      </a>
    
      <a href="/categories/笔记">
        <div class="categories-list-item">
          笔记
          <span class="categories-list-item-badge">4</span>
        </div>
      </a>
    
      <a href="/categories/实战">
        <div class="categories-list-item">
          实战
          <span class="categories-list-item-badge">1</span>
        </div>
      </a>
    
  </div>
</div>
  </article>
  
  <article class="card card-content tags-widget">
    <div class="tags-card">
  <div class="tags-header"><i class="iconfont icon-biaoqian" style="padding-right: 2px;"></i>热门标签</div>
  <div class="tags-list">
    
    <a href="\tags\Next-Step-of-ML" title="Next-Step-of-ML"><div class="tags-list-item">Next-Step-of-ML</div></a>
    
    <a href="\tags\note" title="note"><div class="tags-list-item">note</div></a>
    
    <a href="\tags\ML" title="ML"><div class="tags-list-item">ML</div></a>
    
    <a href="\tags\Android" title="Android"><div class="tags-list-item">Android</div></a>
    
    <a href="\tags\笔记" title="笔记"><div class="tags-list-item">笔记</div></a>
    
    <a href="\tags\论文" title="论文"><div class="tags-list-item">论文</div></a>
    
    <a href="\tags\学习" title="学习"><div class="tags-list-item">学习</div></a>
    
    <a href="\tags\总结" title="总结"><div class="tags-list-item">总结</div></a>
    
    <a href="\tags\动态检测" title="动态检测"><div class="tags-list-item">动态检测</div></a>
    
    <a href="\tags\实战" title="实战"><div class="tags-list-item">实战</div></a>
    
    <a href="\tags\crack" title="crack"><div class="tags-list-item">crack</div></a>
    
    <a href="\tags\ELF" title="ELF"><div class="tags-list-item">ELF</div></a>
    
    <a href="\tags\二进制" title="二进制"><div class="tags-list-item">二进制</div></a>
    
    <a href="\tags\Linux" title="Linux"><div class="tags-list-item">Linux</div></a>
    
    <a href="\tags\Hot-Patch" title="Hot-Patch"><div class="tags-list-item">Hot-Patch</div></a>
    
    <a href="\tags\污点分析" title="污点分析"><div class="tags-list-item">污点分析</div></a>
    
  </div>
</div>
  </article>
  
  
</div>
          </aside>
          <aside class="right-column">
            <div class="sticky-widescreen">
  
  
  <article class="card card-content toc-card">
    <div class="toc-header"><i class="iconfont icon-menu" style="padding-right: 2px;"></i>目录</div>
<ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#elmo-bert-gpt"><span class="toc-text"> ELMO, BERT, GPT</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#representation-of-word"><span class="toc-text"> Representation of Word</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%B8%80%E8%AF%8D%E5%A4%9A%E4%B9%89"><span class="toc-text"> 一词多义</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#contextualized-word-embedding"><span class="toc-text"> Contextualized Word Embedding</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#embeddings-from-language-model-elmo"><span class="toc-text"> Embeddings from Language Model (ELMO)</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#bidirectional-encoder-representations-from-transformers-bert"><span class="toc-text"> Bidirectional Encoder Representations from Transformers (BERT)</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#training-of-bert"><span class="toc-text"> Training of BERT</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#how-to-use-bert"><span class="toc-text"> How to use BERT</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#bert-%E5%B1%A0%E6%A6%9C"><span class="toc-text"> BERT 屠榜</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#enhanced-representation-through-knowledge-integration-ernie"><span class="toc-text"> Enhanced Representation through Knowledge Integration (ERNIE)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#what-does-bert-learn"><span class="toc-text"> What does BERT learn?</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#multilingual-bert"><span class="toc-text"> Multilingual BERT</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#generative-pre-training-gpt"><span class="toc-text"> Generative Pre-Training (GPT)</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#result-of-gpt"><span class="toc-text"> Result of GPT</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#visualization"><span class="toc-text"> Visualization</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#gpt-2-write-novel"><span class="toc-text"> GPT-2 write novel</span></a></li></ol></li></ol></li></ol>
  </article>
  
  
  <article class="card card-content">
    <div class="recent-posts-card">
  <div class="recent-posts-header"><i class="iconfont icon-wenzhang_huaban" style="padding-right: 2px;"></i>最近文章</div>
  <div class="recent-posts-list">
    
      <div class="recent-posts-item">
        <div class="recent-posts-item-title">2021-02-15</div>
        <a href="/Note-Towards-Dynamically-Monitoring-Android-Applications-on-Non-rooted-Devices-in-the-Wild/"><div class="recent-posts-item-content">Note 《Towards Dynamically Monitoring Android Applications on Non-rooted Devices in the Wild》</div></a>
      </div>
    
      <div class="recent-posts-item">
        <div class="recent-posts-item-title">2021-02-08</div>
        <a href="/Linux二进制分析笔记(ELF)/"><div class="recent-posts-item-content">Linux二进制分析笔记(ELF)</div></a>
      </div>
    
      <div class="recent-posts-item">
        <div class="recent-posts-item-title">2021-02-01</div>
        <a href="/【实战】某视频app的crack尝试/"><div class="recent-posts-item-content">【实战】某视频app的crack尝试</div></a>
      </div>
    
      <div class="recent-posts-item">
        <div class="recent-posts-item-title">2021-01-29</div>
        <a href="/Note-Automatic-Hot-Patch-Generation-for-Android-Kernels/"><div class="recent-posts-item-content">Note《Automatic Hot Patch Generation for Android Kernels》</div></a>
      </div>
    
  </div>
</div>
  </article>
  
  
</div>
          </aside>
        </div>
      </div>
    </div>
  </div>
  
  <footer class="footer">
  <div class="footer-container">
    <div>
      <div class="footer-dsc">
        <span>Copyright ©
          
          2020 -
          
          2021
        </span>
        &nbsp;
        <a href="/" class="footer-link">ch3nye's blog </a>
      </div>
    </div>

    
    <div class="footer-dsc">
      
      Powered by
      <a href="https://hexo.io/" class="footer-link" target="_blank" rel="nofollow noopener noreferrer">&nbsp;Hexo </a>
      
      
      <span>&nbsp;|&nbsp;</span>
      
      
      Theme -
      <a href="https://github.com/theme-kaze" class="footer-link" target="_blank"
        rel="nofollow noopener noreferrer">&nbsp;Kaze</a>
      
    </div>
    
    
    
    
      <div class="footer-dsc">
        
        本站总访问量<span id="busuanzi_value_site_pv"></span>次
        
        
        <span>&nbsp;|&nbsp;</span>
        
        
        本站总访客数<span id="busuanzi_value_site_uv"></span>次
        
      </div>
      
    
</footer>
  <a role="button" id="scrollbutton" class="basebutton"  aria-label="回到顶部">
  <i class="iconfont icon-arrowleft button-icon"></i>
</a>
<a role="button" id="menubutton" class="basebutton">
  <i class="iconfont icon-menu button-icon"></i>
</a>
<a role="button" id="popbutton" class="basebutton" aria-label="控制中心">
  <i class="iconfont icon-expand button-icon"></i>
</a>
<a role="button" id="darkbutton" class="basebutton darkwidget" aria-label="夜色模式">
  <i class="iconfont icon-weather button-icon"></i>
</a>
<a role="button" id="searchbutton" class="basebutton searchwidget" aria-label="搜索">
  <i class="iconfont icon-search button-icon"></i>
</a>

  
  
  

  
  
    
<link rel="stylesheet" href="//cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.css">

  

  
  
  <script>
  var addImgLayout = function () {
    var img = document.querySelectorAll('.post-content img');
    var i;
    for (i = 0; i < img.length; i++) {
      var wrapper = document.createElement('a');
      wrapper.setAttribute('data-fslightbox', 'gallery');
      wrapper.setAttribute('href', img[i].getAttribute('data-src'));
      wrapper.setAttribute('aria-label', 'illustration');
      wrapper.style.cssText = 'width: 100%; display: flex; justify-content: center;';
      img[i].before(wrapper);
      wrapper.append(img[i]);
    }
    refreshFsLightbox();
  }
</script>
<script>loadScript("//cdn.jsdelivr.net/npm/fslightbox@3.1.0/index.min.js", addImgLayout)</script>
  
  
  
<script src="/js/main.js"></script>

  
  <script>loadScript("/js/lib/busuanzi.min.js")</script>
  
  
  <script>
    var addLazyload = function () {
      var observer = lozad('.lozad', {
        load: function (el) {
          el.srcset = el.getAttribute('data-src');
        },
        loaded: function (el) {
          el.classList.add('loaded');
        }
      });
      observer.observe();
    }
  </script>
  <script>loadScript("/js/lib/lozad.min.js", addLazyload)</script>
  
  
  <script>
    var googleAnalytics = function() {
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-WHGL11014T');
    }
  </script>
  <script>loadScript("https://www.googletagmanager.com/gtag/js?id=" + "G-WHGL11014T", googleAnalytics)</script>
  
</body>

</html>