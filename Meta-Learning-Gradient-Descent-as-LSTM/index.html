<!DOCTYPE html>
<html lang="zh-CN">
  <head>
  <meta charset="UTF-8">
  <meta 
    name="viewport"
    content="width=device-width, initial-scale=1.0, minimum-scale=1.0">
  <meta 
    http-equiv="X-UA-Compatible" 
    content="ie=edge">
  <meta 
    name="theme-color" 
    content="#fff" 
    id="theme-color">
  <meta 
    name="description" 
    content="网络安全 Android ML CTF">
  <link 
    rel="icon" 
    href="/images/logo.png">
  <title>Meta Learning-Gradient Descent as LSTM</title>
  
    
      <meta 
        property="og:title" 
        content="Meta Learning-Gradient Descent as LSTM">
    
    
      <meta 
        property="og:url" 
        content="https://ch3nye.top/Meta-Learning-Gradient-Descent-as-LSTM/index.html">
    
    
      <meta 
        property="og:img" 
        content="/images/logo.png">
    
    
      <meta 
        property="og:img" 
        content="网络安全 Android ML CTF">
    
    
      <meta 
        property="og:type" 
        content="article">
      <meta 
        property="og:article:published_time" 
        content="2020-12-06">
      <meta 
        property="og:article:modified_time" 
        content="2020-12-06">
      <meta 
        property="og:article:author" 
        content="Ch3nYe">
      
        
          <meta 
            property="og:article:tag" 
            content="ML">
        
          <meta 
            property="og:article:tag" 
            content="note">
        
          <meta 
            property="og:article:tag" 
            content="Next-Step-of-ML">
        
      
    
  
  <script>
    function loadScript(url, cb) {
      var script = document.createElement('script');
      script.src = url;
      if (cb) script.onload = cb;
      script.async = true;
      document.body.appendChild(script);
    }
    function loadCSS(href, data, attr) {
      var sheet = document.createElement('link');
      sheet.ref = 'stylesheet';
      sheet.href = href;
      sheet.dataset[data] = attr;
      document.head.appendChild(sheet);
    }
    function changeCSS(cssFile, data, attr) {
      var oldlink = document.querySelector(data);
      var newlink = document.createElement("link");
      newlink.setAttribute("rel", "stylesheet");
      newlink.setAttribute("href", cssFile);
      newlink.dataset.prism = attr;
      document.head.replaceChild(newlink, oldlink);
    }
  </script>
  
    
      
      
      
      
        
        
        
        <script>
          function prismThemeChange() {
            if(document.getElementById('theme-color').dataset.mode === 'dark') {
              if(document.querySelector('[data-prism]')) {
                changeCSS('/js/lib/prism/prism-tomorrow.min.css', '[data-prism]', 'prism-tomorrow');
              } else {
                loadCSS('/js/lib/prism/prism-tomorrow.min.css', 'prism', 'prism-tomorrow');
              }
            } else {
              if(document.querySelector('[data-prism]')) {
                changeCSS('/js/lib/prism/prism.min.css', '[data-prism]', 'prism');
              } else {
                loadCSS('/js/lib/prism/prism.min.css', 'prism', 'prism');
              }
            }
          }
          prismThemeChange()
        </script>
      
      
        
        <link rel="stylesheet" href="/js/lib/prism/prism-line-numbers.min.css">
      
    
  
  <script>
    // control reverse button
    var reverseDarkList = {
      dark: 'light',
      light: 'dark'
    };
    var themeColor = {
      dark: '#1c1c1e',
      light: '#fff'
    }
    // get the data of css prefers-color-scheme
    var getCssMediaQuery = function() {
      return window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'light';
    };
    // reverse current darkmode setting function
    var reverseDarkModeSetting = function() {
      var setting = localStorage.getItem('user-color-scheme');
      if(reverseDarkList[setting]) {
        setting = reverseDarkList[setting];
      } else if(setting === null) {
        setting = reverseDarkList[getCssMediaQuery()];
      } else {
        return;
      }
      localStorage.setItem('user-color-scheme', setting);
      return setting;
    };
    // apply current darkmode setting
  </script>
  
    <script>
      var setDarkmode = function(mode) {
      var setting = mode || localStorage.getItem('user-color-scheme');
      if(setting === getCssMediaQuery()) {
        document.documentElement.removeAttribute('data-user-color-scheme');
        localStorage.removeItem('user-color-scheme');
        document.getElementById('theme-color').content = themeColor[setting];
        document.getElementById('theme-color').dataset.mode = setting;
        prismThemeChange();
      } else if(reverseDarkList[setting]) {
        document.documentElement.setAttribute('data-user-color-scheme', setting);
        document.getElementById('theme-color').content = themeColor[setting];
        document.getElementById('theme-color').dataset.mode = setting;
        prismThemeChange();
      } else {
        document.documentElement.removeAttribute('data-user-color-scheme');
        localStorage.removeItem('user-color-scheme');
        document.getElementById('theme-color').content = themeColor[getCssMediaQuery()];
        document.getElementById('theme-color').dataset.mode = getCssMediaQuery();
        prismThemeChange();
      }
    };
    setDarkmode();
    </script>
  
  
  <link rel="preload" href="//at.alicdn.com/t/font_1946621_i1kgafibvw.css" as="style" >
  <link rel="preload" href="//at.alicdn.com/t/font_1952792_89b4ac4k4up.css" as="style" >
  
  
    <link rel="preload" href="/js/lib/lightbox/baguetteBox.min.js" as="script">
    <link rel="preload" href="/js/lib/lightbox/baguetteBox.min.css" as="style" >
  
  
    <link rel="preload" href="/js/lib/lozad.min.js" as="script">
  
  
  
  
    
    <link rel="prefetch" href="//cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.css" as="script">
  
  
    
    <link rel="prefetch" href="//cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.js" as="script">
  
  
  
  <link rel="stylesheet" href="/css/main.css">
  
  <link rel="stylesheet" href="//at.alicdn.com/t/font_1946621_i1kgafibvw.css">
  
  <link rel="stylesheet" href="//at.alicdn.com/t/font_1952792_89b4ac4k4up.css">
  
    <link rel="stylesheet" href="/js/lib/lightbox/baguetteBox.min.css">
  
<meta name="generator" content="Hexo 5.3.0"></head>

  <body>
    <div class="wrapper">
       
      <nav class="navbar">
  <div class="navbar-logo">
    <span class="navbar-logo-main">
      
        <img 
          class="navbar-logo-img"
          width="32"
          height="32"
          src="/images/logo.png" 
          alt="blog logo">
      
      <span class="navbar-logo-dsc">ch3nye's blog</span>
    </span>
  </div>
  <div class="navbar-menu">
    
      <a 
        href="/" 
        class="navbar-menu-item">
        
          首页
        
      </a>
    
      <a 
        href="/archives" 
        class="navbar-menu-item">
        
          归档
        
      </a>
    
      <a 
        href="/tags" 
        class="navbar-menu-item">
        
          标签
        
      </a>
    
      <a 
        href="/categories" 
        class="navbar-menu-item">
        
          分类
        
      </a>
    
      <a 
        href="/about" 
        class="navbar-menu-item">
        
          关于
        
      </a>
    
      <a 
        href="/links" 
        class="navbar-menu-item">
        
          友链
        
      </a>
    
      <a 
        href="/shuoshuo" 
        class="navbar-menu-item">
        
          说说
        
      </a>
    
    <a 
      class="navbar-menu-item darknavbar" 
      id="dark">
      <i class="iconfont icon-weather"></i>
    </a>
    <a 
      class="navbar-menu-item searchnavbar" 
      id="search">
      <i 
        class="iconfont icon-search" 
        style="font-size: 1.2rem; font-weight: 400;">
      </i>
    </a>
  </div>
</nav> 
      
      <div 
        id="local-search" 
        style="display: none">
        <input
          class="navbar-menu-item"
          id="search-input"
          placeholder="请输入搜索内容..." />
        <div id="search-content"></div>
      </div>
      
      <div class="section-wrap">
        <div class="container">
          <div class="columns">
            <main class="main-column">
<article class="card card-content">
  <header>
    <h1 class="post-title">
      Meta Learning-Gradient Descent as LSTM
    </h1>
  </header>
  <div class="post-meta post-show-meta">
    <time datetime="2020-12-05T16:00:00.000Z">
      <i 
        class="iconfont icon-calendar" 
        style="margin-right: 2px;">
      </i>
      <span>2020-12-06</span>
    </time>
    
      <span class="dot"></span>
      
        <a 
          href="/categories/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/" 
          class="post-meta-link">
          李宏毅机器学习笔记
        </a>
      
    
    
      <span class="dot"></span>
      <span>3.9k 字</span>
    
  </div>
  
    <div 
      class="post-meta post-show-meta" 
      style="margin-top: -10px;">
      <div style="display: flex; align-items: center;">
        <i 
          class="iconfont icon-biaoqian" 
          style="margin-right: 2px; font-size: 1.15rem;">
        </i>
        
          
          <a 
            href="/tags/ML/" 
            class="post-meta-link">
            ML
          </a>
        
          
            <span class="dot"></span>
          
          <a 
            href="/tags/note/" 
            class="post-meta-link">
            note
          </a>
        
          
            <span class="dot"></span>
          
          <a 
            href="/tags/Next-Step-of-ML/" 
            class="post-meta-link">
            Next-Step-of-ML
          </a>
        
      </div>
    </div>
  
  </header>
  <div 
    id="section" 
    class="post-content">
    <h1 id="meta-learning-gradient-descent-as-lstm"><a class="markdownIt-Anchor" href="#meta-learning-gradient-descent-as-lstm"></a> Meta Learning - Gradient Descent as LSTM</h1>
<p>上节课讲了MAML 和Reptile ，我们说Meta Learning 就是要让机器自己learn 出一个learning 的algorithm。今天我们要讲怎么把我们熟悉的learning algorithm ：Gradient Descent ，当作一个LSTM 来看待，你直接把这个LSTM train下去，你就train 出了Gradient Descent 这样的Algorithm 。（也就是说我现在要把学习算法，即参数的更新算法当作未知数，用Meta Learning 训练出来）</p>
<p><img src="/images/image-20201205165921081.png" alt="image-20201205165921081" / srcset="/images/LoadingImage.gif" data-src="/images/image-20201205165921081.png" class="lozad post-image"></p>
<p>上周我们讲的MAML 和Reptile 都是在Initial Parameters 上做文章，用Meta Learning 训练出一组好的初始化参数，现在我们希望能更进一步，通过Meta Learning 训练出一个好的参数update 算法，上图黄色方块。</p>
<p>我们可以把整个Meta Learning 的算法看作RNN，它和RNN 有点像的，同样都是每次吃一个batch 的data ，RNN 中的memory 可以类比到Meta Learning 中的参数 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>θ</mi></mrow><annotation encoding="application/x-tex">\theta</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.69444em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.02778em;">θ</span></span></span></span> 。</p>
<p>把这个Meta Learning 的算法看作RNN 的思想主要出自两篇paper ：</p>
<blockquote>
<p><a target="_blank" rel="noopener" href="https://openreview.net/forum?id=rJY0-Kcll&amp;noteId=ryq49XyLg">Optimization as a Model for Few-Shot Learning | OpenReview</a></p>
<p>Sachin Ravi, Hugo Larochelle</p>
<p>[<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1606.04474">1606.04474] Learning to learn by gradient descent by gradient descent (arxiv.org)</a><br />
Marcin Andrychowicz, Misha Denil, Sergio Gomez, Matthew W. Hoffman, David Pfau, Tom Schaul, Brendan Shillingford, Nando de Freitas</p>
</blockquote>
<p>第二篇文章的题目非常有趣，也说明了此篇文章的中心：让机器学习用梯度下降学习这件事，使用的方法就是梯度下降。</p>
<h2 id="review-rnn"><a class="markdownIt-Anchor" href="#review-rnn"></a> Review: RNN</h2>
<p>从与之前略微不同的角度快速回顾一下RNN。</p>
<p><img src="/images/image-20201205173024572.png" alt="image-20201205173024572" / srcset="/images/LoadingImage.gif" data-src="/images/image-20201205173024572.png" class="lozad post-image"></p>
<p>RNN就是一个function f，这个函数吃h,x 吐出 h’,y  ，每个step 会有一个x（训练样本数据）作为input，还有一个初始的memory 的值 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>h</mi><mn>0</mn></msub></mrow><annotation encoding="application/x-tex">h_0</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.84444em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault">h</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">0</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> 作为input，这个初始参数有时候是人手动设置的，有时候是可以让模型learn 出来的，然后输出一个y和一个 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msup><mi>h</mi><mn>1</mn></msup></mrow><annotation encoding="application/x-tex">h^1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141079999999999em;vertical-align:0em;"></span><span class="mord"><span class="mord mathdefault">h</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141079999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span></span></span></span></span></span></span></span> 。到下一个step，它吃上一个step 得到的 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msup><mi>h</mi><mn>1</mn></msup></mrow><annotation encoding="application/x-tex">h^1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141079999999999em;vertical-align:0em;"></span><span class="mord"><span class="mord mathdefault">h</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141079999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span></span></span></span></span></span></span></span> 和新的x，也是同样的输出。需要注意的是，h的维度都是一致的，这样同一个f 才能吃前一个step 得到h 。这个过程不断重复，就是RNN。</p>
<p>所以，无论多长的input/output sequence 我们只需要一个函数f 就可以运算，无论你的输入再怎么多，模型的参数量不会变化，这就是RNN 厉害的地方，所以它特别擅长处理input 是一个sequence 的状态。（比如说自然语言处理中input 是一个长句子，用word vector 组成的很长的sequence）</p>
<p>我们如今用的一般都是RNN 的变形LSTM，而且我们现在说使用RNN 基本上就是在指使用LSTM 的技术。那LSTM 相比于RNN 有什么特别的地方呢。</p>
<p><img src="/images/image-20201205182429395.png" alt="image-20201205182429395" / srcset="/images/LoadingImage.gif" data-src="/images/image-20201205182429395.png" class="lozad post-image"></p>
<p>如上图，LSTM（右）相比于RNN ，把input 的h 拆解成两部分，一部分仍然叫做 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>h</mi></mrow><annotation encoding="application/x-tex">h</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.69444em;vertical-align:0em;"></span><span class="mord mathdefault">h</span></span></span></span> ，一部分我们叫做 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>c</mi></mrow><annotation encoding="application/x-tex">c</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em;"></span><span class="mord mathdefault">c</span></span></span></span> 。为什么要这样分呢，你可以想象是因为 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>c</mi></mrow><annotation encoding="application/x-tex">c</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em;"></span><span class="mord mathdefault">c</span></span></span></span> 和 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>h</mi></mrow><annotation encoding="application/x-tex">h</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.69444em;vertical-align:0em;"></span><span class="mord mathdefault">h</span></span></span></span> 扮演了不同的角色。</p>
<ul>
<li><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>c</mi></mrow><annotation encoding="application/x-tex">c</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em;"></span><span class="mord mathdefault">c</span></span></span></span> 变化较慢，通常就是把某个向量加到上一个 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msup><mi>c</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msup></mrow><annotation encoding="application/x-tex">c^{t-1}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141079999999999em;vertical-align:0em;"></span><span class="mord"><span class="mord mathdefault">c</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141079999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">t</span><span class="mbin mtight">−</span><span class="mord mtight">1</span></span></span></span></span></span></span></span></span></span></span></span> 上就得到了新的 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msup><mi>c</mi><mi>t</mi></msup></mrow><annotation encoding="application/x-tex">c^t</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7935559999999999em;vertical-align:0em;"></span><span class="mord"><span class="mord mathdefault">c</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.7935559999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">t</span></span></span></span></span></span></span></span></span></span></span> ，这个 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msup><mi>c</mi><mi>t</mi></msup></mrow><annotation encoding="application/x-tex">c^t</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7935559999999999em;vertical-align:0em;"></span><span class="mord"><span class="mord mathdefault">c</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.7935559999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">t</span></span></span></span></span></span></span></span></span></span></span> 就是LSTM 中memory cell 存储的值，由于这个值变化很慢，所以LSTM 可以记住时间比较久的数据</li>
<li><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>h</mi></mrow><annotation encoding="application/x-tex">h</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.69444em;vertical-align:0em;"></span><span class="mord mathdefault">h</span></span></span></span> 变化较快， <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msup><mi>h</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msup></mrow><annotation encoding="application/x-tex">h^{t-1}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141079999999999em;vertical-align:0em;"></span><span class="mord"><span class="mord mathdefault">h</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141079999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">t</span><span class="mbin mtight">−</span><span class="mord mtight">1</span></span></span></span></span></span></span></span></span></span></span></span> 和 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msup><mi>h</mi><mi>t</mi></msup></mrow><annotation encoding="application/x-tex">h^t</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7935559999999999em;vertical-align:0em;"></span><span class="mord"><span class="mord mathdefault">h</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.7935559999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">t</span></span></span></span></span></span></span></span></span></span></span> 的变化是很大的</li>
</ul>
<h2 id="review-lstm"><a class="markdownIt-Anchor" href="#review-lstm"></a> Review: LSTM</h2>
<p>我们接下来看看LSTM 的做法和结构：</p>
<p><img src="/images/image-20201205184726564.png" alt="image-20201205184726564" / srcset="/images/LoadingImage.gif" data-src="/images/image-20201205184726564.png" class="lozad post-image"></p>
<p><span class="katex"><span class="katex-mathml"><math><semantics><mrow><msup><mi>c</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msup></mrow><annotation encoding="application/x-tex">c^{t-1}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141079999999999em;vertical-align:0em;"></span><span class="mord"><span class="mord mathdefault">c</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141079999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">t</span><span class="mbin mtight">−</span><span class="mord mtight">1</span></span></span></span></span></span></span></span></span></span></span></span> 是memory 记忆单元，把x和h拼在一起乘上一个权重矩阵W，再通过一个tanh 函数得到input z，z是一个向量。同样的x和h拼接后乘上对应的权重矩阵得到对应向量input gate <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msup><mi>z</mi><mi>i</mi></msup></mrow><annotation encoding="application/x-tex">z^i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.824664em;vertical-align:0em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.04398em;">z</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.824664em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">i</span></span></span></span></span></span></span></span></span></span></span> ，forget gate <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msup><mi>z</mi><mi>f</mi></msup></mrow><annotation encoding="application/x-tex">z^f</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8491079999999999em;vertical-align:0em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.04398em;">z</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8491079999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:0.10764em;">f</span></span></span></span></span></span></span></span></span></span></span> ，output  gate <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msup><mi>z</mi><mi>o</mi></msup></mrow><annotation encoding="application/x-tex">z^o</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.664392em;vertical-align:0em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.04398em;">z</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.664392em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">o</span></span></span></span></span></span></span></span></span></span></span> ，接下来：</p>
<p><img src="/images/image-20201205185052977.png" alt="image-20201205185052977" / srcset="/images/LoadingImage.gif" data-src="/images/image-20201205185052977.png" class="lozad post-image"></p>
<p><span class="katex"><span class="katex-mathml"><math><semantics><mrow><msup><mi>z</mi><mi>f</mi></msup><mo>⋅</mo><msup><mi>c</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msup></mrow><annotation encoding="application/x-tex">z^f \cdot c^{t-1}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8491079999999999em;vertical-align:0em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.04398em;">z</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8491079999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:0.10764em;">f</span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">⋅</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.8141079999999999em;vertical-align:0em;"></span><span class="mord"><span class="mord mathdefault">c</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141079999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">t</span><span class="mbin mtight">−</span><span class="mord mtight">1</span></span></span></span></span></span></span></span></span></span></span></span> 决定是否保留上个memory， <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msup><mi>z</mi><mi>i</mi></msup><mo>⋅</mo><mi>z</mi></mrow><annotation encoding="application/x-tex">z^i \cdot z</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.824664em;vertical-align:0em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.04398em;">z</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.824664em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">i</span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">⋅</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.04398em;">z</span></span></span></span> 决定是否把现在的input 存到memory；</p>
<p>通过 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msup><mi>z</mi><mi>o</mi></msup><mo>⋅</mo><mi>t</mi><mi>a</mi><mi>n</mi><mi>h</mi><mo stretchy="false">(</mo><msup><mi>c</mi><mi>t</mi></msup><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">z^o \cdot tanh(c^t)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.664392em;vertical-align:0em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.04398em;">z</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.664392em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">o</span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">⋅</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1.043556em;vertical-align:-0.25em;"></span><span class="mord mathdefault">t</span><span class="mord mathdefault">a</span><span class="mord mathdefault">n</span><span class="mord mathdefault">h</span><span class="mopen">(</span><span class="mord"><span class="mord mathdefault">c</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.7935559999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">t</span></span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span> 得到新的 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msup><mi>h</mi><mi>t</mi></msup></mrow><annotation encoding="application/x-tex">h^t</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7935559999999999em;vertical-align:0em;"></span><span class="mord"><span class="mord mathdefault">h</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.7935559999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">t</span></span></span></span></span></span></span></span></span></span></span> ；</p>
<p><span class="katex"><span class="katex-mathml"><math><semantics><mrow><msup><mi>W</mi><mo mathvariant="normal">′</mo></msup></mrow><annotation encoding="application/x-tex">W&#x27;</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.751892em;vertical-align:0em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.13889em;">W</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.751892em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span></span></span></span> 乘上新的 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msup><mi>h</mi><mi>t</mi></msup></mrow><annotation encoding="application/x-tex">h^t</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7935559999999999em;vertical-align:0em;"></span><span class="mord"><span class="mord mathdefault">h</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.7935559999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">t</span></span></span></span></span></span></span></span></span></span></span> ，再通过一个sigmoid function 得到当前step 的output <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msup><mi>y</mi><mi>t</mi></msup></mrow><annotation encoding="application/x-tex">y^t</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.9879959999999999em;vertical-align:-0.19444em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.03588em;">y</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.7935559999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">t</span></span></span></span></span></span></span></span></span></span></span> ；</p>
<p>重复上述步骤，就是LSTM 的运作方式：</p>
<p><img src="/images/image-20201205185732047.png" alt="image-20201205185732047" / srcset="/images/LoadingImage.gif" data-src="/images/image-20201205185732047.png" class="lozad post-image"></p>
<p>好，讲了这么多，它和Gradient Descent 到底有什么样的关系呢？</p>
<h2 id="lstm-similar-to-gradient-descent-based-algorithm"><a class="markdownIt-Anchor" href="#lstm-similar-to-gradient-descent-based-algorithm"></a> LSTM similar to gradient descent based algorithm</h2>
<p>我们把梯度下降参数θ更新公式和LSTM 的memory c更新公式都列出来，如下图所示：</p>
<p><img src="/images/image-20201205191715041.png" alt="image-20201205191715041" / srcset="/images/LoadingImage.gif" data-src="/images/image-20201205191715041.png" class="lozad post-image"></p>
<p>我们知道在gradient descent 中我们在每个step 中，把旧的参数减去，learning rate 乘梯度，作为更新后的新参数，如上图所示，此式，和LSTM 中memory 单元 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>c</mi></mrow><annotation encoding="application/x-tex">c</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em;"></span><span class="mord mathdefault">c</span></span></span></span> 有些相似，我们就把 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>c</mi></mrow><annotation encoding="application/x-tex">c</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em;"></span><span class="mord mathdefault">c</span></span></span></span> 替换成 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>θ</mi></mrow><annotation encoding="application/x-tex">\theta</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.69444em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.02778em;">θ</span></span></span></span> 看看：</p>
<p><img src="/images/image-20201205192114023.png" alt="image-20201205192114023" / srcset="/images/LoadingImage.gif" data-src="/images/image-20201205192114023.png" class="lozad post-image"></p>
<p>接下来我们再做一些变换。输入<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msup><mi>h</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msup></mrow><annotation encoding="application/x-tex">h^{t-1}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141079999999999em;vertical-align:0em;"></span><span class="mord"><span class="mord mathdefault">h</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141079999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">t</span><span class="mbin mtight">−</span><span class="mord mtight">1</span></span></span></span></span></span></span></span></span></span></span></span> 来自上一个step，<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msup><mi>x</mi><mi>t</mi></msup></mrow><annotation encoding="application/x-tex">x^t</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7935559999999999em;vertical-align:0em;"></span><span class="mord"><span class="mord mathdefault">x</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.7935559999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">t</span></span></span></span></span></span></span></span></span></span></span> 来自外界输入，我们就把<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msup><mi>h</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msup></mrow><annotation encoding="application/x-tex">h^{t-1}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141079999999999em;vertical-align:0em;"></span><span class="mord"><span class="mord mathdefault">h</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141079999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">t</span><span class="mbin mtight">−</span><span class="mord mtight">1</span></span></span></span></span></span></span></span></span></span></span></span> <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msup><mi>x</mi><mi>t</mi></msup></mrow><annotation encoding="application/x-tex">x^t</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7935559999999999em;vertical-align:0em;"></span><span class="mord"><span class="mord mathdefault">x</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.7935559999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">t</span></span></span></span></span></span></span></span></span></span></span> 换成$-\nabla_\theta l $ 。然后我们假设从input 到z 的公式中乘的matrix 是单位矩阵，所以z 就等于$-\nabla_\theta l $ 。再然后，我们把<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msup><mi>z</mi><mi>f</mi></msup></mrow><annotation encoding="application/x-tex">z^f</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8491079999999999em;vertical-align:0em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.04398em;">z</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8491079999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:0.10764em;">f</span></span></span></span></span></span></span></span></span></span></span> 定位全1的列向量，<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msup><mi>z</mi><mi>i</mi></msup></mrow><annotation encoding="application/x-tex">z^i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.824664em;vertical-align:0em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.04398em;">z</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.824664em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">i</span></span></span></span></span></span></span></span></span></span></span> 定位全为learning rate 的列向量，此时LSTM 的memory <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>c</mi></mrow><annotation encoding="application/x-tex">c</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em;"></span><span class="mord mathdefault">c</span></span></span></span> 的更新公式变得和Gradient Descent 一摸一样：</p>
<p><img src="/images/image-20201205192919663.png" alt="image-20201205192919663" / srcset="/images/LoadingImage.gif" data-src="/images/image-20201205192919663.png" class="lozad post-image"></p>
<p>所以你可以说Gradient Descent 就是LSTM 的简化版，LSTM中input gate 和forget gate是通过机器学出来的，而在梯度下降中input gate 和forget gate 都是人设的，input gate 永远都是学习率，forget gate 永远都是不可以忘记。😮</p>
<p>现在，我们考虑能不能让机器自己学习gradient descent 中的input gate 和forget gate 呢？</p>
<p>另外，input的部分刚才假设只有gradient 的值，实作上可以拿更多其他的数据作为input，比如常见的做法，可以把 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msup><mi>c</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msup></mrow><annotation encoding="application/x-tex">c^{t-1}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141079999999999em;vertical-align:0em;"></span><span class="mord"><span class="mord mathdefault">c</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141079999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">t</span><span class="mbin mtight">−</span><span class="mord mtight">1</span></span></span></span></span></span></span></span></span></span></span></span> 在现在这个step算出来的loss 作为输入来control 这个LSTM的input gate 和forget gate 的值。</p>
<p><img src="/images/image-20201205195417567.png" alt="image-20201205195417567" / srcset="/images/LoadingImage.gif" data-src="/images/image-20201205195417567.png" class="lozad post-image"></p>
<p>如果们可以让机器自动的学input gate 和forget gate 的值意味着什么，意味着我们可以拥有动态的learning rate，每一个step 中learning rate 都是不一样的而不是一个不变的值。而 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msup><mi>z</mi><mi>f</mi></msup></mrow><annotation encoding="application/x-tex">z^f</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8491079999999999em;vertical-align:0em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.04398em;">z</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8491079999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:0.10764em;">f</span></span></span></span></span></span></span></span></span></span></span> 就像一个正则项，它做的事情是把前一个step 算出来的参数缩小。我们以前做的L2 regularization 又叫做Weight Decade ，为什么叫Weight Decade，因为如果你把它微分的式子拿出来看，每个step 都会把原来的参数稍微变小，现在这个<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msup><mi>z</mi><mi>f</mi></msup></mrow><annotation encoding="application/x-tex">z^f</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8491079999999999em;vertical-align:0em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.04398em;">z</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8491079999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:0.10764em;">f</span></span></span></span></span></span></span></span></span></span></span> 就扮演了像是Weight Decade 的角色。但是我们现在不是直接告诉机器要做多少Weight Decade 而是要让机器学出来，它应该做多少Weight Decade 。</p>
<h2 id="lstm-for-gradient-descent"><a class="markdownIt-Anchor" href="#lstm-for-gradient-descent"></a> LSTM for Gradient Descent</h2>
<p>我们来看看一般的LSTM和for Gradient Descent 的LSTM：</p>
<p><img src="/images/image-20201205202752227.png" alt="image-20201205202752227" / srcset="/images/LoadingImage.gif" data-src="/images/image-20201205202752227.png" class="lozad post-image"></p>
<p>Typical LSTM 就是input x ，output c 和 h，每个step 会output 一个y ，希望y 和label 越接近越好。</p>
<p>Gradient Descent 的LSTM是这样：我们先sample 一个初始参数θ ，然后sample 一个batch 的data ，根据这一组data 算出一个gradient <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi mathvariant="normal">∇</mi><mi>θ</mi></msub><mi>l</mi></mrow><annotation encoding="application/x-tex">\nabla_\theta l</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.84444em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord">∇</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.33610799999999996em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:0.02778em;">θ</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mord mathdefault" style="margin-right:0.01968em;">l</span></span></span></span> ，把负的gradient input 到LSTM 中进行训练，这个LSTM 的参数过去是人设死的，我们现在让参数在Meta Learning 的架构下被硬learn 出来。上述的这个update 参数的公式就是：</p>
<p class='katex-block'><span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><msup><mi>θ</mi><mi>t</mi></msup><mo>=</mo><msup><mi>z</mi><mi>f</mi></msup><mo>⋅</mo><msup><mi>θ</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msup><mo>+</mo><msup><mi>z</mi><mi>i</mi></msup><mo>⋅</mo><mo>−</mo><msub><mi mathvariant="normal">∇</mi><mi>θ</mi></msub><mi>l</mi></mrow><annotation encoding="application/x-tex">\theta^t = z^f \cdot \theta^{t-1} + z^i \cdot -\nabla_\theta l
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.843556em;vertical-align:0em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.02778em;">θ</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.843556em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">t</span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.8991079999999999em;vertical-align:0em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.04398em;">z</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8991079999999999em;"><span style="top:-3.1130000000000004em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:0.10764em;">f</span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">⋅</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.947438em;vertical-align:-0.08333em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.02778em;">θ</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.864108em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">t</span><span class="mbin mtight">−</span><span class="mord mtight">1</span></span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.8746639999999999em;vertical-align:0em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.04398em;">z</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8746639999999999em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">i</span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">⋅</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.84444em;vertical-align:-0.15em;"></span><span class="mord">−</span><span class="mord"><span class="mord">∇</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.33610799999999996em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:0.02778em;">θ</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mord mathdefault" style="margin-right:0.01968em;">l</span></span></span></span></span></p>
<p><span class="katex"><span class="katex-mathml"><math><semantics><mrow><msup><mi>z</mi><mi>f</mi></msup></mrow><annotation encoding="application/x-tex">z^f</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8491079999999999em;vertical-align:0em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.04398em;">z</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8491079999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:0.10764em;">f</span></span></span></span></span></span></span></span></span></span></span> <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msup><mi>z</mi><mi>i</mi></msup></mrow><annotation encoding="application/x-tex">z^i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.824664em;vertical-align:0em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.04398em;">z</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.824664em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">i</span></span></span></span></span></span></span></span></span></span></span> 以前是人设死的，现在LSTM 可以自动把它学出来。</p>
<p>现在就可以output 新的参数<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msup><mi>θ</mi><mn>1</mn></msup></mrow><annotation encoding="application/x-tex">\theta^1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141079999999999em;vertical-align:0em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.02778em;">θ</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141079999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span></span></span></span></span></span></span></span> ，接着就是做一样的事情：再sample 一组数据，算出梯度作为新的input，放到LSTM 中就得到output <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msup><mi>θ</mi><mn>2</mn></msup></mrow><annotation encoding="application/x-tex">\theta^2</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141079999999999em;vertical-align:0em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.02778em;">θ</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141079999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span></span></span></span> ，以此类推，不断重复这个步骤。最后得到一组参数<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msup><mi>θ</mi><mn>3</mn></msup></mrow><annotation encoding="application/x-tex">θ^3</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141079999999999em;vertical-align:0em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.02778em;">θ</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141079999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">3</span></span></span></span></span></span></span></span></span></span></span>（这里假设只update 3次，实际上要update 更多次），拿这组参数去应用到Testing data 上算一下loss ： <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>l</mi><mo stretchy="false">(</mo><msup><mi>θ</mi><mn>3</mn></msup><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">l(θ^3)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.064108em;vertical-align:-0.25em;"></span><span class="mord mathdefault" style="margin-right:0.01968em;">l</span><span class="mopen">(</span><span class="mord"><span class="mord mathdefault" style="margin-right:0.02778em;">θ</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141079999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">3</span></span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span> ，这个loss 就是我们要minimize 的目标，然后你就要用gradient descent 调LSTM 的参数，去minimize 最后的loss 。</p>
<blockquote>
<p>到这里可能比较懵了，我在这里写一下我的理解不一定对，欢迎指正。看完下面的[Experimental Results](#Experimental Results) 一节可以回来再看一遍这个解释：</p>
<p>一般来说我们使用network 作为模型，其中会有很多参数θ，这些参数每一个都会拿到这个LSTM 中做如上述训练，一方面在train LSTM 中的参数，一方面在train 每一个参数θ。当network中所有θ都经过一轮上述的LSTM 的训练以后，得到的一组参数放回network 中，用testing data 计算loss of θ，据此用梯度下降回调LSTM 参数。如此往复，去minimize loss，最后就得到了一组比较好的参数，使得network 能在testing data 上取得比较好的成绩，这个过程中LSTM 担任了以前使用的梯度下降来update 参数的角色，而且LSTM 中的<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msup><mi>z</mi><mi>f</mi></msup></mrow><annotation encoding="application/x-tex">z^f</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8491079999999999em;vertical-align:0em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.04398em;">z</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8491079999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:0.10764em;">f</span></span></span></span></span></span></span></span></span></span></span> <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msup><mi>z</mi><mi>i</mi></msup></mrow><annotation encoding="application/x-tex">z^i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.824664em;vertical-align:0em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.04398em;">z</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.824664em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">i</span></span></span></span></span></span></span></span></span></span></span> 还是动态变化的，可能要比经典梯度下降效果好。</p>
</blockquote>
<p>这里有一些需要注意的地方。在一般的LSTM 中c 和x 是独立的，LSTM 的memory 存储的值不会影响到下一次的输入，但是Gradient Descent LSTM 中参数θ会影响到下一个step 中算出的gradient 的值，如上图虚线所示。所以说在Gradient Descent LSTM 中现在的参数会影响到未来看到的梯度。所以当你做back propagation 的时候，理论上你的error signal 除了走实线的一条路，它还可以走θ到<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mo>−</mo><msub><mi mathvariant="normal">∇</mi><mi>θ</mi></msub><mi>l</mi></mrow><annotation encoding="application/x-tex">-\nabla_\theta l</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.84444em;vertical-align:-0.15em;"></span><span class="mord">−</span><span class="mord"><span class="mord">∇</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.33610799999999996em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:0.02778em;">θ</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mord mathdefault" style="margin-right:0.01968em;">l</span></span></span></span> 虚线这一条路，可以通过gradient 这条路更新参数。但是这样做会很麻烦，和一般的LSTM 不太一样了，一般的LSTM c 和x 是没有关系的，现在这里确实有关系，为了让它和一般的LSTM 更像，为了少改一些code ，我们就假设没有虚线那条路，结束。现在的文献上其实也是这么做的。</p>
<p>另外，在LSTM input 的地方memory 中的初始值可以通过训练直接被learn 出来，所以在LSTM中也可以做到和MAML相同的事，可以把初始的参数跟着LSTM一起学出来。</p>
<h2 id="real-implementation"><a class="markdownIt-Anchor" href="#real-implementation"></a> Real Implementation</h2>
<p>LSTM 的memory 就是要训练的network 的参数，这些参数动辄就是十万百万级别的，难道要开十万百万个cell 吗？平常我们开上千个cell 就会train 很久，所以这样是train不起来的。在实际的实现上，我们做了一个非常大的简化：我们所learn 的LSTM 只有一个cell 而已，它只处理一个参数，所有的参数都公用一个LSTM。所以就算你有百万个参数，都是使用这同一个LSTM 来处理。</p>
<p><img src="/images/image-20201206114843695.png" alt="image-20201206114843695" / srcset="/images/LoadingImage.gif" data-src="/images/image-20201206114843695.png" class="lozad post-image"></p>
<p>也就是说如上图所示，现在你learn 好一个LSTM以后，它是直接被用在所有的参数上，虽然这个LSTM 一次只处理一个参数，但是同样的LSTM 被用在所有的参数上。<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msup><mi>θ</mi><mn>1</mn></msup></mrow><annotation encoding="application/x-tex">θ^1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141079999999999em;vertical-align:0em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.02778em;">θ</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141079999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span></span></span></span></span></span></span></span> 使用的LSTM 和<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msup><mi>θ</mi><mn>2</mn></msup></mrow><annotation encoding="application/x-tex">θ^2</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141079999999999em;vertical-align:0em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.02778em;">θ</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141079999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span></span></span></span> 使用的LSTM 是同一个处理方式也相同。那你可能会说，<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msup><mi>θ</mi><mn>1</mn></msup></mrow><annotation encoding="application/x-tex">θ^1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141079999999999em;vertical-align:0em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.02778em;">θ</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141079999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span></span></span></span></span></span></span></span> 和 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msup><mi>θ</mi><mn>2</mn></msup></mrow><annotation encoding="application/x-tex">θ^2</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141079999999999em;vertical-align:0em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.02778em;">θ</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141079999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span></span></span></span> 用的处理方式一样，会不会算出同样的值呢？会不，因为他们的初始参数是不同的，而且他们的gradient 也是不一样的。在初始参数和算出来的gradient 不同的情况下，就算你用的LSTM的参数是一样的，就是说你update 参数的规则是一样的， 最终算出来的也是不一样的 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msup><mi>θ</mi><mn>3</mn></msup></mrow><annotation encoding="application/x-tex">θ^3</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141079999999999em;vertical-align:0em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.02778em;">θ</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141079999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">3</span></span></span></span></span></span></span></span></span></span></span> 。</p>
<p>这就是实作上真正implement LSTM Gradient Descent 的方法。</p>
<p>这么做有什么好处：</p>
<ul>
<li>在模型规模上问题上比较容易实现</li>
<li>在经典的gradient descent 中，所有的参数也都是使用相同的规则，所以这里使用相同的LSTM ，就是使用相同的更新规则是合理的</li>
<li>训练和测试的模型架构可以是不一样的，而之前讲的MAML 需要保证训练任务和测试任务使用的model architecture 相同</li>
</ul>
<h2 id="experimental-results"><a class="markdownIt-Anchor" href="#experimental-results"></a> Experimental Results</h2>
<p><img src="/images/image-20201206121356125.png" alt="image-20201206121356125" / srcset="/images/LoadingImage.gif" data-src="/images/image-20201206121356125.png" class="lozad post-image"></p>
<blockquote>
<p><a target="_blank" rel="noopener" href="https://openreview.net/forum?id=rJY0-Kcll&amp;noteId=ryq49XyLg">https://openreview.net/forum?id=rJY0-Kcll&amp;noteId=ryq49XyLg</a></p>
</blockquote>
<p>我们来看一个文献上的实验结果，这是做在few-shot learning 的task上。横轴是update 的次数，每次train 会update 10次，左侧是forget gate <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msup><mi>z</mi><mi>f</mi></msup></mrow><annotation encoding="application/x-tex">z^f</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8491079999999999em;vertical-align:0em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.04398em;">z</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8491079999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:0.10764em;">f</span></span></span></span></span></span></span></span></span></span></span> 的变化，不同的红色线就是不同的task 中forget gate 的变化，可以看出<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msup><mi>z</mi><mi>f</mi></msup></mrow><annotation encoding="application/x-tex">z^f</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8491079999999999em;vertical-align:0em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.04398em;">z</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8491079999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:0.10764em;">f</span></span></span></span></span></span></span></span></span></span></span> 的值多数时候都保持在1附近，也就是说LSTM 有learn 到<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msup><mi>θ</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msup></mrow><annotation encoding="application/x-tex">θ^{t-1}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141079999999999em;vertical-align:0em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.02778em;">θ</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141079999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">t</span><span class="mbin mtight">−</span><span class="mord mtight">1</span></span></span></span></span></span></span></span></span></span></span></span> 是很重要的东西，没事就不要给他忘掉，只做一个小小的weight decade，这和我们做regularization 时候的思想相同，只做一个小小的weight decade 防止overfitting 。</p>
<p>右侧是input gate <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msup><mi>z</mi><mi>i</mi></msup></mrow><annotation encoding="application/x-tex">z^i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.824664em;vertical-align:0em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.04398em;">z</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.824664em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">i</span></span></span></span></span></span></span></span></span></span></span> 的变化，红线是不同的task，可以看出它的变化有点复杂，但是至少我们知道，它不是一成不变的固定值，它是有学到一些东西的，是动态变化的，放到经典梯度下降中来说就是learning rate 是动态变化的。</p>
<h2 id="lstm-for-gradient-descent-v2"><a class="markdownIt-Anchor" href="#lstm-for-gradient-descent-v2"></a> LSTM for Gradient Descent (v2)</h2>
<p>只有刚才的架构还不够，我们还可以更进一步。想想看，过去我们在用经典梯度下降更新参数的时候我们不仅会考虑当前step 的梯度，我们还会考虑过去的梯度，比如RMSProp、Momentum 等。</p>
<p><img src="/images/image-20201206123036529.png" alt="image-20201206123036529" / srcset="/images/LoadingImage.gif" data-src="/images/image-20201206123036529.png" class="lozad post-image"></p>
<p><img src="/images/image-20201206123040978.png" alt="image-20201206123040978" / srcset="/images/LoadingImage.gif" data-src="/images/image-20201206123040978.png" class="lozad post-image"></p>
<p>在刚才的架构中，我们没有让机器去记住过去的gradient ，所以我们可以做更进一步的延伸。我们在过去的架构上再加一层LSTM，如下图所示：</p>
<p><img src="/images/image-20201206123232954.png" alt="image-20201206123232954" / srcset="/images/LoadingImage.gif" data-src="/images/image-20201206123232954.png" class="lozad post-image"></p>
<p>蓝色的一层LSTM 是原先的算learning rate、做weight decade 的LSTM，我们再加入一层LSTM ，让算出来的gradient <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mo>−</mo><msub><mi mathvariant="normal">∇</mi><mi>θ</mi></msub><mi>l</mi></mrow><annotation encoding="application/x-tex">-\nabla_\theta l</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.84444em;vertical-align:-0.15em;"></span><span class="mord">−</span><span class="mord"><span class="mord">∇</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.33610799999999996em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:0.02778em;">θ</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mord mathdefault" style="margin-right:0.01968em;">l</span></span></span></span> 先通过这个LSTM ，把这个LSTM 吐出来的东西input 到原先的LSTM 中，我们希望绿色的这一层能做到记住以前算过的gradient 这件事。这样，可能就可以做到Momentum 可以做的的事情。</p>
<p>上述的这个方法，是老师自己想象的，在learning to learn by gradient descent by gradient descent 这篇paper 中上图中蓝色的LSTM 使用的是一般的梯度下降算法，而在另一篇paper 中只有上面没有下面，而老师觉得这样结合起来才是实现，能考虑过去的gradient 的gradient descent 算法的完全体。</p>
<h2 id="experimental-result-2"><a class="markdownIt-Anchor" href="#experimental-result-2"></a> Experimental Result 2</h2>
<p>learning to learn by gradient descent by gradient descent 这篇paper 的实验结果。</p>
<p><img src="/images/image-20201206123958457.png" alt="image-20201206123958457" / srcset="/images/LoadingImage.gif" data-src="/images/image-20201206123958457.png" class="lozad post-image"></p>
<blockquote>
<p><a target="_blank" rel="noopener" href="https://arxiv.org/abs/1606.04474">https://arxiv.org/abs/1606.04474</a></p>
</blockquote>
<p>第一个实验图，是做在toy example 上，它可以制造一大堆训练任务，然后测试在测试任务上，然后发现，LSTM 来当作gradient descent 的方法要好过人设计的梯度下降方法。</p>
<p>第二张图把这个技术应用带MNIST 上，这个实验是训练任务测试任务都是MNIST。</p>
<p>第三张图是说虽然训练和测试任务都是相同的dataset也是相同的，但是train 和test 的时候network 的架构是不一样的。 在train 的时候network 是只有一层，该层只有20个neuron。这张图是training 的结果。</p>
<p>第四张图是上述改变network 架构后在testing 的结果，testing 的时候network 只有一层该层40个neuron。从图上看还是做的起来，而且比一般的gradient descent 方法要好很多。</p>
<p>第五张图是上述改变network 架构后在testing 的结果，testing 的时候network 有两层。从图上看还是做的起来，而且比一般的gradient descent 方法要好很多。</p>
<p>第六张图是上述改变network 激活函数后在testing 的结果，training 的时候激活函数是sigmoid 而testing 的时候改成ReLU。从图上看做不起来，崩掉了，training 和testing 的network 的激活函数不一样的时候，LSTM 没办法跨model 应用。</p>

  </div>
  <div>
    
      <div 
        class="post-note note-info copyright" 
        style="margin-top: 42px">
        <p>
          <span style="font-weight: bold;">作者：</span><a 
            target="_blank" 
            rel="nofollow noopener noreferrer" 
            href="/about">
            Ch3nYe
          </a>
        </p>
        <p>
          <span style="font-weight: bold;">文章链接：</span><a 
            target="_blank" 
            rel="nofollow noopener noreferrer" 
            href="https://ch3nye.top/Meta-Learning-Gradient-Descent-as-LSTM/">
            https://ch3nye.top/Meta-Learning-Gradient-Descent-as-LSTM/
          </a>
        </p>
        <p><span style="font-weight: bold;">版权声明：</span>本博客所有文章除特别声明外，均采用<a target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh">CC BY-NC-SA 4.0 协议</a>。转载请注明出处！</p>
      </div>
    
  </div>
</article>
<div class="nav">
  
    <div class="nav-item-prev">
      <a 
        href="/Meta-Learning-Metric-based/" 
        class="nav-link">
        <i class="iconfont icon-left nav-prev-icon"></i>
        <div>
          <div class="nav-label">上一篇</div>
          
            <div class="nav-title">Meta Learning-Metric-based </div>
          
        </div>
      </a>
    </div>
  
  
    <div class="nav-item-next">
      <a 
        href="/Meta-Learning-MAML/" 
        class="nav-link">
        <div>
          <div class="nav-label">下一篇</div>
          
            <div class="nav-title">Meta Learning-MAML </div>
          
        </div>
        <i class="iconfont icon-right nav-next-icon"></i>
      </a>
    </div>
  
</div>

  <div 
    class="card card-content comment-card" 
    style="margin-top: 16px;">
    <div class="comment-card-title">评论</div>
    
  <div id="gitalk-container"></div>
  
<link rel="stylesheet" href="//cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.css">

  
<script src="//cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.js"></script>

  
<script src="/js/lib/md5.min.js"></script>

  <script>
    var gitalk = new Gitalk({
      clientID: '049b30eb10ea05082ef1',
      clientSecret: 'c33ad95041c69907b3136b895008320f000987db',
      repo: 'Ch3nYe.github.io',
      owner: 'Ch3nYe',
      admin: "Ch3nYe",
      id: md5(location.href),
      distractionFreeMode: false,
      language: 'navigator.language || navigator.userLanguage',
      labels: ["Gitalk"],
      perPage: 10
    })

    gitalk.render('gitalk-container')
  </script>

  </div>

<div 
  class="card card-content toc-card" 
  id="mobiletoc">
  <div class="toc-header">
  <i 
    class="iconfont icon-menu" 
    style="padding-right: 2px;">
  </i>目录
</div>
<ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#meta-learning-gradient-descent-as-lstm"><span class="toc-text"> Meta Learning - Gradient Descent as LSTM</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#review-rnn"><span class="toc-text"> Review: RNN</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#review-lstm"><span class="toc-text"> Review: LSTM</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#lstm-similar-to-gradient-descent-based-algorithm"><span class="toc-text"> LSTM similar to gradient descent based algorithm</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#lstm-for-gradient-descent"><span class="toc-text"> LSTM for Gradient Descent</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#real-implementation"><span class="toc-text"> Real Implementation</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#experimental-results"><span class="toc-text"> Experimental Results</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#lstm-for-gradient-descent-v2"><span class="toc-text"> LSTM for Gradient Descent (v2)</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#experimental-result-2"><span class="toc-text"> Experimental Result 2</span></a></li></ol></li></ol>
</div></main>
            <aside class="left-column">
              
              <div class="card card-author">
                
  <img 
    src="/images/logo.png" 
    class="author-img"
    width="88"
    height="88"
    alt="author avatar">

<p class="author-name">Ch3nYe</p>
<p class="author-description">如果有文章有任何错误请留言，谢谢🙏</p>
<div class="author-message">
  <a 
    class="author-posts-count" 
    href="/archives">
    <span>46</span>
    <span>文章</span>
  </a>
  <a 
    class="author-categories-count" 
    href="/categories">
    <span>7</span>
    <span>分类</span>
  </a>
  <a 
    class="author-tags-count" 
    href="/tags">
    <span>70</span>
    <span>标签</span>
  </a>
</div>

  <div class="author-card-society">
    
      <div class="author-card-society-icon">
        <a target="_blank" rel="noopener" href="https://github.com/ch3nye/">
          <i class="iconfont icon-github society-icon"></i>
        </a>
      </div>
    
      <div class="author-card-society-icon">
        <a href="mailto:sud0su@163.com">
          <i class="iconfont icon-mail society-icon"></i>
        </a>
      </div>
    
  </div>

              </div>
               <div class="sticky-tablet">
  
  
    <article class="display-when-two-columns spacer">
      <div class="card card-content toc-card">
        <div class="toc-header">
  <i 
    class="iconfont icon-menu" 
    style="padding-right: 2px;">
  </i>目录
</div>
<ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#meta-learning-gradient-descent-as-lstm"><span class="toc-text"> Meta Learning - Gradient Descent as LSTM</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#review-rnn"><span class="toc-text"> Review: RNN</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#review-lstm"><span class="toc-text"> Review: LSTM</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#lstm-similar-to-gradient-descent-based-algorithm"><span class="toc-text"> LSTM similar to gradient descent based algorithm</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#lstm-for-gradient-descent"><span class="toc-text"> LSTM for Gradient Descent</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#real-implementation"><span class="toc-text"> Real Implementation</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#experimental-results"><span class="toc-text"> Experimental Results</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#lstm-for-gradient-descent-v2"><span class="toc-text"> LSTM for Gradient Descent (v2)</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#experimental-result-2"><span class="toc-text"> Experimental Result 2</span></a></li></ol></li></ol>
      </div>
    </article>
  
  
  <article class="card card-content categories-widget">
    <div class="categories-card">
  <div class="categories-header">
    <i 
      class="iconfont icon-fenlei" 
      style="padding-right: 2px;">
    </i>分类
  </div>
  <div class="categories-list">
    
      <a href="/categories/%E5%A4%87%E5%BF%98/">
        <div class="categories-list-item">
          备忘
          <span class="categories-list-item-badge">2</span>
        </div>
      </a>
    
      <a href="/categories/%E7%94%9F%E6%B4%BB/">
        <div class="categories-list-item">
          生活
          <span class="categories-list-item-badge">2</span>
        </div>
      </a>
    
      <a href="/categories/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/">
        <div class="categories-list-item">
          李宏毅机器学习笔记
          <span class="categories-list-item-badge">15</span>
        </div>
      </a>
    
      <a href="/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/">
        <div class="categories-list-item">
          论文阅读
          <span class="categories-list-item-badge">14</span>
        </div>
      </a>
    
      <a href="/categories/%E5%AE%9E%E6%88%98/">
        <div class="categories-list-item">
          实战
          <span class="categories-list-item-badge">3</span>
        </div>
      </a>
    
      <a href="/categories/%E7%AC%94%E8%AE%B0/">
        <div class="categories-list-item">
          笔记
          <span class="categories-list-item-badge">7</span>
        </div>
      </a>
    
      <a href="/categories/%E7%BF%BB%E8%AF%91/">
        <div class="categories-list-item">
          翻译
          <span class="categories-list-item-badge">3</span>
        </div>
      </a>
    
  </div>
</div>
  </article>
  
  <article class="card card-content tags-widget">
    <div class="tags-card">
  <div class="tags-header">
    <i 
      class="iconfont icon-biaoqian" 
      style="padding-right: 2px;">
    </i>热门标签
  </div>
  <div class="tags-list">
    
      <a 
        href="/tags/%E7%AC%94%E8%AE%B0/" 
        title="笔记">
        <div class="tags-list-item">笔记</div>
      </a>
    
      <a 
        href="/tags/%E8%AE%BA%E6%96%87/" 
        title="论文">
        <div class="tags-list-item">论文</div>
      </a>
    
      <a 
        href="/tags/note/" 
        title="note">
        <div class="tags-list-item">note</div>
      </a>
    
      <a 
        href="/tags/ML/" 
        title="ML">
        <div class="tags-list-item">ML</div>
      </a>
    
      <a 
        href="/tags/Next-Step-of-ML/" 
        title="Next-Step-of-ML">
        <div class="tags-list-item">Next-Step-of-ML</div>
      </a>
    
      <a 
        href="/tags/Android/" 
        title="Android">
        <div class="tags-list-item">Android</div>
      </a>
    
      <a 
        href="/tags/Binary/" 
        title="Binary">
        <div class="tags-list-item">Binary</div>
      </a>
    
      <a 
        href="/tags/%E6%80%BB%E7%BB%93/" 
        title="总结">
        <div class="tags-list-item">总结</div>
      </a>
    
      <a 
        href="/tags/Rust/" 
        title="Rust">
        <div class="tags-list-item">Rust</div>
      </a>
    
      <a 
        href="/tags/Deep-Learning/" 
        title="Deep-Learning">
        <div class="tags-list-item">Deep-Learning</div>
      </a>
    
      <a 
        href="/tags/%E7%BF%BB%E8%AF%91/" 
        title="翻译">
        <div class="tags-list-item">翻译</div>
      </a>
    
      <a 
        href="/tags/%E5%AD%A6%E4%B9%A0/" 
        title="学习">
        <div class="tags-list-item">学习</div>
      </a>
    
      <a 
        href="/tags/%E5%AE%9E%E6%88%98/" 
        title="实战">
        <div class="tags-list-item">实战</div>
      </a>
    
      <a 
        href="/tags/crack/" 
        title="crack">
        <div class="tags-list-item">crack</div>
      </a>
    
      <a 
        href="/tags/%E4%BA%8C%E8%BF%9B%E5%88%B6/" 
        title="二进制">
        <div class="tags-list-item">二进制</div>
      </a>
    
      <a 
        href="/tags/HTTPS/" 
        title="HTTPS">
        <div class="tags-list-item">HTTPS</div>
      </a>
    
  </div>
</div>
  </article>
  
  
</div>
            </aside>
            <aside class="right-column">
              <div class="sticky-widescreen">
  
  
    <article class="card card-content toc-card">
      <div class="toc-header">
  <i 
    class="iconfont icon-menu" 
    style="padding-right: 2px;">
  </i>目录
</div>
<ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#meta-learning-gradient-descent-as-lstm"><span class="toc-text"> Meta Learning - Gradient Descent as LSTM</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#review-rnn"><span class="toc-text"> Review: RNN</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#review-lstm"><span class="toc-text"> Review: LSTM</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#lstm-similar-to-gradient-descent-based-algorithm"><span class="toc-text"> LSTM similar to gradient descent based algorithm</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#lstm-for-gradient-descent"><span class="toc-text"> LSTM for Gradient Descent</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#real-implementation"><span class="toc-text"> Real Implementation</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#experimental-results"><span class="toc-text"> Experimental Results</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#lstm-for-gradient-descent-v2"><span class="toc-text"> LSTM for Gradient Descent (v2)</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#experimental-result-2"><span class="toc-text"> Experimental Result 2</span></a></li></ol></li></ol>
    </article>
  
  
  <article class="card card-content">
    <div class="recent-posts-card">
  <div class="recent-posts-header">
    <i 
      class="iconfont icon-wenzhang_huaban" 
      style="padding-right: 2px;">
    </i>最近文章
  </div>
  <div class="recent-posts-list">
    
      <div class="recent-posts-item">
        <div class="recent-posts-item-title">2022-10-25</div>
        <a href="/%E5%9F%BA%E4%BA%8E%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E5%87%BD%E6%95%B0%E5%90%8D%E6%81%A2%E5%A4%8D%E6%80%BB%E7%BB%93%EF%BC%88%E4%B8%8A%EF%BC%89/"><div class="recent-posts-item-content">基于机器学习的函数名恢复总结（上）</div></a>
      </div>
    
      <div class="recent-posts-item">
        <div class="recent-posts-item-title">2022-10-05</div>
        <a href="/Note-SymLM-Predicting-Function-Names-in-Stripped-Binaries/"><div class="recent-posts-item-content">Note 《SymLM Predicting Function Names in Stripped Binaries via Context-Sensitive Execution-Aware Code Embeddings》</div></a>
      </div>
    
      <div class="recent-posts-item">
        <div class="recent-posts-item-title">2022-09-13</div>
        <a href="/Note-SafeDrop-Detecting-Memory-Deallocation-Bugs-of-Rust/"><div class="recent-posts-item-content">Note 《SafeDrop Detecting Memory Deallocation Bugs of Rust Programs via Static Data-Flow Analysis》</div></a>
      </div>
    
      <div class="recent-posts-item">
        <div class="recent-posts-item-title">2022-06-01</div>
        <a href="/Note-SoK-Demystifying-Binary-Lifters-Through-the-Lens-of-Downstream-Applications/"><div class="recent-posts-item-content">Note 《SoK Demystifying Binary Lifters Through the Lens of Downstream Applications》</div></a>
      </div>
    
  </div>
</div>
  </article>
  
  
</div>
            </aside>
          </div>
        </div>
      </div>
    </div>
     
    <footer class="footer">
  <div class="footer-container">
    <div>
      <div class="footer-dsc">
        <span>
          Copyright ©
          
            2020 -
          
          2022
        </span>
        &nbsp;
        <a 
          href="/" 
          class="footer-link">
          ch3nye's blog
        </a>
      </div>
    </div>

    
      <div class="footer-dsc">
        
          Powered by
          <a 
            href="https://hexo.io/" 
            class="footer-link" 
            target="_blank" 
            rel="nofollow noopener noreferrer">
            &nbsp;Hexo
          </a>
        
        
          <span>&nbsp;|&nbsp;</span>
        
        
          Theme -
          <a 
            href="https://github.com/theme-kaze" 
            class="footer-link" 
            target="_blank"
            rel="nofollow noopener noreferrer">
            &nbsp;Kaze
          </a>
        
      </div>
    
    
    
    
      <div class="footer-dsc">
        
          本站总访问量<span id="busuanzi_value_site_pv"></span>次
        
        
          <span>&nbsp;|&nbsp;</span>
        
        
          本站总访客数<span id="busuanzi_value_site_uv"></span>次
        
      </div>
      
    
<!-- Default Statcounter code for My Blog https://ch3nye.top -->
<script type="text/javascript">
var sc_project=12696483; 
var sc_invisible=1; 
var sc_security="0f70f90d"; 
</script>
<script type="text/javascript"
src="https://www.statcounter.com/counter/counter.js" async></script>
<noscript><div class="statcounter"><a title="Web Analytics"
href="https://statcounter.com/" target="_blank"><img class="statcounter"
src="https://c.statcounter.com/12696483/0/0f70f90d/1/" alt="Web Analytics"
referrerPolicy="no-referrer-when-downgrade"></a></div></noscript>
<!-- End of Statcounter Code -->
</footer> 
    
  <a 
    role="button" 
    id="scrollbutton" 
    class="basebutton" 
    aria-label="回到顶部">
    <i class="iconfont icon-arrowleft button-icon"></i>
  </a>

<a 
  role="button" 
  id="menubutton"
  aria-label="menu button"
  class="basebutton">
  <i class="iconfont icon-menu button-icon"></i>
</a>
<a 
  role="button" 
  id="popbutton" 
  class="basebutton" 
  aria-label="控制中心">
  <i class="iconfont icon-expand button-icon"></i>
</a>
<a 
  role="button" 
  id="darkbutton" 
  class="basebutton darkwidget" 
  aria-label="夜色模式">
  <i class="iconfont icon-weather button-icon"></i>
</a>
<a 
  role="button" 
  id="searchbutton" 
  class="basebutton searchwidget" 
  aria-label="搜索">
  <i class="iconfont icon-search button-icon"></i>
</a> 
     
     
      

  
  
    
<link rel="stylesheet" href="//cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.css">

  
 
     
     
      <script>
  var addImgLayout = function () {
    var img = document.querySelectorAll('.post-content img')
    var i
    for (i = 0; i < img.length; i++) {
      var wrapper = document.createElement('a')
      wrapper.setAttribute('href', img[i].getAttribute('data-src'))
      wrapper.setAttribute('aria-label', 'illustration')
      wrapper.style.cssText =
        'width: 100%; display: flex; justify-content: center;'
      if (img[i].alt) wrapper.dataset.caption = img[i].alt
      wrapper.dataset.nolink = true
      img[i].before(wrapper)
      wrapper.append(img[i])
      var divWrap = document.createElement('div')
      divWrap.classList.add('gallery')
      wrapper.before(divWrap)
      divWrap.append(wrapper)
    }
    baguetteBox.run('.gallery')
  }
</script>
<script>
  loadScript(
    "/js/lib/lightbox/baguetteBox.min.js",
    addImgLayout
  )
</script>
 
     
     
    <script src="/js/main.js"></script> 
    
      <script> 
        loadScript('/js/lib/busuanzi.min.js') 
      </script>
     
    
      <script>
        var addLazyload = function () {
          var observer = lozad('.lozad', {
            load: function (el) {
              el.srcset = el.getAttribute('data-src')
            },
            loaded: function (el) {
              el.classList.add('loaded')
            },
          })
          observer.observe()
        }
      </script>
      <script>
        loadScript('/js/lib/lozad.min.js', addLazyload)
      </script>
     
    
      <script>
        var googleAnalytics = function () {
          window.dataLayer = window.dataLayer || []
          function gtag() {
            dataLayer.push(arguments)
          }
          gtag('js', new Date())
          gtag('config', 'G-WHGL11014T')
        }
    </script>
      <script>
        loadScript(
          'https://www.googletagmanager.com/gtag/js?id=' +
            'G-WHGL11014T',
          googleAnalytics
        )
      </script>
    
    
      <script>
        setTimeout(() => {localSearch("search.json")}, 0)
      </script>
    
  </body>
</html>
