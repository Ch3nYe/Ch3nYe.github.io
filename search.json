[{"title":"Note 《SymLM Predicting Function Names in Stripped Binaries via Context-Sensitive Execution-Aware Code Embeddings》","url":"/Note-SymLM-Predicting-Function-Names-in-Stripped-Binaries/","content":" SymLM: Predicting Function Names in Stripped Binaries via Context-Sensitive Execution-Aware Code Embeddings\n\nPublisher: CCS 2022\n\n 0 Abstract\n现有方法尚未成功建模以穷举所有函数的执行行为，并且对于未见过的二进制的泛化能力很差。为了进一步推动函数名恢复任务的研究，我们提出了一个用于函数符号名预测和二进制语言建模的框架（SymLM Symbol name prediction and binary Language Modeling framework），该框架使用新型神经网络架构——一个融合encoder，该网络可以通过联合建模“the execution behavior of the calling context and instructions”（学习函数指令的语义和它在过程间CFG上的callers和callees的语义），来学习二进制函数的综合语义。我们从27个开源工程中收集了1,431,169个二进制函数来评估我们的框架，这些工程通过交叉编译得到（O0-O3，x86、x64、ARM、MIPS，4 obfuscations）。SymLM 在 precision、recall 和 F1 方面优于之前最先进的函数名预测工具 15.4%、59.6% 和 35.0%，具有更好的泛化性和抗混淆性。消融实验证明我们的设计（例如calling context 组件和execution behavior 组件）大幅提升了函数名预测模型的表现。最终我们的样例研究进一步展示了 SymLM 在分析固件映像中的实际表现。\n\ncalling context（调用上下文）：指一个函数调用的函数，和调用它的函数。\n\n 1 INTRODUCTION\nOur Approch.\n为了解决函数名中自然语言的一些问题，比如同义词问题，OOV问题，我们构建了CodeWordNet来捕捉特定领域的函数名token的表征。\n我们广泛的实验表明 SymLM 是准确的：在各种架构和优化选项中实现了 0.634 precision、0.677 recall 和 0.655 F1。在 F1 上SymLM超过SOTA的工具35%。通过学习上下文敏感的函数语义，我们证明了SymLM比现有工作具有更好的泛化性和健壮性。在针对未见过的二进制函数的评估中，其F1比SOTA工具高出295.5%。另外，消融研究进一步证明了 SymLM 设计：融合calling context 的有效性。例如，语义融合encoder可以将 SymLM 的 F1 分数提高多达 36.2%。我们还将 SymLM 应用于现实世界的物联网固件，它成功地预测了固件映像中的函数名称\nContributions.\n\n我们设计了一个新型神经网络架构，SymLM，用于学习保存在调用上下文和函数指令的执行行为中的综合函数语义。\n我们构建了CodeWordNet模块，以使用指定领域的分布式表征来测量函数名的语义距离，并通过预处理解决 OOV 问题。\n我们通过超越现有工作推进了函数名称预测的最新技术，并展示了 SymLM 的通用性、抗混淆性、组件有效性和实际用例。我们的代码和数据集发布在 https://github.com/OSSecLab/SymLM\n\n 2 BACKGROUND AND MOTIVATION\n 2.1 Problem Definition\n。。。\n 2.2 Challenges\nC1: Limited Semantic Information in Stripped Binaries.\n\nC2: Variety of Compilation Settings and Obfuscations.\n\nC3: Ambiguous Function Names and Various Naming Methods\n函数名中的单词常出现原始词干的变形，比如时态，缩写，甚至拼写错误，而且同样功能的函数常被赋予不同的名字。开发人员使用的分隔符是多种多样的，比如大写字母、下划线甚至数字。\n因此，函数名的模糊性和命名方法的多样性使得函数名的预测变得困难。\n\nC4: The Out-Of-Vocabulary (OOV) Issue.\n在整个函数名级别，我们对二进制数据集的调查显示，21.7% 的函数名只出现过一次。尽管我们可以通过将名称拆分为各个单词来缓解这个问题，但 OOV 比率仍然相对较高，例如 ARM 二进制文件为 5.8%（test 中的label 出现train 中没有的单词）。\n\nC5: Calling Context Modeling.\n现有的作品都没有考虑调用上下文的执行行为，包括已经初步尝试对函数的执行行为建模的最新工作（Sec 2.4 TREX）。另外，虽然 NERO 构建了函数调用图并将函数调用点的信息作为其模型的输入，但它并没有充分考虑被调用函数的行为。\n 2.3 Prior Efforts and Our Motivations\n。。。\n 2.4 Microtrace-Based Pretraining\n本文课题组先前的工作，TREX 提出了一种通过预训练micro-execution trace value 的方式建模执指令的执行语义。TREX 在二进制相似度搜索领域取得了不错的成果。\n\n上图Fig2 展示了TREX 的预训练模型，模型输入指令，和对应的由微执行产生的值。当盖住操作符，模型要预测执行逻辑 eax?3=4 。预训练后，TREX 生成每条指令的语义嵌入（向量表示），可进一步用于检测具有相似执行行为的二进制函数。StateFormer 作为其后续研究也通过学习函数执行语义来推断变量类型。而这两者都不是为函数名预测设计的，所以他们不能解决上述C3 C4两个挑战。\nMissing Calling Context. 不幸的是，TREX 也没有考虑函数调用的副作用，所以它只能学习部分函数语义， StateFormer 也存在同样的问题。这限制了它们在面临挑战 C5 时对二进制函数语义的理解。为了弥补这个缺陷，我们提出了 SymLM，基于 microtrace 的预训练模型，来学习完整函数语义用于预测函数名。\n 3 OVERVIEW\n\n上图展示了SymLM的工作流，主要是3个关键步骤：\n1）Function Semantics Encoding：理解二进制函数综合语义并编码到向量表征形式；\n2）Function Name Preprocessing and CodeWordNet Training：建模函数名语义，缓解自然语言的模糊性，减少OOV的token；\n3）Training and Inference：训练模型将函数语义向量映射到函数名。\n\n下面将分别阐述一下这三个步骤中的细节，辅助理解fig3 SymLM的工作流。\n Function Semantics Encoding\n首先，构建过程间控制流图ICFGs，如Fig3 step1，其中节点3（函数）可能调用内部节点4（内部函数）或者外部节点5（外部函数）。\n接下来，SymLM 采取下面两个步骤构造函数内部指令的嵌入 EIE_\\mathcal{I}EI​：\n\n使用microtrace-based 预训练模型（fig2）生成函数tokens 的embedding，这个预训练模型在fig3中就是Transformer\n下采样token embeddings 生成结构化表征（EIE_\\mathcal{I}EI​）。例如，对于节点3，使用 E3inE^{in}_3E3in​ 指代这个函数的指令嵌入。\n\n为了产生函数的上下文嵌入 ECE_\\mathcal{C}EC​ ，需要同时嵌入节点callers和callees的执行行为信息。对于其他的内部函数可以用上述方法分别生成其 EIE_\\mathcal{I}EI​ 向量，而对于外部函数比如节点5，SymLM 通过查找embedding table 来获取其嵌入 EIE_\\mathcal{I}EI​。\n最后，SymLM 融合一个节点的callers和callees的嵌入向量来生成语义嵌入 ECE_\\mathcal{C}EC​ ，并通过结合 EIE_\\mathcal{I}EI​ 和 ECE_\\mathcal{C}EC​ 得到最终的函数表征向量。\n\n关于如何下采样计算内部函数的 EIE_\\mathcal{I}EI​. 和TREX 中直接使用token embeddings的均值不同，SymLM 使用一个特别的pooling 策略来下采样获取更好的模型表现。具体来说，我们观察到，在自然语言处理任务中，直接使用 [CLS] 的嵌入或者所有token嵌入的平均值作为句子嵌入，这两者已被证明是无效的，它们会分别导致 17.5% 和 10.3% 性能下降。根本原因是基于 BERT 的预训练模型生成各向异性token 嵌入。我们将多种下采样嵌入向量拼接起来，包括[CLS] embedding，所有token embedding的均值，以及token-position-insensitive embedding ，三者拼接作为函数语义嵌入 EIE_\\mathcal{I}EI​ 。其中token-position-insensitive embedding：从上图fig3 step1 HHH 中取前 k （实验中k=5）行最大的向量作为HkH^kHk。关于如何比较每一行的大小，文章并没提到（盲猜直接求和一行）。\n\n关于外部函数的嵌入. 基于这样一个观察：“外部函数的语义通常是固定的，且很大程度上体现在其函数名上”，本文使用一个随机初始化的embedding layer 来根据外部调用函数的函数名获取其 EIE_\\mathcal{I}EI​ 。相当于一个词汇表的嵌入，而这个词汇表是通过训练集中的外部调用函数名构建的。在模型训练期间更新这个嵌入层以获得外部函数名的最佳嵌入。当外部函数名不可用时，我们放弃融合对应的函数调用。\n\n关于如何融合多个 EIE_\\mathcal{I}EI​ 获得 ECE_\\mathcal{C}EC​ . 我们提出了一个具有选择性的调用上下文融合策略：1）SymLM 根据caller和callee的call和called by 目标函数的频率排序，2）拼接top-n个caller和top-n个callee的向量：\nEC={EI1+,…,EIn+,EI1−,…,EIn−}E_C=\\left\\{E_{I_1^{+}}, \\ldots, E_{I_n^{+}}, E_{I_1^{-}}, \\ldots, E_{I_n^{-}}\\right\\}EC​={EI1+​​,…,EIn+​​,EI1−​​,…,EIn−​​}\n其中，EIi+E_{I_i^{+}}EIi+​​ 和 EIj−E_{I_j^{-}}EIj−​​ 分别是 𝑖-th caller and 𝑗-th callee 的嵌入向量。\n\n关于如何融合目标函数的 EIE_\\mathcal{I}EI​ 和 ECE_\\mathcal{C}EC​ 得到最终的模型输入. 拼接两个向量。\n Function Name Preprocessing and CodeWordNet Training\n总结了7种OOV问题，如table 2，设计了多种方法来解决这些问题。\n\n首先，在tokenization 过程中，我们对带数字的单词和单词的变形两种情况进行词形还原，以解决表 2 中的 6 和 7。其次，我们基于 unigram 语言模型构建了一个分词模型，对常见连词进行分隔，解决1 和 2。其余问题归结为 3、4 和 5。OOV 类别 3、4 和 5 中的词的关键原因是使用单词的变体（例如，同义词、缩写和拼写错误），我们设计了 CodeWordNet 模块，它可以在函数名称的上下文中生成单词的语义向量表示。借助该模块，SymLM 可以测量词的语义距离，建模函数名称语义，并解决类别 3、4 和 5 中的 OOV 问题。\n Training and Inference\n为了学习从函数语义向量到函数名的映射，我们使用带有调试信息的二进制文件提取数据集训练 SymLM，其中函数名标签从符号表中解析得到。对于训练，SymLM 首先将完整的函数语义编码为嵌入，然后预处理函数名标签，通过多层感知 (MLP) 解码器将函数语义嵌入解码为要预测的名称，最后通过最小化预测损失进行训练。在推理阶段，SymLM 将剥离的二进制文件作为输入，并根据函数的语义嵌入预测函数名tokens，其中生成函数名时 SymLM 也利用到了 CodeWordNet 生成多个同义词。\n 4 DETAILED DESIGN\n。。。\n 5 EVALUATION\n使用Ghidra 作为反编译工具，基于TREX 实现了microtrace-based pretrained model，增加了1864 行新代码。我们使用NLTK 和SentencePiece 构建了我们的函数名处理组件，使用Gensim 构建了CodeWordNet 。开发了一个Ghidra插件和脚本来反编译二进制和解析调试信息。其他部分使用Pytorch 和fairseq 实现。\n评估指标（同NFRE和NERO）：\n。。。\n RQ1: How effective is SymLM in function name prediction?\n\nSymLM 在预测二进制函数名任务上非常有效，它在不同的架构和优化中实现了 0.634 的精度、0.677 的召回率和 0.655 的 F1 分数。\n RQ2: How does SymLM compare to the state of the art?\n\n\nSymLM 比之前最好的工作更有效。例如，它在准确率、召回率和 F1 分数上比 NERO 高出 15.4%、59.6% 和 35.0%。\n RQ3: How can SymLM generalize to unknown binaries and resist obfuscations?\n\n泛化性能更好，但是还是不够好\n\n健壮性，抗混淆能力不错。\n RQ4: How can SymLM’s components improve its performance?\n\n\n\nfig8 (a) 是预训练模型对预测模型的提升，也就是基于TREX 的指令token嵌入模型。\n在前20 epoch的提升比较明显，而在超过20 epoch后基本可以忽略不计。\n\n\nfig8 (b) 是调用上下文对预测模型效果的提升，即是否包含目标函数的上下文函数的嵌入 EIE_\\mathcal{I}EI​ 。\n调用上下文的加入对模型能力上限有提升，7.9%。\n\n\n\nfig9 是fig3 step1中对函数tokens 的embedding 矩阵 HHH 池化策略的不同，对模型的影响。\n本文的池化策略对模型的提升还是比较显著的，加入 token-position-insensitive embedding 后提升明显。\n\nfig10 是对函数名token预处理后OOV问题的减轻程度，以及预测模型性能的提升。\n\n不同的预测模型（解码器），MLP略优于LSTM。\n 6 CASE STUDY\n\n。。。\n 7 DISCUSSION\n本节讨论我们工作的局限性和未来的方向。我们考虑以下可能使我们的评估结果产生偏差的案例，解决它们都可能是有趣的未来工作。\nDataset Size. 当前数据集规模不够大，更大的数据集应该能使模型具有更好的泛化性。\nObfuscation. 本文中仅考虑编译器混淆，而没有考虑其他形式的混淆，例如加密和加壳。我们将其看作和本文正交的研究问题，任何对这些问题的研究进展都能促进我们的方法提升。\nOperating System. 我们仅关注Linux上的二进制，其他OS或许有不同的调用上下文语义，这可能会使SymML 失效。\nNoise in Ground Truth. 虽然我们试图解决模棱两可的函数名称问题，但由于 SymLM 中使用的词嵌入算法的限制，该问题并未完全解决。此外，本文只考虑token级别的语义相似性，而未考虑具有多个token的相似短语。我们将在未来的研究中解决这些问题。\n","categories":["论文阅读"],"tags":["论文","笔记","Binary","Transformer","MLP","Word2Vec","Reassignment","ICFG"]},{"title":"Note 《SafeDrop Detecting Memory Deallocation Bugs of Rust Programs via Static Data-Flow Analysis》","url":"/Note-SafeDrop-Detecting-Memory-Deallocation-Bugs-of-Rust/","content":" SafeDrop: Detecting Memory Deallocation Bugs of Rust Programs via Static Data-Flow Analysis\n论文链接: https://arxiv.org/abs/2103.15420 Publisher: 2021, TOSEM, XuHui\n\n因为写论文加上身体出毛病等等各种原因，最近一个季度啥都没post，本来计划每个月至少发一篇笔记之类的来督促自己学习，现在看来还是小命要紧，以后放慢进度，稳健工作，遇到有趣的工作我就尽量写点笔记发出来。😁\n\n\nTL;DR 在Rust 编译过程中产生的 MIR 上，结合别名分析+数据流的静态分析方法，该方法针对它所分析的对象（MIR）设计了几种内存错误的pattern，通过静态分析检测内存释放bug。\n\n\n 0 摘要\nRust 是一个新兴的编程语言，旨在防止内存安全bug。然而现阶段 Rust 的设计也带来了一些副作用，使得内存安全问题的仍然存在。具体来说，它使用 OBRM（ownership-based resource management）和对未使用的资源强制自动回收（无垃圾回收器）。这使得Rust 程序也许会出现无效内存回收导致use after free 或 double free。文本研究无效的内存回收，提出了SafeDrop，这是一个静态的path-sensitive 数据流分析方法，用于探测上述bugs。我们的方法根据数据流图递归地分析每个Rust crate 的 API ，并且抽取每个数据流的所有aliases。实验结果证明我们的方法可以成功的检测到所有现存的CVEs。分析增加的开销对比原始的编译时间（编译阶段分析）从1%到110.7%不等。我们进一步应用我们的工具在真实世界的Rust crates，并且找到了8个存在无效内存释放的Rust crates。\n 1 Intro\n本文工作针对真实世界中的Rust Crates 查找一种特定类型的内存安全bug，这项工作与 RAII（资源获取即初始化）相关。具体来说Rust 使用 OBRM (ownership-based resource management) 模型，这个模型假定资源在其所有者创建时被分配内存，一旦其所有者离开当前区域这个资源就应该被释放。理想情况下，Rust 应该保证没有悬垂指针和内存泄漏，即使程序产生异常。然而实际上我们观察到真实世界中Rust 存在 UAF (e.g., CVE-2019-16140) 和 Double Free 漏洞 (e.g., CVE-2019-16144)。\n一般来说，内存释放错误是由unsafe 的代码触发的。 Rust 中需要unsafe 的 API 来提供对实现更细节的低级控制和抽象。而误用unsafe 代码可能会造成上述内存中的错误。而且更要命的是，当前的Rust 编译器在不安全代码的内存安全风险方面做得很少，只是假设开发人员应该负责使用它们。由于内存安全是Rust 提倡的最重要的特性，如果可能的话，减少这种风险是极其重要的。\n为了解决这个问题，本文提出了 SafeDrop，一种静态路径敏感的数据流分析方法，用于检测由于自动释放机制导致的内存安全错误。SafeDrop 迭代地分析 Rust crate 的每个 API，并检查 Rust MIR（中级中间表示）中的每个 drop 语句是否可以安全执行。我们的方法采用meet-over-path（MOP）[1] 方法，并基于改进的Tarjan算法[11]提取每个函数的所有有价值的路径，这有效地消除了具有相同别名关系的循环中的冗余路径。\n我们已将我们的方法实现为 Rust 编译器 v1.52 的一个 query（pass），并对此类类型的现有 CVEs 进行了实际实验。实验结果证明我们的方法可以在有限的误报之下成功地检测到所有现存的CVEs。\n\n贡献总结：\n\n我们的论文是第一次尝试研究“与 RAII 的副作用相关的“ Rust 中的内存释放错误。我们系统地讨论了该问题并提取了此类错误的几种常见模式。虽然我们的工作焦点是 Rust，但这个问题也可能存在于其他具有 RAII 的编程语言中。\n我们设计并实现了一种新的路径敏感的数据流分析方法来检测内存释放错误。它包含多种精心设计，以便在不牺牲太多精度的情况下实现可扩展性，包括用于路径敏感分析的改进的 Tarjan 算法和用于过程间分析的基于缓存的策略。\n我们已经对现有的 Rust CVEs 进行了实际实验，并验证了我们方法的有效性和效率。此外，我们发现 8 个 Rust crate 包含尚未发现的无效内存释放问题。\n\n 2 Preliminaries\n主要阐述了以下四个方面内容，不赘述\n\nRust 的内存管理\nRust 编译器的基础知识\nRust 的借用（Borrow）检查器\nRust 编译器的Soundness 限制\n\n\n上图是Rust编译器的处理过程。\n\n本章总结，当前Rust 编译器并没有算法来保证unsafe 代码中Raw pointer 和内存回收的安全性，这是本文分析的造成Rust 内存回收错误的主要原因。\n 3 Problem Statement\nRust 强制执行 RAII 并自动释放未使用的资源。在实践中，这种机制可能会错误地drop一些内存单元，并且容易出现内存安全问题。基于有缺陷的 Drop() 的出现，我们将这个问题分类为两种情况：1）在正常的执行路径上的无效drop；2）在异常处理路径上的无效drop。\n\n上图是Rust 中会引发内存安全问题的 PoC（proof-of-concept）样例。\n 3.1 Motivating Example\n Invalid Drop of Normal Execution.\nFig 2a 中 genvec() 函数是一个内部不安全的构造函数，它基于raw指针 ptr 创建向量 v。在这个 PoC 中，字符串 s 和向量 v 共享相同的内存空间。函数return后字符串自动释放，向量 v 成为一个指向已释放缓冲区的悬空指针，之后在 main 函数中使用此向量将导致 use-after-free。即使不再使用向量，当控制流超出 v 的作用范围，它将被自动丢弃并导致双重释放。在实践中发现了很多这样的bug，比如CVE-2019-16140是use-after-free，CVE-2018-20996 和 CVE-2019-16144 是double free。\nFig 2b 将上述问题以 MIR 的形式展示出来。_1 在 bb0 中创建了一个新的字符串并且在 bb5 中返回了基于_1创建的向量 _0 ，其别名传播链是1-&gt;5-&gt;4-&gt;3-&gt;2-&gt;8-&gt;0. 因此 _0 包含一个 _1 的别名指针。也就是说，drop _1 会导致 _0 变成悬垂指针。\n为什么借用检查器不能添加对raw指针的支持来检测此类问题？问题在于Rust处理函数调用时的权衡设计。例如，_2 是通过调用deref_mut(mov _5)创建的 并且它的参数 _5 是 _1 的可变别名。为了简单起见，Rust 假定每个参数都应该：要么将它的所有权转移给callee函数，要么通过 copy-trait 复制变量，并且复制体不与母本共享所有权。这样的假定使得借用检查器免于做复杂的过程间分析。而由于这个决定，使得 unsafe Rust 代码可能出现漏洞。请注意，from_raw_parts() 是一个 unsafe 函数，并且它在本例中是导致内存回收的原因。‘unsafe’ 标记在 MIR 层就被剥离了，Rust 编译器在 MIR 中是看不到的，也就不能区分安全代码的边界了。此外，当前 Rust 的编译器没有加入为 raw pointer做 alias checking 的功能。因此，上述问题解决起来是不太容易的。\n Invalid Drop of Exception Handling.\n在一些现实世界的错误中，内存释放问题仅存在于异常处理路径中，因为程序会出现恐慌并进入unwinding（栈展开）过程。例如，如果程序崩溃，CVE-2019-16880 和 CVE-2019-16881 就会出现双重释放问题； CVE-2019-15552 和 CVE 2019-15553 可能会在异常处理中 drop 未初始化的内存。Fig 2 中的 PoCs 展示了这个问题。\n假设开发者通过增加一个 mem::forget() 修复了这个bug，在 Fig 2a+b 中能防止bb6 drop(_1)。然而，如果你在 mem::forget() 和创建向量 v 之间访问 v ，那么你的访问仍然是漏洞程序。如果程序在这些点上panic了，那么根据RAII的原则，Rust应该在栈展开的时候调用 Drop() 释放资源。如果存在具有“别名 drop-trait” 的实例（变量），删除这些实例会导致双重释放问题。此外，mem::forget(s) 拿走了字符串 s 的全部所有权，它通常尽可能晚地使用，以便其他语句仍然可以使用该字符串。\n同样，drop 未初始化的内存也是异常处理过程中的一个常见问题。Fig 2c 和 Fig 2d 演示了一个 PoC，它首先应用一个未初始化的缓冲区，然后再对其进行初始化。但是，如果程序在缓冲区完全初始化之前发生恐慌，堆栈展开过程将丢弃那些未初始化的内存，如果缓冲区有指针，这类似于 use-after-free，毕竟调用drop的时候这些指针还未初始化。\n 3.2 Problem Definition\nDefinition 3.1 (Dropping buffers in use). 如果程序错误的释放了一些后续会被访问的缓冲区, 这可能导致悬垂指针最终导致 use-after-free 和 double-free.\nDefinition 3.2 (Dropping invalid pointers). 如果无效指针成为悬垂状态，那么移除这个指针会造成double-free; 如果无效指针指向的未初始化的空间中包含指针类型，删除该指针可能会递归删除其嵌套指针并导致无效的内存访问。\n 3.3 Typical Patterns\n清单2：\n// Listing 2: Typical patterns checked for invalid memory deallocation in SafeDrop.\n// (UAF: use after free; DF: double free; IMA: invalid memory access)\nPattern 1: GetPtr() -> UnsafeConstruct() -> Drop() -> Use() => UAF\nPattern 2: GetPtr() -> UnsafeConstruct() -> Drop() -> Drop() => DF\nPattern 3: GetPtr() -> Drop() -> UnsafeConstruct() -> Use() => UAF\nPattern 4: GetPtr() -> Drop() -> UnsafeConstruct() -> Drop() => DF\nPattern 5: GetPtr() -> Drop() -> Use() => UAF\nPattern 6: Uninitialized() -> Use() => IMA\nPattern 7: Uninitialized() -> Drop() => IMA\n我们说明了此类错误的几种典型模式。请注意，由于正常执行路径和异常处理路径的数据流方法是相似的，因此我们不会在这些模式中进一步区分它们。清单 2 从语句序列的角度总结了 7 种典型的无效内存释放模式。\n模式 1 和模式 2 对应于我们的Fig 2b 的 PoC，它会导致 UAF 或 DF。我们使用 GetPtr() 作为获取指向具有 drop-trait 的实例的 raw 指针的一般表示，并使用 UnsafeConstruct() 来表示用于构造新别名的 unsafe 函数调用（例如，from_raw_part()）。\n模式 3 和模式 4 类似，但交换了 UnsafeConstruct() 和 Drop() 的顺序。\n模式 5 没有 UnsafeConstruct()，但直接使用悬空指针并导致 use-after-free。\n模式 6 和模式 7 对应于我们的Fig 2d 的 PoC，它与未初始化的内存有关。我们使用 Uninitialized() 来表示没有初始化的具有 drop-trait 的实例的构造函数。使用未初始化的内存或直接删除它都容易受到无效内存访问 (IMA) 的影响。\n 4 Approach\n 4.1 Overall Framework\n我们使用路径敏感的数据流分析方法来解决这个问题，并将其集成到名为 SafeDrop 的Rust 编译器中。下图Fig 3为总览：\n\n方法的关键步骤：\n\n\nPath Extraction\nSafeDrop 采用 meet-over-paths 方法来实现路径敏感。由于函数的路径可能是无限的，我们采用一种基于 Tarjan 算法的改进算法来合并冗余路径并生成“生成树”。遍历这棵树，最后枚举出所有有价值的路径。\n\n\n\n\n\nAlias Analysis\nSafeDrop 对字段敏感（field-sensitivity）。此步骤分析变量之间的别名关系以及每个数据流的复合类型的字段。 SafeDrop 也是过程间的，上下文无关的。它缓存并重用获得到的 “被调用者(callees)在返回值和参数之间的别名关系”。\n\n\n\n\n\nInvalid Drop Detection\n基于上面建立的别名集合，这一步在所有数据流上和记录的可疑代码片段上，搜索所有漏洞pattern。\n\n\n\n下面来详细说一下这几个步骤。\n 4.2 Path Extraction\nmeet-over-paths method 会遍历函数的cfg，枚举所有有价值的路径。在SafeDrop中，有价值路径应该满足两个标准：\n1）有价值的路径应该是多个代码块的唯一的集合，其中必须包含开始节点（函数入口）和推出节点。\n2）不能是任何其他有价值路径的子集。\n如果在第二个标准有冲突的路径，我们只选择具有更多独特代码块的路径。这样，我们就不必重复遍历循环，而只需要考虑循环块的最大集合。有效性可以根据SSA形式的代码的别名分析规则自我解释。\n我们使用修改的Tarjan算法来移除冗余路径。这个算法的细节如下：\n\n1-6 首先使用改进的 Trajan 算法生成图的生成树\n7 遍历生成树访问所有有价值的路径\n传统的 Tarjan 算法用于分解图的强连通分量（SCC）并简洁地去除循环（line 2）。因此，SafeDrop 可以将强连通分量收缩为点，并生成该图的生成树（line 3）。理想情况下，生成树枚举所有有价值的路径，而不需要重复遍历循环。但是，由于某些特定的 Rust 语句，这种粗粒度的方法可能会导致不正确的分析结果。例如，Rust 对枚举类型引入了一种批判性设计，这使得传统的 Tarjan 算法不太准确，因此我们应该进行一些修改。Fig 4 展示了这样一个例外情况以及我们相应的解决方案:\n\n解释一下，Fig 4 使用了一种特定类型的终结符 SwitchInt() ，这就是导致不准确的原因。 SwitchInt() 的参数是一个枚举类型，枚举的变化（A or B）决定控制流的走向，例如Fig 4a 中的node 3 在收缩SCCs到一个点后，由 SwitchInt() 链接的分支互相排斥的，因为 A 和 B 是互斥体。\n因此，SafeDrop 明确地将连续的 𝑘 个 SwitchInt() 指向相同的枚举参数，其中包含 𝑛 个可能的变体（line 5），为所有终止符枚举每个变体一次以解决互斥冲突（line 4-6 ）。这种做法可以修剪不可达路径，并且为不同变体（Fig 4b 中node 1 的上部分支）构建独立路径，而不影响其他终结符（Fig 4b 中node 1 的下部分支）。该方法最终将本期的时间复杂度从𝑂(𝑛𝑘)𝑂(𝑛^𝑘)O(nk)降低到𝑂(𝑛)𝑂(𝑛)O(n) 。\n接下来遍历所有生成树，该过程将抽取出所有有价值的路径。\n由于 SafeDrop 实现了meet-over-paths方案，因此它为每条路径分配了独立的遍历状态，在遇到多分支节点时备份状态，从递归返回时将恢复状态（line 18）。这个操作保证了进入不同分支前遍历状态的一致性。\n尽管我们利用修改后的 Tarjan 算法来删除冗余路径，但路径爆炸并非完全不可避免。未解决的路径爆炸主要是由嵌套的条件语句而不是循环产生的，因此我们为其设置了一个较大的阈值。如果计数器超过上限，SafeDrop 将执行传统的半格方案，这是精度和速度之间的权衡。\n 4.3 Alias Analysis\nSafeDrop 对每个路径执行别名分析并为每个程序点建立别名集（算法1 Line 11）。在本小节中，我们首先讨论别名分析的基本规则，然后介绍我们如何执行过程间别名分析。\n Basic Rules for Alias Analysis.\n… 暂时省略，待补充\n\n清单3：别名分析中的类型过滤器。当且仅当以递归方式过滤所有字段并且这些类型应同时实现 Copy trait 时，才会过滤带有星号的复合类型。\nValue := Type::Bool          : The primitive boolean type.\n    | := Type::Char          : The primitive character type.\n    | := Type::Int           : The primitive signed-integer type.\n    | := Type::UInt          : The primitive unsigned-integer type.\n    | := Type::Float         : The primitive floating-point type.\n    | := Type::Array        *: The homogeneous product types.\n    | := Type::Structure    *: The named product types.\n    | := Type::Tuple        *: The anonymous product types.\n    | := Type::Enumeration  *: The tagged union types.\n    | := Type::Union        *: The untagged union types.\n清单4：别名关系的标准以及赋值中对右值的匹配操作。SafeDrop 对move赋值使用零开销的设计，因为这是右值的所有权和别名关系的转移。SafeDrop 在分析之前构造一个指向变量（field）的映射，并在遇到move赋值时将映射从 RValue 传输到 LValue。\nLValue := Use::Copy(RValue)    => e.g. _2 = _1 Use Copy Trait\n     | := Use::Move(RValue)  * => e.g. _2 = move _1 Use Drop Trait\n     | := Cast::Copy(RValue)   => e.g. _2 = _1 as i32 Cast Types\n     | := Cast::Move(RValue) * => e.g. _2 = move _1 as i32\n     | := Ref(RValue)          => e.g. _2 = &amp;mut _1 Create Reference\n     | := AddressOf(RValue)    => e.g. _2 = *mut _1 Create Raw Pointer\n Inter-Procedural Alias Analysis.\n基本的别名分析是过程内方法。它流敏感地分析每条语句，将每个程序的别名关系存储为不相交的集合。然而，这样的分析是unsound的，因为它忽略了函数调用。如果执行路径上包含函数调用，我们将执行过程间的别名分析，来获取调用参数和返回值之间的别名关系。\n\nCallee Analysis. 程序的调用链嵌入在每个基本块的终结符中，包含参数、返回值和内部被调用者ID。随着函数调用的到来，SafeDrop 调用一个查询，通过其内部 ID 向编译器询问被调用者的最终优化的 MIR。 然后 SafeDrop 遍历这个 MIR 并为其变量建立别名集。被调用者上的别名分析最终返回一个包含参数和返回值之间的别名关系的结果给它的调用者。\n\nRecursion Refinement. SafeDrop 采用**不动点迭代(fixed-point iteration)**的方法解决递归调用问题，它维护一个调用栈来确保每个函数ID只在栈上出现一次。函数第一次到达时将它的ID入栈，SafeDrop 将在返回值和参数之间设置默认别名关系（false），并中止调用它自己以防止无限递归。考虑到递归调用通常会退出并且别名关系可能同时发生变化，分析器应该使用更新的别名结果重新执行 SafeDrop 的分析算法到堆栈中的函数，直到最终的别名关系达到不动点。\n\nCache Refinement. 由于 SafeDrop 是一种通过meet-over-paths的方法，因此参数和返回值之间的别名结果是所有有价值路径的并集（算法1 line15）。即，一个参数与返回值有别名关系，只要它至少有一个路径。 SafeDrop 只需要对给定函数分析一次。第一次遍历该函数后，分析结果会被缓存到哈希表中（line 9），再次遇到该函数时分析器可以直接得到结果。\n 4.4 Rules for Invalid Drop Detection\n一次内存释放的安全性通常取决于别名集。对于每条路径，SafeDrop 对每个内存释放语句都会执行流敏感的检测，以确认它是否会引发内存安全问题（line 12）。如果检测到内存释放错误，SafeDrop 将会记录此问题的类型和相关源代码，然后将结果合并到path末尾（line 15）。\n\nSafeDrop 维护一个污点集来记录释放的缓冲区以及返回的悬空指针。它将dropping变量添加到污点集中，如果在path终止符中找到 Drop() 就将其标记为污点源。对于复合类型的变量，SafeDrop 会增加每个drop-trait 字段到污点集中，而不时整个变量。污点源在别名集中传播并污染其他别名。 然后 SafeDrop 迭代检查程序使用的变量和每个程序点的污点集中的所有元素之间的别名关系。有个例外，我们将 uninitialized() 构造的变量插入到污点集作为变量声明的时间，如果稍后初始化，则删除该变量。\n\n根据Sec3中描述的几种存在漏洞的典型模式。我们总结了四个规则来判断path是否存在由无效内存释放引起的内存安全问题，所有这些规则都将应用于正常执行和异常处理path上，如下所示。\n\nUse After Free: The taint set contains the alias of the using variable in a statement or passing this variable to a function.\nDouble Free: The taint set contains the alias of the dropping variable in a Drop() terminator.\nInvalid Memory Access: The taint set contains an uninitialized variable at the time of using and dropping.\nDangling Pointer: The taint set contains the alias of the returned pointer. Although it loses the using context, this pointer is actually buggy and highly unsafe to use.\n\n\n使用SafeDrop分析后，一条规则标记为 true 标志表示此函数内部存在对应类型的内存安全问题。这些标志被缓存到哈希表中，作为过程间分析（line 9）之后的分析结果的一部分。对于每个路径，当 SafeDrop 完成对选定路径的遍历（line 15）时，这些标志以及参数和返回值之间的别名关系将被合并。\n 4.5 Corner Case Handling\n\n修改后的 Tarjan 算法生成生成树以提取有价值的路径，这个过程消除了循环中的冗余遍历。但是，存在一种引入大量误报的极端情况，我们以Fig 5 为例。在Fig 5 中，_1被初始化并drop在 SCC (2-3-4-5-6-7-8-9)上。 在路径 1-2-3-4-5-6-10 中节点 10 的释放是安全的，在路径 1-(2-3-4-5-6-7-8-9)-10 中被视为无效drop。第二条路径确实是误报，因为 _1 在节点 4 中更新，但我们省略了从节点 2 到节点 6 的两次遍历。\n因此我们做出了细化：SafeDrop会记录变量的定义块，并验证是否“指向同一变量的重复drop的分支” 是在 “SCC中定义块和释放块之间”。细化的正确性是基于别名分析中别名关系合并的单调性。\n 5 Experiment\nTabel 1：已被报告过的CVEs上的实验结果。这些CVEs包含四种bug（UAF=use after free, DF=double free, DP=dangling pointer, IMA=Invalid memory access）。这里的结果已通过人工检查和分类。报告的 bug 在表中被分类为 True Positive/False Positive (TP/FP)（使用 “-” 代表 0/0 ），我们还统计了方法数和代码行数来表示 crate 大小.\n\n\nTabel 2：适应性的实验结果。我们选择了一些使用unsafe代码块的crates来测试本文方法。这8个crate中的问题是未曾被人报告过的。\n\n\n下图是一个样例，这是一个异常处理path上的无效drop，匹配了清单2 中的pattern 2。\n\n\n效率：\n我们在比较了原始编译器的编译时间和SafeDrop分析时间，可见SafeDrop不会消耗过长的分析时间（通常不会超过原始编译时间的 2 倍）。\n\n validity\nFalse Positives.\n过程间别名分析严重依赖于query optimize_mir 来获取被调用者的 MIR 。但是，由于编译器的内部设计，SafeDrop 无法捕获某些 MIR，例如某些特定的 trait 实现。SafeDrop的补救措施是，在运行类型过滤器之后，我们的方法会在未过滤的参数和返回值之间建立别名关系。通过这种操作提升别名辨识的soundness，虽然这可能导致一些false positive，但是就不会丢失别名信息了。由于这些情况并不普遍，并且类型过滤器可以识别大多数明显的错误，因此在 SafeDrop 中的误报仍然是可管理的（可接受的）。特别是，我们删除了从 Clone trait 建立的别名，因为函数 clone() 的返回值是对原始变量的深拷贝。\n\nFalse Negatives.\nRust 对内联函数采用了一些tricky的设计，这可能导致一些误报。query可以成功地从编译器获取这些函数的 MIR，但 MIR 内容是空的。因此 SafeDrop 会丢失一些别名关系。此外，SafeDrop 不支持primitive array type, closure, raw pointer offset, 和 function pointer，这可能会导致漏报。\n\n参考文献：\n论文链接: https://arxiv.org/abs/2103.15420 Publisher: 2021, TOSEM\n【1】Glenn Ammons and James R. Larus. 1998. Improving Data-Flow Analysis with Path Profiles. In Proceedings of the ACM SIGPLAN 1998 Conference on Programming Language Design and Implementation (Montreal, Quebec, Canada) (PLDI ’98). Association for Computing Machinery, New York, NY, USA, 72–84. https://doi.org/10.1145/277650.277665\n【11】Harold N. Gabow and Robert Endre Tarjan. 1983. A Linear-Time Algorithm for a Special Case of Disjoint Set Union. In Proceedings of the Fifteenth Annual ACM Symposium on Theory of Computing. Association for Computing Machinery.\n","categories":["论文阅读"],"tags":["论文","笔记","Rust","static-analysis"]},{"title":"Note 《SoK Demystifying Binary Lifters Through the Lens of Downstream Applications》","url":"/Note-SoK-Demystifying-Binary-Lifters-Through-the-Lens-of-Downstream-Applications/","content":" SoK: Demystifying Binary Lifters Through the Lens of Downstream Applications\n\nLink: https://www.computer.org/csdl/proceedings-article/sp/2022/131600a453/1wKCev3wlbO\nPublisher: S&amp;P, 2022\n\n 摘要\n最近的研究有表明，二进制lifter 可以生成功能和逻辑正确的 LLVM IR 代码，即使对于复杂的情况也能成功。\n本文从新的评估视角对二进制lifter 进行了深入研究。解释二进制lifter 的”表现力“，揭露其恢复出来的LLVM IR 可以多好地支持下游二进制分析任务。为此，我们通过编译 C/C++ 程序和提升相应的可执行文件来生成相对应的两种 LLVM IR 代码。然后，我们将这两种 LLVM IR 代码提供给三个关键下游应用程序（指针分析、可辨别性分析（discriminability analysis）和反编译），并检查是否产生了不一致的分析结果。我们从由各种编译器、不同的优化选项和不同的架构生成的总共 252,063 个可执行文件中研究了由行业或学术界开发的四种流行的静态和动态 LLVM IR lifter 。\n我们的研究结果表明，现代二进制lifter 提供了非常适合可辨别性分析和反编译的 IR 代码，并表明这种二进制lifter 可以应用于常见的基于相似性或代码理解的安全分析（例如，binary diffing）。然而，提升的 IR 代码似乎不适合严格的静态分析（例如，指针分析）。\n为了更全面地了解二进制lifter 的实用性，我们还在三个安全分析任务（Asan 地址错误检查、binary diffing和 C 反编译）上比较了 支持lifter的方法 与 仅使用二进制汇编 的工具之间的性能。我们总结了我们的发现，并为正确使用和进一步增强二进制lifter 提出了建议。我们还探索了使用提升的 IR 代码提高指针分析准确性的实用方法：用lifted IR 增强 Debin（一种用于预测调试信息的工具）。\n\nDebin! 世界线收束了。\n\n 0x1 Introduction\n贡献\n\n提出了一个对二进制lifter 评估的新视角，区别于过去的功能正确性视角，我们关注在lifted IR 需求的视角，即其得到的IR是否能很好的支持下游任务。\n我们选取了三种安全研究场景下的基本下游任务，为了顺利的评估各种静态和动态的lifter，我们投入了相当大的人工。我们的研究是在交叉编译器、交叉优化和跨架构设置中进行的。为了获取更综合的评估结构，我们还在三个主流的安全任务中直接比较了支持lifter和纯二进制汇编两种 方法。\n我们研究的结果和意义：现代二进制lifter 的限制——支持严格的静态分析的能力，其解决方案——使用Debin。我们的发现为用户提供了关于如何使用 LLVM 基础设施分析低级二进制代码以实现安全目的的指南，并指出了开发人员需要对Lifter 改进的方向。\n\n 0x2 Preliminaries\n预备知识。\n\n静态和动态lifter的工作流。”Static“和”Dynamic“分别表示静态反汇编和动态二进制翻译。\n本章介绍了上图中每个阶段大概是怎么做的，以及现代lifter 在各个阶段的不同做法，或者说特性。\n 0x3 Motivation Example\n\nfig2 给出了一个study case，关于C源码© 其编译的LLVM IR，和通过编译后的二进制文件lifted IR。\nEmulation-Style IR (EIR) vs. High-Level IR (HIR).\n关于这两种IR，后文Sec6 给出常见的lifter 生成的类似fig2©的EIR，或者更高级的HIR类似于fig2(b)编译的IR code。Sec2 中讲述的Refinement 技术就是把EIR转换成HIR的过程。Sec2 指出为了弥合EIR和CIR 之间在局部变量上的差别，现代lifter 实现了一些analysis passes 和启发式方法去恢复局部变量，另外，对变量类型的精确恢复也有助于静态分析，见Sec6-B。然而不是所有lifter 都实现了refinement 过程，本节中的例子McSema 就没有refinement，其他lifter 的情况我们会在Sec6详细展示。\n为了便于表述，由 Clang 从源代码编译的 LLVM IR 被称为Compiled IR (CIR)，Lifter根据emulation 范式产生的emulation-style IR 称为EIR，通过refinement之后的EIR称为high-level IR (HIR)。\nTransformations Involved in Emitting EIR and HIR.\n在构建lifter codebase 过程中投入了大量的人工努力，我们把汇编代码提升到IR 的过程中使用到的transformations 分成三类：\n\n\noptimizations offered by the LLVM framework\nsimplifies lifted IR code in a functionality-preserving manner.\n\n\nsimple optimizations developed by lifters\n移除臃肿的 utility functions, 如果正确实现的话这个transformation也应该生成 functionality-preserving\n\n\ntransformations recovering high-level program features developed by lifters\n尝试恢复编译期间扔掉的信息\n\n\n根据人工分析总结，Lifter使用1和2能生成EIR，结合123才能生成HIR。\n 0x4 Study Overview\n关于如何度量IR的好坏，我们将Compiled IR Code 的表现作为上界，使用三个下游任务作为评估方法，通过lifted IR Code 和Compiled IR Code 之间的性能的接近程度作为衡量标准。具体到每个任务上的评估度量标准后面会讲。\n接下来介绍了三个下游任务：\n\nPointer Analysis\n\n本研究的第一部分：是否可以使用lifted IR 代码傻瓜式（即不对IR做修改）的启动指针分析。\n使用SOTA的 LLVM pointer analysis library: SVF。\n\n\nDiscriminability Analysis\n\n可辨别性分析是各种基于相似度分析的安全应用的基础，比如恶意代码聚类，代码剽窃检测等应用。\n我们扩展了在 LLVM 框架中开发的代码嵌入(code embedding)工具 ncc 。将代码映射到数值空间，根据其余弦距离判断相似度。ncc 使用”Contextual Flow Graph“来理解LLVM IR代码，这个Graph 结合了IR 数据流和控制流特征。然后ncc 使用基于图神经网络的模型生成程序的数值向量。ncc也提供了一个分类模型用于分类GNN生成的数值向量，这个模型训练在POJ-104。\n使用ncc的分类来评估IR代码的好坏，即越高的分类精度表明更容易确定两个 LLVM IR 程序的（不）相似性。\n\n\nC Decompilation\n\n这里评估的方法是：compiled IR 和lifted IR 代码是否可以反编译得到相似质量的反编译 C 代码。\n使用的反编译工具是llvmir2hll，它是retdec的一部分。\n\n\n\n 0x5 Study Setup\n A. Binary Lifters\n本文所调研的Lifter如下表所示：\n\n提一嘴BinRec，这是最近才发布的动态lifter，它以可执行文件作为输入，采用符号执行引擎S2ES^2ES2E 挖掘可执行路径，该引擎运行在QEMU上，它对于每条记录下来的路径都可以生成一条LLVM IR 路径。S2ES^2ES2E 提供RevGen 用于生成LLVM IR，RevGen 使用IDA-Pro 和McSema 作为前端，这和我们的评估有重叠，所以本文实验中BinRec 不使用RevGen 。BinRec 代表最先进的动态lifter，其设计目标之一就是functional correctness，这在我们后面的实验中得到了很好的证明。\n作者对上述四个工具做了一番介绍，然后总结：这四个工具可能是本文撰写时三个最好的静态lifter和一个最好的动态lifter。\n B. Downstream Task Setup and Test Case Selection\nPointer Analysis.\n我们使用 SVF 提供的所有 24 个流量敏感测试用例。对于每个测试程序，一些pointer pairs 使用 MustAlias 、MayAlias 或 NoAlias 进行注释。它们是指针分析任务的true label。我们研究提升和编译的 IR 代码是否可以支持 SVF 生成正确的指针分析结果。\nDiscriminability Analysis.\n我们使用 ncc 论文中的默认设置将 POJ-104 数据集中的程序拆分为训练、验证和测试集。每个数据集都将被编译或提升为 LLVM IR 程序。所以，最后我们会分别创建5个train、 valid、 test set，对应五个被测试的lifter tools。\n使用ncc 做分类任务，准确率越高，说明提供给ncc 的IR 质量越好。\nC Decompilation.\n将IR 代码丢给llvmir2hll 做反编译，根据反编译代码的质量评估IR的质量。\n代码的结构性越好说明越可读，关于代码结构性的度量标准参考了别人的评估标准，选取了一个指标：统计 #goto 语句的数量，越少的 goto 语句说明可读性越好。\n另一个指标是，函数平均代码长度LOC（lines of Code），基于朴素的理解代码越冗长可读性越差。由于McSema 中包含用于模拟机器指令的工具函数，这会增加IR 代码长度，所以我们计算每个用户定义的函数的平均 LOC，而不是整个二进制文件的反编译代码。\n从SPEC INT 2006测试套件（共九个程序）收集C程序，排除了三个C++程序。\n C. Changes Made on IR Lifters and Downstream Applications\n\n上表是我们为了使数据集中的二进制文件可以正确的被处理所做出的人工努力，我们对几个lifter 和下游任务的ncc、llvmir2hll都做出了很多修改，才使得任务可以进行。具体修改参考。\n 0x6 Findings\n A. Binary Lifting Results\n\n上表是上一小节中三个任务上的所有测试样例，和其中做lifting 失败的样例数量统计。\n简单分析一下：\n数据集：SVF和POJ数据集中的文件比较小，可以在几秒内完成lift，而SPEC中的二进制程序比较大需要数分钟至数小时才能完成。\nLifters：\n\nBinRec：速度远远慢于其他lifter，因为BinRec 使用符号执行探索程序路径。SVF中有两个程序是BinRec 无法分析的（异常退出）, POJ数据集程序数量众多，我们设置超时时长为10mins，21.6%的程序没有分析完成，最终该数据集上的函数覆盖率是93%。这里分析速度较慢不能怪BinRec，因为我们在Sec5中说了不使用BinRec的 S2ES^2ES2E 的RevGen技术，如果我们用上这个技术或者换成其他更好的符号执行引擎，是可以很快的完成分析的。在SPEC 上我们对每个用例执行了50h的分析，平均函数覆盖率是23%，这样的覆盖率阻碍了IR 反编译性能的评估，在Sec6-D详细描述。\nmctoll：在SVF数据集中，有7个不能分析，有7个生成了broken IR。SPEC数据集中的所有样本都不能分析。\nRetDec：总体来说取得了最好的可用性，不能分析的样本最少。\n\n B. Pointer Analysis\n\nSVF数据集上对测试程序中pointer pairs 做了注释（MustAlias MayAlias NoAlias），测试lifted IR能否正确的恢复出这些注释。后面几列是SVF对各种IR做SVFG分析的时候产生的空间和时间消耗。\n结论就是，所有lifter产生的IR都完蛋了，原因是这些IR很难正确的执行严格的静态分析。\n只有RetDec完成了一点指针分析，我们来简单说一下。\n\nRetDec对SVF数据集中的24个程序的分析结果的人工检查，可以看到仅有1个分析正确，由16个虽然是不正确但是可以通过人工修复。以下是修复结果：\n\n人工修复了16个SVF数据集中由RetDec提升得到的IR程序，用于修正指针分析结果。人工修正了213个不精确的变量类型，50个函数原型错误，新加入和调整了几个语句。完成对IR的人工修正后，最后实际上通过指针分析得到的变量恢复也就只有两个。。。\nLifted IR 难以用于严格的指针分析的原因总结为：\n（1）类型恢复的困难，准确来说是难以区分指针类型和非指针类型。恢复复合数据类型比如结构体和数组，仍然是一个难题。\n（2）函数类型恢复困难，准确来说，要使得lifted IR 和编译生成的IR 相似是很难的。\n可能的改进方法：\n\n我们进一步人工研究了lifters的源码，发现在评估中CIR和HIR的不一致性并不是主要来自于工具的bug。而是，这些lifter 没有完全实现本领域（指针分析）的研究成果。现存研究所提出的进行变量恢复和类型推断的静态分析技术，例如VSA。函数原型推断也可以使用相似的方法或者使用AI技术。\n而且，我们也注意到McSema 使用商业工具IDApro作为它的逆向前端，它目前只是使用IDApro获取函数边界，我们建议McSema利用IDApro的函数原型信息。\n\n总的来说，我们lifters 开发人员更多的采用学术研究成果，或者尽可能发掘其使用的第三方工具的全部潜力。\n C. Discriminability Analysis\n在POJ-104 生成的各种IR数据集上，使用ncc 做分类任务。\n\nPOJ上平均分类准确率，准确率越高说明lifter 生成的IR 越好，越利于各种基于相似度的任务。\n\n不同类别而上各个lifter 生成的IR 的表现。RetDec 在某些类别上甚至好于Clang\n总结：RetDec 和mctoll 展现了很不错的在Discriminability Analysis 上的性能，我们还做了binary-diffing tasks，其结果和这里一致，放在论文的附录中。\n这里的结果虽然不能说是非常精确，但是至少能给研究者在使用binary lifter上提供一些选择的参考，尤其是在做相似度任务的时候，例如code patch search。\n原文这里还分析了有效性威胁，关于DNN-based Discriminability Analysis的潜在偏差。\n D. Decompilation Analysis\n\n可见clang 和retdec 得到的IR通过llvmir2hll 得到的类源码的可读性最好。\n总结：RetDec恢复的紧凑IR 代码在很大程度上提高了反编译C代码的结构性和可读性。\n E. Functionality Correctness of Lifted IR Programs\n所谓Functionality Correctness我们的测试方法是重编译IR代码，看编译的二进制文件是否能完成正确执行。\n这里SVF数据集中的程序相对简单没有输出，所以我们只采用POJ和SPEC中的程序作为功能正确性的测试。\n对于POJ 中的104个任务，我们人工阅读源码，来构造输入，我们最终确定了86个程序的输入输出。\n对于九个SPEC 程序，我们使用脚本进行测试。\n结果如下：\n\n在BinRec的论文中，作者用SPEC做了函数功能正确性的测试，所以我们直接根据作者的建议正确配置了 S2ES^2ES2E 符号执行引擎去提升POJ测试用例，结果是全都成功了。\n此外本文还对跨平台和跨编译选项做了进一步研究。\n 0x7 Cross-Platform Evaluation\n本节对lifters在ARM架构上的表现。RetDec和McSema支持64bit ARM架构。\n\n可见只有McSema有1%的失败，其他都lifting 成功了。\n测试结果：\n\n指针分析的结果比x86平台还差一些，\ndiscriminability分析的准确率和x86平台差不多，\nC反编译任务上，RetDec和x86平台稍好一些；McSema在LOC上和x86平台持平，在goto语句数量上比x86平台高很多。\n对于功能正确性来说，这两个lifter 在ARM 平台上的表现都很差。\n 0x8. CROSS-COMPILER &amp; OPTIMIZATION EVALUATION\n跨编译器和优化选项，之前的实验都是使用不带任何优化的clang编译的，我们使用gcc（7.5.0版），和完整的编译器优化（-O3）进行实验。关于lifting 成功和失败的样本情况，在附录中给出，总的来说就是和TABLE V一致。\n\n本组实验中McSema0给出了非常差的结构，所以这里就不展示McSema0了。NA是因为mctoll lifted IR 没能成功完成反编译。BinRec 中的括号内是函数全覆盖的数值（应该是BinRec 动态分析太慢，没有完成所有函数的覆盖就人工停掉了）。\n 0x9 COMPARISON WITH BINARY-ONLY TOOLS\n原文附录ABC做了三个实验探讨纯binary tools 和lifter 在三个分析任务上的优劣。\n\n\nAppx. A RetDec、McSema和RetroWrite 在 Address sanitizer(Asan)(地址错误检查) 任务。\n原理上来讲，RetroWrite没有像lifters一样努力恢复变量，特别是RetDec虽然不精确但是可以恢复变量和函数的局部栈空间。这从概念上将RetroWrite于lifters 区分开来，鉴于它只能粗粒度的栈帧级的Asan insertion。\n从经验上来讲，我们通过在Juliet 测试数据集上插入Asan 检查来评估三个工具。由于RetDec生成的IR 在功能正确性上就有严重缺失，所以对其lifted IR 做异常检测任务评估是没有意义的。McSema正确提升了2,187个用例，在这些用例上，其表现和RetroWrite相当。\n关于更多两者之间的优点和缺点的分析，见附录A。\n\n\nAppx. B 比较RetDec 与 DeepBinDiff、BinDiff 在binary diffing 任务。\n\nRetDec 和纯binary的两个工具具有相似的性能，这说明了lifters在恶意软件聚类和CVE/补丁搜索上具有良好的潜力。\n\n\nAppx. C 比较lifters 和 IDAPro、Ghidra 在C 反编译任务。\n\n多数情况下IDA能生成结构性更好的代码，但是lifters 也表相出相当不错的反编译质量，并且他们都是免费的。XD\n\n\n 0x10 总结\n\n根据你想做的任务选择你的英雄。\n\n\n","categories":["论文阅读"],"tags":["论文","笔记","Binary","CFG","Disassembly","Sok"]},{"title":"Rust类型系统图灵完备的证明","url":"/Rust%E7%B1%BB%E5%9E%8B%E7%B3%BB%E7%BB%9F%E5%9B%BE%E7%81%B5%E5%AE%8C%E5%A4%87%E7%9A%84%E8%AF%81%E6%98%8E/","content":"\n原文链接: https://sdleffler.github.io/RustTypeSystemTuringComplete/\n翻译：ChenYe\n选题：aresbit\n本文由 Rustt 翻译，StudyRust 荣誉推出\n\n Rust类型系统图灵完备的证明\n\nTL;DR: 本文使用 Rust 类型系统实现了 Smallfuck 的所用功能，证明了 Rust 的类型系统是图灵完备的。\n译者前言：阅读本文需要了解 Rust 的 trait 机制，associated types ，以及泛型的相关知识。阅读时长约2h。\n\n（注意：“fuck” 在本文中出现多次，但这个词在这里并不是污言秽语。）\n不久前，有人在 Reddit Rust 子论坛上提出挑战：虽然每个人都喜欢说 Rust 的类型系统是图灵完备的，但实际上似乎还没有人给出确凿的证据。作为对那篇文章的回应，我以使用Rust 实现 Smallfuck（一种已知的图灵完备语言）的方式给出了一个证明，说明 Rust 的类型系统是图灵完备的。这篇文章将阐述 Smallfuck 的 Rust 实现的内部工作原理，以及这对 Rust 类型系统的意义。\n那么什么是图灵完备性呢？图灵完备性是大多数编程语言的一个属性，它表明这些语言可以模拟通用图灵机。与之相关的概念是图灵等效。图灵等效语言可以模拟图灵机也可以被图灵机模拟——因此，如果你有任何两种图灵等效语言，则一定可以将使用一种语言编写的任何程序翻译成另一种语言所编写的程序。大多数图灵完备的系统也被认为是图灵等效的。（wiki）\n关于图灵完备性，有几件重要的事情需要注意。假定你知道停机问题， 如果一个系统是图灵完备的，那么也意味着停机问题不可解决。也就是说，如果你有一种图灵完备的语言，它可以模拟任何通用图灵机，那么它必须能够无限循环。这就是为什么知道 Rust 的类型系统是否是图灵完备的很有用——这意味着，如果你能够将图灵完备的语言编码到 Rust 的类型系统中，那么检查 Rust 程序以确保它的类型是否正确的过程必须是不可判定的问题。即类型检查器必须能够进行无限循环。\n我们如何证明 Rust 类型系统是图灵完备的？最直接的方法（实际上我不知道其他方法）是在其中实现一种已知的图灵完备语言。如果你可以用一种语言实现图灵完备的语言，那么你可以清楚地在其中模拟任何通用图灵机，也就说明这种语言是图灵完备的。\n Smallfuck: 有啥用\n那么，什么是 Smallfuck？ Smallfuck 是一种极简的编程语言，当内存限制被解除时，它就是图灵完备的。我选择将它在 Rust 类型系统中实现而不是其他图灵完备的语言，就是因为它很简单。\n实际上 Smallfuck 非常接近图灵机本身。 根据 Smallfuck 的原始规范声明，它是运行在内存有限的机器上的。然而，如果我们解除这个限制并允许它访问理论上无限的内存数组，那么 Smallfuck 就变成了图灵完备的。所以在这里，我们考虑一种具有无限内存的 Smallfuck 的变体。 本文中 Smallfuck 机器由无限的内存磁带组成，该磁带由包含位的“单元”以及指向该单元的指针组成。\n                 Pointer |\n                         v\n...000001000111000001000001111...\nSmallfuck 程序是由五个指令组成的字符串：\n&lt; | Pointer decrement\n&gt; | Pointer increment\n* | Flip current bit\n[ | If current bit is 0, jump past the matching ]; else, go to the next instruction\n] | Jump back to the matching [ instruction\n利用这几种指令就能够选择内存单元格并制作循环程序。这是一个简单的示例程序：\n&gt;*&gt;*&gt;*[*&lt;]\n这是一个非常简单且完全没用的程序（大多数 Smallfuck 程序都没啥用，这要归因于它完全没有任何类型的 I/O），它只是将指针所指位置的后三位设置为 1，然后使用循环将它们全部置回 0，最后指针停在它开始时候的位置。下面我们来看一下这个过程：\n首先，以下是初始状态：\nInstruction pointer\n|               Memory pointer\nv               v\n&gt;*&gt;*&gt;*[*&lt;] | ...0...\n第一条指令将指针向右移动。所有单元格默认为 0：\n v               v\n&gt;*&gt;*&gt;*[*&lt;] | ...00...\n下一条指令是“翻转当前位”指令，因此我们将指针处的位从 0 翻转到 1。\n v               v\n&gt;*&gt;*&gt;*[*&lt;] | ...01...\n这种情况发生了 3 次。我们跳过重复来到循环的开头：\n      v            v\n&gt;*&gt;*&gt;*[*&lt;] | ...0111...\n现在我们处于循环的开始。 [ 指令是说，“如果当前位为零，则跳转到匹配的 ]；否则，转到下一条指令。” 当前指针所指是 1，所以我们转到下一条指令。\n       v           v\n&gt;*&gt;*&gt;*[*&lt;] | ...0111...\n这会将当前位翻转回零；然后，我们将内存指针移回一个位置。\n         v        v\n&gt;*&gt;*&gt;*[*&lt;] | ...0110...\n现在我们来到循环结束符]，无条件跳回到循环开始处。\n      v           v\n&gt;*&gt;*&gt;*[*&lt;] | ...0110...\n现在我们再次分支。当前单元格为 0 吗？显然不为 0，然后我们继续：\n       v          v\n&gt;*&gt;*&gt;*[*&lt;] | ...0110...\n\n        v         v\n&gt;*&gt;*&gt;*[*&lt;] | ...0100...\n\n         v       v\n&gt;*&gt;*&gt;*[*&lt;] | ...0100...\n\n      v          v\n&gt;*&gt;*&gt;*[*&lt;] | ...0100...\n在最后一次循环之后，我们结束了：\n      v         v\n&gt;*&gt;*&gt;*[*&lt;] | ...0000...\n这正是我们开始的地方：所有单元格归零，并且指针位于其起始位置。\n Smallfuck 在 Rust 中的 Runtime\n那么在 Rust 中这个简单的实现会是什么样子呢？首先我要介绍我实现的 Smallfuck Runtime 的实现，以验证 Rust type-level 和 Smallfuck 运行时实现是否一致。我们会将 Smallfuck 程序存储为枚举类型定义的 AST 节点，如下形式：\nenum Program &#123;\n    Empty,\n    Left(Box&lt;Program>),\n    Right(Box&lt;Program>),\n    Flip(Box&lt;Program>),\n    Loop(Box&lt;(Program, Program)>),\n&#125;\n这种表示方法对于字符串形式的 Smallfuck 程序并非十分精确，但它更容易理解。我们还需要一个类型来表示正在运行的 Smallfuck 程序的状态：\nstruct State &#123;\n    ptr: u16,\n    bits: [u8; (std::u16::MAX as usize + 1) / 8],\n&#125;\n尽管要实现图灵完备，我们在技术上需要无限长的磁带，但我们的运行时实现仅仅是为了检查 Rust 类型系统的语义正确性。所以说，这里我们使用一个有限长的磁带作为近似就可以了。通过使用长度为(std::u16::MAX as usize + 1) / 8的bits，我们确保每个u16指针可以指向的范围内都是可用的。现在我们来定义实现 Smallfuck 程序解释器时需要用到的一些操作：\nimpl State &#123;\n    fn get_bit(&amp;self, at: u16) -> bool &#123;\n        self.bits[(at >> 3) as usize] &amp; (0x1 &lt;&lt; (at &amp; 0x7)) != 0\n    &#125;\n\n    fn get_current_bit(&amp;self) -> bool &#123;\n        self.get_bit(self.ptr)\n    &#125;\n\n    fn flip_current_bit(&amp;mut self) &#123;\n        self.bits[(self.ptr >> 3) as usize] ^= 0x1 &lt;&lt; (self.ptr &amp; 0x7);\n    &#125;\n&#125;\n这是一些标准的位操作。我们将 8 位存储到我们的位数组中的一个单元格中，这样一来要找出给定的指针指向哪个单元格：将指针向右移位三位，然后使用指针的低三位作为单元格内部的索引，就找到了目标 bit 位。\nptr &#x3D; 0b1100011010011001\nIndex into the bytes of &#96;bits&#96; --&gt; 1100011010011 \\ 001 &lt;-- Which bit in the byte\n具体来说，这个寻址操作就如是上面的函数get_bit所示，将指针at&gt;&gt;3作为单元格寻址，通过bits[(at &gt;&gt; 3) as usize]取出单元格，然后使用0x7即0b111作为掩码取at的低三位，作为单元格内的偏移量，将0x1左移偏移量，再按位与上单元格就得到了目标bit是1还是0，对应函数返回true和false。同样的flip_current_bit也按相同的方式取出目标bit位，然后异或上0x1，将原bit位取反。\n现在我们有了这些原语，让我们继续实现我们的解释器。我们将通过递归调用一个在枚举类型 Program 上进行模式匹配的函数来实现：\nimpl Program &#123;\n    fn big_step(&amp;self, state: &amp;mut State) &#123;\n        use self::Program::*;\n\n        match *self &#123;\n            Empty => unimplemented!(),\n            Left(ref next) => unimplemented!(),\n            Right(ref next) => unimplemented!(),\n            Flip(ref next) => unimplemented!(),\n            Loop(ref body_and_next) => unimplemented!(),\n        &#125;\n    &#125;\n&#125;\nEmpty 程序不修改任何状态，因此其实现非常简单：\nEmpty => &#123;&#125;,\nLeft 和 Right 只是将指针加/减一。由于我们在简单实现中选择使用有限的内存磁带，因此我们使用 wrapping_add 和 wrapping_sub：\nLeft(ref next) => &#123;\n    state.ptr = state.ptr.wrapping_sub(1);\n    next.big_step(state);\n&#125;,\nRight(ref next) => &#123;\n    state.ptr = state.ptr.wrapping_add(1);\n    next.big_step(state);\n&#125;,\nFlip 非常简单，因为我们已经写了方便的 flip_current_bit 函数：\nFlip(ref next) => &#123;\n    state.flip_current_bit();\n    next.big_step(state);\n&#125;,\n最后是循环。我们需要检查当前位。如果是 1，那么我们执行循环体，然后以更新的状态再次执行循环指令。如果为 0，我们继续循环体后的下一条指令：\nLoop(ref body_and_next) => &#123;\n    let (ref body, ref next) = **body_and_next;\n    if state.get_current_bit() &#123;\n        body.big_step(state);\n        self.big_step(state);\n    &#125; else &#123;\n        next.big_step(state);\n    &#125;\n&#125;,\n注意，我们必须对 body_and_next 进行双重解引用，因为我们是将一个元组(Program, Program)装入Box&lt;&gt;中（见 Program 的定义）。我们通过 ref 来避免将 body 和 next 的所有权从 Box 中移走。最终我们为枚举类型 Program 完成的 .big_step() 函数如下所示：\nimpl Program &#123;\n    fn big_step(&amp;self, state: &amp;mut State) &#123;\n        use self::Program::*;\n    \n        match *self &#123;\n            Empty => &#123;&#125;,\n            Left(ref next) => &#123;\n                state.ptr = state.ptr.wrapping_sub(1);\n                next.big_step(state);\n            &#125;,\n            Right(ref next) => &#123;\n                state.ptr = state.ptr.wrapping_add(1);\n                next.big_step(state);\n            &#125;,\n            Flip(ref next) => &#123;\n                state.flip_current_bit();\n                next.big_step(state);\n            &#125;,\n            Loop(ref body_and_next) => &#123;\n                let (ref body, ref next) = **body_and_next;\n                if state.get_current_bit() &#123;\n                    body.big_step(state);\n                    self.big_step(state);\n                &#125; else &#123;\n                    next.big_step(state);\n                &#125;\n            &#125;,\n        &#125;\n    &#125;\n&#125;\n我们不会为State实现任何类型的调试打印，因为我们不需要。当使用我们的实现 Smallfuck 运行时解释器来检查 Rust type-level 解释器时，我们将通过检查 Rust type-level 解释器的输出，和 Smallfuck 运行时解释器输出，的一致性来完成检验。现在，我们终于可以开始做些有趣的事情了！\n Type-Level Smallfuck in Rust\nRust 有一个称为 traits 的特性。 Traits 是一种进行编译时静态分派的方法，也可以用于执行运行时分派（尽管在实践中很少见），这里假设读者知道 Rust 中有这些特征并且已经大量使用过。为了实现 Smallfuck，我们将依赖于被称为 associated types 的特性。\n\n这里 associated types 可参考：关联类型 。\n\n Trait resolution and unification\n在 Rust 中，为了调用 trait methods 或解析 associated types ，编译器必须经过一个称为 trait resolution 的过程。为了解析一个 trait ，编译器必须寻找一个与所涉及的 types 相对应的 impl。Unification 是求解 types 之间等式的过程。如果没有解，我们就说 unification 失败了。举个例子：\ntrait Foo&lt;B> &#123;\n   type Associated;\n&#125;\n\n\nimpl&lt;B> Foo&lt;B> for u16 &#123;\n    type Associated = bool;\n&#125;\n\n\nimpl Foo&lt;u64> for u8 &#123;\n    type Associated = String;\n&#125;\n为了解析对一个 associated type （e.g. &lt;F as Foo&lt;T&gt;&gt;::Associated）的引用，Rust 编译器必须找到一个 impl ，这个实现能正确匹配 F 和 T 。假设我们有F == u16和T == u64。如果编译器尝试第二个 impl Foo&lt;u64&gt; for u8，它会发现 F 不匹配 u8，这个实现是不可行的。如果它尝试第一个实现，就得到 F == u16，此时 F 匹配正确。现在我们必须匹配 T == B。这里就是最 magic 的地方。\nRust 编译器将尝试 unify T 和 B 。由于 B 是一个泛型——不是具体类型，它的值可以是 String 或 u64 等等可以被随意赋予。所以现在 B 被 u64 替换，我们有了一个 Foo&lt;u64&gt; for u16 的 impl，这是一个可行的解析。Unification 是一个相当简单的过程：它接受一个可能有泛型的类型，每次它遇到一个尚未分配的泛型，且这个泛型匹配任何其他的类型，就将这个泛型分配给这个类型（即使其他类型也是泛型）。然后，任何时候在 unifies 同一个类型时遇到该泛型，它被其分配的值替换，相反，unification 尝试将分配的类型与另一个类型统一起来。\nUnification 操作可以理解为一组“代换”——泛型到类型的映射，这样当你采用类型时，你就在试图 unify 并替换所有出现的泛型，你最终会得到两个相同的类型。下面是一个独立的 unification 示例，尝试使用Foo&lt;Baz, Y&gt; unify Foo&lt;X, Bar&lt;u16, Z&gt; （X，Z 为泛型，Foo，Baz，Bar 为具体类型）：\nFoo&lt;X, Bar&lt;u16, Z>> == Foo&lt;Baz, Y>\n首先，我们检查类型的头部——我们得到 Foo vs. Foo ，并没有失败。如果我们有，比如说，Foo == Bar 那么 unification 就直接失败了。接下来，我们将其分解为两个子问题。我们知道 Foo&lt;A, B&gt; == Foo&lt;C, D&gt; 当且仅当 A == C 和 B == D：\nX == Baz, Bar&lt;u16, Z> == Y\n我们现在已经解决了一个变量——X，它有一个明确定义的值：Baz 。可以将这个泛型和类型之间的关系添加到我们的代换表中，[X -&gt; Baz]。然后，我们将该代换表应用于第二项 Bar&lt;u16, Z&gt; == Y。因为代换表里没有出现 X，所以什么也没有发生。然后，我们得到了 Y 的替换方案。此时，代换表：[X -&gt; Baz, Y -&gt; Bar&lt;u16, Z&gt;]。让我们观察当我们将其应用于我们想要检查的原始方程时会发生什么：\nFoo&lt;X, Bar&lt;u16, Z>> == Foo&lt;Baz, Y> [X -> Baz, Y -> Bar&lt;u16, Z>]\n转换为：\nFoo&lt;Baz, Bar&lt;u16, Z>> == Foo&lt;Baz, Bar&lt;u16, Z>>\n这个等式方程显然是成立的。这两个变量现在相等了！那么 unification 失败的例子是什么样的？咱们试试吧：\nFoo&lt;Bar, X> == Foo&lt;X, Baz>\n这分解为两个等式：\nBar == X, X == Baz\n所以我们求解第一个方程，得到代换表 [X -&gt; Bar]。应用代换表到 x == Baz 产生了 Bar == Baz 这显然是错误的，unification 失败了——没有可行的解。\nUnification 是一个非常有用的过程，实际上存在一种编程语言 Prolog ，它通过逻辑表达式描述程序，其程序的执行过程就是表达式的 unification 的过程。这听起来有点让人摸不着头脑。实际上，如果有一种基于 Unification 的图灵完备语言 Prolog，那么 Rust 的 trait 解析（通过尝试unify trait impl 直到找到可行的实现）似乎也应该是图灵完备的.\n Computing with traits\n现在我们已经完成了样板文件，让我们看看我们如何在 Rust 中实际编写实现Smallfuck。我们将使用我几个月前编写的一个名为 type_operators! 的宏，它将 DSL 编译成 Rust struct definitions、trait defination 和 trait impls 的集合。接下来我们首先要来看看type_operators!的实现。\n我们将从简单开始，我们如何在 Rust 类型中表示 Smallfuck 状态的位？\ntype_operators! &#123;\n    [ZZ, ZZZ, ZZZZ, ZZZZZ, ZZZZZZ]\n    \n    concrete Bit => bool &#123;\n        F => false,\n        T => true,\n    &#125;\n&#125;\n这里有几点需要注意。第一个就是这个有点奇怪的 list，[ZZ, ZZZ, ZZZZ, ZZZZZ, ZZZZZZ] 。这很难解释，但它与 Rust 宏无法创建唯一类型名称有关。因此，你必须通过提供自己的 list 的方式来解决这个问题。你可以暂时忽略这一点，因为它与实际实现无关。但是， concrete Bit 是我们必须要搞清楚的！\ntype_operators! 接受了两种类型级别的伪数据类型定义：data 和 concrete 。这两个宏关键字的区别在于 type_operators! 生成 traits 以确保你不会与这些 type-level 的数据类型不匹配。上面的宏代码将会编译为以下定义：\npub trait Bit &#123;\n    fn reify() -> bool;\n&#125;\n\n\npub struct T;\npub struct F;\n\n\nimpl Bit for T &#123;\n    fn reify() -> bool &#123; true &#125;\n&#125;\n\nimpl Bit for F &#123;\n    fn reify() -> bool &#123; false &#125;\n&#125;\n希望你能看明白发生了什么。T 和 F 成了单个的实现了 Bit 的 structs 。Bit 为它们提供了一个 reify() 函数，可让你将类型 T 和 F 转换为相应的布尔表示。所以你可以写 &lt;T as Bit&gt;::reify() 它将返回 true 。这很有用，因为只要你有一个 Bit 类型的变量 B: Bit，你就可以使用 B::reify() 返回其对应的 bool 值。我使用它来将 Smallfuck 解释器的输出转换为可以检查其运行时实现的值。\n希望我表达的足够清楚！让我们看看另一个使用 concrete 的定义：\nconcrete List => BitVec &#123;\n    Nil => BitVec::new(),\n    Cons(B: Bit, L: List = Nil) => &#123; let mut tail = L; tail.push(B); tail &#125;,\n&#125;\n这里有点复杂了。让我们慢慢讲一讲。\n这是一个 type-level cons-list 。我们从代码中首先可以看到一对结构类型，Nil 和 Cons ，它们将被宏解析为：\npub struct Nil;\npub struct Cons&lt;B: Bit, L: List = Nil>(PhantomData&lt;(B, L)>);\n所以，现在我们得到了 bits 和 bits 的 lists 。我们可以构造一个列表[T, F, F]作为Cons&lt;T, Cons&lt;F, Cons&lt;F, Nil&gt;&gt;&gt;。除此之外，宏还使我们获得了一些 traits：\npub trait List &#123;\n    fn reify() -> BitVec;\n&#125;\n\n\nimpl List for Nil &#123;\n    fn reify() -> BitVec &#123; BitVec::new() &#125;\n&#125;\n\nimpl&lt;B: Bit, L: List> List for Cons&lt;B, L> &#123;\n    fn reify() -> BitVec &#123;\n        let mut tail = &lt;L as List>::reify();\n        tail.push(&lt;B as Bit>::reify());\n        tail\n    &#125;\n&#125;\n所以要注意的一件事是，我们写的调用宏的代码中：\nCons(B: Bit, L: List = Nil) => &#123; let mut tail = L; tail.push(B); tail &#125;\n它会产生一种语法糖，其中 L 和 B 被自动具体化为具体类型，然后绑定到这些类型中。这是为了绕过 macro hygiene 的一些限制，并允许用户实际使用这些值。\n所以希望现在你已经弄清楚了，让我们看看我们实现 Smallfuck 将使用的最后两个使用 concrete 的定义：\nconcrete ProgramTy => Program &#123;\n    Empty => Program::Empty,\n    Left(P: ProgramTy = Empty) => Program::Left(Box::new(P)),\n    Right(P: ProgramTy = Empty) => Program::Right(Box::new(P)),\n    Flip(P: ProgramTy = Empty) => Program::Flip(Box::new(P)),\n    Loop(P: ProgramTy = Empty, Q: ProgramTy = Empty) => Program::Loop(Box::new((P, Q))),\n&#125;\n\nconcrete StateTy => StateTyOut &#123;\n    St(L: List, C: Bit, R: List) => &#123;\n        let mut bits = L;\n        let loc = bits.len();\n        bits.push(C);\n        bits.extend(R.into_iter().rev());\n\n        StateTyOut &#123;\n            loc: loc,\n            bits: bits,\n        &#125;\n    &#125;,\n&#125;\n希望你现在已经弄清楚了所有的模式。以下是一个完整的清单，展示了所有这些使用 type_operators! 宏的定义是如何编译成 Rust structs， traits， 和 impls ：\n// `Bit` trait and `T` and `F` types.\n\npub trait Bit &#123;\n    fn reify() -> bool;\n&#125;\n\n\npub struct F;\npub struct T;\n\n\nimpl Bit for F &#123;\n    fn reify() -> bool &#123;\n        false\n    &#125;\n&#125;\n\nimpl Bit for T &#123;\n    fn reify() -> bool &#123;\n        true\n    &#125;\n&#125;\n\n\n// `List` trait and `Nil` and `Cons&lt;H, T>` types.\npub trait List &#123;\n    fn reify() -> BitVec;\n&#125;\n\n\npub struct Nil;\npub struct Cons&lt;B: Bit, L: List = Nil>(PhantomData&lt;B>, PhantomData&lt;L>);\n\n\nimpl List for Nil &#123;\n    fn reify() -> BitVec &#123;\n        BitVec::new()\n    &#125;\n&#125;\n\nimpl&lt;B: Bit, L: List> List for Cons&lt;B, L> &#123;\n    #[allow(non_snake_case)]\n    fn reify() -> BitVec &#123;\n        let B = &lt;B>::reify();\n        let L = &lt;L>::reify();\n        &#123;\n            let mut tail = L;\n            tail.push(B);\n            tail\n        &#125;\n    &#125;\n&#125;\n\n\n// `ProgramTy` trait and `Empty`, `Left&lt;P>`, `Right&lt;P>`, `Flip&lt;P>`, and `Loop&lt;P, Q>` types.\npub trait ProgramTy &#123;\n    fn reify() -> Program;\n&#125;\n\n\npub struct Empty;\npub struct Left&lt;P: ProgramTy = Empty>(PhantomData&lt;P>);\npub struct Right&lt;P: ProgramTy = Empty>(PhantomData&lt;P>);\npub struct Flip&lt;P: ProgramTy = Empty>(PhantomData&lt;P>);\npub struct Loop&lt;P: ProgramTy = Empty, Q: ProgramTy = Empty>(PhantomData&lt;P>, PhantomData&lt;Q>);\n\n\nimpl ProgramTy for Empty &#123;\n    fn reify() -> Program &#123;\n        Program::Empty\n    &#125;\n&#125;\n\nimpl&lt;P: ProgramTy> ProgramTy for Left&lt;P> &#123;\n    #[allow(non_snake_case)]\n    fn reify() -> Program &#123;\n        let P = &lt;P>::reify();\n        Program::Left(Box::new(P))\n    &#125;\n&#125;\n\nimpl&lt;P: ProgramTy> ProgramTy for Right&lt;P> &#123;\n    #[allow(non_snake_case)]\n    fn reify() -> Program &#123;\n        let P = &lt;P>::reify();\n        Program::Right(Box::new(P))\n    &#125;\n&#125;\n\nimpl&lt;P: ProgramTy> ProgramTy for Flip&lt;P> &#123;\n    #[allow(non_snake_case)]\n    fn reify() -> Program &#123;\n        let P = &lt;P>::reify();\n        Program::Flip(Box::new(P))\n    &#125;\n&#125;\n\nimpl&lt;P: ProgramTy, Q: ProgramTy> ProgramTy for Loop&lt;P, Q> &#123;\n    #[allow(non_snake_case)]\n    fn reify() -> Program &#123;\n        let P = &lt;P>::reify();\n        let Q = &lt;Q>::reify();\n        Program::Loop(Box::new((P, Q)))\n    &#125;\n&#125;\n\n\n// `StateTy` trait and `St` type.\npub trait StateTy &#123;\n    fn reify() -> StateTyOut;\n&#125;\n\n\npub struct St&lt;L: List, C: Bit, R: List>(PhantomData&lt;L>, PhantomData&lt;C>, PhantomData&lt;R>);\n\n\nimpl&lt;L: List, C: Bit, R: List> StateTy for St&lt;L, C, R> &#123;\n    #[allow(non_snake_case)]\n    fn reify() -> StateTyOut &#123;\n        let L = &lt;L>::reify();\n        let C = &lt;C>::reify();\n        let R = &lt;R>::reify();\n        &#123;\n            let mut bits = L;\n            let loc = bits.len();\n            bits.push(C);\n            bits.extend(R.into_iter().rev());\n            StateTyOut &#123;\n                loc: loc,\n                bits: bits,\n            &#125;\n        &#125;\n    &#125;\n&#125;\n\nfrom: https://gist.github.com/sdleffler/46700a8add454bc23cdb399a766cb273#file-rust-smallfuck-concretes-rs\n\n从上面的代码可见 ProgramTy 相当简单，希望你现在明白为什么我选择将 Smallfuck 程序的运行时编码为 AST 的形式—— 为了尽可能清晰地反映 Rust type-level encoding。 StateTy  trait 下的 St 类型是一个 zipper list ，同时表示 Smallfuck 解释器的指针位置和内存。 L 和 R 列表表示 C 两侧的内存，C 是指针下的当前位。\n现在我们可以看看最有趣的部分——Smallfuck 解释器的实际实现。它由一个 trait 和十几个 impl 组成。以下是 type_operators! 宏中的代码：\n(Run) Running(ProgramTy, StateTy): StateTy &#123;\n    forall (P: ProgramTy, C: Bit, R: List) &#123;\n        [(Left P), (St Nil C R)] => (# P (St Nil F (Cons C R)))\n    &#125;\n    forall (P: ProgramTy, L: List, C: Bit) &#123;\n        [(Right P), (St L C Nil)] => (# P (St (Cons C L) F Nil))\n    &#125;\n    forall (P: ProgramTy, L: List, C: Bit, N: Bit, R: List) &#123;\n        [(Left P), (St (Cons N L) C R)] => (# P (St L N (Cons C R)))\n        [(Right P), (St L C (Cons N R))] => (# P (St (Cons C L) N R))\n    &#125;\n    forall (P: ProgramTy, L: List, R: List) &#123;\n        [(Flip P), (St L F R)] => (# P (St L T R))\n        [(Flip P), (St L T R)] => (# P (St L F R))\n    &#125;\n    forall (P: ProgramTy, Q: ProgramTy, L: List, R: List) &#123;\n        [(Loop P Q), (St L F R)] => (# Q (St L F R))\n        [(Loop P Q), (St L T R)] => (# (Loop P Q) (# P (St L T R)))\n    &#125;\n    forall (S: StateTy) &#123;\n        [Empty, S] => S\n    &#125;\n&#125;\n这些使用宏的代码将编译成一大堆东西。首先来说有一个 trait ，如下代码所示。\n这就是奇怪的 gensym list 发挥作用的地方。由于 gensyms 只在这里使用，它们不会与用户编写的任何内容发生冲突，因为这里没有写入任何涉及泛型的内容。由于写 ZZ, ZZZ 等是一件痛苦的事情，这里我选择忽略，不过多叙述我实际放到 gensym list 中的东西，只是简单的把它写好就得了。\n\n上面一段并不重要，是原作者的自言自语，如果需要可以自行了解 Rust 的宏生成系统.\n\npub trait Running&lt;S: StateTy>: ProgramTy &#123;\n    type Output: StateTy;\n&#125;\n\npub type Run&lt;P: ProgramTy, S: StateTy> = &lt;P as Running&lt;S>>::Output;\n(Run) Running(ProgramTy, StateTy): StateTy 成了一个 type-level 函数，输入参数是实现了 ProgramTy 和 StateTy 的两种 trait 的类型，输出是实现了 StateTy trait 的类型。\n现在来看看里面的东西。让我们先看一个简单的定义：\nforall (P: ProgramTy, C: Bit, R: List) &#123;\n    [(Left P), (St Nil C R)] => (# P (St Nil F (Cons C R)))\n&#125;\n它编译为一个 trait impl ，如下所示：\nimpl&lt;P: ProgramTy, C: Bit, R: List> Running&lt;St&lt;Nil, C, R>> for Left&lt;P>\n    where P: Running&lt;St&lt;Nil, F, Cons&lt;C, R>>> &#123;\n    type Output = &lt;P as Running&lt;St&lt;Nil, F, Cons&lt;C, R>>>>::Output;\n&#125;\n(# P (St Nil F (Cons C R))) 的意思是：“递归地‘调用’带有参数 P (St Nil F (Cons C R)) 的 type-level 函数。” type_operators! 使用 lisp-like DSL 来简化解析； 写作 (A B C) ，编译为类型： A&lt;B, C&gt; 。 ‘# function’ 在 type_operators! 中是特殊的语法，因为必须追踪 ‘#’ 的使用情况，具体来说：每当调用 ‘#’ 时，必须在 where 子句中添加一个约束。你可以通过上面的示例看到这种用宏生成代码的动作是如何进行的。\n现在类型级函数定义的语义已经明确了，我们来看看 Smallfuck 是怎么定义的。我们有四种不同的定义来处理 Left 和 Right指令：\nforall (P: ProgramTy, C: Bit, R: List) &#123;\n    [(Left P), (St Nil C R)] => (# P (St Nil F (Cons C R)))\n&#125;\nforall (P: ProgramTy, L: List, C: Bit) &#123;\n    [(Right P), (St L C Nil)] => (# P (St (Cons C L) F Nil))\n&#125;\nforall (P: ProgramTy, L: List, C: Bit, N: Bit, R: List) &#123;\n    [(Left P), (St (Cons N L) C R)] => (# P (St L N (Cons C R)))\n    [(Right P), (St L C (Cons N R))] => (# P (St (Cons C L) N R))\n&#125;\n第一个定义了当指针向左移动时会发生什么，但我们的拉链列表（zipper list）中左侧的 cons-list 是空的。我们必须创建一个新的 F bit（表示 0），并将当前位移动到右侧的 cons-list 中。左侧的 cons-list 保持为 Nil。\n这里的第二个定义同上，现在是指针必须向右移动而右侧的 cons-list 为 Nil 的情况。\n第三和第四个定义分别处理指针向左、右移动，且左、右的 cons-list 分别为 Cons&lt;N, L&gt; 和 Cons&lt;N, R&gt; 的情况。在这种情况下，我们可以从 cons-list 中弹出一个值并将其移动到当前位；然后将当前位压入另一个 cons-list 。下面来形象的说明这个四个过程：\nThe zipper list is like this:\n[...L...] C [...R...]\nWhere ...L... represents a list which may be Cons or Nil; we don&#39;t care. It&#39;s\nopaque.\n\n[...L..., LN] C [RN, ...R...]\nHere is a list where the left-hand list is Cons&lt;LN, L&gt; and the right-hand list\nis Cons&lt;RN, R&gt;.\n\nThe variables used here are meaningful: L &#x3D; Left, R &#x3D; Right, N &#x3D; Next, C &#x3D; Current.\n\nPointer moves left, left-hand side is Nil:\n[] C [...R...] &#x3D;&gt; [] 0 [C, ...R...]\n\nPointer moves right, right-hand side is Nil:\n[...L...] C [] &#x3D;&gt; [...L..., C] 0 []\n\nPointer moves left, left-hand side is Cons&lt;N, L&gt;:\n[...L..., N] C [...R...] &#x3D;&gt; [...L...] N [C, ...R...]\n\nPointer moves right, right-hand side is Cons&lt;N, R&gt;:\n[...L...] C [N, ...R...] &#x3D;&gt; [...L..., C] N [...R...]\n那么 Rust 究竟是如何“执行”这些向左向右动作的呢？假设当前程序为 Left&lt;P&gt; 当前状态是 St&lt;Nil, F, R&gt;。当我们将它们放入 Run&lt;P, S&gt; type synonym，使其成为 Run&lt;Left&lt;P&gt;, St&lt;Nil, F, R&gt;&gt; 时，我们最终得到 &lt;Left&lt;P&gt; as Running&lt;St&lt;Nil, F, R&gt;&gt;&gt;::Output 。这会使得 Rust 尝试搜索一个 impl 来做 unification 。\n我们之前说过，由于 Rust 编写 trait 和 impl 的规则，对于任何给定的类型，Rust 尝试为其寻找 impl 最多只能有一个有效的 impl。 Rust 找到了我们手写的宏编译后会生成的 impl ：\nimpl&lt;P: ProgramTy, C: Bit, R: List> Running&lt;St&lt;Nil, C, R>> for Left&lt;P>\n    where P: Running&lt;St&lt;Nil, F, Cons&lt;C, R>>> &#123;\n    type Output = &lt;P as Running&lt;St&lt;Nil, F, Cons&lt;C, R>>>>::Output;\n&#125;\nRust 将 trait 中的 Left&lt;P1&gt; （即宏代码中的 P）与我们要求它 unify 的 Left&lt;P2&gt; 统一起来。 （为 P1 和 P2 是不同的） Rust 将 P1 与 P2 统一起来，统一成功。然后，Rust 将 St&lt;Nil, C, R1&gt; 与 St&lt;Nil, F, R2&gt; 统一起来。这也可以成功； Nil 是具体的类型，Nil == Nil ，这没问题也可以统一。将具体类型 F 分配给泛型 C。最后，泛型 R1 和 R2 统一起来，顺利结束 unification。\n现在我们有了代换表 [P1 -&gt; P2, C -&gt; F, R1 -&gt; R2] ，它被替换到上述 trait impl 的主体中。我们最终得到了这个：\nimpl Running&lt;St&lt;Nil, F, R2>> for Left&lt;P2>\n    where P2: Running&lt;St&lt;Nil, F, Cons&lt;F, R2>>> &#123;\n    type Output = &lt;P2 as Running&lt;St&lt;Nil, F, Cons&lt;F, R2>>>>::Output;\n&#125;\n所以宏编译生成后的 “output type” 是 &lt;P2 as Running&lt;St&lt;Nil, F, Cons&lt;F, R2&gt;&gt;&gt;&gt;::Output 。\n现在我们已经掌握了宏的运作原理，让我们看看 Running 中 Flip 指令的处理：\nforall (P: ProgramTy, L: List, R: List) &#123;\n    [(Flip P), (St L F R)] => (# P (St L T R))\n    [(Flip P), (St L T R)] => (# P (St L F R))\n&#125;\n实际上这部分这相当简单。左右列表根本没有修改。我们在 Flip 指令之后对程序递归调用 Run，并通过模式匹配改变 F -&gt; T 和 T -&gt; F ，结束。\nforall (P: ProgramTy, Q: ProgramTy, L: List, R: List) &#123;\n    [(Loop P Q), (St L F R)] => (# Q (St L F R))\n    [(Loop P Q), (St L T R)] => (# (Loop P Q) (# P (St L T R)))\n&#125;\n循环指令 Loop&lt;P, Q&gt; 是最复杂的指令。我们通过模式匹配检查当前位。如果它是 F 即 0 ，则在运行时的表达就是：我们递归地运行循环之后的程序部分，也就是跳过循环体。如果它是 T 即 1 ，则在运行时的表达就是：我们进行两次递归调用。第一个通过运行循环体产生一个新状态。然后，我们再次使用新状态运行 Loop&lt;P, Q&gt; 。\n现在我们已经完成了所有复杂的部分！最后要处理的指令只是 Empty。 Empty 所做的只是返回未修改的当前状态，而不是递归：\nforall (S: StateTy) &#123;\n    [Empty, S] => S\n&#125;\n现在，你已经完成了 type-level 实现 Smallfuck 运行时的所有困难工作。以下是代码，展示了 Running trait 及其 impls 的完整扩展版本：\npub trait Running&lt;B: StateTy>: ProgramTy &#123;\n    type Output: StateTy;\n&#125;\n\npub type Run&lt;A: ProgramTy, B: StateTy> = &lt;A as Running&lt;B>>::Output;\n\n\n// [(Left P), (St Nil C R)] => (# P (St Nil F (Cons C R)))\nimpl&lt;P: ProgramTy, C: Bit, R: List> Running&lt;St&lt;Nil, C, R>> for Left&lt;P>\n    where P: Running&lt;St&lt;Nil, F, Cons&lt;C, R>>>\n&#123;\n    type Output = &lt;P as Running&lt;St&lt;Nil, F, Cons&lt;C, R>>>>::Output;\n&#125;\n\n// [(Right P), (St L C Nil)] => (# P (St (Cons C L) F Nil))\nimpl&lt;P: ProgramTy, L: List, C: Bit> Running&lt;St&lt;L, C, Nil>> for Right&lt;P>\n    where P: Running&lt;St&lt;Cons&lt;C, L>, F, Nil>>\n&#123;\n    type Output = &lt;P as Running&lt;St&lt;Cons&lt;C, L>, F, Nil>>>::Output;\n&#125;\n\n// [(Left P), (St (Cons N L) C R)] => (# P (St L N (Cons C R)))\nimpl&lt;P: ProgramTy, L: List, C: Bit, N: Bit, R: List> Running&lt;St&lt;Cons&lt;N, L>, C, R>> for Left&lt;P>\n    where P: Running&lt;St&lt;L, N, Cons&lt;C, R>>>\n&#123;\n    type Output = &lt;P as Running&lt;St&lt;L, N, Cons&lt;C, R>>>>::Output;\n&#125;\n\n// [(Right P), (St L C (Cons N R))] => (# P (St (Cons C L) N R))\nimpl&lt;P: ProgramTy, L: List, C: Bit, N: Bit, R: List> Running&lt;St&lt;L, C, Cons&lt;N, R>>> for Right&lt;P>\n    where P: Running&lt;St&lt;Cons&lt;C, L>, N, R>>\n&#123;\n    type Output = &lt;P as Running&lt;St&lt;Cons&lt;C, L>, N, R>>>::Output;\n&#125;\n\n// [(Flip P), (St L F R)] => (# P (St L T R))\nimpl&lt;P: ProgramTy, L: List, R: List> Running&lt;St&lt;L, F, R>> for Flip&lt;P>\n    where P: Running&lt;St&lt;L, T, R>>\n&#123;\n    type Output = &lt;P as Running&lt;St&lt;L, T, R>>>::Output;\n&#125;\n\n// [(Flip P), (St L T R)] => (# P (St L F R))\nimpl&lt;P: ProgramTy, L: List, R: List> Running&lt;St&lt;L, T, R>> for Flip&lt;P>\n    where P: Running&lt;St&lt;L, F, R>>\n&#123;\n    type Output = &lt;P as Running&lt;St&lt;L, F, R>>>::Output;\n&#125;\n\n// [(Loop P Q), (St L F R)] => (# Q (St L F R))\nimpl&lt;P: ProgramTy, Q: ProgramTy, L: List, R: List> Running&lt;St&lt;L, F, R>> for Loop&lt;P, Q>\n    where Q: Running&lt;St&lt;L, F, R>>\n&#123;\n    type Output = &lt;Q as Running&lt;St&lt;L, F, R>>>::Output;\n&#125;\n\n// [(Loop P Q), (St L T R)] => (# (Loop P Q) (# P (St L T R)))\nimpl&lt;P: ProgramTy, Q: ProgramTy, L: List, R: List> Running&lt;St&lt;L, T, R>> for Loop&lt;P, Q>\n    where Loop&lt;P, Q>: Running&lt;&lt;P as Running&lt;St&lt;L, T, R>>>::Output>,\n          P: Running&lt;St&lt;L, T, R>>\n&#123;\n    type Output = &lt;Loop&lt;P, Q> as Running&lt;&lt;P as Running&lt;St&lt;L, T, R>>>::Output>>::Output;\n&#125;\n\n// [Empty, S] => S\nimpl&lt;S: StateTy> Running&lt;S> for Empty &#123;\n    type Output = S;\n&#125;\n\nfrom: https://gist.github.com/sdleffler/a432103278432140f84f6b17869f8b52/raw/f69435eb4e3800af4a60f5462b2e3840dd308b90/rust-type-level-smallfuck-running-trait.rs\n\n 测试和结论\n除了 type-level 的实现，还实现了两个宏，sf! 和 sf_test! ，用于测试我们的实现：\n// A Smallfuck state which is filled with `F` bits - a clean slate.\npub type Blank = St&lt;Nil, F, Nil>;\n\n\n// Convert nicely formatted Smallfuck into type-encoded Smallfuck.\nmacro_rules! sf &#123;\n    (&lt; $($prog:tt)*) => &#123; Left&lt;sf!($($prog)*)> &#125;;\n    (> $($prog:tt)*) => &#123; Right&lt;sf!($($prog)*)> &#125;;\n    (* $($prog:tt)*) => &#123; Flip&lt;sf!($($prog)*)> &#125;;\n    ([$($inside:tt)*] $($outside:tt)*) => &#123; Loop&lt;sf!($($inside)*), sf!($($outside)*)> &#125;;\n    () => &#123; Empty &#125;;\n&#125;\n\n\nmacro_rules! sf_test &#123;\n    ($($test_name:ident $prog:tt)*) => &#123;\n         $(\n            #[test]\n            fn $test_name() &#123;\n                let prog = &lt;sf! $prog as ProgramTy>::reify();\n\n                let typelevel_out = &lt;Run&lt;sf! $prog, Blank> as StateTy>::reify();\n                let runtime_out = prog.run();\n\n                println!(\"Program: &#123;:?&#125;\", prog);\n                println!(\"Type-level output: &#123;:?&#125;\", typelevel_out);\n\n                let offset = runtime_out.ptr.wrapping_sub(typelevel_out.loc as u16);\n\n                for (i, b1) in typelevel_out.bits.into_iter().enumerate() &#123;\n                    let b2 = runtime_out.get_bit((i as u16).wrapping_add(offset));\n                    println!(\"[&#123;&#125;] &#123;&#125; == &#123;&#125;\",\n                            i,\n                            if b1 &#123; \"1\" &#125; else &#123; \"0\" &#125;,\n                            if b2 &#123; \"1\" &#125; else &#123; \"0\" &#125;);\n                    assert_eq!(b1, b2);\n                &#125;\n            &#125;\n         )*\n    &#125;\n&#125;\n我必须感谢 durka 修复了我的 sf! 宏，原本的几乎不能用。非常感谢！\n完整源代码还包括两个测试：\nsf_test! &#123;\n    back_and_forth &#123;\n        > * > * > * > * &lt; [ * &lt; ]\n    &#125;\n    forth_and_back &#123;\n        &lt; * &lt; * &lt; * &lt; * > [ * > ] > > >\n    &#125;\n&#125;\n测试只检查了type-level 实现相对于指针位置设置的比特位，但足以让我确定我的实现是正确的。 Smallfuck 非常简单，几乎没有出错的余地；尽管如此，如果有人想贡献更多的测试用例，我很欢迎 pr 请求。\n所以，Rust 的类型系统是图灵完备的。这是什么意思？\n老实说，这几乎没有任何意义。类型系统确实可以进入无限循环，但我们已经在类型检查器中有递归限制，所以这个证明几乎没啥用。当然，我们可以在类型系统中编写 Smallfuck 之类的东西。好吧，最后这个还有一点点炫。\n在类型检查器达到递归限制的大多数情况下，你的程序会无法编译。只有当你真的很向嘲讽 Rust 的类型系统时，那你可以尝试用它编写非常大的 Smallfuck 程序，此时，你可能会达到递归限制。\n如果你正在挑战极限并尝试将整数或其他信息编码为 Rust 中的类型——比如 typenum、peano、type-level-logic——那么本文的证明会有点用，因为基于本文的证明，如果你搞砸了，那么你最终可能会在类型检查器中产生无限循环。\n由于 Peano 问题是不可判定的，如果你真的想以一种甚至可以做逻辑推理的方式滥用一个类型系统，那你就必须有一个图灵完备的类型系统。\n为了进一步了解图灵完备性，我建议查看 Glasgow Haskell Compiler 的手册，特别是关于 typeclasses 的扩展，例如 - XUndecidableInstances  。维基百科也有很多关于可计算性理论和逻辑科学的学习资料。\n最后，该项目的完整源代码可在此处获取，位于 GitHub 上。\nhttps://github.com/sdleffler/tarpit-rs\n原文写于 March 7, 2017，翻译于 May 4, 2022\n翻译用词和表意可能不够精准，如有错误和误翻请联系Ch3nYe或到Rust中文社区提交pr。\n","categories":["翻译"],"tags":["翻译","Rust - Smallfuck - turing-complete"]},{"title":"从python调用rust-使用rust加速你的python代码","url":"/%E4%BB%8Epython%E8%B0%83%E7%94%A8rust-%E4%BD%BF%E7%94%A8rust%E5%8A%A0%E9%80%9F%E4%BD%A0%E7%9A%84python%E4%BB%A3%E7%A0%81/","content":"\n原文链接: http://saidvandeklundert.net/learn/2021-11-06-calling-rust-from-python/\n翻译：ChenYe\n选题：minikiller\n本文由 Rustt 翻译，StudyRust 荣誉推出\n\n 从python调用rust-使用rust加速你的python代码\nPython 是一门很棒的语言！至少，在我看来是这样。它有着丰富的第三方库，可以非常好的完成很多工作。但作为解释型语言，Python 并不是最快的。如果要加速代码运行速度，你可以使用 C/C++ 。这种编程语言上的扩展使得你可以在 Python 中调用这些其他语言写的函数。这种方式提供了更快的执行速度，同时你也不会被全局解释器锁困扰。\n除了使用 C 或 C++ 之外，我们还可以使用 Rust 。Rust 提供了 FFI（Foreign Function Interface 外部函数接口），它允许你将 Rust 函数导出到 C 语言。通过让 Python 使用这些导出的 C 函数，你可以将 Python 和 Rust 缝合在一起。如此一来就可以在需要的地方使用外部的 Rust 程序。\n在本文中，我将介绍一些关于如何从 Python 调用多个 Rust 函数的基本示例。在 Rust 一侧，我将使用 std 中的 ffi，在 Python 一侧，我将仍然使用 ctypes：\n\n 在 Python 中调用 Rust 函数打印一个字符串.\n首先，我们将编写一个打印字符串的 Rust 函数。下图说明具体发生了什么：\n\n在 Python 侧我们做了以下几件事：\n\nRust 导出为 C 编译生成的库文件被导入\n将 string 转换为 bytes\n调用从 Rust 中导入的函数\nPython 侧的 Rust 函数的参数是 UTF-8 编码的 bytes\n\n在 Rust 侧我们做了以下几件事：\n\n创建一个新的库\n通过 Cargo.toml 指明我们要构建一个 Rust 动态库\n用 Rust 编写与 C 兼容的外部函数接口\n读取经由 C 传递来的 Python 侧的输入，类型是 Char *\n编译 Rust 库\n\n完成之后，我们就可以运行代码了！\n Python 侧.\nimport ctypes\n\nrust = ctypes.CDLL(\"target/release/librust_lib.so\")\n\n\nif __name__ == \"__main__\":\n    SOME_BYTES = \"Python says hi inside Rust!\".encode(\"utf-8\")\n    rust.print_string(SOME_BYTES)\n首先，我们导入 ctypes ，然后，我们指定 .so 文件的问题。在我这里，命名为 print_string.py 的 Python 脚本被放在构建 rust 库的目录下。在我运行完 cargo build --release 后，会产生以下目录结构：\nrust_lib&#x2F;\n├── target&#x2F;\n|   ├── lib.rs\n├── target&#x2F;\n│   ├── release&#x2F;\n│       ├── librust_lib.so\n├── print_string.py\n在 Python 代码的 __main__ 部分，我先通过调用 .encode(&quot;utf-8&quot;) 将字符串转换为 bytes 。这一步将得到 UTF-8 编码的 bytes 。然后，我通过 rust.print_string(SOME_BYTES) 调用 Rust 函数，rust 是对我们之前加载的库的引用。print_string 是导出的 Rust 函数名。SOME_BYTES 作为参数传递给目标函数。\n我们还可以以另一种方式为 Rust 函数提供参数。试试将以下内容添加到 print_string_script.py ：\nrust.print_string(\n    ctypes.c_char_p(\"Another way of sending strings to Rust via C.\".encode(\"utf-8\"))\n)\n在这种情况下，我们使用 ctypes.c_char_p 将值传递给 rust 函数。 c_char_p 是一个指向字符串的指针。\n Rust 侧.\n在 Rust 侧，我们先使用 cargo new --lib 创建一个库。然后我们编辑 Cargo.toml 文件：\n[package]\nname = \"rust_from_python\"\nversion = \"0.1.0\"\nauthors = [\"Said van de Klundert\"]\n\n\n[lib]\nname = \"rust_lib\"\ncrate-type = [\"dylib\"]\n这表明我们正在构建一个 Rust 动态库，并且我们将使用 libc 。\n然后，我们编写 lib.rs 文件：\nuse std::ffi::CStr;\nuse std::os::raw::c_char;\nuse std::str;\n\n/// Turn a C-string into a string slice and print to console:\n#[no_mangle]\npub extern \"C\" fn print_string(c_string_ptr: *const c_char) &#123;\n    let bytes = unsafe &#123; CStr::from_ptr(c_string_ptr).to_bytes() &#125;;\n    let str_slice = str::from_utf8(bytes).unwrap();\n    println!(\"&#123;&#125;\", str_slice);\n&#125;\n第一行将 CStr 引入，这是一个表示 C 字符串的借用的结构体，它的文档是这样说的：\nThis type represents a borrowed reference to a nul-terminated array of bytes. It can be constructed safely from a &amp;[u8] slice, or unsafely from a raw *const c_char. It can then be converted to a Rust &amp;str by performing UTF-8 validation, or into an owned CString.\n我们将用它，将 CStr 转换成 Rust 中的 &amp;str 。\n第二行 use std::os::raw::c_char; ，将 c_char 类型引入。\n第三行 use std::os::raw::c_char; ，使得我们能访问 from_utf8 ，使用这个方法将 bytes 转换为 Rust 的 &amp;str 。\n现在，我们看看这个函数：\n#[no_mangle]\npub extern \"C\" fn print_string(c_string_ptr: *const c_char) &#123;\n    let bytes = unsafe &#123; CStr::from_ptr(c_string_ptr).to_bytes() &#125;;\n    let str_slice = str::from_utf8(bytes).unwrap();\n    println!(\"&#123;&#125;\", str_slice);\n&#125;\nextern 关键字用于创建 FFI（Foreign Function Interface）。它可用于调用其他语言的函数或创建允许其他语言调用 Rust 函数接口。\n以下引用来自 book of Rust：\nWe also need to add a #[no_mangle] annotation to tell the Rust compiler not to mangle the name of this function. Mangling is when a compiler changes the name we’ve given a function to a different name that contains more information for other parts of the compilation process to consume but is less human readable. Every programming language compiler mangles names slightly differently, so for a Rust function to be nameable by other languages, we must disable the Rust compiler’s name mangling.\n函数参数类型指定为 c_string_ptr: *const c_char 。其中，*const 表示原始指针，c_char 是 C 语言 char 类型。送一组合起来，就是一个指向 c_char 的原始指针。\n为了解引用这个原始指针，我们使用 unsafe 代码块。在这个代码块中，我们使用 CStr::from_ptr ，它会使用 CStr 包装器包装由参数传进来的指针。之后，我们调用 to_bytes() 将 C 字符串转换为 byte 切片。这些字符都将存储在 bytes 变量中。\n将字符切片传给 str::from_utf8() 并解包返回值，就会得到一个 &amp;str 类型的值，最后将其打印出来。\n当上述一切都写好之后，使用命令 cargo build --release 就可以得到以下目录结构：\nrust_lib&#x2F;\n├── target&#x2F;\n|   ├── lib.rs\n├── target&#x2F;\n│   ├── release&#x2F;\n│       ├── librust_lib.so\n├── print_string.py\n从 rust_lib 目录，我们运行 Python 脚本：\nroot@rust:&#x2F;python&#x2F;rust&#x2F;rust_lib# .&#x2F;print_string.py  \nPython says hi inside Rust!\n此时，我们就已经从 Python 调用了一个 Rust 函数并将一个字符串打印到控制台。\n 在 Python 中调用一个 Rust 函数打印一个整数.\n当从 Python 向 Rust 传递参数时，我们需要考虑 Python、C 和 Rust 中使用的类型。现在我们试试在屏幕上打印一个整数。首先，我们创建 Python 脚本：\nimport ctypes\n\nrust_lib &#x3D; ctypes.CDLL(&quot;target&#x2F;release&#x2F;librust_lib.so&quot;)\n\nif __name__ &#x3D;&#x3D; &quot;__main__&quot;:\n    SOME_BYTES &#x3D; (2).to_bytes(4, byteorder&#x3D;&quot;little&quot;)\n    rust_lib.print_int(SOME_BYTES)\n接下来，我们在 Rust 端并将以下内容添加到我们的 lib.rs 中：\nuse std::os::raw::c_int;\n\n// Turn a C-int into a &amp;i32 and print to console:\n#[no_mangle]\npub extern \"C\" fn print_int(c_int_ptr: *const c_int) &#123;\n    let int_ptr = unsafe &#123; c_int_ptr.as_ref().unwrap() &#125;;\n    println!(\"Python gave us number &#123;&#125;\", int_ptr);\n&#125;\n我们将另一种数据类型 c_int 引入 。该函数将指向 C 整型的指针作为参数。在我们执行命令 cargo build --release 之后，我们可以运行 Python 脚本：\nroot@rust:&#x2F;python&#x2F;rust&#x2F;rust_lib# python3 print_number.py  \nPython gave us number 2\nRust、Python 和 C 中有许多类型。这些类型之间的对应可能会把人搞的很头疼！\n 从 Python 调用多类型的 Rust 函数\n现在，我们使用 Python 调用一个 start_procedure 。为了专注研究跨语言调用，该函数仅仅获取一个结构并返回另一个结构。在 Python 侧，我们使用 Pydantic basemodel 来创建 Rust 函数所需的输入。Pydantic basemodel 将具有与 Rust 结构相同的字段。我们对 Rust 的返回值做同样的事情。我们创建了一个 Pydantic basemodel ，它是 Rust struct 在 Python 侧的镜像。 Rust struct 和 Pydantic basemodel 将包含多种不同类型的字段。这是我们将以最简单的方式（至少在我看来）处理的这件事：使用 C 语言中的 Char *。\n\n上图说明了该过程会发生什么。在 Rust 和 Python 之间，JSON 字符串用于传递值。在 Rust 侧，我们将 JSON 解析到相应的结构中。在 Python 侧，我们将 JSON 加载到相应的 Pydantic basemodel 中。这样做的优点是我们只需要在 C 中使用 Char *，而不必使用 C 结构体或 C 中的其他类型。\n Python 侧.\n我们将使用的 Python 脚本如下：\nimport ctypes\nfrom pydantic import BaseModel\nfrom typing import List\n\nrust = ctypes.CDLL(\"target/release/librust_lib2.so\")\n\n\nclass ProcedureInput(BaseModel):\n    timeout: int\n    retries: int\n    host_list: List[str]\n    action: str\n    job_id: int\n\n\nclass ProcedureOutput(BaseModel):\n    job_id: int\n    result: str\n    message: str\n    failed_hosts: List[str]\n\n\nif __name__ == \"__main__\":\n    procedure_input = ProcedureInput(\n        timeout=10,\n        retries=3,\n        action=\"reboot\",\n        host_list=[\"server1\", \"server2\"],\n        job_id=1,\n    )\n\n    ptr = rust.start_procedure(procedure_input.json().encode(\"utf-8\"))\n\n    returned_bytes = ctypes.c_char_p(ptr).value\n\n    procedure_output = ProcedureOutput.parse_raw(returned_bytes)\n    print(procedure_output.json(indent=2))\n我们先加载了目标库，然后定义了两个类。这脸各类是 Pydantic basemodels ，它在运行时强制执行类型提示。ProcedureInput 类是 Rust 函数的参数，ProcedureOutput 类是我们希望从 Rust 函数中返回的东西。\n完成定义后，我们实例化一个 ProcedureInput 对象。我们使用 rust.start_procedure 调用 Rust 函数。当我们进行 Rust 函数调用时，procedure_input.json().encode(&quot;utf-8&quot;) 这个表达式将会以 JSON 字符串的形式输出类实例的字段，并将字符串转成 bytes 。\n我们会收到从 Rust 中返回的 ptr 。接下来，将其转换为 bytes，并且使用 parse_raw 方法将使我们能够从这些 bytes 构建 ProcedureOutput 实例。\n当我们运行 Python 代码时，将得到以下输出：\nroot@rust:&#x2F;python&#x2F;rust&#x2F;rust_lib# python3 call_rust_function.py \n&#123;\n  &quot;job_id&quot;: 1,\n  &quot;result&quot;: &quot;success&quot;,\n  &quot;message&quot;: &quot;1 host failed&quot;,\n  &quot;failed_hosts&quot;: [\n    &quot;server1&quot;\n  ]\n&#125;\n Rust 侧.\n以下是 Rust 侧的代码：\nextern crate serde;\nextern crate serde_json;\n\nuse serde::&#123;Deserialize, Serialize&#125;;\nuse std::ffi::CStr;\nuse std::ffi::CString;\nuse std::os::raw::c_char;\nuse std::str;\n\n#[derive(Debug, Serialize, Deserialize)]\nstruct ProcedureInput &#123;\n    timeout: u8,\n    retries: u8,\n    host_list: Vec&lt;String>,\n    action: String,\n    job_id: i32,\n&#125;\n\n#[derive(Serialize, Deserialize)]\nstruct ProcedureOutput &#123;\n    result: String,\n    message: String,\n    failed_hosts: Vec&lt;String>,\n    job_id: i32,\n&#125;\n\n#[no_mangle]\npub extern \"C\" fn start_procedure(c_string_ptr: *const c_char) -> *mut c_char &#123;\n    let bytes = unsafe &#123; CStr::from_ptr(c_string_ptr).to_bytes() &#125;;\n    let string = str::from_utf8(bytes).unwrap();\n    let model: ProcedureInput = serde_json::from_str(string).unwrap();\n    let result = long_running_task(model);\n    let result_json = serde_json::to_string(&amp;result).unwrap();\n    let c_string = CString::new(result_json).unwrap();\n    c_string.into_raw()\n&#125;\n\nfn long_running_task(model: ProcedureInput) -> ProcedureOutput &#123;\n    let result = ProcedureOutput &#123;\n        result: \"success\".to_string(),\n        message: \"1 host failed\".to_string(),\n        failed_hosts: vec![\"server1\".to_string()],\n        job_id: model.job_id,\n    &#125;;\n    return result;\n&#125;\n在这段代码中，我们先定义了两个结构体作为 Pydantic basemodels 在 Rust 侧的镜像：\n#[derive(Debug, Serialize, Deserialize)]\nstruct ProcedureInput &#123;\n    timeout: u8,\n    retries: u8,\n    host_list: Vec&lt;String>,\n    action: String,\n    job_id: i32,\n&#125;\n\n#[derive(Serialize, Deserialize)]\nstruct ProcedureOutput &#123;\n    result: String,\n    message: String,\n    failed_hosts: Vec&lt;String>,\n    job_id: i32,\n&#125;\n然后，定义了 start_procedure 函数：\n#[no_mangle]\npub extern \"C\" fn start_procedure(c_string_ptr: *const c_char) -> *mut c_char &#123;\n    let bytes = unsafe &#123; CStr::from_ptr(c_string_ptr).to_bytes() &#125;;\n    let string = str::from_utf8(bytes).unwrap();\n    let model: ProcedureInput = serde_json::from_str(string).unwrap();\n    let result = long_running_task(model);\n    let result_json = serde_json::to_string(&amp;result).unwrap();\n    let c_string = CString::new(result_json).unwrap();\n    c_string.into_raw()\n&#125;\n与之前相似，我们先从原始指针构建一个字符串。然后我们得到了从 Python 传过来的 JSON 字符串，我们将其解析为 ProcedureInput 结构体。我们将该结构传递给 long_running_task 方法，它会为我们返回一个 ProcedureOutput 对象。使用 serde_json::to_string 可以将结构体转为 JSON 字符串。我们使用 CString::new 从字节容器创建新的兼容 C 的字符串。使用 into_raw() 方法将 c_string 的所有权转移给 C 中的调用者。into_raw() 方法返回一个指针，我们之前在 Python 侧用它来读取返回值。\n 从 Python 调用带有内存泄漏的 Rust 函数\n如果我们把 start_procedure 放在一个 while 循环里，让它运行一段时间，我们可以看到进程会逐渐开始消耗越来越多的内存。是因为从 Rust 返回的值没有被清理。\n通过对 Python 脚本进行以下更改，我们将持续调用 start_procedure ：\nwhile True:\n    ptr &#x3D; rust.start_procedure(procedure_input.json().encode(&quot;utf-8&quot;))\n    returned_bytes &#x3D; ctypes.c_char_p(ptr).value\n    procedure_output &#x3D; ProcedureOutput.parse_raw(returned_bytes)\n    print(procedure_output.json(indent&#x3D;2))\n调用脚本，我们可以看到 Python 脚本的内存使用量在慢慢增加。\n 修复内存泄露问题\n为了解决这个内存泄漏问题，我们首先需要在 Rust 中创建一个清理内存的函数：\n#[no_mangle]\npub extern \"C\" fn free_mem(c_string_ptr: *mut c_char) &#123;\n    unsafe &#123; CString::from_raw(c_string_ptr) &#125;;\n&#125;\n此函数将使用 from_raw 获取已转移到 C 的 CString 的所有权。当函数结束时，变量将不被任何所有者拥有，值被删除，释放内存。\n我们需要在 Python 侧调用这个函数。 free_mem 函数的输入应该是 Rust 返回给 Python 的值。\n以下 Python 可以连续运行而不会泄漏内存：\nwhile True:\n    ptr = rust.start_procedure(procedure_input.json().encode(\"utf-8\"))\n    returned_bytes = ctypes.c_char_p(ptr).value\n    procedure_output = ProcedureOutput.parse_raw(returned_bytes)\n    print(procedure_output)\n    rust.free_mem(ptr)\nstart_procedure 返回的值就是我们传递给 free_mem 的值。另一件需要注意的事情是我们在处理完值后调用 free_mem。\n在下面的示例中，我们在 Python 代码中使用它之前释放该值：\nptr &#x3D; rust.start_procedure(procedure_input.json().encode(&quot;utf-8&quot;))\nreturned_bytes &#x3D; ctypes.c_char_p(ptr).value\nrust.free_mem(ptr)\nprocedure_output &#x3D; ProcedureOutput.parse_raw(returned_bytes)\n在让 Rust 释放内存之后，我们尝试在 Python 侧读取相同的字节。当我们运行这段代码时，我们完成了一次 double free ：\nroot@rust:&#x2F;python&#x2F;rust&#x2F;rust_lib# python3 call_rust_continuously_free_mem.py\njob_id&#x3D;1 result&#x3D;&#39;success&#39; message&#x3D;&#39;1 host failed&#39; failed_hosts&#x3D;[&#39;server1&#39;]\nfree(): double free detected in tcache 2\nAborted\n 总结\n从 Python 中使用 Rust 是我这段时间一直想尝试的事情。即使我已经研究 Rust 一段时间了，完全转向 Rust 对我现在参与的任何项目都没有任何意义。在某些情况下，我使用的库在 Rust 中不可用，而在其他情况下，我正在工作的项目太大而无法在 Rust 中重写。另外，其实我对于 Rust 还处于学习阶段。\n像这样从 Python 调用 Rust 为我铺平了道路：\n\n将 Rust 合并到现有的 Python 项目中并从小处着手\n对现有的 Python 项目更有信心，知道如果速度真的成为问题，我可以用 Rust 来加速\n同时使用 Python 和 Rust！\n\nRust 社区的人们在使用 Rust 来加速 Python 方面做出了很多努力。在很多情况下，你会看到这些项目使用 pyo3。这个库提供了 “Python 的 Rust 绑定，包括用于创建原生 Python 扩展库的工具” 。使用 pyo3 的一个例子是 polars 。\n这篇文章中的例子可以在这里找到。\n注意：写这篇文章时，我使用了 CPython 3.9.2、Pydantic 版本 1.8.2 和 Rust 1.56.0。\n","categories":["翻译"],"tags":["Rust","翻译"]},{"title":"如何用 Rust 实现朴素贝叶斯分类器","url":"/%E5%A6%82%E4%BD%95%E7%94%A8%20Rust%20%E5%AE%9E%E7%8E%B0%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%88%86%E7%B1%BB%E5%99%A8/","content":"\n原文链接: https://www.freecodecamp.org/news/implement-naive-bayes-with-rust/\n翻译：Ch3nYe\n选题：Ch3nYe\n本文由 Rustt 翻译，StudyRust 荣誉推出\n\n 如何用 Rust 实现朴素贝叶斯分类器\n我想精进我的 Rust 编程技能，同时也帮助你磨练技能。所以我决定写一系列关于 Rust 编程语言的文章。\n在实际使用 Rust 写东西的时候，我能学到很广泛的技术概念。在本期中，我们将学习如何使用 Rust 实现朴素贝叶斯分类器。\n您可能会在本文中遇到一些不熟悉的术语或概念。不要气馁，如果你有时间可以自行学习，但无论如何，希望你不要偏离本文的主要思路。\n 什么是朴素贝叶斯分类器\n朴素贝叶斯分类器是一个基于贝叶斯理论的机器学习算法。贝叶斯理论是一种通过给定一些数据 D ，来更新一个假设 H 的概率的方法。\n数学表达为：\nP(H∣D)=P(D∣H)P(H)P(D)P(H \\mid D)=\\frac{P(D \\mid H) P(H)}{P(D)}\nP(H∣D)=P(D)P(D∣H)P(H)​\nP(H∣D)P(H|D)P(H∣D) 是给出数据 D 假设 H 成立的概率。\n如果我们统计更多数据，就可以根据这些数据更新P(H∣D)P(H|D)P(H∣D) 。\n朴素贝叶斯模型基于一个大假设：数据集中是否存在数据点与该数据集中已经存在的数据无关（参考）。也就是说，每条数据之间相互独立。\n显然，这个假设是比较弱的，现实中难以完全成立。但它仍然很有用，它允许我们创建一个用起来还不错的的高效分类器（参考）。\n对朴素贝叶斯的描述就停在这里，本文的重点是练习 Rust 。\n如果您想了解有关该算法的更多信息，这里有一些资源：\n\nJosh Starmer 的视频讲解非常好.\nJoel Grus 在《Data Science from Scratch》这本书中关于贝叶斯一节的描述是本文实现的启发。\n如果你更喜欢数学的形式化定义, try section 6.6.3 of *The Elements of Statisical Learning.*\n一篇关于算法工作原理的有用文章\n\n朴素贝叶斯分类器的典型应用是垃圾邮件分类器。这就是我们要实现的东西。代码在这：\nhttps://github.com/josht-jpg/shaking-off-the-rust\n我们从使用 Cargo 创建一个新的库开始：\ncargo new naive_bayes --lib\ncd naive_bayes\n Tokenization in Rust\n我们的分类器会将邮件消息内容作为输入并返回其是否为垃圾邮件的分类。\n为了处理我们收到的消息，我们需要对其进行tokenize（分词）。我们的词汇表将是一堆小写的单词，忽略顺序和重复单词。Rust 的 std::collections::HashSet 结构正合适来实现词汇表。\n我们将编写分词的函数将需要使用 regex crate。确保在 Cargo.toml 文件中包含以下依赖项：\n[dependencies]\nregex = \"^1.5.4\"\ntokenize 分词函数：\n// lib.rs\n\n// We'll need HashMap later\nuse std::collections::&#123;HashMap, HashSet&#125;;\n\nextern crate regex;\nuse regex::Regex;\n\npub fn tokenize(lower_case_text: &amp;str) -> HashSet&lt;&amp;str> &#123;\n    Regex::new(r\"[a-z0-9']+\")\n        .unwrap()\n        .find_iter(lower_case_text)\n        .map(|mat| mat.as_str())\n        .collect()\n&#125;\n此函数使用正则表达式匹配所有数字和小写字母。每当我们遇到不同类型的符号（通常是空格或标点符号）时，我们就会拆分输入并将自上次拆分后遇到的所有数字和字母组合在一起（你可以在这里阅读有关 Rust 正则表达式的更多内容）。也就是说，我们正在识别和分割输入文本中的单词。\n 构造结构体\n使用 struct 来表示消息是很好的方法。struct 将包含消息文本的字符串切片，以及指示消息是否为垃圾邮件的布尔值：\npub struct Message&lt;'a> &#123;\n    pub text: &amp;'a str,\n    pub is_spam: bool,\n&#125;\n'a 是声明周期注释。如果你不熟悉生命周期，我推荐你阅读 section 10.3 of The Rust Programming Language Book 。\n 什么是拉普拉斯平滑？\n假设——在我们的训练数据中——单词 fubar 出现在一些非垃圾邮件中，但没有出现在任何垃圾邮件中。此时，朴素贝叶斯分类器把任何包含单词 fubar（参考）的消息认定为非垃圾邮件，也就是说该消息是垃圾邮件的概率为 0 。\n显然，仅仅因为它还没有发生就给它分配 0 的概率是不合适的。\n加入拉普拉斯平滑就是用来解决这个问题的，将一个常数 α\\alphaα 加在每个单词出现的次数统计上。我们来观察一下拉普拉斯平滑常数加入前后，在垃圾邮件中看到单词 w 的概率为：\nP(w∣S)=number of spam messages containing wtotal number of spamsP(w|S) = \\frac{number\\ of\\ spam\\ messages\\ containing\\ w}{total\\ number\\ of\\ spams}\nP(w∣S)=total number of spamsnumber of spam messages containing w​\n使用拉普拉斯平滑后就是：\nP(w∣S)=α+number of spam messages containing w2α+total number of spamsP(w|S) = \\frac{\\alpha + number\\ of\\ spam\\ messages\\ containing\\ w}{2\\alpha + total\\ number\\ of\\ spams}\nP(w∣S)=2α+total number of spamsα+number of spam messages containing w​\n具体到分类器结构体就是：\npub struct NaiveBayesClassifier &#123;\n    pub alpha: f64,\n    pub tokens: HashSet&lt;String>,\n    pub token_ham_counts: HashMap&lt;String, i32>,\n    pub token_spam_counts: HashMap&lt;String, i32>,\n    pub spam_messages_count: i32,\n    pub ham_messages_count: i32,\n&#125;\nNaiveBayesClassifier 的实现将围绕一个 train 方法和一个 predict 方法。\n 如何训练分类器\ntrain 方法将接收多个 Message ，并循环对每个 Message 进行以下步骤：\n\n检查邮件是否为垃圾邮件并相应地更新 spam_messages_count 或 ham_messages_count。我们为此创建辅助函数 increment_message_classifications_count 。\n使用 tokenize 函数将消息分词。\n遍历消息中的每个单词，然后：\n将单词插入词汇表 HashSet ，然后更新token_spam_counts 或 token_ham_counts 。我们为此创建辅助函数 increment_token_count\n\n下面是 train 方法的伪代码。如果你愿意，尝试将伪代码转换为 Rust，然后再查看下面的实现。\nimplementation block for NaiveBayesClassifier &#123;\n\n\ttrain(self, messages) &#123;\n\t\tfor each message in messages &#123;\n\t\t\tself.increment_message_classifications_count(message)\n\t\t\t\n\t\t\tlowercase_text = to_lowercase(message.text)\n\t\t\tfor each token in tokenize(lowercase_text) &#123;\n\t\t\t\tself.tokens.insert(tokens)\n\t\t\t\tself.increment_token_count(token, message.is_spam)\n\t\t\t&#125;\t\t\t\n\t\t&#125;\n\t&#125;\n\n\tincrement_message_classifications_count(self, message) &#123;\n\t\tif message.is_spam &#123;\n\t\t\tself.spam_messages_count = self.spam_messages_count + 1\n\t\t&#125; else &#123;\n\t\t\tself.ham_messages_count = self.ham_messages_count + 1\n\t\t&#125;\n\t&#125;\n\n\tincrement_token_count(&amp;mut self, token, is_spam) &#123;\n\t\tif token is not a key of self.token_spam_counts &#123;\n\t\t\tinsert record with key=token and value=0 into self.token_spam_counts\n\t\t&#125;\n\n\t\tif token is not a key of self.token_ham_counts &#123;\n\t\t\tinsert record with key=token and value=0 into self.token_ham_counts\n\t\t&#125;\n\n\t\tif is_spam &#123;\n\t\t\tself.token_spam_counts[token] = self.token_spam_counts[token] + 1\n\t\t&#125; else &#123;\n\t\t\tself.token_ham_counts[token] = self.token_ham_counts[token] + 1\n\t\t&#125;\n\t&#125;\n\n&#125;\n下面是 Rust 的实现：\nimpl NaiveBayesClassifier &#123;\n    pub fn train(&amp;mut self, messages: &amp;[Message]) &#123;\n        for message in messages.iter() &#123;\n            self.increment_message_classifications_count(message);\n            for token in tokenize(&amp;message.text.to_lowercase()) &#123;\n                self.tokens.insert(token.to_string());\n                self.increment_token_count(token, message.is_spam)\n            &#125;\n        &#125;\n    &#125;\n\n    fn increment_message_classifications_count(&amp;mut self, message: &amp;Message) &#123;\n        if message.is_spam &#123;\n            self.spam_messages_count += 1;\n        &#125; else &#123;\n            self.ham_messages_count += 1;\n        &#125;\n    &#125;\n\n    fn increment_token_count(&amp;mut self, token: &amp;str, is_spam: bool) &#123;\n        if !self.token_spam_counts.contains_key(token) &#123;\n            self.token_spam_counts.insert(token.to_string(), 0);\n        &#125;\n\n        if !self.token_ham_counts.contains_key(token) &#123;\n            self.token_ham_counts.insert(token.to_string(), 0);\n        &#125;\n\n        if is_spam &#123;\n            self.increment_spam_count(token);\n        &#125; else &#123;\n            self.increment_ham_count(token);\n        &#125;\n    &#125;\n\n    fn increment_spam_count(&amp;mut self, token: &amp;str) &#123;\n        *self.token_spam_counts.get_mut(token).unwrap() += 1;\n    &#125;\n\n    fn increment_ham_count(&amp;mut self, token: &amp;str) &#123;\n        *self.token_ham_counts.get_mut(token).unwrap() += 1;\n    &#125;\n&#125;\n请注意，在 HashMap 中增加一个值是非常耗时的。新手 Rust 程序员很难理解下面这行代码在做什么：\n*self.token_spam_counts.get_mut(token).unwrap() += 1\n为了使代码更明确，我创建了 increment_spam_count 和 increment_ham_count 函数。但我对此并不满意——这仍然很麻烦。如果您对更好的方法有建议，请与我联系。\n 如何使用分类器做预测\npredict 方法接手一个字符串切片，返回模型对于该消息是否为垃圾邮件的预测结果。\n我们创建两个辅助函数 probabilities_of_message 和 robabilites_of_token 来完成 predict 的任务。\nprobabilities_of_message returns P(Message|Spam) and P(Message|ham)\nprobabilities_of_token returns P(Token|Spam) and P(Token|ham)\n计算输入消息是垃圾邮件的概率需要将每个单词在垃圾邮件中出现的概率相乘。\n概率是介于 0 和 1 之间的浮点数，将许多概率相乘会导致下溢（参考）。这是因为当计算产生的数字小于计算机可以准确存储的数字（请参阅这里和这里）。因此，我们将使用对数和指数将任务转换为一系列加法：\nΠi=0npi=exp⁡(∑i=0nlog⁡(pi))\\Pi_{i=0}^{n} p_{i}=\\exp \\left(\\sum_{i=0}^{n} \\log \\left(p_{i}\\right)\\right)\nΠi=0n​pi​=exp(i=0∑n​log(pi​))\n因为对于任何实数 a 和 b，\nab=exp(log(ab))=exp(log(a)+log(b))ab = exp(log(ab))=exp(log(a)+log(b))\nab=exp(log(ab))=exp(log(a)+log(b))\n我将再次先给出 predict 方法的伪代码：\nimplementation block for NaiveBayesCalssifier &#123;\n\t/*...*/\n\n\tpredict(self, text) &#123;\n\t\tlower_case_text = to_lowercase(text)\n\t\tmessage_tokens = tokenize(text)\n\t\t(prob_if_spam, prob_if_ham) = self.probabilities_of_message(message_tokens)\n\t\treturn prob_if_spam / (prob_if_spam + prob_if_ham)\n\t&#125;\n\t\n\tprobabilities_of_message(self, message_tokens) &#123;\n\t\tlog_prob_if_spam = 0\n\t\tlog_prob_if_ham = 0\n\n\t\tfor each token in self.tokens &#123;\n\t\t\t(prob_if_spam, prob_if_ham) = self.probabilites_of_token(token)\n\n\t\t\tif message_tokens contains token &#123;\n\t\t\t\tlog_prob_if_spam = log_prob_if_spam + ln(prob_if_spam)\n\t\t\t\tlog_prob_if_ham = log_prob_if_ham + ln(prob_if_ham)\n\t\t\t&#125; else &#123;\n\t\t\t\tlog_prob_if_spam = log_prob_if_spam + ln(1 - prob_if_spam)\n\t\t\t\tlog_prob_if_ham = log_prob_if_ham + ln(1 - prob_if_ham)\n\t\t\t&#125;\n\t\t&#125;\n\n\t\tprob_if_spam = exp(log_prob_if_spam)\n\t\tprob_if_ham = exp(log_prob_if_ham)\n\n\t\treturn (prob_if_spam, prob_if_ham)\n\t&#125;\n\n\tprobabilites_of_token(self, token) &#123;\n\t\tprob_of_token_spam = (self.token_spam_counts[token] + self.alpha) \n\t\t\t\t\t\t/ (self.spam_messages_count + 2 * self.alpha)\n        \n\t\tprob_of_token_ham = (self.token_ham_counts[token] + self.alpha) \n\t\t\t\t\t\t/ (self.ham_messages_count + 2 * self.alpha)\n\n\t\treturn (prob_of_token_spam, prob_of_token_ham)\n\t&#125;\n\t\n\t\n&#125;\nRust 的具体实现：\nimpl NaiveBayesClassifier &#123;\n\n\t\t/*...*/\n\n\tpub fn predict(&amp;self, text: &amp;str) -> f64 &#123;\n        let lower_case_text = text.to_lowercase();\n        let message_tokens = tokenize(&amp;lower_case_text);\n        let (prob_if_spam, prob_if_ham) = self.probabilities_of_message(message_tokens);\n\n        return prob_if_spam / (prob_if_spam + prob_if_ham);\n    &#125;\n\n    fn probabilities_of_message(&amp;self, message_tokens: HashSet&lt;&amp;str>) -> (f64, f64) &#123;\n        let mut log_prob_if_spam = 0.;\n        let mut log_prob_if_ham = 0.;\n\n        for token in self.tokens.iter() &#123;\n            let (prob_if_spam, prob_if_ham) = self.probabilites_of_token(&amp;token);\n\n            if message_tokens.contains(token.as_str()) &#123;\n                log_prob_if_spam += prob_if_spam.ln();\n                log_prob_if_ham += prob_if_ham.ln();\n            &#125; else &#123;\n                log_prob_if_spam += (1. - prob_if_spam).ln();\n                log_prob_if_ham += (1. - prob_if_ham).ln();\n            &#125;\n        &#125;\n\n        let prob_if_spam = log_prob_if_spam.exp();\n        let prob_if_ham = log_prob_if_ham.exp();\n\n        return (prob_if_spam, prob_if_ham);\n    &#125;\n\n    fn probabilites_of_token(&amp;self, token: &amp;str) -> (f64, f64) &#123;\n        let prob_of_token_spam = (self.token_spam_counts[token] as f64 + self.alpha)\n            / (self.spam_messages_count as f64 + 2. * self.alpha);\n\n        let prob_of_token_ham = (self.token_ham_counts[token] as f64 + self.alpha)\n            / (self.ham_messages_count as f64 + 2. * self.alpha);\n\n        return (prob_of_token_spam, prob_of_token_ham);\n    &#125;\n&#125;\n 如何测试分类器\n让我们对模型做个测试。下面的代码中的样例手动打上了分类标签，然后检查我们的模型是否给出了相同的结果。\n检查代码逻辑是很有必要的，你可以将代码粘贴到 lib.rs 文件的底部以检查你的代码是否有效。\n// ...lib.rs\n\npub fn new_classifier(alpha: f64) -> NaiveBayesClassifier &#123;\n    return NaiveBayesClassifier &#123;\n        alpha,\n        tokens: HashSet::new(),\n        token_ham_counts: HashMap::new(),\n        token_spam_counts: HashMap::new(),\n        spam_messages_count: 0,\n        ham_messages_count: 0,\n    &#125;;\n&#125;\n\n#[cfg(test)]\nmod tests &#123;\n    use super::*;\n\n    #[test]\n    fn naive_bayes() &#123;\n        let train_messages = [\n            Message &#123;\n                text: \"Free Bitcoin viagra XXX christmas deals 😻😻😻\",\n                is_spam: true,\n            &#125;,\n            Message &#123;\n                text: \"My dear Granddaughter, please explain Bitcoin over Christmas dinner\",\n                is_spam: false,\n            &#125;,\n            Message &#123;\n                text: \"Here in my garage...\",\n                is_spam: true,\n            &#125;,\n        ];\n\n        let alpha = 1.;\n        let num_spam_messages = 2.;\n        let num_ham_messages = 1.;\n\n        let mut model = new_classifier(alpha);\n        model.train(&amp;train_messages);\n\n        let mut expected_tokens: HashSet&lt;String> = HashSet::new();\n        for message in train_messages.iter() &#123;\n            for token in tokenize(&amp;message.text.to_lowercase()) &#123;\n                expected_tokens.insert(token.to_string());\n            &#125;\n        &#125;\n\n        let input_text = \"Bitcoin crypto academy Christmas deals\";\n\n        let probs_if_spam = [\n            1. - (1. + alpha) / (num_spam_messages + 2. * alpha), // \"Free\"  (not present)\n            (1. + alpha) / (num_spam_messages + 2. * alpha),      // \"Bitcoin\"  (present)\n            1. - (1. + alpha) / (num_spam_messages + 2. * alpha), // \"viagra\"  (not present)\n            1. - (1. + alpha) / (num_spam_messages + 2. * alpha), // \"XXX\"  (not present)\n            (1. + alpha) / (num_spam_messages + 2. * alpha),      // \"christmas\"  (present)\n            (1. + alpha) / (num_spam_messages + 2. * alpha),      // \"deals\"  (present)\n            1. - (1. + alpha) / (num_spam_messages + 2. * alpha), // \"my\"  (not present)\n            1. - (0. + alpha) / (num_spam_messages + 2. * alpha), // \"dear\"  (not present)\n            1. - (0. + alpha) / (num_spam_messages + 2. * alpha), // \"granddaughter\"  (not present)\n            1. - (0. + alpha) / (num_spam_messages + 2. * alpha), // \"please\"  (not present)\n            1. - (0. + alpha) / (num_spam_messages + 2. * alpha), // \"explain\"  (not present)\n            1. - (0. + alpha) / (num_spam_messages + 2. * alpha), // \"over\"  (not present)\n            1. - (0. + alpha) / (num_spam_messages + 2. * alpha), // \"dinner\"  (not present)\n            1. - (1. + alpha) / (num_spam_messages + 2. * alpha), // \"here\"  (not present)\n            1. - (1. + alpha) / (num_spam_messages + 2. * alpha), // \"in\"  (not present)\n            1. - (1. + alpha) / (num_spam_messages + 2. * alpha), // \"garage\"  (not present)\n        ];\n\n        let probs_if_ham = [\n            1. - (0. + alpha) / (num_ham_messages + 2. * alpha), // \"Free\"  (not present)\n            (1. + alpha) / (num_ham_messages + 2. * alpha),      // \"Bitcoin\"  (present)\n            1. - (0. + alpha) / (num_ham_messages + 2. * alpha), // \"viagra\"  (not present)\n            1. - (0. + alpha) / (num_ham_messages + 2. * alpha), // \"XXX\"  (not present)\n            (1. + alpha) / (num_ham_messages + 2. * alpha),      // \"christmas\"  (present)\n            (0. + alpha) / (num_ham_messages + 2. * alpha),      // \"deals\"  (present)\n            1. - (1. + alpha) / (num_ham_messages + 2. * alpha), // \"my\"  (not present)\n            1. - (1. + alpha) / (num_ham_messages + 2. * alpha), // \"dear\"  (not present)\n            1. - (1. + alpha) / (num_ham_messages + 2. * alpha), // \"granddaughter\"  (not present)\n            1. - (1. + alpha) / (num_ham_messages + 2. * alpha), // \"please\"  (not present)\n            1. - (1. + alpha) / (num_ham_messages + 2. * alpha), // \"explain\"  (not present)\n            1. - (1. + alpha) / (num_ham_messages + 2. * alpha), // \"over\"  (not present)\n            1. - (1. + alpha) / (num_ham_messages + 2. * alpha), // \"dinner\"  (not present)\n            1. - (0. + alpha) / (num_ham_messages + 2. * alpha), // \"here\"  (not present)\n            1. - (0. + alpha) / (num_ham_messages + 2. * alpha), // \"in\"  (not present)\n            1. - (0. + alpha) / (num_ham_messages + 2. * alpha), // \"garage\"  (not present)\n        ];\n\n        let p_if_spam_log: f64 = probs_if_spam.iter().map(|p| p.ln()).sum();\n        let p_if_spam = p_if_spam_log.exp();\n\n        let p_if_ham_log: f64 = probs_if_ham.iter().map(|p| p.ln()).sum();\n        let p_if_ham = p_if_ham_log.exp();\n\n        // P(message | spam) / (P(messge | spam) + P(message | ham)) rounds to 0.97\n        assert!((model.predict(input_text) - p_if_spam / (p_if_spam + p_if_ham)).abs() &lt; 0.000001);\n    &#125;\n&#125;\n现在可以通过 cargo test 进行测试，如果你成功通过了测试，你用 Rust 实现的朴素贝叶斯模型没有问题了！\n感谢你看到这里。如果您有任何问题或建议，请随时与我们联系。\n References\n\nGrus, J. (2019). Data Science from Scratch: First Principles with Python, 2nd edition. O’Reilly Media.\nDowney, A. (2021). Think Bayes: Bayesian Statistics in Python, 2nd edition. O’Reilly Media.\nMurphy, K. (2012). Machine Learning: A Probabilistic Perspective. MIT Press.\nDhinakaran, V. (2017). Rust Cookbook. Packt.\nNg, A. (2018). Stanford CS229: Lecture 5 - GDA &amp; Naive Bayes.\nBurden, R. Faires, J. Burden, A. (2015). Numerical Analysis, 10th edition. Brooks Cole.\n*Underflow.* Technopedia.\n\n","categories":["翻译"],"tags":["Rust","翻译","Bayes"]},{"title":"关于用Rust写算法题的姿势探讨以及Windows11调试Rust的环境配置","url":"/%E5%85%B3%E4%BA%8E%E7%94%A8Rust%E5%86%99%E7%AE%97%E6%B3%95%E9%A2%98%E7%9A%84%E5%A7%BF%E5%8A%BF%E6%8E%A2%E8%AE%A8%E4%BB%A5%E5%8F%8AWindows11%E8%B0%83%E8%AF%95Rust%E7%9A%84%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE/","content":" 0x0 前言\n用Rust写算法应该怎么组织目录结构比较好，我到各种论坛上找了都没见到有人谈这个话题，可能大家都有自己的组织方式，我作为Rust新人就有点懵. 幸运的是我在GitHub上找到一个大哥维护的LeetCode in Rust项目感觉非常不错，就抄了他的结构，本文做一个简要叙述，方便Rust新人通过写算法题的方式上手Rust. 可能我的方法不是Rust写算法题组织文件的最优解，但对我来说足够了，也希望对你有帮助.\n首先来配置一下Rust的开发和调试环境.\n 0x1 Rust开发和调试环境配置\nClion自带的Bundled MinGW就可以编译和调试Rust\n\n安装Clion\nSettings-Build, Execution, Deployment-toolchains 自动识别\nClion Rust插件\n安装rust\n\n自定义安装 选2\n输入指定toolchain x86_64-pc-windows-gnu\n其他的默认就好\n\n\n\n如果已经装过了，可以通过rustup default 查看默认toolchain，\n通过  rustup default stable-x86_64-pc-windows-gnu  修改默认toolchain.\n结束.\n 0x2 Rust工程目录简述\n 主要目录\nD:\\RustProject\\LeetCode\n├─src // 源码目录\n│  ├─problem_0164_maximum_gap // 每个算法问题目录\n│  │    ├─mod.rs // 模块声明 or 模块接口\n│  │    ├─buckets_and_pigeonhole.rs // 解法1\n│  │    └─solve2.rs // 解法2\n│  ├─problem_0372_super_pow\n│  ├─…\n│  ├─problem_xxxx_k_radius_subarray_averages\n│  └─main.rs\n├─target // 编译生成目录\n│   └─…\n├─Cargo.toml // cargo 配置文件\n└─README.md // 如果设置git同步到远端的话，写一个简单的仓库说明\n 解题工作流\n以算法题目0164 maximum_gap为例：\nleetcode-cn 网站上给的Rust模板为：\nimpl Solution &#123;\n    pub fn maximum_gap(nums: Vec&lt;i32>) -> i32 &#123;\n        \n    &#125;\n&#125;\n\n我们在src目录创建子目录problem_0164_maximum_gap\n\n​\t遵从 problem_id_title 的格式，我这里的问题编号是从leetcode-cn.com上取的，不是主站 leetcode.com.\n\n在 main.rs 中调用当前模块\n\n将你当前调试的模块导入到main.rs中，以problem_0164_maximum_gap 为例：\n// mod problem_xxxx_xxxxxx; \nmod problem_0164_maximum_gap;\n// other problem\nfn main() &#123;\n    println!(\"[+]main.rs| Compile Success\");\n&#125;\n导入的目的是让Clion在你写子模块的时候进行代码分析，帮你自动补全.\n注意，调试完的模块就注释掉防止使用 cargo test 测试时测试到不需要的模块.\n\n在子目录创建文件 mod.rs ，将测试输入写在这个文件中\n\nmod.rs\npub mod buckets_and_pigeonhole; // 解法1\npub mod solve2; // 解法2\n\npub trait Solution &#123;\n    fn maximum_gap(nums: Vec&lt;i32>) -> i32;\n&#125;\n\n#[cfg(test)]\nmod tests &#123;\n    use super::Solution;\n\n    pub fn run&lt;S: Solution>() &#123;\n        let test_cases = [\n            (&amp;[3, 6, 9, 1] as &amp;[_], 3),\n            (&amp;[10], 0),\n            (&amp;[1, 3, 100], 97),\n            (&amp;[1, 1, 1, 1], 0),\n            (&amp;[1, 11], 10),\n            (&amp;[1, 1, 1, 1, 1, 5, 5, 5, 5, 5], 4),\n        ];\n\n        for (nums, expected) in test_cases &#123;\n            assert_eq!(S::maximum_gap(nums.to_vec()), expected);\n        &#125;\n    &#125;\n&#125;\n上述代码中定义了名为Solution的trait，规定了我们在写具体的算法的时候必须实现的函数接口.\n最上面的buckets_and_pigeonhole和solve2是我们写的具体算法的文件名.\n#[cfg(test)] 注解下是我们写的测试代码，其中test_cases变量中保存了我们的测试样例，每条样例中包括输入和预期输出.\n测试算法时就通过 cargo test 即可测试这些样例.\n\n具体算法实现\n\n在相应的问题文件夹下创建rust文件求解，以 buckets_and_pigeonhole.rs 为例：\npub struct Solution;\n\nimpl Solution &#123;\n    pub fn maximum_gap(nums: Vec&lt;i32>) -> i32 &#123;\n        let mut result = 0;\n        let maybe_min_value = nums.iter().copied().min();\n        let maybe_max_value = nums.iter().copied().max();\n\n        if maybe_min_value != maybe_max_value &#123;\n            let min_value = maybe_min_value.unwrap();\n            let max_value = maybe_max_value.unwrap();\n            let length = max_value - min_value;\n\n            // The maximum gap must be greater than or equal to length / (n - 1), so as long as the bucket size is less\n            // than length / (n - 1) + 1, the maximum gap can not be inside one bucket. So we only need to check gaps\n            // between buckets.\n\n            let bucket_size = &#123;\n                let n = nums.len() as i32;\n\n                (length + n - 2) / (n - 1)\n            &#125;;\n\n            let mut buckets = vec![(-1, 0); (length / bucket_size + 1) as _];\n\n            for num in nums &#123;\n                let (left, right) = &amp;mut buckets[((num - min_value) / bucket_size) as usize];\n\n                if *left == -1 &#123;\n                    *left = num;\n                    *right = num;\n                &#125; else if num &lt; *left &#123;\n                    *left = num;\n                &#125; else if num > *right &#123;\n                    *right = num;\n                &#125;\n            &#125;\n\n            let mut bucket_iter = buckets.into_iter().filter(|(left, _)| *left != -1);\n            let mut previous = bucket_iter.next().unwrap().1;\n\n            for (left, right) in bucket_iter &#123;\n                result = result.max(left - previous);\n\n                previous = right;\n            &#125;\n        &#125;\n\n        result\n    &#125;\n&#125;\n\n// ------------------------------------------------------ snip ------------------------------------------------------ //\n\nimpl super::Solution for Solution &#123;\n    fn maximum_gap(nums: Vec&lt;i32>) -> i32 &#123;\n        Self::maximum_gap(nums)\n    &#125;\n&#125;\n\n#[cfg(test)]\nmod tests &#123;\n    #[test]\n    fn test_solution() &#123;\n        super::super::tests::run::&lt;super::Solution>();\n    &#125;\n&#125;\nsnip注释下面的代码是为了将当前的Solution方法作为mod.rs中trait的实现，用于测试.\n使用Clion做测试调试时，可以通过下图debug按钮调试：\n\n 0x3 Git同步\n如有需要，将除了target目录以外的文件夹加入git，同步到远端，这样就可以在多台设备上优雅的同步代码了.\n 0x4 其他\n在大哥的代码库中包含了:\nmod data_structures;\nmod test_utilities;\n这两个模块，和main.rs同级目录，其作用是一些比较重要的且rust标准库不提供的数据结构，还有一些测试用的函数，在某些问题中你可能需要用到这些.\n此外，他还配置了一些github action用于自动化编译和生成展示网页：LeetCode Progress Report (efanzh.org)\n对于我这样的新手就没什么用，所以也不需要配置了.\nRef：\nhttps://github.com/EFanZh/LeetCode 快给大哥一个Star\nhttps://github.com/Ch3nYe/LeetCode 也可以给我一个Star 😘\n","categories":["备忘"],"tags":["备忘","Rust","algorithm","environment"]},{"title":"Note 《Neural reverse engineering of stripped binaries using augmented control flow graphs》","url":"/Note-Nero/","content":" Neural reverse engineering of stripped binaries using augmented control flow graphs\n 0x1 INTRODUCTION\n从二进制代码出发，遍历CFG从Entry到Sink（就是一个目标call site）的所有path，利用pointer-aware slice技术重构汇编码形式的API call，重构为有一定可读性的类似高级语言的形式（ eg: connect(RET, ARG, 16) ）.\n然后通过学习，如何学习，将函数名embedding，参数的抽象表达也做embedding. (如何学习到embedding不知道，没看懂)\n最后，将一个procedure表示为从Entry到Sink的路径集，将这个路径集送入LSTM/Transformer-based seq2seq model 还有GNN网络训练，label是procedure对应的函数名.\n 0x2 overview\n整体介绍了一下本文的思路，但是读完容易不知所云，所以建议读完文章，再回来读一遍原文这一节.\n每个augmented call site 作为一个node，将程序表示为graph，从起始结点开始每条path都表示一个可能的运行时执行顺序.\n两种训练函数名预测的方式：\n\n基于这些graph应用GNN模型做encoder，用LSTM decoder输出边长序列（函数名）；\n从graph上生成path，喂给LSTM-based、Transformer-based model.\n\n关键点：\n\n使用静态分析，重构API call sites\n使用pointer-aware slices技术Sec4，使用具体值或者抽象值，替换调用点参数\n\n二进制程序切片是创建一个变量必要的程序片段，本文中从CFG上取path切slices\n参数变量替换为具体值或者抽象值，抽象值包括：argument=ARG，global value=GLOBAL，value from calling a procedur=RET，value on stack=STK\n这一步提供了relative improvement 20%\n\n\n通过分析CFG，可从call sites的近似运行时的顺序中，获知增强的call sites. Sec2\n增强的call sites  能有效的表示二进制过程\n运用了大量的神经网络技术\n\n0x3 model 的介绍就不写了.\n 0x4 Representation Binary Procedures\n Reconstruct call site\n重构三种调用点的方法\n\n内部调用，elf内部的固定位置的过程调用\n\nstripped binary 去除了符号表，所以无法恢复目标函数名（procedure name）；\n\n\n外部调用，动态加载的外部库中的过程调用\n\nelf导入表中包含这些函数名，且通过依赖库的debug信息可以得知函数参数信息；\n\n\n间接调用，调用时地址存储在寄存器中的间接调用\n\n通过CFG上取路径分析可能的目标方法，如果能解析成外部调用，就解析，否则就不能恢复目标函数名.\n\n\n\n这里完成了对调用点目标方法名的解析.\n 4.1 Augmenting call sites with concrete and abstract values\n首先为一个call site的argument register创建pointer-aware slice-tree.\n第一步：\n对于程序切片采用了Lyle and Binkley[1993]的定义，对于一条指令inst定义以下集合：\n\nVwrite(inst): 指令inst中写入操作的目标寄存器\nVread(inst): 指令inst中读取操作的目标寄存器，或者指令中使用的数值\nPread(inst): 指令inst中写操作的目标内存地址\nPread(inst): 指令inst中读操作的目标内存地址\n\n前两个集合捕捉了所有指令中读写的值（寄存器）和立即数，后两集合捕捉了所有读写操作的指针.\n注意，控制流指令和栈指针EIP+ESP排除在外，因为他们与数据流不相关.\n上述集合的定义结合下面例子理解：\n\n第二步：\n对于一个指令序列，可以定义：\n\nVlast-write(inst, r): 包含寄存器 r∈Vwrite(inst′)r \\in V_{write}(inst&#x27;)r∈Vwrite​(inst′) 的最大指令inst &gt; inst’\nPlast-write(inst, p): 包含指针 p∈Vwrite(inst′)p \\in V_{write}(inst&#x27;)p∈Vwrite​(inst′) 的最大指令inst &gt; inst’\n\n大于是指后执行为大.\n通过这个定义就可以从一条CFG path上找到，call site处某个寄存器的相关执行. 如下图fig3(a)中rsi（green block）的三条相关指令.\n第三步：\n为了生成在指令 inst 处的 r 寄存器的pointer-aware slice，需要重复以下步骤：𝑉𝑙𝑎𝑠𝑡−𝑤𝑟𝑖𝑡𝑒 → 𝑉𝑟𝑒𝑎𝑑 and 𝑃𝑙𝑎𝑠𝑡−𝑤𝑟𝑖𝑡𝑒 → 𝑃𝑟𝑒𝑎𝑑，直到达到空集. 这是一个逻辑与操作，前者是指令中读取的寄存器或值，后者是指令中读取的指针.\n通过这个步骤就可以得到fig3(b)这样的slice-tree.\n\nFig. 3(a) shows the pointer-aware slice for the rsi register (green) that is used as an argument for connect;\nFig. 3(b) represents the same slice as a tree, we denote as a “slice tree”.\n然后，分析pointer-aware slice-trees.\n先以下步骤做初始化tag assignments：\n\n被procedure P接受的参数打“ARG”标签.\n所有的调用指令打“RET”标签，注意这里是给存返回值的rax寄存器打“RET”标签.\nP的开始时rbp寄存器打“STK”标签，在传播过程中，这个标签也许会传递给rsp或者其他寄存器.\n当指针指向全局变量且该不能解析出具体值时，打“GLOBAL”标签.\n\n完成初始化以后，我们沿着(b)所示的slice-tree向下传播这些变量，知道sink点也就是API call site处的寄存器调用，按照以下顺序选择value：\n(1) concrete value\n(2) ARG\n(3) GLOBAL\n(4) RET\n(5) STK\n(6) 空集（为了顺利执行传播算法，空集永远不会到sink点）\n关于传播，以fig3©为例，具体过程如下：\n\n先给2和4打tag，“ARG”和“STK”，初始化的tag用黑色下划线画出\n两个tag，空集和ARG传播到3，然后3处只有ARG能传播到6\n在6处，根据优先级，只有ARG能传播到7\n6处传播下来的ARG到达了sink点，替换rsi寄存器.\n\n到这里，就完成了调用点call site处参数的reconstruction.\n\nAn augmented call site graph example\n\n\n上图展示了一个CFG和一个增强的调用图 augmented call site graph. CFG中node 4在增强图中表示为两个不同的路径上的API call，原因是对node 4处的调用中的第二个参数做slice分析时选取的CFG的path不同从而导致创建的slice也不同，之后得到的augmented arguments representation也不同.\n同时也注意到node 1中的一个内部调用，缺少了过程名，就用UnknownInternel表示，这也对表征程序P有一定帮助.\n\nDealing with aliasing and complex constant expressions\n\n基于作者先前的工作，重新优化了整个procedure，还在分析之前优化了每个slice，这可以推导出参数值. 一个复杂的常量计算被替换为一个优化过的方式表达. 例如 xor rax,rax; inc rax; 简化为 mov rax, 1 这节省了做表达式简化的繁重工作.\n\nFacilitating augmented call site graphs creation\n\n为了高效分析大量的巨大的CFGs，本文执行了以下几个优化措施：\n\n路径前缀的缓存值计算\n将不包含调用的子图融合\n移除重复指令序列\n因为所有的call指令被RET label替换，在遇到所有call指令后，就停止切片.\n\n 0x5 Model\n这一节说一下，Model如何吃上述得到的东西.\n 5.1 Call site Encoder\n先通过学习得到一个embedding vocab：E namesE^{\\ names}E names，将训练数据库中的函数名分割成token，根据vocab给每个token赋一个vector.\n此外，通过学习给每个抽象的参数值定一个embedding，对于具体数值也是如此. 记作：E valuesE^{\\ values}E values\n\nencoding a call site:\n\n对于一个call site 把上述subtoken的vector求和得到一个向量作为encoding的第一部分；把kargsk_{args}kargs​个参数的vector 首位相接，作为第二部分. 形式化为：\n\n使用一个专门的no_arg符号的vector补全 kargsk_{args}kargs​ 个参数.\n 5.2+3 Encoding Call Site Sequences with LSTMs and Transformers\n在LSTMs和Transformer的seq2seq模型中，本文抽取一个procedure上所有可能的Entry到Sink的path，用这样一个集合表征这个procedure.\n注意，这里和传统的seq2seq范式的区别在于，输入不是标准的单一符号序列，而是一组call site序列，所以是这些model可以说是“set-of-sequences-to-sequence”.\n 5.4 Encoding Call Site Sequences with Graph Neural Networks\n使用GNN模型的时候，直接把一个procedure的CFG图作为输入，具体使用的是GCN.\n这里由于直接输入CFG不像上面输入call site 序列，所以要处理一下CFG：\n从节点是基本块的CFG开始，一个基本块上可能有多个API call. 因此，我们将每个基本块拆分为一个call site chain，以便所有传入的边缘连接到第一个call site，最后一个call site连接到基本块的所有传出边.\n此外本文在重构call site参数时，选取的CFG上的path不同构造出的参数抽象值或具体值可能不同，所以这里复制了这些可能不同的调用点，组合所有参数的可能的值，如图fig4.\n论文链接：\nLink: https://dl.acm.org/doi/abs/10.1145/3428293\nPublisher: 2020, ACM期刊\nCode: https://github.com/tech-srl/Nero\n","categories":["论文阅读"],"tags":["论文","笔记","Binary","CFG","Graph","LSTM","Transformer","PL"]},{"title":"Note 《Structural Attack against Graph Based Android Malware Detection》","url":"/Note-Structural-Attack-against-Graph-Based-Android-Malware-Detection/","content":" Structural Attack against Graph Based Android Malware Detection\n 主要解决什么问题\n目前有很多基于函数调用图FCG的恶意软件检测方法，本文提出一种结构攻击方法，攻击基于图的Android恶意软件检测器. 解决了feature space和problem space之间的逆变换问题[1].\n这里写一下我对这两个空间和两者之间变换的理解：\n\n特征空间feature space：原始输入（比如image，pdf，software）通过特征映射函数得到的特征向量，也就是数值向量.\n问题空间problem space：（不知道这样翻译是否准确）指原始输入.\n两者之间的变换：从样本空间到特征向量空间的映射，和，从特征向量空间到样本空间的映射.\n值得说明的是，从原始输入计算特征向量的这个映射函数，它往往不是双射的，比如从一个软件的函数调用图上生成特征向量的映射函数，任意给出一个向量大概率找不到对应的函数调用图. 这意味着一个特征空间的向量叠加上扰动以后，很可能没有对应的problem space中的样本. 这就使得Inverse Feature-Mapping Problem成了一个难题.\n\n\n这篇文章[1]对上述概念进行了较为详细的阐述和形式化表达，如果想进一步深入了解，可以通过文末链接阅读原文. 文中Sec III对两个space上的攻击，space之间的变换，变换的难点，以及可能的解决的方法（在段落B.Inverse Feature-Mapping Problem）进行了形式化描述.\n有趣的是，这篇文章Sec IV直接使用问题空间攻击，在Andorid恶意程序检测模型上进行攻击尝试.\n\n回到本文，\n设计了一个启发式优化模型与强化学习框架集成到一起，用于优化我们的结构攻击(HRAT：Heuristic optimization model integrated with Reinforcement learning framework structural ATtack). HRAT包括四种类型的攻击：插入和删除节点，增加边和改写，对应APP中的插入和删除方法，增加调用关系和改写.\n通过对超过 30k Android 应用程序的广泛实验，HRAT在feature space（超过 90% 的攻击成功率）和problem space（多数情况下高达 100% 的攻击成功率）都展示了出色的攻击性能.\n另外，需要阐述本文要解决的现有的对抗样本攻击方法的问题：\n\n\nL1-system-specifc attack methods：现存的攻击方法多数高度依赖目标系统的特征提取方法，比如Android HIV实现了problem space attack on Mamadroid 通过修改某些样本特征，以扰动特征向量，所以如果目标系统更换特征提取方法，那需要重新设计problem space transformation（这里的变换指problem space中原始样本到对抗样本的变换）. 本文structural attack 扰动图结构而不是特征向量，所以目标系统的特征提取方法对本文攻击工作流无影响.\n（怎么会没有影响呢，那我要是不从函数调用图上提取特征了，你还能生成个球的对抗样本. 所以我觉得，在problem space的扰动，最终还是对特征向量的扰动，是不能绕靠目标系统更换特征提取方法这个问题的.）\n\n\nL2-limited software modification operations：为了保持功能的一致性，现有的 App 修改方法仅限于插入死代码.\n\n\nL3-inconsistent transformation relation：现存的对抗样本的生成方法以特征扰动到app modification为指导，尽管他们可以按照需要随意插入代码，但是为了一致性（或者说Plausibility）只能插入no-op code无动作代码. 而我们的攻击方法确保了修改图后仍能保持应用程序的一致性. （其实这条和上一条不是一个意思吗.\n\n\n其实上述L3在[1]中段落Problem-Space Constraints也有提到，就是关于问题空间transformation生成对抗样本的约束条件.\nchallenges\n\nhow to determine a operation type.\nhow to select the most effective objects (nodes or edges) to modify.\n\n为此设计了本文HRAT，原文Sec 3.3对上述问题的解决进行了描述.\n\n总结：\n\n本文要解决的问题就是右下角的问号.\n 目标系统\n默认大家都对下述Android恶意程序检测工作有基本的了解，这里就简要地提一下.\n Malscan\n从app和提取FCGs图，并使用图上敏感节点的中心作为特征. 所谓敏感节点就是能反应恶意软件性质的android敏感APIs.\n Mamadroid\n从FCG中抽取函数调用关系作为特征.\n APIGraph\n是一个提升Android Malware Detection的框架. 它增强了特征的表征能力，并利用Android的特性聚合具有相似语义的API. 也就是做API聚类，在特征提取过程中，使用一个代表性API表示一个cluster中的所有API.\n 攻击方法\n 威胁模型\n目标系统攻击这来说是可访问的白盒. 也就是说，攻击者可以访问目标系统的数据集、特征空间和模型参数. 我们的攻击目标是在修改恶意应用程序的函数调用图以逃避目标系统的检测. .两种基于 FCG 的 AMD，即 Malscan (ASE 2019)和 Mamadroid (NDSS 2017)，以及一种 Android Malware Detection 增强方法，即 APIGraph，用于评估我们攻击的有效性.\n上述两种检测模型拥有很高的检测性能：98% detection accuracy for Malscan and 99% F1 score for Mamadroid.\n 攻击方法的形式化\n攻击方法K\\mathcal{K}K的输入是一个FCG（邻接矩阵），该方法会通过在邻接矩阵上加一个扰动矩阵δ的方式修改这个图：\nG~=K(G)=G+δ\\tilde{G}=\\mathcal{K}(G)=G+\\delta\nG~=K(G)=G+δ\nG~\\tilde{G}G~就是对抗样本图. 使用f(G,θ)f(G,\\theta)f(G,θ)表示恶意软件检测模型在模型参数θ下的检测结果，攻击目标是让模型将恶意样本错误分类为良性，形式化为：f(G)≠f(G~)f(G) \\ne f(\\tilde{G})f(G)​=f(G~)\n在与原来的功能相同的情况下，我们设计了四种类型的修改动作，即添加边、重写、插入节点和删除节点，这也是考虑到了Apps的特点. 接下来，我们首先介绍了约束条件的定义，然后描述每个动作的定义和属性.\n上文已经提到过四种修改了，这里对重写，删除这两种修改动作做个解释.\n本文定义了Constraints，它列出了APP中methods的可修改性. 当HRAT修改FCGs中的节点和边时，需要遵循这个约束条件. HRAT只能修改modifiable methods和call relations. 我们通过Android，Flowdroid和soot的性质决定了以下不可行修改的约束：\n\nFramework APIs：framework API是在Android系统而不是应用程序中定义和实现的，因此它们是不可修改的.\nLifecycle methods：生命周期方法有Android系统调用，如果我们删除或增加这样的函数调用则容易使APP崩溃.\nFlowdroid methods：因为Flowdroid引入了额外的methods便于分析APPs，FCGs也会包含这些方法.\n\n说回重写这个修改动作：\n将三个不在约束条件中的node： Ar={vbeg,vend,vmid}A_r = \\{v_{beg},v_{end},v_{mid}\\}Ar​={vbeg​,vend​,vmid​} 将开始节点和结束节点之间的边删除，增加从开始节点到中间节点的边，增加从中间节点到结束节点之间的边.\n另外，删除动作：\n删除一个不在约束中的节点并连接被删除节点的caller和callee.\n 启发式优化基于结构攻击的强化学习\n结构化攻击过程有一系列针对图的攻击动作组成. 具体来说，每次攻击动作由两个阶段：决定一个攻击类型和选择一个攻击对象. 前者决定动作类型的问题（adding edge, rewriting, inserting/deleting node），后者决定那条边或者节点要被修改.\n我们的优化目标是得到尽可能小的图上的修改序列，而不是某种隐藏的结构，所以这里选择交互式的学习算法—强化学习模型. 这里的隐藏结构就是说，该目标不是传统的在数据集上找某种隐藏的扰动结构可以使得模型出错，而是生成一个某种约束下的改动序列.\n state space\n状态空间存储所有攻击过程中出现的中间状态. HRAT选择从中间图上抽取出的特征向量进行存储. 例如，当HRAT攻击Malscan模型时，状态空间存储所有过程中产生的centrality features of sensitive APIs. 特征向量的长度固定，适合训练策略网络. 我们不使用邻接矩阵，因为如何动态地处理图形尺度的变化超出了本文的范围.\n此外，HRAT只存储上一次修改后的图（邻接矩阵的形式），这有利于下一次在图上的修改. 我们每次修改只基于当前的图和状态，存储历史图结构是浪费内存.\n Policy Model\n策略模型确定攻击行为，包括决定行为类型和选择攻击目标. HRAT基于强化学习，具体来说是deep Q-learning来确定攻击行为.\n文中对deep Q-learning用于structural attack的算法有详细描述，这里就不展开，我们避免陷入过于细节的地方而失去了对文章整体的把握.\n Action Space\naction space存储了修改图的操作，每个操作表示为一个四元组：{动作类型，对象*3}.\n Reward Function\n\n反馈函数评估所选的动作的在当前状态上的效果.\n\n因为目标是让通过尽可能小的修改使得系统分类出错，所以反馈函数定义为：\nR(st,at)={1 if f(G^)≠f(G)−(ΔNnode +ΔNedge ) if f(G^)=f(G)\\mathcal{R}\\left(s_{t}, a_{t}\\right)= \\begin{cases}1 &amp; \\text { if } f(\\hat{G}) \\neq f(G) \\\\ -\\left(\\Delta N_{\\text {node }}+\\Delta N_{\\text {edge }}\\right) &amp; \\text { if } f(\\hat{G})=f(G)\\end{cases}\nR(st​,at​)={1−(ΔNnode ​+ΔNedge ​)​ if f(G^)​=f(G) if f(G^)=f(G)​\nΔNnode ,ΔNedge \\Delta N_{\\text {node }},\\Delta N_{\\text {edge }}ΔNnode ​,ΔNedge ​代表当前修改后的graph和原始的graph中间在node数量和edge数量上的差别.\n\n反馈函数评估所选的攻击动作类型和攻击对象\n\n\nadding edge的反馈是-1，因为无论梯度搜索选择了哪一个边，只有一个边被修改了.\nrewriting 和inserting node的反馈是-3和-2.\nremoving node删除一个节点并连接被删除节点的caller和callee，其反馈是：1+该节点caller的数量*该节点callee的数量.\n\n 结构化攻击的优点和有效性\n\nStructural attacks bridge the gap between feature-space attacks, which only perturb feature vectors to deceive the classifier, and problem-space attacks, which generate adversarial objects.\n\n也就是[1]中提到的inverse feature-mapping problem. 现存的问题空间的攻击受限于inverse feature-mapping problem(即，优化feature-space attacks 不能被很好的映射到problem-space attacks) 我们的Structural attacks基于App的FCG修改nodes and edges，因此，四个FCG修改动作对应了App上的修改.\n暗涵，我们的攻击也涉及到了向量空间，毕竟还是将向量送给目标模型，而且我们也在problem space上找到了对应修改，所以我们就能说我们弥补了feature-space attacks和problem-space attacks之间的鸿沟.\n Android Application Manipulation\n阐述本文开发的基于graph modification自动生成App Manipulation的工具（APPMOD）. 遵循以下两个原则：\n\na) Functional Consistency：APP的功能在修改前后保持一致\nb) Valid Modifications：插入的代码不会被静态分析移除. 具体来说静态分析会删除永远不会执行的dead code.\n\nHRAT的原型（即APPMOD）构建在soot上. APPMOD使用soot修改Apps，soot将会在无源码的情况下翻译android字节码到中间表示. 根据方法签名APPMOD将会定位到App中的方法并进行修改.\n方法签名示例：&lt;packageName.className returnType methodName(paraList)&gt;\nAPPMOD的工作流如下图所示.\n\n原文中也对App修改的具体方法进行了详细介绍Sec4.2-4.5，这里也不展开了.\n 结果评估\n\n本文主要工作，HRAT的完整工作流如上图所示.\n① 抽取FCG\n② 生成一个graph modification sequence（GMS）这个序列中每一项都表示一个对node或edge的扰动\n③ 将GMS转换到实际App上的Manipulation\n④ App依照GMS进行逐项修改\n⑤ 最后我们评估生产的重打包的App是否能避免被目标系统检测出来\n⑥ 下面通过回答以下五个问题评估HRAT的表现\n\nRQ1: Effectiveness analysis. How effective is HRAT against the state-of-the-art AMD techniques?\nRQ2: Modification efficiency comparisons. Compared with other attack methods, how is the modification efficiency of HRAT?\nRQ3: Effectiveness of IMA. How effective is individual modification action (IMA)?\nRQ4: Resilience to code obfuscation. How is the resilience of HRAT to malware with different obfuscation techniques?\nRQ5: Functional consistency assessment. Do the adversarial app generated by HRAT preserve the functionality as the original one?\n\n Dataset\n使用11,613 benign Apps and 11,583 malicious Apps from 2011 to 2018 in Malscan 评估HRAT（for RQ1-3&amp;5）. 所有App都来自AndroidZoo，并且每个样本都经过多个反病毒引擎测试才决定了其标签. 对于RQ4&amp;5，我们使用了一个先前工作中构建的数据集，来评估HRAT的有效性. 这个数据集包含不同家族的恶意软件，其中6586个经过变量重命名混淆，1090个通过字符串加密混淆，1172包含反射.\n Metrics\n为了评估feature-space 和 problem-space的攻击的有效性，本文设计了三种attack success rates（ASRs）：\n Init_ASR =Ng/N Rela_ASR =Np/Ng Abs_ASR =Ns/Np\\begin{aligned}\\text { Init\\_ASR } &amp;=N_{g} / N \\\\\\text { Rela\\_ASR } &amp;=N_{p} / N_{g} \\\\\\text { Abs\\_ASR } &amp;=N_{s} / N_{p}\\end{aligned}\n Init_ASR  Rela_ASR  Abs_ASR ​=Ng​/N=Np​/Ng​=Ns​/Np​​\n\nInitialization ASR：N是恶意样本总数，Ng是改变了FCG并成功避免系统识别的样本数量，且一个样本最多500个修改，这里的门限值回答了RQ2，我们发现几乎100%的样本在500次以内的修改就可以逃避检测. 需要注意，由于anti-repackage protection的存在，并不是所有FCGs上的修改都可以被重打包. （这其实可以看作feature-space attack）\nRelative ASR：反应了成功重打包的比例. Np为，在Ng中成功重打包的比例. （也就是feature-space attack能inverse mapping 到problem-space的比例）\nAbsolute ASR：反映了HRAT在problem space上的有效性，即是否重打包的样本可以逃避检测并且保持功能性. Ns是Np中成功避免检测的样本数量. （就是problem-space attack成功率）\n\n 实验结果\n本文设计了以下三个数据集做HRAT的有效性分析：\n\n第一个数据集中，训练集(TRo) 和测试集(TEo)是在同一时间段被收集的.\n第二个数据集中，测试集(TE1 and TE2)是在训练集之后被收集的. 这个数据集模拟了使用已知样本训练检测器并使用它检测未知恶意样本的实际使用情形.\n第三个数据集使用最新的恶意软件(TR2)训练分类器使用旧的恶意软件(TE1)做测试.\n\n结果如下图所示：\n\n使用不同算法的攻击成功率如下图所示（using TRo for training and TE1 for testing）：\n\n使用SACK，HACK和EPACK的Init_ASR比强化学习低，因为evolutionary algorithms 把结构攻击引导向，随机地选择攻击行为. 受益于结构攻击，基于演化的算法实现了和强化学习相同的Abs ASR.\n关于这几种算法可以查看原文附录，SACK: simulated annealing based structural attack，HACK: hill-climbing based structural attack，EPACK: evolutionary programming based structural attack.\n总结：\n\nAnswer to RQ1: HRAT实现了在500次修改内高达99.94%的攻击成功率. 如果不做严格的修改次数限制HRAT可以达到100%的problem-space attack成功率.\n\n再来看一下HRAT的修改效率：\n\nCDF：cumulative distribution of the required number of modifications for evasion 就是需要修改的最少次数能避免被检测的样本累计统计.\n看图可知HRAT是几种算法中效率最好的，因为多数样本在100次修改就可以完成欺骗.\n\nAnswer to RQ2: HRAT需要的骗过检测器的App修改次数少于其他算法.\n\n文章对其他几个进行了一一解答，这里我们已经看到这篇文章的创新点和实验效果，就不在展开更多实验内容了.\n 对我有什么启发（总结）\n[1]才是这个任务的上游玩家，这篇就文章有点…\n不过本文也是做了不少实验，能把强化学习应用到inverse feature-space mapping这个问题上算是比较有趣的工作，但是文章读下来感觉不够简洁，没能把细节清晰明了的展现给读者，当然也可能是我能力不够XD.\n另外，这篇文章的作者（们）还写过一篇BinarizedAttack: Structural Poisoning Attacks to Graph-based Anomaly Detection[2]，发表在ICDE 2022数据库顶会，这篇文章是对基于图的异常检测模型进行结构化投毒攻击，主要讲对图结构进行修改达到让模型分类出错的问题，给我的感觉作者应该是先做了BinarizedAttack这篇文章，然后才是本文. 然而，本文发表于CCS 2021，至于前者是否是二投我也没问作者，这不太重要. 总而言之，这两篇文章都是对图模型的攻击，虽然关注点不太一样，但不失为很好的参考对象.\n\n论文链接：Structural Attack against Graph Based Android Malware Detection | Proceedings of the 2021 ACM SIGSAC Conference on Computer and Communications Security\n参考文章：\n\n[1] [1911.02142] Intriguing Properties of Adversarial ML Attacks in the Problem Space (arxiv.org)\n[2] [2106.09989] BinarizedAttack: Structural Poisoning Attacks to Graph-based Anomaly Detection (arxiv.org)\n作者主页 Daniel Xiapu Luo’s homepage (polyu.edu.hk)\n\n","categories":["论文阅读"],"tags":["Android","论文","笔记","Graph","Reinforcement-Learning","Structural-Attack","FCG","Android-Malware-Detection"]},{"title":"Note 《ELECTRA Pre-training Text Encoders as Discriminators Rather Than Generators》","url":"/Note-ELECTRA-Pre-training-Text-Encoders-as-Discriminators-Rather-Than-Generators/","content":" ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators\n 摘要\nMLM任务的预训练模型比如BERT，通常需要大量计算才能达到比较好的效果. 本文提出一个简单却更有效的训练任务replaced token detection. 该任务不是MLM那种mask输入中的一个token，而是使用一个小型生成网络生成一些 plausible alternatives（看似可以合理替代的）token去替换输入中的一些token，训练判别模型去判断输入中的每个token是否被替换过. 通过全面的实验我们发现这个训练任务优于MLM任务，因为此任务定义在所有input token上，而不是只关注输入的一个子集. 实验结果表示，我们的任务在模型大小，数据量和计算量上都显著优于MLM. 尤其对于小模型的提升更加明显. 例如，在GLUE这个自然语言理解benchmark上，我们用单GPU训练了4天的模型优于GPT（计算量超过我们30倍）. 我们的方法在大规模上也表现很好，在与RoBERTa和XLNet两个大规模模型表现相同的情况下，我们的模型训练的计算量比他们少四分之一，相同计算量的情况下，我们的模型比他们更加优秀.\n 主要解决什么问题\n作者想证明：基于上述判别任务的训练方法比现存的生成任务的训练方法，在计算效率，模型参数效率上更优秀.\n 使用的方法\n上文中我们描述的替换token检测的预训练任务，如下图：\n\n图2：An overview of replaced token detection. 生成器是可以生成覆盖词汇表中token的任意模型（典型是MLM任务训出来的模型），我们通常使用与鉴别器联合训练的小型masked language模型. 尽管上述架构看起来像是GAN，我们寻来你生成器使用最大似然法而不是adversarially，因为把GAN用于文本是比较困难的. 完成预训练后，我们在下游任务中扔掉生成器只微调鉴别器.\n本文方法训练两个神经网络，生成器G和鉴别器D，两者都包含一个encoder hG &amp; hDh_G \\ \\&amp; \\ h_DhG​ &amp; hD​（eg. a Transformer network），该encoder将一个输入序列x=[x1,...,xn]x=[x_1,...,x_n]x=[x1​,...,xn​]映射到一个上下文化（contextualiized 或者说语境化?）的向量序列表示h(x)=[h1,...,hn]h(x)=[h_1,...,h_n]h(x)=[h1​,...,hn​].\n对于一个给定的位置t（在本文中t只取被mask的位置），生成器输出生成一个特定token xtx_txt​的概率，这一步是通过以下softmax layer（也就是说G会计算它认为masked位置t最有可能的词汇的概率，这个词汇就是hG(x)th_G(\\boldsymbol{x})_thG​(x)t​）：\npG(xt∣x)=exp⁡(e(xt)ThG(x)t)/∑x′exp⁡(e(x′)ThG(x)t)p_{G}\\left(x_{t} \\mid \\boldsymbol{x}\\right)=\\exp \\left(e\\left(x_{t}\\right)^{T} h_{G}(\\boldsymbol{x})_{t}\\right) / \\sum_{x^{\\prime}} \\exp \\left(e\\left(x^{\\prime}\\right)^{T} h_{G}(\\boldsymbol{x})_{t}\\right)\npG​(xt​∣x)=exp(e(xt​)ThG​(x)t​)/x′∑​exp(e(x′)ThG​(x)t​)\ne表示token embedding函数，hGh_GhG​表示生成器的encoder. 上式分子是行向量乘一个列向量，其实这里没太想明白：一个token的embedding 内积 序列通过G的encoder得到的向量h（表示一个token），比上，序列中每个token的embedding 内积 序列通过G的encoder得到的向量h 的和，这是啥概率？\n对于特定位置t，鉴别器输出 xtx_txt​ 是”original“还是”replaced“，这一步通过以下sigmoid output layer：\nD(x,t)=sigmoid⁡(wThD(x)t)D(\\boldsymbol{x}, t)=\\operatorname{sigmoid}\\left(w^{T} h_{D}(\\boldsymbol{x})_{t}\\right)\nD(x,t)=sigmoid(wThD​(x)t​)\n这里的ωT\\omega^TωT应该是sigmoid output layer前一层网络的输出.\n随机选出k个位置用[mask]替换这些位置上的token，构成xmaskedx^{masked}xmasked，这就是G的输入. 然后，生成器学习预masked token的原值.\n将G生成的token替换xmaskedx^{masked}xmasked就得到了xcorruptedx^{corrupted}xcorrupted，这就是D的输入. 鉴别器经过训练，可以在区分输入序列中token似乎否被替换过.\n总结，G和D的输入形式化为：\nmi∼unif⁡{1,n} for i=1 to kxmasked =REPLACE⁡(x,m,[ MASK ])x^i∼pG(xi∣xmasked ) for i∈mxcorrupt =REPLACE⁡(x,m,x^)\\begin{array}{ll}m_{i} \\sim \\operatorname{unif}\\{1, n\\} \\text { for } i=1 \\text { to } k &amp; \\boldsymbol{x}^{\\text {masked }}=\\operatorname{REPLACE}(\\boldsymbol{x}, \\boldsymbol{m},[\\text { MASK }]) \\\\\\hat{x}_{i} \\sim p_{G}\\left(x_{i} \\mid \\boldsymbol{x}^{\\text {masked }}\\right) \\text { for } i \\in \\boldsymbol{m} &amp; \\boldsymbol{x}^{\\text {corrupt }}=\\operatorname{REPLACE}(\\boldsymbol{x}, \\boldsymbol{m}, \\hat{\\boldsymbol{x}})\\end{array}\nmi​∼unif{1,n} for i=1 to kx^i​∼pG​(xi​∣xmasked ) for i∈m​xmasked =REPLACE(x,m,[ MASK ])xcorrupt =REPLACE(x,m,x^)​\nps：Typically k = ⌈0.15n⌉, i.e., 15% of the tokens are masked out.\n尽管这个架构很像GAN，但是这里有几点关键不同：\n\n如果生成器恰好生成了一个正确的token比如上图所示的the，则这个token在鉴别器这里应该考虑为”original”而不是”replaced“. 我们发现这种做法可以一定程度地改善下游任务的结果.\n生成器通过最大似然训练而不是通过adversarially训练，以骗过鉴别器.\n\nAdversally训练生成器是困难的，因为不可能通过生成器的取样反向传播（文本取样为离散值）\n虽然我们尝试通过强化学习（reinforcement learning）绕过这个问题，但是这样做表现很差.\n\n\n我们没有给生成器一个噪声向量作为输入，而这在GAN中是典型做法.\n\nD，G的loss和最小化的总loss：\nLMLM(x,θG)=E(∑i∈m−log⁡pG(xi∣xmasked))LDisc(x,θD)=E(∑t=1n−1(xtcorrupt=xt)log⁡D(xcorrupt,t)−1(xtcorrupt≠xt)log⁡(1−D(xcorrupt,t)))\\begin{aligned}&amp;\\mathcal{L}_{\\mathrm{MLM}}\\left(\\boldsymbol{x}, \\theta_{G}\\right)=\\mathbb{E}\\left(\\sum_{i \\in \\boldsymbol{m}}-\\log p_{G}\\left(x_{i} \\mid \\boldsymbol{x}^{\\mathrm{masked}}\\right)\\right) \\\\&amp;\\mathcal{L}_{\\mathrm{Disc}}\\left(\\boldsymbol{x}, \\theta_{D}\\right)=\\mathbb{E}\\left(\\sum_{t=1}^{n}-\\mathbb{1}\\left(x_{t}^{\\mathrm{corrupt}}=x_{t}\\right) \\log D\\left(\\boldsymbol{x}^{\\mathrm{corrupt}}, t\\right)-\\mathbb{1}\\left(x_{t}^{\\mathrm{corrupt}} \\neq x_{t}\\right) \\log \\left(1-D\\left(\\boldsymbol{x}^{\\mathrm{corrupt}}, t\\right)\\right)\\right)   \\end{aligned}\n​LMLM​(x,θG​)=E(i∈m∑​−logpG​(xi​∣xmasked))LDisc​(x,θD​)=E(t=1∑n​−1(xtcorrupt​=xt​)logD(xcorrupt,t)−1(xtcorrupt​​=xt​)log(1−D(xcorrupt,t)))​\nmin⁡θG,θD∑x∈XLMLM(x,θG)+λLDisc(x,θD)\\min _{\\theta_{G}, \\theta_{D}} \\sum_{\\boldsymbol{x} \\in \\mathcal{X}} \\mathcal{L}_{\\mathrm{MLM}}\\left(\\boldsymbol{x}, \\theta_{G}\\right)+\\lambda \\mathcal{L}_{\\mathrm{Disc}}\\left(\\boldsymbol{x}, \\theta_{D}\\right)\nθG​,θD​min​x∈X∑​LMLM​(x,θG​)+λLDisc​(x,θD​)\n训练在一个大raw text语料库X上. 我们用一个简单的样本来近似估计损失中的期望值. 我们因为sampling这步不能做反向传播，完成预训练之后，丢掉G，在下游任务上fine-tune G.\n本文对方法的描述比较简单精炼，如果觉得这里的翻译和理解不准确，可以翻看原文，文末有链接.\n 结果评估\nELECTERA和BERT的参数量基本相同，大多数实验的预训练数据和BERT保持相同，使用3.3 Billion tokens from Wikipedia and BooksCorpus (Zhu et al., 2015). 在以下任务上做了评估：\n\nGeneral Language Understanding Evaluation (GLUE) benchmark (Wang et al., 2019)\nStanford Question Answering (SQuAD) dataset (Rajpurkar et al., 2016)\n\n另外还设计了几个扩展模型来改进本文方法，除非另有说明否则使用的参数量和数据集都和基本模型一致. 其中，ELECTERA-large上使用了33Billion data from ClueWeb (Callan et al., 2009), CommonCrawl, and Gigaword (Parker et al., 2011). 都是英文数据集.\n SIZE\n\n图3：左，GLUE任务得分对于不同尺寸的生成器和鉴别器的情况. 有趣的是，生成器比鉴别器小时结果比较好. 右，比较不同的训练算法的效率. 注意这里使用的是FLOPs而不是训练步骤，由于ELECTERA还有生成器，所以相同FLOPs的训练步骤比BERT少.\n上图中two-stage ELECTRA：\n\nTrain only the generator with LMLM\\mathcal{L}_{MLM}LMLM​ for n steps.\nInitialize the weights of the discriminator with the weights of the generator. Then train the discriminator with LDisc\\mathcal{L}_{Disc}LDisc​ for n steps, keeping the generator’s weights frozen\n\n\n表1：在GLUE任务dev 数据集上比较小模型. BERT-Small/Base 是我们实现的和ELECTERA-Small/Base 相同参数量的模型. Infer FLOPs 是假定单个输入长度128. 训练时间应该谨慎对待，因为它们是针对不同的硬盘和未优化的代码. 即使在单GPU上训练，ELECTRA也表现良好. 在GLUE任务上比同类的BERT模型高出5%，甚至超过了更大的GPT模型.\n SMALL MODELS\n\n表2：大模型在GLUE dev set数据集上的表现. ELECTRA和RoBERTa在不同的预训练步骤下的情况，-后面表示训练步数. ELECTRA在使用少于1/4的预训练计算量时，与XLNet和RoBERTa的表现相当. 而在给定类似的预训练计算量时，表现优于它们。BERT dev的结果是来自Clark et al. (2019).\n\n表3：GLUE test-set results for large models. 本表中的模型采用了额外的技巧诸如ensemble来提高分数（详见附录B）. 有些模型没有QNLI得分，因为它们将QNLI作为一项排名任务，而最近GLUE基准不允许这样做. 为了与这些模型进行比较，我们报告了不包括QNLI的平均分数(Avg.*)，以及GLUE排行榜上的分数(Score). &quot;ELECTRA &quot;和 &quot;RoBERTa &quot;是指 完全训练的ELECTRA-1.75M和RoBERTa-500K模型.\nLARGE MODELS\n\n表4：Results on the SQuAD for non-ensemble models\n EFFICIENCY ANALYSIS\n本文的初始想法是MLM任务训练只将一小部分数据集作为目标会导致训练效率低下，但是事实并非如此，毕竟即使之预测一部分token，模型也输入了一个sample所有的token，为了更好的知道这件事到底造成了多大的影响或者说瓶颈，我们在ELECTRA和BERT之间的差别设置了一系列实验：\n\nELECTRA 15%：这个模型与ELECTRA相同，只是判别器的损失只来自于输入中被mask的15%的token. 换句话说，鉴别器的loss综合从1-n变成了1-k.\nReplace MLM：这一目标与Mask Language Model相同，只是没有用[MASK]代替被盖住的token，这些被盖住的token直接替换为generator生成的token. 也就是说生成器不会看到[mask]标记而是直接根据一条完整的sample预测特定位置可能出现的token. 这一目标测试了ELECTRA的收益在多大程度上来自于，解决在预训练期间将模型暴露于[MASK]标记的差异.\nAll-Tokens MLM：和Replace MLM很想，被mask的token被替换为生成器的sample. 此外，该生成器还预测了输入中所有的token，而不仅仅是那些 被mask掉的.\n\n\n图4：左边和中间，比较BERT和ELECTRA对不同的模型尺寸. 右边，Small ELECTRA 模型比BERT收敛到更高的下游任务准确率，表明改进不仅仅来自更快的训练.\n首先，我们发现ELECTRA极大地受益于对所有输入标记定义的损失，而不仅仅是一个子集. ELECTRA 15%的表现比ELECTRA差很多.\n第二，我们发现BERT的性能受到轻微损害，原因是预训练和微调的[MASK] token 不匹配，因为 Replace MLM 稍微优于 BERT. 我们注意到，BERT（包括我们的实现）已经包括了一个帮助解决预训练/微调差异的技巧：被mask的token在10%的时间里被替换成随机token，在10%的时间里保持不变. 然而，我们的结果表明，这些简单的启发式方法不足以完全解决这个问题.\n最后，我们发现All-Tokens MLM，这个生成模型对所有token而不是子集进行预测，而不是一个子集，缩小了BERT和ELECTRA之间的大部分差距.\n总的来说，这些结果表明，ELECTRA的很大一部分改进可归因于从所有token中学习，还有一小部分归功于缓解了预训练和微调阶段的不匹配问题.\n 对我有什么启发\n启发就是以后可以用这种结构，水一下二进制任务. 😁\n论文链接：[2003.10555] ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators (arxiv.org)\n参考文章：\n\nsource code https://github.com/google-research/electra\n\n","categories":["论文阅读"],"tags":["论文","笔记","Deep-Learning","Pre-Training","MLM"]},{"title":"2021年终总结","url":"/2021%E5%B9%B4%E7%BB%88%E6%80%BB%E7%BB%93/","content":" 2021年终总结\nthis is a personal post…\n","categories":["生活"],"tags":["日常","手记"]},{"title":"USTC宿舍装修理论与应用","url":"/USTC%E5%AE%BF%E8%88%8D%E8%A3%85%E4%BF%AE%E7%90%86%E8%AE%BA%E4%B8%8E%E5%BA%94%E7%94%A8/","content":" USTC宿舍装修理论与应用\n 0x0 胡话\n这是第一次写生活类post，感觉偶尔分享一下生活也挺好，抑郁症远离我，冲冲冲😁\n多图警告！！！\n 0x1 绪论\n关于USTC宿舍装修理论与应用这门课我颇有心得和体会，在这里把我的学习总结分享给大家，仅供理论参考.\n以下内容均不属实.\n 总览\n\n 0x2 桌上书架的维修理论\n关于桌子上的书架大概是宿舍最大的槽点了吧，放一张图自行感受一下：\n\n\n灵魂拷问：不到两扎的高度，怎么放显示器？后面的挡板是怕我们在桌子上玩的时候掉下来吗，怎么装机械臂？桌子上连个孔都没有怎么走线？\n\n相信大家都会为设计师的智商感到捉急，其实也可以原谅他，毕竟他大概没想整个高新校区第一年就来这么多计算机相关专业的学生. 可能是专为理科生准备的宿舍，工科什么时候能站起来（废理兴工！！！\n据说开学头两天就有大哥暴力强拆了右侧第一层挡板，佩服大哥.\n我和室友研究了一下午，有了现在这个半暴力拆解方法. 理论上是这样的：\n\n\n\n上图向左侧的螺钉结构共四个分布在四角，将水平木板与左右纵向木板固定\n\n\n远端的螺钉比较简单仅有两个，将水平木板与后侧背板固定\n\n\n解法：\n\n将奇怪的螺钉疯狂旋转，当它比较松的时候轻轻转动，如果运气好它会自动脱落，实际上他是与横向螺钉咬合. 如法炮制四次.\n\n如果运气不好就用一字螺丝刀插进去把他撬下来，动作要轻柔\n\n\n将一字螺丝插进孔洞破坏横向钉子下的复合板，如法炮制四次.\n此时横向木板已经开始晃动，施加一个向外的力，并上下晃动它，木板会与后面的钉子脱离开.\n\n结束.\n此时理论上木板状态如下（非真实）：\n\n桌面上的情况如下：\n\n\n仔细观察，这个钉子的屁股上是平的且带螺纹的，所以毕业的时候可以拧回去再把木板放上去，通过强力胶水的方式固定木板，达到和拆卸前相似的功能体验.\n ox3 宿舍装修光学理论与实践\n 灯带\nyeelight rgb 灯带接入米家\n通过wifi接入小爱触屏版音箱控制\n\n 台灯\n米家台灯同样通过wifi接入小爱音箱控制\n\n 屏幕挂灯\n比较可惜的是米家的屏幕挂灯不能通过任何方式接入音箱控制，yeelight 299那一款挂灯可以实现，家境贫寒打扰了.\n\n可以实现的是通过小爱音箱控制：\n\n台灯的亮度和色温\n灯带的亮度、色温和颜色\n\n比较可惜的是不能通过音箱切换灯带的模式（单色/流光）\n ox4 宿舍装修空间收纳学理论与实践\n 洞洞板\n淘宝220购入自选零件的洞洞板子，分两面：\n\n衣柜侧粘胶固定 - 放置不取拿的物品\n墙壁侧无痕钉固定 - 放置频繁取拿的物品\n\nps: 无痕钉也会有一个洞洞看得出来😥\n\n这里有一个我比较满意的小设计，手机可以架在洞洞板上充电，不影响鼠标操作，充电线用一个数据线固定器固定在侧边，方便取拿. 充电器插在桌下插排上，桌下有个置物架后面会看到照片.\n\n我真的不是二次元😐 两个摆件都是20r左右从淘宝按斤买的.\n 小桌板\n淘宝18.5r入手，便宜，但是很薄，我小米2k 165hz那款显示器上桌板有点压弯了，不过还撑得住.\n\n刚刚能放进去我的键盘，好险.\n 电线收纳槽\n53r入手，80cm豪华款，很实用，用胶贴的方式固定到桌子底板上，比较牢固. 路由器也在这上面，电线很乱，手动捂脸.\n\n 笔记本电脑立式支架\n根据我的设想，在寝室不办公，用不到双屏，所以搞了个笔记本电脑立式支架24r，把笔记本立起来，节省空间.\n\n 0x5 宿舍装修网络工程学理论与实践\n路由器红米AC2100，刷了 padavan 的固件，用得到的扩展功能：\n\n网易云灰色音乐解锁（此文完成时貌似仅PC客户端可以用）\n代理服务\n端口映射（把笔记本3389端口暴露到学校内网，实验室可以连回寝室）\n网络唤醒（笔记本开机键已经够不到了，只能网络唤醒）\n\n路由器接宿舍有线网，同样需要进入科大校园网登录认证. 寝室所有设备都接入我的路由器，统一管理，美滋滋.\n 0x6 我很满意的小细节\n 衣柜吸顶灯\n这个yeelight吸顶灯是感应人体和环境光的，暗光环境伸手就亮，还是比较灵敏，非常不错.\n\n 防撞胶贴\n桌子边边割手，这样就很舒服，真有我的😁.\n\n 0x7 结束语\n生活不易，让自己舒服点，挺好，特别好.\n带一张今天下午路上拍到的晚霞：\n\n","categories":["生活"],"tags":["日常","手记"]},{"title":"Glibc内存管理笔记","url":"/Glibc%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86%E7%AC%94%E8%AE%B0/","content":" Glibc内存管理笔记\n 0x0 胡话\n计划：\n\nLinux 下的内存管理：Glibc\nWindows 下的内存管理：HeapApi\nDouble Free\nUAF\n\nwindows 闭源，所以从Linux的内存管理开始学习\n 0x1 这篇笔记在写什么鬼\n这篇文章主要目的是，记录我在学习Glibc内存管理的过程中阅读的优秀参考文章，遇到的问题和对应的自主解答（见经典自问自答环节）。对读者的作用仅仅是推荐一些学习这部分知识的优秀参考文章，顶多再参考下我遇到的问题。并不是详细讲解Glibc的源码，也没有对Glibc内存管理策略的系统性描述。为什么不写？因为不想重复造轮，我写的怎么可能超过拥有多年内核层经验的前辈。\n\nGlibc就是GNU C库，是GNU计划所实现的C标准库。尽管其名字中带有“C库”，但它现在也直接支持C++（以及间接支持其他编程语言）。它为GNU系统，GNU/Linux系统和一些其他的类Unix系统提供了系统核心库。这些库提供了关键的API，包括ISO C11、POSIX.1-2008和BSD所规定的API和一些底层API，包括open、read、write、malloc、printf、getaddrinfo、dlopen、pthread_create、crypt、login、exit等。\nref: GNU C库 - 维基百科，自由的百科全书 (wikipedia.org)\n\n像这种底层内核源码库，乍一看感觉很难读懂，实际上就是不懂的知识点太多了，使用了大量的自定义的数据结构，眼花缭乱的宏定义，让人一下子抓不住要点，理解不了整体框架。就好像刚学完高中英语4500词，给我一篇二进制学术论文，没见过的单词太多了，当然理解不了文章。所以说不要着急，从别人的分析文章开始看，最后读一读源码，这样的过程多反复进行几遍就能对它有个很好的理解了。我是属于天资愚钝的那类人😥，这种源码阅读一般都要耗费个把月，但是沃觉得很值，也是必要的，毕竟基本功要打扎实。\n 0x2 知识点?\n chunk 相关\nref: (22条消息) linux堆内存管理malloc分析（3）_dongyu_1989的博客-CSDN博客\n文章内容：\n\n\nsamll bins &amp; large bins 中chunk的申请与释放\n\n\nlarge bins中每个双向循环链表chunk size的大小区间\n\n\n给出一张较为详细且清晰的Bins snapshot\n\n\nBins的初始化\n\n\n\n这是一个系列，感觉这个作者写的比较易懂，且语病相对较少，可以读一读同系列其他文章。\n 核心结构体\nmalloc_state\n每个分配区是struct malloc_state的一个实例，ptmalloc使用malloc_state来管理分配区.\nstruct malloc_state\n&#123;\n  /* Serialize access.  */\n  __libc_lock_define (, mutex);\n\n  /* Flags (formerly in max_fast).  */\n  int flags;\n\n  /* Set if the fastbin chunks contain recently inserted free blocks.  */\n  /* Note this is a bool but not all targets support atomics on booleans.  */\n  int have_fastchunks;\n\n  /* Fastbins */\n  mfastbinptr fastbinsY[NFASTBINS];\n\n  /* Base of the topmost chunk -- not otherwise kept in a bin */\n  mchunkptr top;\n\n  /* The remainder from the most recent split of a small request */\n  mchunkptr last_remainder;\n\n  /* Normal bins packed as described above */\n  mchunkptr bins[NBINS * 2 - 2];\n\n  /* Bitmap of bins */\n  unsigned int binmap[BINMAPSIZE];\n\n  /* Linked list */\n  struct malloc_state *next;\n\n  /* Linked list for free arenas.  Access to this field is serialized\n     by free_list_lock in arena.c.  */\n  struct malloc_state *next_free;\n\n  /* Number of threads attached to this arena.  0 if the arena is on\n     the free list.  Access to this field is serialized by\n     free_list_lock in arena.c.  */\n  INTERNAL_SIZE_T attached_threads;\n\n  /* Memory allocated from the system in this arena.  */\n  INTERNAL_SIZE_T system_mem;\n  INTERNAL_SIZE_T max_system_mem;\n&#125;;\nmalloc_par\n参数管理使用struct malloc_par，全局拥有一个唯一的malloc_par实例.\nstruct malloc_par\n&#123;\n  &#x2F;* Tunable parameters *&#x2F;\n  unsigned long trim_threshold;\n  INTERNAL_SIZE_T top_pad;\n  INTERNAL_SIZE_T mmap_threshold;\n  INTERNAL_SIZE_T arena_test;\n  INTERNAL_SIZE_T arena_max;\n\n  &#x2F;* Memory map support *&#x2F;\n  int n_mmaps;\n  int n_mmaps_max;\n  int max_n_mmaps;\n  &#x2F;* the mmap_threshold is dynamic, until the user sets\n     it manually, at which point we need to disable any\n     dynamic behavior. *&#x2F;\n  int no_dyn_threshold;\n\n  &#x2F;* Statistics *&#x2F;\n  INTERNAL_SIZE_T mmapped_mem;\n  INTERNAL_SIZE_T max_mmapped_mem;\n\n  &#x2F;* First address handed out by MORECORE&#x2F;sbrk.  *&#x2F;\n  char *sbrk_base;\n\n#if USE_TCACHE\n  &#x2F;* Maximum number of buckets to use.  *&#x2F;\n  size_t tcache_bins;\n  size_t tcache_max_bytes;\n  &#x2F;* Maximum number of chunks in each bucket.  *&#x2F;\n  size_t tcache_count;\n  &#x2F;* Maximum number of chunks to remove from the unsorted list, which\n     aren&#39;t used to prefill the cache.  *&#x2F;\n  size_t tcache_unsorted_limit;\n#endif\n&#125;;\nmalloc_chunk\nstruct malloc_chunk &#123;\n\n  INTERNAL_SIZE_T      mchunk_prev_size;  &#x2F;* Size of previous chunk (if free).  *&#x2F;\n  INTERNAL_SIZE_T      mchunk_size;       &#x2F;* Size in bytes, including overhead. *&#x2F;\n\n  struct malloc_chunk* fd;         &#x2F;* double links -- used only if free. *&#x2F;\n  struct malloc_chunk* bk;\n\n  &#x2F;* Only used for large blocks: pointer to next larger size.  *&#x2F;\n  struct malloc_chunk* fd_nextsize; &#x2F;* double links -- used only if free. *&#x2F;\n  struct malloc_chunk* bk_nextsize;\n&#125;;\nheap_info\ntypedef struct _heap_info\n&#123;\n  mstate ar_ptr; &#x2F;* Arena for this heap. *&#x2F;\n  struct _heap_info *prev; &#x2F;* Previous heap. *&#x2F;\n  size_t size;   &#x2F;* Current size in bytes. *&#x2F;\n  size_t mprotect_size; &#x2F;* Size in bytes that has been mprotected\n                           PROT_READ|PROT_WRITE.  *&#x2F;\n  &#x2F;* Make sure the following data is properly aligned, particularly\n     that sizeof (heap_info) + 2 * SIZE_SZ is a multiple of\n     MALLOC_ALIGNMENT. *&#x2F;\n  char pad[-6 * SIZE_SZ &amp; MALLOC_ALIGN_MASK];\n&#125; heap_info;\n 0x-1经典自问自答环节\n Chunk 大小的计算公式？\n\n\n一个使用中的chunk的大小：in_use_size  =  (用户请求大小+2*SIZE_SZ-SIZE_SZ)  align to 8B，准确来说是malloc分配的bins上的chunk\n参见：glibc/malloc.c at master · lattera/glibc (github.com)\nmmap分配时没有相邻的上下chunk，此时in_use_size  =  (用户请求大小+2*SIZE_SZ) align to 8B\n\n\n空闲chunk至少需要4个SIZE_SZ的大小，用于存储prev_size，size，fd和bk\n参见：glibc/malloc.c at master · lattera/glibc (github.com)\n\n\n由于空闲chunk和使用中的chunk使用同一块空间，所以肯定要取两者最大的一个作为最终的分配空间：chunk_size  =  max(in_use_size, 16)\n\n\n 为什么使用中的Chunk可以占用下一个chunk的prev_size域？\nprev_size字段虽然在当前chunk块结构体内，记录的却是前一个邻接chunk块的信息，这样做的好处，或者说目的，就是我们通过本块chunk结构体就可以直接获取到前一chunk块的地址，从而方便做进一步的处理操作。相对的，当前chunk块的foot信息就存在于下一个邻接chunk块的结构体内。字段prev_size记录的信息，有两种情况：\n1）如果前一个邻接chunk块空闲，那么当前chunk块结构体内的prev_size字段记录的是前一个邻接chunk块的大小。这就是由当前chunk指针获得前一个空闲chunk地址的依据。宏prev_chunk§就是依赖这个假设实现的。\n2）如果前一个邻接chunk在使用中，则当前chunk的prev_size的空间被前一个chunk借用中，其中的值是前一个chunk的内存内容，对当前chunk没有任何意义。此时上一个chunk不能用作分配，管理器不需要获取上一个chunk的地址，所以这个prev_size也就没有用了。字段size记录了本chunk的大小，无论下一个chunk是空闲状态或是被使用状态，都可以通过本chunk的地址加上本chunk的大小，得到下一个chunk的地址，由于size的低3个bit记录了控制信息，需要屏蔽掉这些控制信息，取出实际的size在进行计算下一个chunk地址，这是next_chunk§的实现原理。\n&#x2F;* Ptr to previous physical malloc_chunk.  Only valid if !prev_inuse (P).  *&#x2F;\n#define prev_chunk(p) ((mchunkptr) (((char *) (p)) - prev_size (p)))\n&#x2F;* Ptr to next physical malloc_chunk. *&#x2F;\n#define next_chunk(p) ((mchunkptr) (((char *) (p)) + chunksize (p)))\n宏chunksize§用于获得chunk的实际大小，需要屏蔽掉size中的控制信息。\n&#x2F;* Get size, ignoring use bits *&#x2F;\n#define chunksize(p) (chunksize_nomask (p) &amp; ~(SIZE_BITS))\n宏chunk_at_offset(p, s)将p+s的地址强制看作一个chunk。\n&#x2F;* Treat space at ptr + offset as a chunk *&#x2F;\n#define chunk_at_offset(p, s)  ((mchunkptr) (((char *) (p)) + (s)))\n 为什么Chunk的size域低三位可以复用？\nchunk在分割时总是以地址对齐，默认是8字节对齐，对齐值可以自定义，但是必须比默认值大且是2n2^n2n，这样的话低三位总是0，就可以复用。\n\n\n以第0位作为P状态位，标记前一chunk块是否在使用中，为1表示使用，为0表示空闲。\n\n\n以第1位作为M状态位，标记本chunk块是否是使用mmap()直接从进程的mmap映射区域分配的，为1表示是，为0表示否。\n\n\n以第2位作为A状态位，标记本chunk是否属于非主分配区，为1表示是，为0表示否。\n\n\n Chunk Bins 如何初始化？\n分配区main_arena初始化函数:\nstatic void\nmalloc_init_state (mstate av)&#x2F;&#x2F;mstate &#x3D;&#x3D; malloc_state\n&#123;\n  int i;\n  mbinptr bin;\n\n  &#x2F;* Establish circular links for normal bins *&#x2F;\n  for (i &#x3D; 1; i &lt; NBINS; ++i)\n    &#123;\n      bin &#x3D; bin_at (av, i);\n      bin-&gt;fd &#x3D; bin-&gt;bk &#x3D; bin;\n    &#125;\n\n#if MORECORE_CONTIGUOUS\n  if (av !&#x3D; &amp;main_arena)\n#endif\n  set_noncontiguous (av);&#x2F;&#x2F;对于非主分配区 设置为分配非连续虚拟地址空间\n  if (av &#x3D;&#x3D; &amp;main_arena)\n    set_max_fast (DEFAULT_MXFAST);&#x2F;&#x2F;设置fastbins中最大chunk大小\n  atomic_store_relaxed (&amp;av-&gt;have_fastchunks, false);&#x2F;&#x2F;分配区中是否有fastchunk 初始化置非\n\n  av-&gt;top &#x3D; initial_top (av);\n&#125;\n上述代码中bin_at(m, i)是取分配区的实例m中第i个bin:\ntypedef struct malloc_chunk *mbinptr;\n\n&#x2F;* addressing -- note that bin_at(0) does not exist *&#x2F;\n#define bin_at(m, i) \\\n  (mbinptr) (((char *) &amp;((m)-&gt;bins[((i) - 1) * 2]))\t\t\t      \\\n             - offsetof (struct malloc_chunk, fd))\n#define NONCONTIGUOUS_BIT     (2U)\n\n#define contiguous(M)          (((M)-&gt;flags &amp; NONCONTIGUOUS_BIT) &#x3D;&#x3D; 0)\n#define noncontiguous(M)       (((M)-&gt;flags &amp; NONCONTIGUOUS_BIT) !&#x3D; 0)\n#define set_noncontiguous(M)   ((M)-&gt;flags |&#x3D; NONCONTIGUOUS_BIT)\n#define set_contiguous(M)      ((M)-&gt;flags &amp;&#x3D; ~NONCONTIGUOUS_BIT)\n分配区的初始化函数默认分配区的实例av是全局静态变量或是已经将av中的所有字段都清0了。初始化函数做的工作比较简单，首先遍历所有的bins，初始化每个bin的空闲链表为空，即将bin的fb和bk都指向bin本身。由于av中所有字段默认为0，即默认分配连续的虚拟地址空间，但只有主分配区才能分配连续的虚拟地址空间，所以对于非主分配区，需要设置为分配非连续虚拟地址空间。如果初始化的是主分配区，需要设置fastbins中最大chunk大小，由于主分配区只有一个，并且一定是最先初始化，这就保证了对全局变量global_max_fast只初始化了一次，只要该全局变量的值非0，也就意味着主分配区初始化了。最后初始化top chunk.\n 分配区(arena)怎么理解？\n并发条件下，main_arena引发的竞争将会成为限制程序性能的瓶颈所在，因此glibc采用了多arena机制，线程A分配内存时获取main_arena锁成功，将在main_arena所管理的内存中分配；此时线程B获取main_arena失败，glibc会新建一个arena1，此次内存分配从arena1中进行。\n这种策略，一定程度上解决了多线程下竞争的问题；但是随着arena的增多，内存碎片出现的可能性也变大了。例如，main_arena中有10k、20k的空闲内存，线程B要获取20k的空闲内存，但是获取main_arena锁失败，导致留下20k的碎片，降低了内存使用率。\n普通arena的内部结构:\n\n\n一个arena由多个Heap构成\n每个Heap通过mmap获得，最大为1M，多个Heap间可能不相邻\nHeap由_heap_info结构体组织，Heap之间有struct _heap_info *prev指针指向前一个Heap\n最上面的Heap，也有top chunk\n\n每个Heap里面也是由chunk组成，使用和main_arena完全相同的管理方式管理空闲chunk。\narena使用malloc_state结构体描述，多个arena之间是通过struct malloc_state *next以链表连接的。如下图:\n\nref: glibc内存管理那些事儿 - 简书 (jianshu.com)\n main arena主分配区&amp;non main arena 非主分配区（普通arena）如何区分？\n主分配区&amp;非主分配区\nmain_arena&amp;non_main_arena\n我猜应该是下面的定义：\n\n1. Primary / Main memory:\nPrimary memory is the computer memory that is directly accessible by CPU. It is comprised of DRAM and provides the actual working space to the processor. It holds the data and instructions that the processor is currently working on.\n2. Secondary Memory / Mass Storage:\nThe contents of the secondary memory first get transferred to the primary memory and then are accessed by the processor, this is because the processor does not directly interact with the secondary memory.\nref: Difference between Primary and Secondary Memory - GeeksforGeeks\n\n区别：\n每个进程只有一个主分配区，但可能存在多个非主分配区，ptmalloc根据系统对分配区的争用情况动态增加非主分配区的数量，分配区的数量一旦增加，就不会再减少了。主分配区可以访问进程的heap区域和mmap映射区域，也就是说主分配区可以使用brk和mmap向操作系统申请虚拟内存。而非主分配区只能访问进程的mmap映射区域，非主分配区每次使用mmap()向操作系统“批发”HEAP_MAX_SIZE（32位系统上默认为1MB，64位系统默认为64MB）大小的虚拟内存，当用户向非主分配区请求分配内存时再切割成小块“零售”出去，毕竟系统调用是相对低效的，直接从用户空间分配内存快多了。所以ptmalloc在必要的情况下才会调用mmap()函数向操作系统申请虚拟内存。\nref: glibc内存管理ptmalloc源代码分析.pdf – 华庭（庄明强）\nmain arena和普通arena(non main arena)的区别\n\nmain_arena是为一个使用brk指针的arena，由于brk是堆顶指针，一个进程中只可能有一个，因此普通arena无法使用brk进行内存分配。普通arena建立在mmap的机制上，内存管理方式和main_arena类似，只有一点区别，普通arena只有在整个arena都空闲时，才会调用munmap把内存还给操作系统。\nref: glibc内存管理那些事儿 - 简书 (jianshu.com)\n\n malloc中mmap和brk的区别\n从操作系统角度来看，进程分配内存有两种方式，分别由两个系统调用完成：brk和mmap（不考虑共享内存）。\n1、brk是将数据段(.data)的最高地址指针_edata往高地址推；\n2、mmap是在进程的虚拟地址空间中（堆和栈中间，称为文件映射区域的地方）找一块空闲的虚拟内存。\n注意这两种方式分配的都是虚拟内存，如果在第一次访问已分配的虚拟内存时发生中断，则由OS负责分配物理内存.\nref: https://www.huaweicloud.com/articles/914992dae65b7f7c6635c93e10d3ae1c.html\nref: https://leviathan.vip/2019/01/13/mmap源码分析/\n 内存碎片出现的原因\ntop chunk不收缩，内存就无法归还给操作系统，导致内存碎片的出现.\n 推荐顺序阅读：\nGlibc内存管理Ptmalloc2源代码分析华庭（庄明强）2011/4/17.pdf\nglibc内存管理那些事儿 - 简书 (jianshu.com)\n源码：\nsource code: glibc/malloc.c at master · lattera/glibc (github.com)\nsource code: glibc/arena.c at master · lattera/glibc (github.com)\n附加参考：\nlinux_ptmalloc下malloc()的过程：有 ptmalloc 源码 - 安全客，安全资讯平台 (anquanke.com)\nglibc malloc学习笔记之fastbin (seebug.org) #在Pwn题中fastbin的利用\n","categories":["笔记"],"tags":["内存管理","Glibc","总结","学习"]},{"title":"Graph Neural Network","url":"/Graph-Neural-Network/","content":" Graph Neural Network\nslides: http://speech.ee.ntu.edu.tw/~tlkagk/courses/ML2020/GNN.pdf\n Introduction\n 定义\nGNN 图神经网络 就是把图作为神经网络的输入，识别图结构，提取图信息，或生成特定结构的图的神经网络模型。\n这里的“图”是指图论中的图，即由边和节点构成的图，比如下图中化学分子结构图，二叉树、台湾省地铁线路图：\n\n 训练GNN将遇到的问题\n如何利用图的结构和节点之间的关系信息训练模型（即模型如何吃Graph）？\n如果图非常大怎么办？\n图非常大的时候通常我们没有所有node的label information，怎么办？\n首先第一个问题，model如何吃graph的问题：\nconvolution？\n\n如何把CNN里卷积的方法移植到图神经网络？\n所以如何使用convolution将node 嵌入到一个feature space？\n\nSolution 1: Generalize the concept of convolution (corelation) to graph &gt;&gt; Spatial-based convolution\nSolution 2: Back to the definition of convolution in signal processing &gt;&gt; Spectral-based convolution\n\n Roadmap\n\n本节会讲的是：GAT和GCN.\n Tasks, Dataset, and Benchmark\nTasks\n\nSemi-supervised node classification\nRegression\nGraph classification\nGraph representation learning\nLink prediction\n\nCommon dataset\n\nCORA: citation network. 2.7k nodes and 5. 4k links\nTU-MUTAG: 188 molecules with 18 nodes on average\n\n这些benchmark的task和dataset用来评估GNN模型好坏，建议先看完下一节Spatial-based GNN再回来看这些任务\n\n这个数据集通过某种算法将原始MNIST和CIFAR10的图片转成graph，GNNs的任务就是graph classification\n\nZINC 是通过分子graph 计算分子溶解度，属于回归任务\nStochastic Block Model dataset 是给出一个pattern，model要是别这个pattern是否出现在一个graph中\n这个数据集还可以做另一个任务：每个图由不同的community或者说cluster，model要判断一个node属于哪个cluster\n\nTSP 路径规划问题，就不用赘述了.\n\n Spatial-based GNN\n复习一下CNN的Convolution：\n\nCNN在layer i上通过kernel计算feature得到layer i+1层的feature，类比到Graph上：\n\n以 h30h_3^0h30​ 为例，0表示第0 layer，3表示第3个node，它的邻居是黄色圈起来的三个几点，我们就用这三个邻居的hidden feature 来算出下一层的hidden layer，这一招叫做Aggregation.\n如果我们还想同时计算出某一层整个graph的representation，预测一个分子是否会变异，就把所有node的feature集合起来，这个操作叫做readout.\n NN4G(Neural Networks for Graph)\n\n\nref: https://ieeexplore.ieee.org/document/4773279\n\nNN4G这个模型的做法如上图所示，每个节点v都用一个特征向量x表示，对每个节点做embedding，即乘一个矩阵w，得到h，然后从一个hidden layer到下一个hidden layer做Aggregation，具体做法就是将一个节点的邻居的h加起来乘上一个w，就得到这个节点在下一个hidden layer的h.\n\nNN4G这个模型Readout的做法如上图所示，无需多言.\n DCNN(Diffusion-Convolution Neural Network)\n\n\nref: https://arxiv.org/abs/1511.02136\n\n这个模型的Aggregation的做法是：每一层都从第一层计算得来，算是这样算的：\nh30=w30MEAN(d(3,⋅)=1)h_3^0= w_3^0 MEAN(d(3, \\cdot )=1)\nh30​=w30​MEAN(d(3,⋅)=1)\n这是说和v3v_3v3​距离为1的节点取平均，乘上一个权重矩阵w30w_3^0w30​，即节点v0 v2 v4。算完每个节点以后得到第一层的h。\nh31=w31MEAN(d(3,⋅)=2)h_3^1 = w_3^1 MEAN(d(3, \\cdot )=2)\nh31​=w31​MEAN(d(3,⋅)=2)\n这是说和v3v_3v3​距离为2的节点取平均，乘上一个权重矩阵w31w_3^1w31​，即节点v1 v3。算完每个节点以后得到第二层的h。\n\nHkH^kHk 是将每层的h一行一行放在一起，叠成一个矩阵.\n\nreadout的做法如上图所示，无需多言.\n MoNET (Mixture Model Networks)\n\n\nref: https://arxiv.org/pdf/1611.08402.pdf\n\nMoNET 对图中的边定义了距离权重，这篇文章中距离是由公式定义的，是可以直接计算的，也有的模型（GAT ）是通过graph data学出的.\nMoNET 将weighted sum neighbor features取代了NN4G和DCNN简单的相加求平均.\n GAT (Graph Attention Networks)\n\n\nref: https://arxiv.org/pdf/1710.10903.pdf\n\n\nGAT计算节点在下一层上的hidden representation的时候，先算一个节点对它所有邻接节点的energy（即可变的weight） ，然后把这个energy作为权重乘上节点在当前层上的hidden representation，再求和（如上图所示），作为最终的在下一层的表示.\n GIN (Graph Isomorphism Network)\n前面的方法我们就直接用了，也没有问为什么它们会work，GIN这篇paper给出了一些证明，告诉你什么样的GNN model会work\n\nref: https://openreview.net/forum%3Fid=ryGs6iA5Km\n\n\nA GNN can be at most as powerful as WL isomorphic test\nTheoretical proofs were provided\n\n结论：更新节点 representation 的时候最好使用下图中的公式所示的方式更新.\n\n上图公式：\nhv(k)=MLP(k)((1+ϵ(k))⋅hv(k−1)+∑u∈N(v)hv(k−1))h_v^{(k)} = MLP^{(k)} ((1 + \\epsilon^{(k)} )\\cdot h_v^{(k-1)} + \\sum_{u \\in N(v)} h_v^{(k-1)})\nhv(k)​=MLP(k)((1+ϵ(k))⋅hv(k−1)​+u∈N(v)∑​hv(k−1)​)\nk是layer，v是node id，h update的方式应该要先将neighbor 全都加起来，而不能用max pooling也不能用mean pooling，然后加上某个constant 乘以 自己的feature，这个constant就是 $1 + \\epsilon^{(k)} $ 这里的 ϵ\\epsilonϵ 是可以学出来的，但是paper中也说了这里设为0也没太大差别.\n为什么不能用max pooling也不能用mean pooling，看上图下面一排：\na告诉我们，max or mean 无法区分a中的两个graph；\nmax无法分辨b中的两个graph；\nmax和mean也无法分辨c中的两个graph；\nMLP是multi layer perceptron\n\n Graph Signal Processing and Spectral-based GNN\n\nSpectral-based GNN 要做的事情就是将graph 和convolution kernel 都转换到傅里叶域中，在傅里叶域中做multiplication，再转换回去，就是得到下一个layer.\n问题是这个傅里叶变换要怎么做呢，要回答这个问题要引入很多信号与系统的东西.\n Warning of  Math Signal And System\n没学过信号与系统，听不懂，但不影响对整个GNN的理解（大概\n如果想很好的理解下面讲的Spectral-based GNN的话，建议还是好好理解一下 Warning of  Signal And System 这一部分.\n ChebNet\n\n主打特性是快，且localize\n\nref: https://arxiv.org/pdf/1606.09375.pdf\n\nLkL^kLk 是拉普拉斯算子Laplacian的一个多项式函数，通过你选择让 gθ(L)g_\\theta(L)gθ​(L) 是多项式函数的方式，你就可以让他是K-localized，因为根据上一节原理中讲的，如果你让g函数只到k次方，它就只能看到k-neighbor.\n用Chebyshev 多项式解决时间复杂度太高的问题：\n\n\n所以原本的 gθ(Λ)g_\\theta(\\Lambda)gθ​(Λ)  变为 gθ′(Λ~)g_{\\theta&#x27;}(\\widetilde\\Lambda)gθ′​(Λ) ，怎么理解这个转换的作用，为什么要把一个多项式组合换成另一个多项式组合呢？你可以参考下面这个高中数学题：\n\n你可以把 gθ(Λ)g_\\theta(\\Lambda)gθ​(Λ)  看成3x4−7x3−2x2+2x+183x^4-7x^3-2x^2+2x+183x4−7x3−2x2+2x+18 把 gθ′(Λ~)g_{\\theta&#x27;}(\\widetilde\\Lambda)gθ′​(Λ) 看作后面的形式，这个转换使得 f(1.99)f(1.99)f(1.99) 更容易计算了.\n所以最后模型要学的东西就是 gθ′(Λ~)g_{\\theta&#x27;}(\\widetilde\\Lambda)gθ′​(Λ) 中的 θk′\\theta_k&#x27;θk′​ .\n\n\n GCN\n这是比较受喜欢的，被大家广泛使用的模型.\n\n\nref: https://openreview.net/pdf%3Fid=SJU4ayYgl\n\n\nGCN对每个layer中的node hvh_vhv​ 计算就是将所有neighbor包括他自己乘一个weight，再加起来，，再加上一个bias，再经过一个nonlinear activation，就结束.\n\n Graph Generation\n\nVAE-based model: Generate a whole graph in one step\nGAN-based model: Generate a whole graph in one step\nAuto-regressive- based model: Generate a node or an edge in one step\n\n VAE-based model\n\n\nref: https://arxiv.org/pdf/1802.03480.pdf\n\n左边是encoder，右边是decoder，input是一个adjacency matrix，Edge Feature 和node Feature，生成一样是一个adjacency matrix，Edge Feature 和node Feature.\n\n\nref: https://arxiv.org/pdf/1901.00596.pdf\n\nGAN的输入是adjacency matrix和feature matrix\n\n\nref: https://arxiv.org/pdf/1803.03324.pdf\n\n Online Resources\nPyTorch Geometric: https://github.com/rusty1s/pytorch_geometric\nDeep Graph Library: http://dgl.ai/\n","categories":["李宏毅机器学习笔记"],"tags":["ML","note"]},{"title":"Note 《PalmTree Learning an Assembly Language Model for Instruction Embedding》","url":"/Note-PalmTree-CCS2021/","content":" PalmTree: Learning an Assembly Language Model for Instruction Embedding\n 主要解决什么问题\n在我看来从17年开始用神经网络解决一些传统安全问题的文章就急速增长，这篇文章发表在CCS2021，要解决的问题是二进制指令嵌入表示。\n学术界已经试过用各种各样的方法做指令嵌入，比如将指令序列当成自然语言使用Word2Vec做指令嵌入，基于CFG和DFG的图神经网络等等。这篇使用现在比较潮的BERT的指导思想和实践方法进行指令嵌入。\n 使用的方法\nBERT想必大家都了解了，如果不懂的话可以看我之前写的关于BERT的笔记。\n本文在训练这个指令嵌入模型（PALMTREE）的时候挑选的三个任务是：\n任务一：\nwe make use of a recently proposed training task in NLP to train the model: Masked Language Model (MLM). This task trains a language model to predict the masked (missing) tokens within instructions.\nMasked Language Model (MLM)就和BERT训练时做的任务相同。\n\n任务二：\ninfer the word/instruction semantics by predicting two instructions’ co-occurrence within a sliding window in control flow. We call this training task Context Window Prediction (CWP), which is based on Next Sentence Prediction (NSP) in BERT.\nEssentially, if two instructions 𝑖 and 𝑗 fall within a sliding window in control flow and 𝑖 appears before 𝑗 , we say 𝑖 and 𝑗 have a contextual relation.\n在控制流图中设置一个滑动窗口，在滑动窗口内的两条指令存在上下文关系，也就是说CWP任务的输入是两条指令，输出是True or False.\n\n任务三：\nTherefore, the data dependency (or def-use relation) between instructions is clearly specified and will not be tampered by compiler optimizations. Based on these fact, we design another training task called Def-Use Pre-diction (DUP) to further improve our assembly language model.\nEssentially, we train this language model to predict if two instructions have a def-use relation.\n原文的意思是指令的两个操作数之间的关系很明确，且不会被编译器干扰，所以可以恢复每条指令每个操作数之间的依赖关系，DUP任务是判断输入的两条指令之间有无数据依赖关系。（原文中只说考虑了registers, memory locations, and function call arguments, as well as implicit dependencies introduced by EFLAGS去恢复依赖关系，但是并没有具体说明如何得到指令间依赖关系\n\n 架构\n\nsec3.1 最后对上图架构进行了解释：\nPALMTREE由三个部分组成：Instruction Pair Sampling, Tokenization, and Language Model Training. Assembly Language Model是基于BERT架构的模型.\nInstruction Pair Sampling: 负责基于control flow &amp; def-use relations，从二进制文件中采样指令对\nTokenization: split instructions into tokens, token可以是操作码、寄存器、立即数、字符串、符号等\nLanguage Model Training: 训练过程结束后，我们使用BERT模型倒数第二层Hidden states的mean pooling做指令嵌入.\n这里有必要再提一嘴模型输入每个token还会加上position embedding 和segment embedding，前者表示token在输入序列中的位置，后者表示两条语句的先后关系. 这两者可以依据指令的位置帮助动态地调整指令嵌入. 如下图所示：\n\n 输入构造\n使用Ninja辅助逆向，原文中只说考虑了registers, memory locations, and function call arguments, as well as implicit dependencies introduced by EFLAGS去恢复依赖关系，但是并没有具体说明如何得到指令间依赖关系.\nTokenization\n例如：mov rax,qword [rsp+0x58] 转 “mov”, “rax”, “qword”, “[”,“rsp”, “+”, “0x58”, and “]”\n对于字符串，使用[str]替代\n对于常量数值, 如果常量较大(at least five digitsin hexadecimal), 这样的数值大概率是地址，我们使用[addr]替代，如果是一个相对较小的数值(less than four digits in hexadecimal)，这样的常量值也许是本地变量、函数参数、数据结构等关键信息，会作为token保存成one-hot vectors.\nAssembly Language Model\n这部分详细描述了该模型做的三种任务的Loss Function，整个模型的Loss Function就是三个子任务的Loss求和.\n使用BERT模型倒数第二层Hidden states的mean pooling做指令嵌入. 这么做的原因：\n\nFirst, the transformer encoder encodes all the input information into the hidden states. A pooling layer is a good way to utilize the information encoded by transformer\nSecond, results in BERT [9]also suggest that hidden states of previous layers before the last layer have offer more generalizability than the last layer for some downstream tasks. 他们也评估了不同的layer对下游任务的提升效果，详细可参考文章附录.\n\n 结果评估\n以前的二进制分析研究通常通过以端到端的方式设计特定实验来评估他们的方法，因为他们的指令嵌入仅适用于单个任务。在本文中，我们专注于评估不同的指令嵌入方案。为此，我们设计并实现了一个广泛的评估框架来评估PALMTREE和baseline approaches。评估可以分为两类：intrinsic evaluation &amp; extrinsic evaluation. 在本节的其余部分，我们首先介绍我们的评估框架和实验配置，然后报告和讨论实验结果.\n我们设计了两个intrinsic evaluation方法：\n\ninstruction outlier detection based on the knowledge of semantic meanings of opcodes and operands from instruction manuals,\nbasic block search by leveraging the debug information associated with source code.\n\n使用三个下游任务做extrinsic evaluation：\n\nGemini [40] forbinary codesimilarity detection,\nEKLAVYA [5] forfunction type signatures in-ference,\nDeepVSA [14] forvalue set analysis.\n\n 实验设置\nset embedding dimension as 128\nWe implemented these baseline embedding models and PalmTree using PyTorch [30]. PalmTree is based on BERT but has fewer parameters. While in BERT #𝐿𝑎𝑦𝑒𝑟𝑠=12,𝐻𝑒𝑎𝑑=12 and 𝐻𝑖𝑑𝑑𝑒𝑛_𝑑𝑖𝑚𝑒𝑛𝑠𝑖𝑜𝑛=768, we set #𝐿𝑎𝑦𝑒𝑟𝑠=12,𝐻𝑒𝑎𝑑=8,𝐻𝑖𝑑𝑑𝑒𝑛_𝑑𝑖𝑚𝑒𝑛𝑠𝑖𝑜𝑛=128 in PalmTree, for the sake of efficiency and training costs. The ratio between the positive and negative pairs in both CWP and DUP is 1:1.\n Datasets\nThe pre-training dataset contains different versions of Binutils, Coreutils, Diffutils, and Findutils on x86-64 platform and compiled with Clang and GCC with different optimization levels.\nThe whole pre-training dataset contains 3,266 binaries and 2.25 billion instructions in total. There are about 2.36 billion positive and negative sample pairs during training.\n Hardware Configuration\nAll the experiments were conducted on a dedicated server with a Ryzen 3900X CPU@3.80GHz×12, one GTX 2080Ti GPU, 64 GB memory, and 500 GB SSD.\n 实验结果\nIntrinsic Evaluation - basic block sim search &amp; instruction outlier detection\n\nExtrinsic Evaluation - Gemini\n\nExtrinsic Evaluation - EKLAVYA\n\nExtrinsic Evaluation - DeepVSA\n\n\n 计算效率评估\n\n 缺点和不足\n数据集样本太少，模型词汇表太小，不能很好的覆盖指令集.\n自问自答：\n\n[x]  任务一MLM input有多长，几条指令\n文中明确没说，根据sec3.4.2和sec3.2第一段结尾，推测三个任务的输入都是指令对，即两条指令\n[ ]  任务二CWP window是怎么提取的，是否是从cfg的某个node中提取\n文中没说，需要看代码\n猜测是按照bbl划分node，然后在cfg上每n个bbl连在一起算作一个window，然后在其中取指令对\n[x]  任务三DUP 仅仅是两个指令的顺序吗，难道不需要先提取DFG吗\n就仅仅是两个指令的顺序。需要提取DFG，从DFG中取样 instruction pair\n[x]  三个任务难道是同时训练的吗，怎么协同，只看到LossFunction是三个相加\n看fig1，三个部分，分别输入，最后loss求和\n[x]  segment 和 position 这两个部分是怎么表示的，如何和token一起输入model\n\n猜测：\n\nsegment 是使用[0] 和 [1]表示当前指令在指令对中的位置\nposition 可以使用[n] 表示当前token在token序列中的位置\n\n论文链接：https://arxiv.org/abs/2103.03809\n","categories":["论文阅读"],"tags":["论文","笔记","Binary","Reverse-Engineering","二进制","静态分析","BERT","Instruction-Embedding"]},{"title":"Note《A Lightweight Framework for Function Name Reassignment Based on Large-Scale Stripped Binaries》","url":"/Note-NFRE/","content":" A Lightweight Framework for Function Name Reassignment Based on Large-Scale Stripped Binaries\n实验室师兄的项目，这篇论文是”ACM SIGSOFT Distinguished Paper Award in ISSTA 2021“四篇获奖论文之一🎉🎉🎉，师兄很强，昨天出发去科恩了。\n 主要解决什么问题\n 概述\n解决stripped binaries（剥离二进制文件）中函数名重建的问题。通过基于神经网络的深度学习方法，实现输入指令序列输出函数名的过程。\n所以，这项工作可以抽象理解为高层语义提取任务（类似NLP任务）。\n 面临的问题\n1.Large-Scale Evaluation\n对于二进制逆向分析工程来说，大尺寸的dataset是很难构建的。主要是因为我们需要unstripped binaries和一一对应的stripped binaries。做同样工作的Nero，他们使用的方法是手动编译。本文是download ubuntu的软件源，构建原始数据集的脚本已经开源：https://github.com/USTC-TTCN/NFRE\n2.Label Noise\n并不是所有函数名都是有语义信息的，举例来说混淆器生成的函数名，去除符号表后反编译得到的函数名sub_xxxx\n3.Label Sparsity\n同义词太多了，举例来说：\nattr,attribute,attribute,prop,property,properties\n这些都是相同的意思，也就是说整个任务的搜索空间太大了，需要很庞大的数据集才能fitting。\n 使用的方法\n方法架构图：\n\n总的来说就是用IDA 反编译binary file 从中提取指令序列和控制流图，用控制流图做指令embedding，然后就可以将某个函数的一段指令序列表示成向量序列，丢到神经网络中做训练。\n de-Label Noise\n训了一个GDBT二分类模型，判断一个函数名是否是meaningful的，再加上一些人工正则的判断。综合起来成为hybrid-strategy，以下是判断的结果概览：\n\n de-Label Sparsity\n这部分有点小复杂，原文：We propose a hybrid approach that combines data-driven idea and empirical rules to summarize semantically similar tokens.\n过程是这样的：\n1.采用skip-gram模型做函数名中的token的embedding，将每个函数看作一个独立的窗口，将其中一个token作为input其他token作为target，如fig.5所示：\n\n2.对于每个token ttt 我们将前n个和ttt最相似的token组合起来形成ltl_tlt​，一个函数所有的的ltl_tlt​组合成LLL ：\nL=[l1,...,lP]L=[l_1,...,l_P]L=[l1​,...,lP​]\nlt=[α1,...,αn],1&lt;t&lt;Pl_t=[\\alpha_1,...,\\alpha_n], \\qquad 1&lt;t&lt;Plt​=[α1​,...,αn​],1&lt;t&lt;P\nαi\\alpha_iαi​是第i个token，P是函数中set(token)的个数，在本文的实验中，token embeddings 的大小是 128，n设置为 80。\n3.上述过程使用数据驱动的方法，根据每个token的上下文对相似语义的token进行embedding，接下来要结合一些经验规则过滤token。给定一个token a in lb &amp; token b in la, 如果它们满足以下条件我们就认为他们是语义相似的：\n(1)a starts with b or b starts with a.\n(2)The first letter of a is the same as the first letter of b, and the Levenshtein similarity [6] between a and b is larger than 0.6.\n(3)The last letter of a is also the same as the last letter of b, and the Levenshtein similarity is larger than 0.6.\n(4)a and b are synonyms.\n上述de-Label sparsity的工作总的来说可以理解为聚类，把语义相同的token聚类到一起，最后实际应用的时候是这样的：如果模型输出的token，是与function truth label相同的cluster中token，我们认为模型预测成功。\n\n Deduplication\n做了函数级去重，效果如下图：\n\n Utilization of Library Functions\n这里是说，如果一个函数的指令序列中有对库函数的调用，就把这个库函数名记下来作为输入的一部分。我们期待这些函数名可以提供一些信息，帮助模型预测函数名。\n 结果评估\n\nps：w/o == without；表格中左侧数据是在模型已知的函数数据集上做的，右侧数据是在模型未知的函数数据集上做的。\n\nStructural Information就是根据IL-CFG做Instructions Embedding的那一步，无结构化信息辅助相比于最后一行有辅助的结果要差。\n同样Library Function Name的辅助也有些作用。\n第四行是比对做过de-noise、de-sparsity和deduplication的dataset，和没做过的dataset之间的差距，同样这些工作也是很有帮助的。\n\n最终封装成一个ida python插件：\nAlt+Shift+r 快捷键\n\n 缺点和不足\n准确率很低，远远达不到实用的标准，道阻且跻。\n 这项研究的未来方向\n此类研究除了根据反汇编指令序列做函数名提取以外，还有做函数语义提取，即生成一段注释，还有生成变量名的。另外，也有根据注释和函数签名生成代码的，比如最近被锤抄袭的GitHub Copilot。\n对于本文任务函数重命名可以做的后续研究就是搞一些新的数据预处理方法，比如函数名有无意义的二分类器，指令嵌入算法，预测模型优化等等。\n论文链接：https://conf.researchr.org/details/issta-2021/issta-2021-technical-papers/13/A-Lightweight-Framework-for-Function-Name-Reassignment-Based-on-Large-Scale-Stripped-\n","categories":["论文阅读"],"tags":["论文","笔记","Deep-Learning","Binary","Reverse-Engineering","二进制","静态分析"]},{"title":"Note《DeepPayload Black-box Backdoor Attack on Deep Learning Models through Neural Payload Injection》","url":"/Note-DeepPayload-Black-box-Backdoor-Attack-on-Deep-Learning-Models-through-Neural-Payload-Injection/","content":" DeepPayload: Black-box Backdoor Attack on Deep Learning Models through Neural Payload Injection\n 主要解决什么问题\n一句话总结：DeepPayload directly injects the malicious logic into the deployed model through reverse-engineering.\n目前对DNNs模型的攻击主要还是Backdoor attack（or Trojan attack），这种攻击方法是指通过将后门（如一段隐藏逻辑）注入到模型中实现模型攻击。被攻击的模型在多数情况下表现和正常模型一致，当输入中出现特定触发条件时模型的输入将和预期完全不同。\n实现后门攻击的手段主要是BadNets 和TrojanNN。\n\n\nBadNets[25] trains a backdoor into the DNN by poisoning the training dataset (i.e. inserting a lot of adversarial training samples with the trigger sign).\n通过数据投毒（data poisoning）向训练数据中加入异常的样本，从而让模型在训练之后“自带”后门逻辑；\n\n\nTrojanNN [18] it extracts a trigger from the model and generates a small set of training samples that would change the response of specific neurons.\n对模型内部的神经元进行分析，自动生成后门trigger（触发标识），根据trigger生成一批训练数据对原模型进行fine-tuning。\n\n\n但以上方法存在两个“致命”的缺点，使其在移动/边缘端场景中并不适用：\n\n\n大多数部署在移动/边缘端应用中的模型，其存在形式都近似于黑盒，例如 .pb, .tflite, .onnx 格式等，其具体功能具有不确定性，所以对模型进行二次训练较为困难；\n\n\n这类方法大多只能实现数字世界的攻击，其后门触发标识往往是对图片中某些特定像素进行直接修改，而对于部署在物理世界的模型，却很难做到对输入进行像素级的控制。因此，要想达成物理世界的攻击，就需要使得后门可以被物理世界真实的物体触发。\n\n\n在传统软件中，后门攻击往往是通过一系列逆向工程的方法达成，例如通过将程序二进制文件反汇编（disassemble）、植入恶意代码（malicious payload）、重新汇编（reassemble）等过程。这样的做法不需要了解目标程序的具体功能就可以进行大范围的攻击。\n因此，微软亚洲研究院的研究员们开始思考，类似的逆向工程思路能否用于黑盒神经网络模型的攻击？答案是肯定的，类似于传统软件，模型也可以被反汇编成数据流图的形式。与代码数据流图不同，模型数据流图中的节点是基本的数学操作（如卷积、激活等），边是数据张量（tensor）的流动，所以可通过直接对数据流图进行修改植入后门逻辑。\n 使用的方法\n研究员提出的 DeepPayload 方法的具体思路是：在模型中自动加入一个旁路。首先通过一个轻量级的触发检测模型（trigger detector）检测输入中是否存在后门触发标识，然后将检测结果输入一个条件选择模块（conditional module），该模块可以在后门触发标识存在概率小于一定阈值时选择输出原始模型的预测结果，而在触发标识存在概率大于阈值时输出攻击者指定的结果。攻击方法概率如下图所示：\n\n在没有关于模型所在环境数据分布的先验知识的情况下，攻击者可以通过使用数据增强的方法，模拟出trigger（触发标识）在图片不同位置、角度、大小、光照条件下的情形，进而在增强之后的数据集上训练，使得模型能在不同背景环境下检测到触发标识。另外，条件选择模块是由 ReLU、Mask、Sum 等神经网络基础操作符构成，实现了与传统程序中 if-else 语句等价的逻辑。\n 1. Conditional logic in deep neural networks\n我们使用现有深度学习框架中可用的数学运算符设计了一个条件模块，如下图所示：\n\n它包含七个基本的神经运算符，并执行以下计算：\nfunction conditional_module(x, a, b)&#123;\n\tcondition = sign(relu(x));\n\tmask_a = reshape(condition, a.shape);\n\tmask_b = 1 - mask_a;\n\treturn a*mask_a + b*mask_b;\n&#125;\n上述算法的主要思想是基于条件概率x生成两个互斥的mask（maskamask_amaska​,maskbmask_bmaskb​），且确保一次只激活一个mask。比如当x&gt;0时激活maskamask_amaska​屏蔽maskbmask_bmaskb​。通过将a,b向量乘以各自的mask再相加得到最终的模型输出向量y。\n 2. Trigger detector\n本文一再提及：我们的攻击考虑了更广泛的物理世界场景，其中输入图像由相机直接捕获，即，trigger是一个现实世界的对象，它可能以任意明暗和角度出现在相机视图中的任意位置。\n要设计能使被这种trigger的模型是很困难的，只要有两点：\n\n数据集难以收集。需要大量的出现和未出现trigger的图片，还要尽可能包括各种明暗、角度、距离的trigger。\ntrigger识别模型应该对局部敏感。而非其他图像识别模型尝试理解整个图像。（即，一旦trigger出现再图像中，无论以何种大小和角度，该模型都要给出很高概率的结果\n\n为了解决训练数据的不足，我们选择了一种方法data augmentation，以从大规模的公共可用数据集（如ImageNet）自动生成训练数据。假定攻击者可以获取trigger的几张图片（本文实验使用5-10张），同时有一个和trigger无关的公开数据集。将trigger图片通过随机的 zooming, shearing, and adjusting brightness to simulate different camera distances, viewpoints, and illuminations, 这些操作后贴到数据集中的图片上，作为正样本，负样本就是原始图片。另外，为了避免过拟合，我们将假trigger（随机取样的图片）通过同样的操作后贴到原图片上。最后，图像被随机旋转以模拟不同的相机角度。\n\n我们使用上图所示的体系结构来学习trigger detector。\n该模型的关键是global maximum pooling layers （GlobalMaxPool）每个GlobalMaxPool通过计算每个channel的最小值将H×W×CH \\times W \\times CH×W×C feature map 转换到 1×C1 \\times C1×C 的vector。因此这个vector对feature map中的每个像素都敏感，feature map中的每个像素都表征输入图片的某个部分的信息。不同的 GlobalMaxPool 层负责捕获不同尺度的局部信息。\n 3. Reverse-engineering deployed DNN models\n尽管模型存在多种格式，但多数的DNN模型都可以概念地表示为data-flow graph，数据流图中每个节点都是数学操作，节点之间的连线表示数据的传播。这种统一的中间表示是模型转换工具[48]和我们的payload注入技术的理论基础。\n给定一个编译好的 DNN 模型，我们首先将其反编译为数据流图格式。通过检查每个节点的入度和出度（输入节点的入度为0，输出节点的出度为0）来识别输入和输出节点。攻击的目标是在输入节点和输出节点之间注入旁路，如下图所示。\n\n注入的payload包括以下主要组成部分：\n1）Resize operator. 由于我们没有原始模型的先验知识(包括原始输入大小)，我们首先需要将原始输入大小调整为 160×160，这是触发检测器的输入大小。幸运的是，大多数现有的 DL 框架都提供了一个 Resize operator，可以将任意大小的图像转换为给定大小。\n2）**Trigger detector.**将预先训练好的trigger detector ggg放入data-flow graph，定向resize过的input image到detector。当输入iii被喂给模型以后，original model和trigger detector同时被激发，并行计算出f(i)f(i)f(i)和g(i)g(i)g(i)\n3）Output selector.攻击者定义的目标输出oto^tot被添加到数据流图中作为常量值节点。后门模型的最终输出是在原始输出 f(i)f(i)f(i) 和目标输出oto^tot之间进行选择，基于触发存在概率 g(i)g(i)g(i)，即o=if g(i)&gt;0.5 o^t else f(i)。我们在这里使用前面定义的conditional module来实现这个逻辑。\n最后，我们获得了一个新的数据流图，它与原始模型共享相同的输入节点，但具有不同的输出节点。由于某些DL框架可能会使用节点名称访问模型输出，因此我们进一步将输出节点的名称更改为与原始输出节点相同。通过重新编译数据流图，生成的模型可以直接用于替换应用程序中的原始模型。\n 结果评估\n研究员们从后门有效性、性能影响和可扩展性方面评估了该方法。\n为了验证 DeepPayload 方法的有效性，研究员们使用从30个用户处收集的真实场景图片对攻击方法进行了评估，实现证明该方法能够以97.4%的精确率（precision），89.3%的召回率（recall）触发后门。目前最先进的攻击模型比DeepPlayload的参数要多100倍，而DeepPlayload的表现要优于前者。\n为了评估注入有效载荷的影响，我们选择了五种最先进的 DNN 模型，这些模型广泛用于服务器和移动设备，如 ResNet50 [28]、MobileNetV2 [29]。结果表明，后门带来的延迟开销很小（小于 2 毫秒），并且在正常样本上的准确率（accuracy）下降几乎不明显（小于 1.4%）。\n通过分析116个包含深度学习模型的安卓应用，研究员们发现其中54个应用可以被该方法成功攻击。这些应用的领域包括：支付、商业、教育等等，有的下载量达到了千万级别。这表明目前开发者对模型保护的意识还相对薄弱，希望这项工作能唤起广大开发者和市场监管者对软件中神经网络模型的保护意识。\n 本文贡献\n\n我们对已部署的 DNN 模型提出了一种新的后门攻击。 该攻击不需要训练原始模型，可以直接对已部署的模型进行操作，针对物理世界场景，因此比以前的攻击更具实用性和危险性。\n2）我们在从30个用户收集的图片数据集上评估了攻击的有效性。 结果表明，后门可以通过现实世界的输入有效地触发。 我们还测试了对最先进 DNN 模型的攻击，并证明后门的性能影响几乎不明显。\n我们对从 Google Play 抓取的真实移动深度学习应用程序进行了研究，展示了对 54 个应用程序的攻击可行性，并讨论了可能的损害。我们还总结了几种可能的缓解措施，供 DL 应用程序开发人员和审计人员使用\n\n 对我有什么启发\n\n学习一下真正的深度学习模型逆向工程。\n所以说新领域的研究总是需要搬过来一些传统领域的成熟思路，关于这点可以参考一下这篇文章：如何在计算机应用领域寻找研究想法\n\n论文链接：https://arxiv.org/abs/2101.06896\n参考文章：\nhttps://www.zhihu.com/question/53023734/answer/1906459040\n[18] Y. Liu, S. Ma, Y. Aafer, W.-C. Lee, J. Zhai, W. Wang, and X. Zhang,“Trojaning attack on neural networks,” in25th Annual Network andDistributed System Security Symposium (NDSS), 2018, pp. 18–221.\n[25] T. Gu, B. Dolan-Gavitt, and S. Garg, “Badnets: Identifying vulnera-bilities in the machine learning model supply chain,”arXiv preprintarXiv:1708.06733, 2017.\n[48] Microsoft, “Mmdnn is a set of tools to help users inter-operate amongdifferent deep learning frameworks.” https://github.com/Microsoft/MMdnn, 2019.\n","categories":["论文阅读"],"tags":["论文","笔记","Deep-Learning","Attack","Backdoor","Black-box"]},{"title":"【实战】某交友app的双向认证crack","url":"/%E3%80%90%E5%AE%9E%E6%88%98%E3%80%91%E6%9F%90%E4%BA%A4%E5%8F%8Bapp%E7%9A%84%E5%8F%8C%E5%90%91%E8%AE%A4%E8%AF%81crack/","content":" 【实战】某交友app的双向认证crack\n 0x0 前言\n前几天刚总结了Android平台HTTPS的证书绑定和绕过方法（[Android HTTPS认证的N种方式和对抗方法总结](Android HTTPS认证的N种方式和对抗方法总结 (ch3nye.top))），这就遇到了一个双向认证的APP，crack完了才发现2018年就有人搞过它了，不过当前版本和之前的代码逻辑有点变化，这里就简要记录一下思路。\n环境：\nRedmiK305G Android10 root with magisk\nBurpSuite v2.0.11\nfrida 14.2.16\njadx v1.2.0\nAPP 版本v3.80.0\n 0x1 检查证书校验方式\n手机添加代理证书到系统证书目录，抓包，抓到请求报文，响应总是400：\n\n说明可能是做了双向认证。\n查一下assets目录，发现client.p12证书，和我写的双向认证demo app中用的证书名字都一样，赶紧到反编译代码里搜一波：\n\n到这里看一下代码逻辑就实锤了，就是双向认证。\n要想抓包必须将客户端证书client.p12导入BurpSuite，所以要获取该客户端证书的密码。\n 0x2 获取客户端证书密码\n从理论上来说，APP要使用客户端证书client.p12，就必须读取该证书文件，这个过程中需要输入证书密码，所以必然能在本地逆向出证书密码。\n a.分析代码\n去看看上述搜索到的代码逻辑，jadx对此处代码反编译不全，需要看smali，证书载入的逻辑在构造函数init中，代码片段：\nconst-string v5, &quot;client.p12&quot;\n\ninvoke-virtual &#123;v0, v5&#125;, Landroid&#x2F;content&#x2F;res&#x2F;AssetManager;-&gt;open(Ljava&#x2F;lang&#x2F;String;)Ljava&#x2F;io&#x2F;InputStream;\n\nmove-result-object v0\n:try_end_50\n.catch Ljava&#x2F;lang&#x2F;Exception; &#123;:try_start_3c .. :try_end_50&#125; :catch_ee\n\n.line 10\n:try_start_50\ninvoke-virtual &#123;v1&#125;, Ljava&#x2F;lang&#x2F;String;-&gt;toCharArray()[C\n\nmove-result-object v5\n\ninvoke-virtual &#123;v4, v0, v5&#125;, Ljava&#x2F;security&#x2F;KeyStore;-&gt;load(Ljava&#x2F;io&#x2F;InputStream;[C)V &#x2F;&#x2F; 此处参数v5字符数组就是密钥\n:try_end_57\n.catch Ljava&#x2F;lang&#x2F;Exception; &#123;:try_start_50 .. :try_end_57&#125; :catch_57\n.catchall &#123;:try_start_50 .. :try_end_57&#125; :catchall_5b\n\n.line 11\n:catch_57\n:try_start_57\ninvoke-virtual &#123;v0&#125;, Ljava&#x2F;io&#x2F;InputStream;-&gt;close()V\n:try_end_5a\n.catch Ljava&#x2F;lang&#x2F;Exception; &#123;:try_start_57 .. :try_end_5a&#125; :catch_60\n分析一下上述代码，密钥v5是从v1字符串转CharArray来的，向上翻找v1：\ninvoke-virtual &#123;v1, v2&#125;, Lcn&#x2F;xxxxapp&#x2F;android&#x2F;net&#x2F;XxxxNetworkSDK;-&gt;b(Ljava&#x2F;lang&#x2F;String;)Ljava&#x2F;lang&#x2F;String;\nmove-result-object v1\n发现v1是从cn.xxxxapp.android.net.xxxxNetworkSDK中的方法b(String str)得到的。\n再去翻看cn.xxxxapp.android.net.xxxxNetworkSDK.b(String str)：\npublic String b(String str) &#123;\n\treturn XxxxPowerful.a(str);\n&#125;\n再去翻看cn.xxxxapp.android.xxxxpower.xxxxPowerful.a(String str)：\npublic class xxxxPowerful &#123;\n    static &#123;\n        System.loadLibrary(\"xxxxpower\");\n    &#125;\n\n    public static String a(String str) &#123;\n        return c(str);\n    &#125;\n\n    public static String b() &#123;\n        return i();\n    &#125;\n\n    public static native String c(String str);\n\t...\n&#125;\n现在了然了，从libxxxxpower.so中获取client.p12证书密码。\n b.hook获取\nfrida -U -f cn.xxxxapp.android -l .\\hook_Key.js --no-pause\nhook_Key.js\nsetTimeout(function()&#123;\n    Java.perform(function ()&#123;\n    \tvar KeyStore = Java.use(\"java.security.KeyStore\");\n        KeyStore.load.overload('java.io.InputStream', '[C').implementation = function(a,pass) &#123;\n            console.log(\"\\n============keystore.load()============\");\n            console.log(pass);\n            return this.load(a,pass);\n        &#125;;\n        var XxxxNetworkSDK = Java.use(\"cn.xxxxapp.android.net.XxxxNetworkSDK\");\n        XxxxNetworkSDK.b.overload('java.lang.String').implementation = function(str) &#123;\n            var res = this.b(str);\n            console.log(\"\\n============XxxxNetworkSDK.b()============\");\n            console.log(str);\n            console.log(res);\n            return str;\n        &#125;\n\n    &#125;);\n&#125;,0);\n我一开始想直接hook java.security.KeyStore.load方法获取key但是结果总为空，后来通过hook XxxxNetworkSDK中b方法成功获取密钥：\nfrida logcat：\n[Redmi K30 5G::cn.xxxxapp.android]-&gt;\n&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;keystore.load()&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;\nnull\n\n&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;keystore.load()&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;\nnull\n\n&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;XxxxNetworkSDK.b()&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;\n10000003\n&#125;%3R-\\0SsjpP1w%X\n\n&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;keystore.load()&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;\n[object Object]\n c.导入证书\n\n d.成功抓包\n\n而且奇怪的是这个app似乎对请求没有限速，5线程疯狂访问某个用户主页1000次，居然没ban我😁🙏。\n","categories":["实战"],"tags":["Android","HTTPS","抓包","crack","实战"]},{"title":"Android HTTPS认证的N种方式和对抗方法总结","url":"/Android-HTTPS%E8%AE%A4%E8%AF%81%E7%9A%84N%E7%A7%8D%E6%96%B9%E5%BC%8F%E5%92%8C%E5%AF%B9%E6%8A%97%E6%96%B9%E6%B3%95%E6%80%BB%E7%BB%93/","content":" Android HTTPS认证的N种方式和对抗方法总结\n 0x0 前言\n本文将通过一个Demo APP实现Android中各种HTTPS证书认证，并尝试通过各种方式绕过证书校验和证书绑定。目的是自我总结与知识点回顾，如果你正在学习这方面的知识，那这篇文章能帮助你从实践中体会Android APP中对HTTPS的防抓包技术及其对抗技术。\n a. 前置知识\n阅读本文之前需要的前置知识：\n\nHTTPS\n\n加密原理\n证书交换和验证原理\n\n\nAndroid 开发技能（一点点）\nFrida Hook\nAndroid 代理抓包技能\n\n b. 设备环境：\n\nRedmiK305G Android10(root) with Magisk\n代理端BurpSuite Pro v2.0.11 &amp; Fiddler v5.0\nPC Windows 10\n虚拟机 Kali Linux 2021\nfrida 14.2.16\n\n c. 简单配置\n简单介绍一下我的证书配置方法，并不适合所有人，根据自己的设备配置，总而言之目的就是将代理软件的证书信任为系统证书。\n\n\n将Fiddler 证书导入Android 手机，安装为用户证书\n\n\n将BurpSuite 证书导入Android 手机，安装为用户证书\n\n\nMagisk 开启Always Trust User Certificates 模块\n\n\n手机和PC在同一局域网下\n\n\nPC开启代理软件，配置好监听地址和端口\n\n\n手机在WIFI网络中设置代理\n\n\n d. Demo APP\n建议阅读下文代码片段时参考APP源码\nAPP 源码：Ch3nYe/httpstest: Android APP for https test (github.com)\nAPP Demo 截图：\n\n 0x1 HTTP 直连\n本文中使用的https访问组件是okhttp3，直接访问http协议的url就可以了。不需要进行任何多余的配置，只需要配置好代理，http流量会被代理软件抓到。\n/*\n* http协议\n*/\nbutton_http_connect.setOnClickListener(new View.OnClickListener() &#123;\n    @Override\n    public void onClick(View v) &#123;\n        new Thread(new Runnable() &#123;\n            @RequiresApi(api = Build.VERSION_CODES.N)\n            @Override\n            public void run() &#123;\n                OkHttpClient mClient = client.newBuilder().build();\n                Request request = new Request.Builder()\n                        .url(\"http://www.vulnweb.com/\")\n                        .build();\n                Message message = new Message();\n                message.what = 1;\n                try (Response response = mClient.newCall(request).execute()) &#123;\n                    message.obj = \"http connect access vulnweb.com success\";\n                    Log.d(TAG, \"http connect access vulnweb.com success return code:\"+response.code());\n                &#125; catch (IOException e) &#123;\n                    message.obj = \"http connect access vulnweb.com failed\";\n                    Log.d(TAG, \"http connect access vulnweb.com failed\");\n                    e.printStackTrace();\n                &#125;\n                mHandler.sendMessage(message);\n            &#125;\n        &#125;).start();\n    &#125;\n&#125;);\n成功访问时，APP界面上会显示http connect access vulnweb.com success\nAS(AndroidStudio)中查看日志会看到打印类似 2021-04-30 21:25:38.888 8351-8516/com.example.httpstest D/[+]MainActivity: http connect access vulnweb.com success return code:200\n上述代码中的message对象的目的是：使用Binder服务从子线程，将HTTP/HTTPS请求结果发送到主线程以改变UI。处理函数Handler的实现如下：\n// 注册Handler处理从thread中返回的url请求结果\n@SuppressLint(\"HandlerLeak\") final Handler mHandler = new Handler()&#123;\n    public void handleMessage(Message msg) &#123;\n        // 处理消息\n        super.handleMessage(msg);\n        switch (msg.what) &#123;\n            case 1:\n                textView.setText((CharSequence) msg.obj); break;\n        &#125;\n    &#125;\n&#125;;\n 0x2 HTTPS 忽略证书验证\n正常人不会忽略证书验证，写这个功能纯粹是为了实验而实验。\n/*\n* https协议\n* 忽略证书验证\n*/\nbutton_https_connect_without_ca.setOnClickListener(new View.OnClickListener() &#123;\n    @Override\n    public void onClick(View v) &#123;\n        new Thread(new Runnable()&#123;\n            @RequiresApi(api = Build.VERSION_CODES.N)\n            @Override\n            public void run() &#123;\n                OkHttpClient mClient = client.newBuilder().sslSocketFactory(HttpsTrustAllCerts.createSSLSocketFactory(),new HttpsTrustAllCerts()).hostnameVerifier(new HttpsTrustAllCerts.TrustAllHostnameVerifier()).build();\n                Request request = new Request.Builder()\n                        .url(\"https://www.baidu.com/?q=trustAllCerts\")\n                        .build();\n                Message message = new Message();\n                message.what = 1;\n                try (Response response = mClient.newCall(request).execute()) &#123;\n                    message.obj = \"https connect without ca success\";\n                    Log.d(TAG, \"https connect without ca success return code:\"+response.code());\n                &#125; catch (IOException e) &#123;\n                    message.obj = \"https connect without ca failed\";\n                    Log.d(TAG, \"https connect without ca failed\");\n                    e.printStackTrace();\n                &#125;\n                mHandler.sendMessage(message);\n            &#125;\n        &#125;).start();\n    &#125;\n&#125;);\nHttpsTrustAllCerts对象实现如下，重写checkClientTrusted、checkServerTrusted方法，使其验证逻辑为空，重写域名验证器TrustAllHostnameVerifier 的verify方法，总是返回true，达到信任所有证书的效果:\npublic class HttpsTrustAllCerts implements X509TrustManager &#123;\n\n\n    @SuppressLint(\"TrustAllX509TrustManager\")\n    @Override\n    public void checkClientTrusted(X509Certificate[] chain, String authType) throws CertificateException &#123;\n\n    &#125;\n\n    @SuppressLint(\"TrustAllX509TrustManager\")\n    @Override\n    public void checkServerTrusted(X509Certificate[] chain, String authType) throws CertificateException &#123; // 验证服务端证书需要重写该函数\n\n    &#125;\n\n    @Override\n    public X509Certificate[] getAcceptedIssuers() &#123;\n        return new X509Certificate[0]; //返回长度为0的数组，相当于return null\n    &#125;\n\n\n    public static SSLSocketFactory createSSLSocketFactory() &#123; // SSLSocketFactory 创建器\n        SSLSocketFactory sSLSocketFactory = null;\n        try &#123;\n            SSLContext sc = SSLContext.getInstance(\"TLS\");\n            sc.init(null, new TrustManager[]&#123;new HttpsTrustAllCerts()&#125;,new SecureRandom());\n            sSLSocketFactory = sc.getSocketFactory();\n        &#125; catch (Exception e) &#123;\n        &#125;\n        return sSLSocketFactory;\n    &#125;\n\n\n    public static class TrustAllHostnameVerifier implements HostnameVerifier &#123; // 域名验证器\n        @Override\n        public boolean verify(String s, SSLSession sslSession) &#123;\n            return true;\n        &#125;\n    &#125;\n&#125;\n忽略证书校验的https请求和使用http协议的请求报文一样，可以通过直接设置网络代理抓包解密明文。\n 0x3 HTTPS 系统证书校验\nOkHttp3框架发起HTTPS请求时，默认就是使用的系统信任库证书链对服务端返回的证书进行验证，实现如下:\n/*\n* https协议\n* 默认证书链校验，只信任系统CA(根证书)\n*\n* tips: OKHTTP默认的https请求使用系统CA验证服务端证书（Android7.0以下还信任用户证书，Android7.0开始默认只信任系统证书）\n*/\nbutton_https_connect_with_system_ca.setOnClickListener(new View.OnClickListener() &#123;\n    @Override\n    public void onClick(View v) &#123;\n        new Thread(new Runnable()&#123;\n            @RequiresApi(api = Build.VERSION_CODES.N)\n            @Override\n            public void run() &#123;\n                Request request = new Request.Builder()\n                        .url(\"https://www.baidu.com/?q=defaultCerts\")\n                        .build();\n                Message message = new Message();\n                message.what = 1;\n                try (Response response = client.newCall(request).execute()) &#123;\n                    message.obj = \"https connect with system ca success\";\n                    Log.d(TAG, \"https connect with system ca success return code:\"+response.code());\n                &#125; catch (IOException e) &#123;\n                    message.obj = \"https connect with system ca failed\";\n                    Log.d(TAG, \"https connect with system ca failed\");\n                    e.printStackTrace();\n                &#125;\n                mHandler.sendMessage(message);\n            &#125;\n        &#125;).start();\n    &#125;\n&#125;);\n此时需要安装代理软件的证书到系统证书目录，前面说过我本机已经配置两个抓包软件的证书到系统证书列表了，所以仍然可以抓包解密明文。\n 0x4 SSL PINNING（代码校验）\nSSL PINNING 就是说通过客户端检查服务端证书是否真的是服务端证书来判断是否被中间人攻击。由于客户端需要验证服务端证书的正确性，那大原则上就是说客户端必须要有能判断正确性的依据。对于Android APP来说通常有两种方式：\n\n客户端持有证书公钥hash\n客户端持有证书文件\n\n下面的代码段也是通过这两种方式进行了证书验证。这样代理软件充当服务端的时候发给APP的证书就不能通过SSL PINNING验证。证书的获取方式可以参考代码中的注释。\n/*\n* https协议 SSL Pinning\n* 证书公钥绑定：验证证书公钥 baidu.com 使用CertificatePinner\n* 证书文件绑定：验证证书文件 bing.com  使用SSLSocketFactory\n*/\nbutton_SSL_PINNING_with_key.setOnClickListener(new View.OnClickListener() &#123;\n    @Override\n    public void onClick(View v) &#123;\n        new Thread(new Runnable()&#123;\n            @RequiresApi(api = Build.VERSION_CODES.N)\n            @Override\n            public void run() &#123;\n                final String CA_DOMAIN = \"www.baidu.com\";\n                //获取目标公钥: openssl s_client -connect www.baidu.com:443 -servername www.baidu.com | openssl x509 -pubkey -noout | openssl rsa -pubin -outform der | openssl dgst -sha256 -binary | openssl enc -base64\n                final String CA_PUBLIC_KEY = \"sha256//558pd1Y5Vercv1ZoSqOrJWDsh9sTMEolM6T8csLucQ=\";\n                //只校验公钥\n                CertificatePinner pinner = new CertificatePinner.Builder()\n                        .add(CA_DOMAIN, CA_PUBLIC_KEY)\n                        .build();\n                OkHttpClient pClient = client.newBuilder().certificatePinner(pinner).build();\n                Request request = new Request.Builder()\n                        .url(\"https://www.baidu.com/?q=SSLPinningCode\")\n                        .build();\n                Message message = new Message();\n                message.what = 1;\n                try (Response response = pClient.newCall(request).execute()) &#123;\n                    message.obj = \"https SSL_PINNING_with_key access baidu.com success\";\n                    Log.d(TAG, \"https SSL_PINNING_with_key access baidu.com success return code:\"+response.code());\n                &#125; catch (IOException e) &#123;\n                    message.obj = \"https SSL_PINNING_with_key access baidu.com failed\";\n                    Log.d(TAG, \"https SSL_PINNING_with_key access baidu.com failed\");\n                    e.printStackTrace();\n                &#125;\n\n\n                try &#123;\n                    // 获取证书输入流\n                    // 获取证书 openssl s_client -connect bing.com:443 -servername bing.com | openssl x509 -out bing.pem\n                    InputStream openRawResource = getApplicationContext().getResources().openRawResource(R.raw.bing); //R.raw.bing是bing.com的正确证书，R.raw.bing2_so是hostname=bing.com的so.com的证书，可视为用作测试的虚假bing.com证书\n                    Certificate ca = CertificateFactory.getInstance(\"X.509\").generateCertificate(openRawResource);\n                    // 创建 Keystore 包含我们的证书\n                    KeyStore keyStore = KeyStore.getInstance(KeyStore.getDefaultType());\n                    keyStore.load(null, null);\n                    keyStore.setCertificateEntry(\"ca\", ca);\n                    // 创建一个 TrustManager 仅把 Keystore 中的证书 作为信任的锚点\n                    TrustManagerFactory trustManagerFactory = TrustManagerFactory.getInstance(TrustManagerFactory.getDefaultAlgorithm()); // 建议不要使用自己实现的X509TrustManager，而是使用默认的X509TrustManager\n                    trustManagerFactory.init(keyStore);\n                    // 用 TrustManager 初始化一个 SSLContext\n                    sslContext = SSLContext.getInstance(\"TLS\");  //定义：public static SSLContext sslContext = null;\n                    sslContext.init(null, trustManagerFactory.getTrustManagers(), new SecureRandom());\n\n                    OkHttpClient pClient2 = client.newBuilder().sslSocketFactory(sslContext.getSocketFactory(), (X509TrustManager) trustManagerFactory.getTrustManagers()[0]).build();\n                    Request request2 = new Request.Builder()\n                            .url(\"https://www.bing.com/?q=SSLPinningCAfile\")\n                            .build();\n                    try (Response response2 = pClient2.newCall(request2).execute()) &#123;\n                        message.obj += \"\\nhttps SSL_PINNING_with_CA_file access bing.com success\";\n                        Log.d(TAG, \"https SSL_PINNING_with_CA_file access bing.com success return code:\"+response2.code());\n                    &#125; catch (IOException e) &#123;\n                        message.obj += \"\\nhttps SSL_PINNING_with_CA_file access bing.com failed\";\n                        Log.d(TAG, \"https SSL_PINNING_with_CA_file access bing.com failed\");\n                        e.printStackTrace();\n                    &#125;\n\n                &#125; catch (KeyStoreException | CertificateException | IOException | NoSuchAlgorithmException | KeyManagementException e) &#123;\n                    e.printStackTrace();\n                &#125;\n                mHandler.sendMessage(message);\n            &#125;\n        &#125;).start();\n    &#125;\n&#125;);\n此时可以使用Frida对上述证书绑定进行动作Hook，阻断绑定，hook代码在0x-1附录。\n 0x5 SSL PINNING（配置文件）\n通过res/xml/network_security_config.xml配置文件对证书进行校验是官方推荐使用的方法，配置方式还是可以为两种（同上）\n\n\n公钥校验\n\n\n证书校验\n\n\nxml 配置如下所示：\n&lt;network-security-config xmlns:tools=\"http://schemas.android.com/tools\">\n    &lt;!--允许http访问-->\n    &lt;base-config cleartextTrafficPermitted=\"true\"\n        tools:ignore=\"InsecureBaseConfiguration\" />\n    &lt;!--证书校验-->\n    &lt;domain-config>\n        &lt;domain includeSubdomains=\"true\">sogou.com&lt;/domain>\n        &lt;trust-anchors>\n            &lt;!--获取证书: openssl s_client -connect sogou.com:443 -servername sogou.com | openssl x509 -out sogou.pem-->\n            &lt;certificates src=\"@raw/sogou\"/>\n        &lt;/trust-anchors>\n    &lt;/domain-config>\n\n    &lt;!--公钥校验-->\n    &lt;domain-config>\n        &lt;domain includeSubdomains=\"true\">zhihu.com&lt;/domain>\n        &lt;!--zhihu.com公钥校验\n        获取公钥: openssl s_client -connect zhihu.com:443 -servername zhihu.com | openssl x509 -pubkey -noout | openssl rsa -pubin -outform der | openssl dgst -sha256 -binary | openssl enc -base64\n        -->\n        &lt;pin-set expiration=\"2099-01-01\"\n            tools:ignore=\"MissingBackupPin\">\n            &lt;pin digest=\"SHA-256\">vzXV96/gpZMyyNNhyTdjtX0/NUVYTtmYqWcVVaUtTdQ=&lt;/pin>\n        &lt;/pin-set>\n    &lt;/domain-config>\n&lt;/network-security-config>\n\n\n这样配置好以后就可以对上述指定的域名进行https访问，将会自动对证书进行校验，请求代码：\n/*\n* https协议 SSL PINNING\n* 证书绑定验证 配置在 @xml/network_security_config 中\n* sogou.com 使用 sogou.pem 验证证书\n* so.com 使用 sha256 key 验证\n*/\nbutton_SSL_PINNING_with_CA.setOnClickListener(new View.OnClickListener() &#123;\n    @Override\n    public void onClick(View v) &#123;\n        new Thread(new Runnable()&#123;\n            @RequiresApi(api = Build.VERSION_CODES.N)\n            @Override\n            public void run() &#123;\n                OkHttpClient pClient = client.newBuilder().build();\n                Request request = new Request.Builder()\n                        .url(\"https://www.sogou.com/web?query=SSLPinningXML\")\n                        .build();\n                Request request2 = new Request.Builder()\n                        .url(\"https://www.zhihu.com/\")\n                        .build();\n                Message message = new Message();\n                message.what = 1;\n                try (Response response = pClient.newCall(request).execute()) &#123;\n                    message.obj = \"https SSL_PINNING_with_CA, config in xml with CA.pem file access sogou.com success\";\n                    Log.d(TAG, \"https SSL_PINNING_with_CA, config in xml with CA.pem file access sogou.com success return code:\"+response.code());\n                &#125; catch (IOException e) &#123;\n                    message.obj = \"https SSL_PINNING_with_CA, config in xml with CA.pem file access sogou.com failed\";\n                    Log.d(TAG, \"https SSL_PINNING_with_CA, config in xml with CA.pem file access sogou.com failed\");\n                    e.printStackTrace();\n                &#125;\n                try (Response response = pClient.newCall(request2).execute()) &#123;\n                    message.obj += \"\\nhttps SSL_PINNING_with_CA, config in xml with key access zhihu.com success\";\n                    Log.d(TAG, \"https SSL_PINNING_with_CA, config in xml with key access zhihu.com success return code:\"+response.code());\n                &#125; catch (IOException e) &#123;\n                    message.obj += \"\\nhttps SSL_PINNING_with_CA, config in xml with key access zhihu.com failed\";\n                    Log.d(TAG, \"https SSL_PINNING_with_CA, config in xml with key access zhihu.com failed\");\n                    e.printStackTrace();\n                &#125;\n                mHandler.sendMessage(message);\n            &#125;\n        &#125;).start();\n    &#125;\n&#125;);\n该证书绑定实现，绕过方式同0x4 SSL PINNING（代码校验）可以使用frida hook unpinning。\n 0x6 HTTPS 双向验证\n双向验证，顾名思义就是客户端验证服务器端证书的正确性，服务器端也验证客户端的证书正确性，所以大原则上客户端要持有服务器端证书，服务端也要持有客户端的证书，对于Android APP来说打包发布的时候既要内置一个服务端证书也要生成一个客户端证书给服务器存储起来。这种双向认证非常可以做到非常高的安全性，但是同时服务端要想保持所有服务端的证书比较困难，因此这种方式只适用于某些点对点的高安全性需求的通信场合，对于Android APP来说可能是某类高机密性的内网业务才会使用这种双向HTTPS认证。\n接下来我们需要实现一个server模拟HTTPS双向认证。首先通过以下命令生成一对证书。\n服务端证书：\nopenssl genrsa -out server-key.key 2048\nopenssl req -new -out server-req.csr -key server-key.key\nopenssl x509 -req -in server-req.csr -out server-cert.cer -signkey server-key.key  -CAcreateserial -days 3650\n客户端证书：\nopenssl genrsa -out client-key.key 2048\nopenssl req -new -out client-req.csr -key client-key.key\nopenssl x509 -req -in client-req.csr -out client-cert.cer -signkey client-key.key -CAcreateserial -days 3650\n生成客户端带密码的p12证书（这步很重要，双向认证的话，浏览器访问时候要导入该证书才行；可能某些Android系统版本请求的时候需要把它转成bks来请求双向认证，我的设备用p12格式是可行的）：\nopenssl pkcs12 -export -clcerts -in client-cert.cer -inkey client-key.key -out client.p12\n这里你也可以选择使用我生成好的证书，需要使用的证书都在源码certs目录，我的client.p12密码是clientpassword，server证书的密码是serverpassword。\n服务端代码：\nimport os\nimport sys\nimport ssl\nfrom http.server import HTTPServer, BaseHTTPRequestHandler\n\n#服务端证书和私钥\nserverCerts = \"%s\\\\certs\\\\server-cert.cer\" % os.getcwd()\nserverKey = \"%s\\\\certs\\\\server-key.key\" % os.getcwd()\n#客户端证书\nclientCerts = \"%s\\\\certs\\\\client-cert.cer\" % os.getcwd()\n\nclass RequestHandler(BaseHTTPRequestHandler):\n    def _writeheaders(self):\n        self.send_response(200)\n        self.send_header('Content-type','text/plain')\n        self.end_headers()\n    def do_GET(self):\n        self._writeheaders()\n        self.wfile.write(\"OK\".encode(\"utf-8\"))\n\ndef main():\n    if (len(sys.argv) != 2):\n        port = 443\n    else:\n        port = sys.argv[1]\n    server_address = (\"0.0.0.0\", int(port))\n    server = HTTPServer(server_address, RequestHandler)\n    #双向校验\n    server.socket = ssl.wrap_socket(server.socket, certfile = serverCerts, server_side = True,  \n                               keyfile = serverKey,\n                               cert_reqs = ssl.CERT_REQUIRED,\n                               ca_certs = clientCerts,\n                               do_handshake_on_connect = False\n                               )\n    print(\"Starting server, listen at: %s:%s\" % server_address)\n    server.serve_forever()\n\nif __name__ == \"__main__\":\n    main()\n使用python启动server，在windows上运行可能报错，大概率是由于防火墙禁止启用443端口。\nOSError: [WinError 10013] 以一种访问权限不允许的方式做了一个访问套接字的尝试。\n所以我的方案是在VMware虚拟机中启动这个server，然后再宿主机配置hosts，添加主机域名\n&lt;虚拟机IP&gt; www.test.com\n宿主机安装client.p12证书（windows双击安装好方便），然后使用浏览器访问 www.test.com\n\n然后你就选这个，就可以访问了，访问成功会返回OK字样的空白页。服务端日志：\n\n尽管输出ERROR但是还是访问成功了，错误原因应该是证书为自签名证书，客户端并不想认可这个没娘的证书。\n如果你能做到这里说明你已经正确实现了HTTPS双向认证，接下来我们要在Android APP上复现。\n/*\n* 双向校验\n* 因该测试是自建服务器并自签名，所以需要先在res/xml/network_security_config中配置信任服务端证书\n*/\nbutton_https_twoway.setOnClickListener(new View.OnClickListener() &#123;\n    @Override\n    public void onClick(View v) &#123;\n        new Thread(new Runnable()&#123;\n            @RequiresApi(api = Build.VERSION_CODES.N)\n            @Override\n            public void run() &#123;\n                X509TrustManager trustManager = null;\n                try &#123;\n                    TrustManagerFactory trustManagerFactory = TrustManagerFactory.getInstance(TrustManagerFactory.getDefaultAlgorithm());\n\n                    trustManagerFactory.init((KeyStore) null);\n                    TrustManager[] trustManagers = trustManagerFactory.getTrustManagers();\n                    if (trustManagers.length != 1 || !(trustManagers[0] instanceof X509TrustManager)) &#123;\n                        throw new IllegalStateException(\"Unexpected default trust managers:\" + Arrays.toString(trustManagers));\n                    &#125;\n                    trustManager = (X509TrustManager) trustManagers[0];\n                &#125; catch (Exception e) &#123;\n                    e.printStackTrace();\n                &#125;\n                OkHttpClient mClient = client.newBuilder().sslSocketFactory(Objects.requireNonNull(ClientSSLSocketFactory.getSocketFactory(getApplicationContext())), Objects.requireNonNull(trustManager)).hostnameVerifier(new HostnameVerifier() &#123;\n                    @Override\n                    public boolean verify(String hostname, SSLSession session) &#123;\n                        HostnameVerifier hv = HttpsURLConnection.getDefaultHostnameVerifier();\n                        return hv.verify(\"www.test.com\", session);\n                    &#125;\n                &#125;).build();\n\n                Request request = new Request.Builder()\n                        .url(\"https://www.test.com/?q=TwoWayVerify\")\n                        .build();\n                Message message = new Message();\n                message.what = 1;\n                try (Response response = mClient.newCall(request).execute()) &#123;\n                    Log.d(\"TestReq\", response.peekBody(2048).string());\n                    message.obj = \"请求成功: \" + response.peekBody(2048).string();\n                    mHandler.sendMessage(message);\n                &#125; catch (IOException e) &#123;\n                    message.obj = e.getLocalizedMessage();\n                    mHandler.sendMessage(message);\n                    e.printStackTrace();\n                &#125;\n            &#125;\n        &#125;).start();\n    &#125;\n&#125;);\nnetwork_security_config.xml 中添加配置：\n&lt;!--证书校验-->\n&lt;domain-config>\n    &lt;domain includeSubdomains=\"true\">www.test.com&lt;/domain>\n    &lt;trust-anchors>\n        &lt;certificates src=\"@raw/server_cert\"\n            tools:ignore=\"NetworkSecurityConfig\" />\n    &lt;/trust-anchors>\n&lt;/domain-config>\nserver_cert就是server_cert.cer。到这里完成了客户端PIN服务端证书，然后还要在客户端这边写把客户端证书给服务端验证的代码逻辑，即ClientSSLSocketFactory的实现：\npublic class ClientSSLSocketFactory  &#123;\n    private static final String KEY_STORE_PASSWORD = \"clientpassword\"; // 证书密码\n    private static InputStream client_input;\n\n    public static SSLSocketFactory getSocketFactory(Context context) &#123;\n        try &#123;\n            //客户端证书\n            client_input = context.getResources().getAssets().open(\"client.p12\");\n            SSLContext sslContext = SSLContext.getInstance(\"TLS\");\n            KeyStore keyStore = KeyStore.getInstance(\"PKCS12\");\n            keyStore.load(client_input, KEY_STORE_PASSWORD.toCharArray());\n            KeyManagerFactory keyManagerFactory = KeyManagerFactory.getInstance(KeyManagerFactory.getDefaultAlgorithm());\n            keyManagerFactory.init(keyStore, KEY_STORE_PASSWORD.toCharArray());\n            sslContext.init(keyManagerFactory.getKeyManagers(), null, new SecureRandom());\n            return sslContext.getSocketFactory();\n        &#125; catch (Exception e) &#123;\n            e.printStackTrace();\n        &#125; finally &#123;\n            try &#123;\n                client_input.close();\n            &#125; catch (IOException e) &#123;\n                e.printStackTrace();\n            &#125;\n        &#125;\n        return null;\n    &#125;\n&#125;\n代码写完了，现在我们来缕一缕：\nclient app包含server-cert.cer 对服务端证书PIN，这个过程需要验证服务端证书的签发单位是否为合法CA Issuer，但是我们的证书是自签名的，所以就算是不使用代理还是不能通过SSL PINNING，大概率是报错：\nCaused by: java.security.cert.CertificateException: java.security.cert.CertPathValidatorException: Trust anchor for certification path not found.\n所以，一开始我们就需要用frida hook启动app，这样跳过app端的证书绑定，才可以继续后面的步骤。\n接下来需要配置代理，给代理软件加客户端证书，这里我们使用BurpSuite，Fidller在这个场景不能用：\n\n选择client.p12证书文件，如果你使用我生成的证书文件密码是clientpassword。\n此时确认以下条件：\n\n[x] 服务端能通过PC浏览器访问\n[x] APP 中 client.p12 证书装载成功\n[x] APP 通过frida hook spawn模式启动\n[x] BurpSuite 加载 client.p12 证书成功\n[x] PC 代理软件未开启\n[x] PC hosts 中将 www.test.com 域名定向到正确IP\n\n确定没问题以后，点击 HTTPS 双向验证 按钮访问。\n最终效果：\n\nAPP 页面上显示：请求成功: 200\n\n碎碎念：\n可以把server-cert.cer 连带 server-key.key 导出一份可以装载的证书，这样就可以把这个证书加到系统证书列表中，应该可以通过SSL PINNING。\n如果能把android 系统中hosts在/system/etc/hosts 是只读文件系统，我remount以后修改这个文件系统就会崩溃，随意没有尝试在android手机上直接将域名定向到server ip地址。\n如果上述两点能正确实现，那就可以在局域网下，完成一个正经的HTTPS双向认证了。\n\n 0x7 WebView 忽略证书验证\nWebView 正常应该做成一个跳转页的，但是我为了图省事，就做了个小框在最下面，只是为了看是否访问成功。\n/*\n* https协议\n* WebView 不进行证书校验\n*/\nbutton_webview_ssl_without_ca.setOnClickListener(new View.OnClickListener() &#123;\n    @Override\n    public void onClick(View v) &#123;\n        MyWebViewClient mWebViewClient = new MyWebViewClient();\n        mWebViewClient.setCheckflag(\"trustAllCerts\");\n        mWebview.setWebViewClient(mWebViewClient);\n        mWebview.loadUrl(\"https://www.baidu.com/?q=WebView_without_CAcheck\");\n    &#125;\n&#125;);\n上述代码片段中MyWebViewClient的实现如下：\nprivate class MyWebViewClient extends WebViewClient &#123;\n    private String checkflag=\"checkCerts\"; // 是否忽略证书校验\n\n    public void setCheckflag(String checkflag) &#123;\n        this.checkflag = checkflag;\n    &#125;\n\n    @Override\n    public void onReceivedSslError(WebView view, SslErrorHandler handler, SslError error) &#123;\n        if(\"trustAllCerts\".equals(checkflag))&#123;\n            handler.proceed();\n        &#125;else &#123;\n            handler.cancel();\n            Toast.makeText(MainActivity.this, \"证书异常，停止访问\", Toast.LENGTH_SHORT).show();\n        &#125;\n    &#125;\n&#125;\n这样就做到了在遇到SSL错误时继续访问而中断访问，也不发出任何提醒。这样只需要配置好代理，甚至不需要安装代理证书就可以抓包了。\n 0x8 WebView 系统证书验证\n很遗憾，WebView没有自定义SSL PINNING的实现方法，只能通过network_security_config.xml配置证书绑定。但这确实是一个很好的现象，在我看来把安全相关的业务逻辑交给普通开发者简直就是白给，这么做反而是提升安全性的最佳实践。\n/*\n * https协议\n * WebView 使用系统证书校验\n */\nbutton_webview_ssl_with_system_ca.setOnClickListener(new View.OnClickListener() &#123;\n    @Override\n    public void onClick(View v) &#123;\n        MyWebViewClient mWebViewClient = new MyWebViewClient();\n        mWebViewClient.setCheckflag(\"checkCerts\");\n        mWebview.setWebViewClient(mWebViewClient);\n        mWebview.loadUrl(\"https://www.baidu.com/?q=WebView_with_SystemCAcheck\");\n    &#125;\n&#125;);\nbaidu.com未在network_security_config.xml中配置证书绑定，所以访问 https://www.baidu.com/ 时只使用系统证书进行校验。\n所以只需要将代理软件的证书加入系统证书列表就可以抓包解密明文了。\n 0x9 WebView SSL PINNING\n/*\n* https协议 SSL PINNING WebView\n* 通过network_security_config.xml中定义的证书和密钥进行绑定\n*/\nbutton_webview_ssl_pinning.setOnClickListener(new View.OnClickListener() &#123;\n    @Override\n    public void onClick(View v) &#123;\n        MyWebViewClient mWebViewClient = new MyWebViewClient();\n        mWebViewClient.setCheckflag(\"checkCerts\");\n        mWebview.setWebViewClient(mWebViewClient);\n        mWebview.loadUrl(\"https://www.sogou.com/web?query=WebView_SSLPinningXML\"); // 证书文件校验\n        // mWebview.loadUrl(\"https://www.zhihu.com/\"); // 证书公钥校验\n    &#125;\n&#125;);\nsogou.com和zhihu.com都通过network_security_config.xml做了证书绑定，所以直接访问的过程中就会触发SSL PINNING的验证。\nWebView SSL PINNING还不能通过0x-1附录的hook 代码unpinning，如果你有能unpinning的方法请务必告诉我。\n 0x-1 附录\nFrida Hook Code:\nusage: frida -U -f com.example.httpstest -l ./frida_multiple_unpinning.js --no-pause\n/*  Android ssl certificate pinning bypass script for various methods\n\tby Maurizio Siddu modify by Ch3nYe\n\n\tRun with:\n\tfrida -U -f [APP_ID] -l frida_multiple_unpinning.js --no-pause\n*/\nsetTimeout(function() &#123;\n\tJava.perform(function () &#123;\n\t\tconsole.log('');\n\t\tconsole.log('======');\n\t\tconsole.log('[#] Android Bypass for various Certificate Pinning methods [#]');\n\t\tconsole.log('======');\n\n\n\t\tvar X509TrustManager = Java.use('javax.net.ssl.X509TrustManager');\n\t\tvar SSLContext = Java.use('javax.net.ssl.SSLContext');\n\n\n\t\t// TrustManager (Android &lt; 7) //\n\t\t////////////////////////////////\n\t\tvar TrustManager = Java.registerClass(&#123;\n\t\t\t// Implement a custom TrustManager\n\t\t\tname: 'dev.asd.test.TrustManager',\n\t\t\timplements: [X509TrustManager],\n\t\t\tmethods: &#123;\n\t\t\t\tcheckClientTrusted: function (chain, authType) &#123;&#125;,\n\t\t\t\tcheckServerTrusted: function (chain, authType) &#123;&#125;,\n\t\t\t\tgetAcceptedIssuers: function () &#123;return []; &#125;\n\t\t\t&#125;\n\t\t&#125;);\n\t\t// Prepare the TrustManager array to pass to SSLContext.init()\n\t\tvar TrustManagers = [TrustManager.$new()];\n\t\t// Get a handle on the init() on the SSLContext class\n\t\tvar SSLContext_init = SSLContext.init.overload(\n\t\t\t'[Ljavax.net.ssl.KeyManager;', '[Ljavax.net.ssl.TrustManager;', 'java.security.SecureRandom');\n\t\ttry &#123;\n\t\t\t// Override the init method, specifying the custom TrustManager\n\t\t\tSSLContext_init.implementation = function(keyManager, trustManager, secureRandom) &#123;\n\t\t\t\tconsole.log('[+] Bypassing Trustmanager (Android &lt; 7) request');\n\t\t\t\tSSLContext_init.call(this, keyManager, TrustManagers, secureRandom);\n\t\t\t&#125;;\n\t\t&#125; catch (err) &#123;\n\t\t\tconsole.log('[-] TrustManager (Android &lt; 7) pinner not found');\n\t\t\t//console.log(err);\n\t\t&#125;\n\n\n\n\t\t// OkHTTPv3 (quadruple bypass) //\n\t\t/////////////////////////////////\n\t\ttry &#123;\n\t\t\t// Bypass OkHTTPv3 &#123;1&#125;\n\t\t\tvar okhttp3_Activity_1 = Java.use('okhttp3.CertificatePinner');\n\t\t\tokhttp3_Activity_1.check.overload('java.lang.String', 'java.util.List').implementation = function (a, b) &#123;\n\t\t\t\tconsole.log('[+] Bypassing OkHTTPv3 &#123;1&#125;: ' + a);\n\t\t\t\treturn true;\n\t\t\t&#125;;\n\t\t&#125; catch (err) &#123;\n\t\t\tconsole.log('[-] OkHTTPv3 &#123;1&#125; pinner not found');\n\t\t\t//console.log(err);\n\t\t&#125;\n\t\ttry &#123;\n\t\t\t// Bypass OkHTTPv3 &#123;2&#125;\n\t\t\t// This method of CertificatePinner.check could be found in some old Android app\n\t\t\tvar okhttp3_Activity_2 = Java.use('okhttp3.CertificatePinner');\n\t\t\tokhttp3_Activity_2.check.overload('java.lang.String', 'java.security.cert.Certificate').implementation = function (a, b) &#123;\n\t\t\t\tconsole.log('[+] Bypassing OkHTTPv3 &#123;2&#125;: ' + a);\n\t\t\t\treturn true;\n\t\t\t&#125;;\n\t\t&#125; catch (err) &#123;\n\t\t\tconsole.log('[-] OkHTTPv3 &#123;2&#125; pinner not found');\n\t\t\t//console.log(err);\n\t\t&#125;\n\t\ttry &#123;\n\t\t\t// Bypass OkHTTPv3 &#123;3&#125;\n\t\t\tvar okhttp3_Activity_3 = Java.use('okhttp3.CertificatePinner');\n\t\t\tokhttp3_Activity_3.check.overload('java.lang.String', '[Ljava.security.cert.Certificate;').implementation = function (a, b) &#123;\n\t\t\t\tconsole.log('[+] Bypassing OkHTTPv3 &#123;3&#125;: ' + a);\n\t\t\t\treturn true;\n\t\t\t&#125;;\n\t\t&#125; catch(err) &#123;\n\t\t\tconsole.log('[-] OkHTTPv3 &#123;3&#125; pinner not found');\n\t\t\t//console.log(err);\n\t\t&#125;\n\t\ttry &#123;\n\t\t\t// Bypass OkHTTPv3 &#123;4&#125;\n\t\t\tvar okhttp3_Activity_4 = Java.use('okhttp3.CertificatePinner');\n\t\t\tokhttp3_Activity_4['check$okhttp'].implementation = function (a, b) &#123;\n\t\t\t\tconsole.log('[+] Bypassing OkHTTPv3 &#123;4&#125;: ' + a);\n\t\t\t&#125;;\n\t\t&#125; catch(err) &#123;\n\t\t\tconsole.log('[-] OkHTTPv3 &#123;4&#125; pinner not found');\n\t\t\t//console.log(err);\n\t\t&#125;\n\n\n\n\t\t// Trustkit (triple bypass) //\n\t\t//////////////////////////////\n\t\ttry &#123;\n\t\t\t// Bypass Trustkit &#123;1&#125;\n\t\t\tvar trustkit_Activity_1 = Java.use('com.datatheorem.android.trustkit.pinning.OkHostnameVerifier');\n\t\t\ttrustkit_Activity_1.verify.overload('java.lang.String', 'javax.net.ssl.SSLSession').implementation = function (a, b) &#123;\n\t\t\t\tconsole.log('[+] Bypassing Trustkit &#123;1&#125;: ' + a);\n\t\t\t\treturn true;\n\t\t\t&#125;;\n\t\t&#125; catch (err) &#123;\n\t\t\tconsole.log('[-] Trustkit &#123;1&#125; pinner not found');\n\t\t\t//console.log(err);\n\t\t&#125;\n\t\ttry &#123;\n\t\t\t// Bypass Trustkit &#123;2&#125;\n\t\t\tvar trustkit_Activity_2 = Java.use('com.datatheorem.android.trustkit.pinning.OkHostnameVerifier');\n\t\t\ttrustkit_Activity_2.verify.overload('java.lang.String', 'java.security.cert.X509Certificate').implementation = function (a, b) &#123;\n\t\t\t\tconsole.log('[+] Bypassing Trustkit &#123;2&#125;: ' + a);\n\t\t\t\treturn true;\n\t\t\t&#125;;\n\t\t&#125; catch (err) &#123;\n\t\t\tconsole.log('[-] Trustkit &#123;2&#125; pinner not found');\n\t\t\t//console.log(err);\n\t\t&#125;\n\t\ttry &#123;\n\t\t\t// Bypass Trustkit &#123;3&#125;\n\t\t\tvar trustkit_PinningTrustManager = Java.use('com.datatheorem.android.trustkit.pinning.PinningTrustManager');\n\t\t\ttrustkit_PinningTrustManager.checkServerTrusted.implementation = function () &#123;\n\t\t\t\tconsole.log('[+] Bypassing Trustkit &#123;3&#125;');\n\t\t\t&#125;;\n\t\t&#125; catch (err) &#123;\n\t\t\tconsole.log('[-] Trustkit &#123;3&#125; pinner not found');\n\t\t\t//console.log(err);\n\t\t&#125;\n\n\n\n\n\t\t// TrustManagerImpl (Android > 7) //\n\t\t////////////////////////////////////\n\t\ttry &#123;\n\t\t\tvar TrustManagerImpl = Java.use('com.android.org.conscrypt.TrustManagerImpl');\n\t\t\tTrustManagerImpl.verifyChain.implementation = function (untrustedChain, trustAnchorChain, host, clientAuth, ocspData, tlsSctData) &#123;\n\t\t\t\tconsole.log('[+] Bypassing TrustManagerImpl (Android > 7): ' + host);\n\t\t\t\treturn untrustedChain;\n\t\t\t&#125;;\n\t\t&#125; catch (err) &#123;\n\t\t\tconsole.log('[-] TrustManagerImpl (Android > 7) pinner not found');\n\t\t\t//console.log(err);\n\t\t&#125;\n\n\n\n\t\t// Appcelerator Titanium //\n\t\t///////////////////////////\n\t\ttry &#123;\n\t\t\tvar appcelerator_PinningTrustManager = Java.use('appcelerator.https.PinningTrustManager');\n\t\t\tappcelerator_PinningTrustManager.checkServerTrusted.implementation = function () &#123;\n\t\t\t\tconsole.log('[+] Bypassing Appcelerator PinningTrustManager');\n\t\t\t&#125;;\n\t\t&#125; catch (err) &#123;\n\t\t\tconsole.log('[-] Appcelerator PinningTrustManager pinner not found');\n\t\t\t//console.log(err);\n\t\t&#125;\n\n\n\n\t\t// OpenSSLSocketImpl Conscrypt //\n\t\t/////////////////////////////////\n\t\ttry &#123;\n\t\t\tvar OpenSSLSocketImpl = Java.use('com.android.org.conscrypt.OpenSSLSocketImpl');\n\t\t\tOpenSSLSocketImpl.verifyCertificateChain.implementation = function (certRefs, JavaObject, authMethod) &#123;\n\t\t\t\tconsole.log('[+] Bypassing OpenSSLSocketImpl Conscrypt');\n\t\t\t&#125;;\n\t\t&#125; catch (err) &#123;\n\t\t\tconsole.log('[-] OpenSSLSocketImpl Conscrypt pinner not found');\n\t\t\t//console.log(err);\n\t\t&#125;\n\n\n\n\t\t// OpenSSLEngineSocketImpl Conscrypt //\n\t\t///////////////////////////////////////\n\t\ttry &#123;\n\t\t\tvar OpenSSLEngineSocketImpl_Activity = Java.use('com.android.org.conscrypt.OpenSSLEngineSocketImpl');\n\t\t\tOpenSSLSocketImpl_Activity.verifyCertificateChain.overload('[Ljava.lang.Long;', 'java.lang.String').implementation = function (a, b) &#123;\n\t\t\t\tconsole.log('[+] Bypassing OpenSSLEngineSocketImpl Conscrypt: ' + b);\n\t\t\t&#125;;\n\t\t&#125; catch (err) &#123;\n\t\t\tconsole.log('[-] OpenSSLEngineSocketImpl Conscrypt pinner not found');\n\t\t\t//console.log(err);\n\t\t&#125;\n\n\n\n\t\t// OpenSSLSocketImpl Apache Harmony //\n\t\t//////////////////////////////////////\n\t\ttry &#123;\n\t\t\tvar OpenSSLSocketImpl_Harmony = Java.use('org.apache.harmony.xnet.provider.jsse.OpenSSLSocketImpl');\n\t\t\tOpenSSLSocketImpl_Harmony.verifyCertificateChain.implementation = function (asn1DerEncodedCertificateChain, authMethod) &#123;\n\t\t\t\tconsole.log('[+] Bypassing OpenSSLSocketImpl Apache Harmony');\n\t\t\t&#125;;\n\t\t&#125; catch (err) &#123;\n\t\t\tconsole.log('[-] OpenSSLSocketImpl Apache Harmony pinner not found');\n\t\t\t//console.log(err);\n\t\t&#125;\n\n\n\n\t\t// PhoneGap sslCertificateChecker (https://github.com/EddyVerbruggen/SSLCertificateChecker-PhoneGap-Plugin) //\n\t\t//////////////////////////////////////////////////////////////////////////////////////////////////////////////\n\t\ttry &#123;\n\t\t\tvar phonegap_Activity = Java.use('nl.xservices.plugins.sslCertificateChecker');\n\t\t\tphonegap_Activity.execute.overload('java.lang.String', 'org.json.JSONArray', 'org.apache.cordova.CallbackContext').implementation = function (a, b, c) &#123;\n\t\t\t\tconsole.log('[+] Bypassing PhoneGap sslCertificateChecker: ' + a);\n\t\t\t\treturn true;\n\t\t\t&#125;;\n\t\t&#125; catch (err) &#123;\n\t\t\tconsole.log('[-] PhoneGap sslCertificateChecker pinner not found');\n\t\t\t//console.log(err);\n\t\t&#125;\n\n\n\n\t\t// IBM MobileFirst pinTrustedCertificatePublicKey (double bypass) //\n\t\t////////////////////////////////////////////////////////////////////\n\t\ttry &#123;\n\t\t\t// Bypass IBM MobileFirst &#123;1&#125;\n\t\t\tvar WLClient_Activity_1 = Java.use('com.worklight.wlclient.api.WLClient');\n\t\t\tWLClient_Activity_1.getInstance().pinTrustedCertificatePublicKey.overload('java.lang.String').implementation = function (cert) &#123;\n\t\t\t\tconsole.log('[+] Bypassing IBM MobileFirst pinTrustedCertificatePublicKey &#123;1&#125;: ' + cert);\n\t\t\t\treturn;\n\t\t\t&#125;;\n\t\t\t&#125; catch (err) &#123;\n\t\t\tconsole.log('[-] IBM MobileFirst pinTrustedCertificatePublicKey &#123;1&#125; pinner not found');\n\t\t\t//console.log(err);\n\t\t&#125;\n\t\ttry &#123;\n\t\t\t// Bypass IBM MobileFirst &#123;2&#125;\n\t\t\tvar WLClient_Activity_2 = Java.use('com.worklight.wlclient.api.WLClient');\n\t\t\tWLClient_Activity_2.getInstance().pinTrustedCertificatePublicKey.overload('[Ljava.lang.String;').implementation = function (cert) &#123;\n\t\t\t\tconsole.log('[+] Bypassing IBM MobileFirst pinTrustedCertificatePublicKey &#123;2&#125;: ' + cert);\n\t\t\t\treturn;\n\t\t\t&#125;;\n\t\t&#125; catch (err) &#123;\n\t\t\tconsole.log('[-] IBM MobileFirst pinTrustedCertificatePublicKey &#123;2&#125; pinner not found');\n\t\t\t//console.log(err);\n\t\t&#125;\n\n\n\n\t\t// IBM WorkLight (ancestor of MobileFirst) HostNameVerifierWithCertificatePinning (quadruple bypass) //\n\t\t///////////////////////////////////////////////////////////////////////////////////////////////////////\n\t\ttry &#123;\n\t\t\t// Bypass IBM WorkLight &#123;1&#125;\n\t\t\tvar worklight_Activity_1 = Java.use('com.worklight.wlclient.certificatepinning.HostNameVerifierWithCertificatePinning');\n\t\t\tworklight_Activity_1.verify.overload('java.lang.String', 'javax.net.ssl.SSLSocket').implementation = function (a, b) &#123;\n\t\t\t\tconsole.log('[+] Bypassing IBM WorkLight HostNameVerifierWithCertificatePinning &#123;1&#125;: ' + a);\n\t\t\t\treturn;\n\t\t\t&#125;;\n\t\t&#125; catch (err) &#123;\n\t\t\tconsole.log('[-] IBM WorkLight HostNameVerifierWithCertificatePinning &#123;1&#125; pinner not found');\n\t\t\t//console.log(err);\n\t\t&#125;\n\t\ttry &#123;\n\t\t\t// Bypass IBM WorkLight &#123;2&#125;\n\t\t\tvar worklight_Activity_2 = Java.use('com.worklight.wlclient.certificatepinning.HostNameVerifierWithCertificatePinning');\n\t\t\tworklight_Activity_2.verify.overload('java.lang.String', 'java.security.cert.X509Certificate').implementation = function (a, b) &#123;\n\t\t\t\tconsole.log('[+] Bypassing IBM WorkLight HostNameVerifierWithCertificatePinning &#123;2&#125;: ' + a);\n\t\t\t\treturn;\n\t\t\t&#125;;\n\t\t&#125; catch (err) &#123;\n\t\t\tconsole.log('[-] IBM WorkLight HostNameVerifierWithCertificatePinning &#123;2&#125; pinner not found');\n\t\t\t//console.log(err);\n\t\t&#125;\n\t\ttry &#123;\n\t\t\t// Bypass IBM WorkLight &#123;3&#125;\n\t\t\tvar worklight_Activity_3 = Java.use('com.worklight.wlclient.certificatepinning.HostNameVerifierWithCertificatePinning');\n\t\t\tworklight_Activity_3.verify.overload('java.lang.String', '[Ljava.lang.String;', '[Ljava.lang.String;').implementation = function (a, b) &#123;\n\t\t\t\tconsole.log('[+] Bypassing IBM WorkLight HostNameVerifierWithCertificatePinning &#123;3&#125;: ' + a);\n\t\t\t\treturn;\n\t\t\t&#125;;\n\t\t&#125; catch (err) &#123;\n\t\t\tconsole.log('[-] IBM WorkLight HostNameVerifierWithCertificatePinning &#123;3&#125; pinner not found');\n\t\t\t//console.log(err);\n\t\t&#125;\n\t\ttry &#123;\n\t\t\t// Bypass IBM WorkLight &#123;4&#125;\n\t\t\tvar worklight_Activity_4 = Java.use('com.worklight.wlclient.certificatepinning.HostNameVerifierWithCertificatePinning');\n\t\t\tworklight_Activity_4.verify.overload('java.lang.String', 'javax.net.ssl.SSLSession').implementation = function (a, b) &#123;\n\t\t\t\tconsole.log('[+] Bypassing IBM WorkLight HostNameVerifierWithCertificatePinning &#123;4&#125;: ' + a);\n\t\t\t\treturn true;\n\t\t\t&#125;;\n\t\t&#125; catch (err) &#123;\n\t\t\tconsole.log('[-] IBM WorkLight HostNameVerifierWithCertificatePinning &#123;4&#125; pinner not found');\n\t\t\t//console.log(err);\n\t\t&#125;\n\n\n\n\t\t// Conscrypt CertPinManager //\n\t\t//////////////////////////////\n\t\ttry &#123;\n\t\t\tvar conscrypt_CertPinManager_Activity = Java.use('com.android.org.conscrypt.CertPinManager');\n\t\t\tconscrypt_CertPinManager_Activity.isChainValid.overload('java.lang.String', 'java.util.List').implementation = function (a, b) &#123;\n\t\t\t\tconsole.log('[+] Bypassing Conscrypt CertPinManager: ' + a);\n\t\t\t\treturn true;\n\t\t\t&#125;;\n\t\t&#125; catch (err) &#123;\n\t\t\tconsole.log('[-] Conscrypt CertPinManager pinner not found');\n\t\t\t//console.log(err);\n\t\t&#125;\n\n\n\n\t\t// CWAC-Netsecurity (unofficial back-port pinner for Android&lt;4.2) CertPinManager //\n\t\t///////////////////////////////////////////////////////////////////////////////////\n\t\ttry &#123;\n\t\t\tvar cwac_CertPinManager_Activity = Java.use('com.commonsware.cwac.netsecurity.conscrypt.CertPinManager');\n\t\t\tcwac_CertPinManager_Activity.isChainValid.overload('java.lang.String', 'java.util.List').implementation = function (a, b) &#123;\n\t\t\t\tconsole.log('[+] Bypassing CWAC-Netsecurity CertPinManager: ' + a);\n\t\t\t\treturn true;\n\t\t\t&#125;;\n\t\t&#125; catch (err) &#123;\n\t\t\tconsole.log('[-] CWAC-Netsecurity CertPinManager pinner not found');\n\t\t\t//console.log(err);\n\t\t&#125;\n\n\n\n\t\t// Worklight Androidgap WLCertificatePinningPlugin //\n\t\t/////////////////////////////////////////////////////\n\t\ttry &#123;\n\t\t\tvar androidgap_WLCertificatePinningPlugin_Activity = Java.use('com.worklight.androidgap.plugin.WLCertificatePinningPlugin');\n\t\t\tandroidgap_WLCertificatePinningPlugin_Activity.execute.overload('java.lang.String', 'org.json.JSONArray', 'org.apache.cordova.CallbackContext').implementation = function (a, b, c) &#123;\n\t\t\t\tconsole.log('[+] Bypassing Worklight Androidgap WLCertificatePinningPlugin: ' + a);\n\t\t\t\treturn true;\n\t\t\t&#125;;\n\t\t&#125; catch (err) &#123;\n\t\t\tconsole.log('[-] Worklight Androidgap WLCertificatePinningPlugin pinner not found');\n\t\t\t//console.log(err);\n\t\t&#125;\n\n\n\n\t\t// Netty FingerprintTrustManagerFactory //\n\t\t//////////////////////////////////////////\n\t\ttry &#123;\n\t\t\tvar netty_FingerprintTrustManagerFactory = Java.use('io.netty.handler.ssl.util.FingerprintTrustManagerFactory');\n\t\t\t//NOTE: sometimes this below implementation could be useful\n\t\t\t//var netty_FingerprintTrustManagerFactory = Java.use('org.jboss.netty.handler.ssl.util.FingerprintTrustManagerFactory');\n\t\t\tnetty_FingerprintTrustManagerFactory.checkTrusted.implementation = function (type, chain) &#123;\n\t\t\t\tconsole.log('[+] Bypassing Netty FingerprintTrustManagerFactory');\n\t\t\t&#125;;\n\t\t&#125; catch (err) &#123;\n\t\t\tconsole.log('[-] Netty FingerprintTrustManagerFactory pinner not found');\n\t\t\t//console.log(err);\n\t\t&#125;\n\n\n\n\t\t// Squareup CertificatePinner [OkHTTP&lt;v3] (double bypass) //\n\t\t////////////////////////////////////////////////////////////\n\t\ttry &#123;\n\t\t\t// Bypass Squareup CertificatePinner  &#123;1&#125;\n\t\t\tvar Squareup_CertificatePinner_Activity_1 = Java.use('com.squareup.okhttp.CertificatePinner');\n\t\t\tSquareup_CertificatePinner_Activity_1.check.overload('java.lang.String', 'java.security.cert.Certificate').implementation = function (a, b) &#123;\n\t\t\t\tconsole.log('[+] Bypassing Squareup CertificatePinner &#123;1&#125;: ' + a);\n\t\t\t\treturn;\n\t\t\t&#125;;\n\t\t&#125; catch (err) &#123;\n\t\t\tconsole.log('[-] Squareup CertificatePinner &#123;1&#125; pinner not found');\n\t\t\t//console.log(err);\n\t\t&#125;\n\t\ttry &#123;\n\t\t\t// Bypass Squareup CertificatePinner &#123;2&#125;\n\t\t\tvar Squareup_CertificatePinner_Activity_2 = Java.use('com.squareup.okhttp.CertificatePinner');\n\t\t\tSquareup_CertificatePinner_Activity_2.check.overload('java.lang.String', 'java.util.List').implementation = function (a, b) &#123;\n\t\t\t\tconsole.log('[+] Bypassing Squareup CertificatePinner &#123;2&#125;: ' + a);\n\t\t\t\treturn;\n\t\t\t&#125;;\n\t\t&#125; catch (err) &#123;\n\t\t\tconsole.log('[-] Squareup CertificatePinner &#123;2&#125; pinner not found');\n\t\t\t//console.log(err);\n\t\t&#125;\n\n\n\n\t\t// Squareup OkHostnameVerifier [OkHTTP v3] (double bypass) //\n\t\t/////////////////////////////////////////////////////////////\n\t\ttry &#123;\n\t\t\t// Bypass Squareup OkHostnameVerifier &#123;1&#125;\n\t\t\tvar Squareup_OkHostnameVerifier_Activity_1 = Java.use('com.squareup.okhttp.internal.tls.OkHostnameVerifier');\n\t\t\tSquareup_OkHostnameVerifier_Activity_1.verify.overload('java.lang.String', 'java.security.cert.X509Certificate').implementation = function (a, b) &#123;\n\t\t\t\tconsole.log('[+] Bypassing Squareup OkHostnameVerifier &#123;1&#125;: ' + a);\n\t\t\t\treturn true;\n\t\t\t&#125;;\n\t\t&#125; catch (err) &#123;\n\t\t\tconsole.log('[-] Squareup OkHostnameVerifier pinner not found');\n\t\t\t//console.log(err);\n\t\t&#125;\n\t\ttry &#123;\n\t\t\t// Bypass Squareup OkHostnameVerifier &#123;2&#125;\n\t\t\tvar Squareup_OkHostnameVerifier_Activity_2 = Java.use('com.squareup.okhttp.internal.tls.OkHostnameVerifier');\n\t\t\tSquareup_OkHostnameVerifier_Activity_2.verify.overload('java.lang.String', 'javax.net.ssl.SSLSession').implementation = function (a, b) &#123;\n\t\t\t\tconsole.log('[+] Bypassing Squareup OkHostnameVerifier &#123;2&#125;: ' + a);\n\t\t\t\treturn true;\n\t\t\t&#125;;\n\t\t&#125; catch (err) &#123;\n\t\t\tconsole.log('[-] Squareup OkHostnameVerifier pinner not found');\n\t\t\t//console.log(err);\n\t\t&#125;\n\n\n\n\t\t// Android WebViewClient (double bypass) //\n\t\t///////////////////////////////////////////\n\t\ttry &#123;\n\t\t\t// Bypass WebViewClient &#123;1&#125; (deprecated from Android 6)\n\t\t\tvar AndroidWebViewClient_Activity_1 = Java.use('android.webkit.WebViewClient');\n\t\t\tAndroidWebViewClient_Activity_1.onReceivedSslError.overload('android.webkit.WebView', 'android.webkit.SslErrorHandler', 'android.net.http.SslError').implementation = function (obj1, obj2, obj3) &#123;\n\t\t\t\tconsole.log('[+] Bypassing Android WebViewClient &#123;1&#125;');\n\t\t\t&#125;;\n\t\t&#125; catch (err) &#123;\n\t\t\tconsole.log('[-] Android WebViewClient &#123;1&#125; pinner not found');\n\t\t\t//console.log(err)\n\t\t&#125;\n\t\ttry &#123;\n\t\t\t// Bypass WebViewClient &#123;2&#125;\n\t\t\tvar AndroidWebViewClient_Activity_2 = Java.use('android.webkit.WebViewClient');\n\t\t\tAndroidWebViewClient_Activity_2.onReceivedSslError.overload('android.webkit.WebView', 'android.webkit.WebResourceRequest', 'android.webkit.WebResourceError').implementation = function (obj1, obj2, obj3) &#123;\n\t\t\t\tconsole.log('[+] Bypassing Android WebViewClient &#123;2&#125;');\n\t\t\t&#125;;\n\t\t&#125; catch (err) &#123;\n\t\t\tconsole.log('[-] Android WebViewClient &#123;2&#125; pinner not found');\n\t\t\t//console.log(err)\n\t\t&#125;\n\n\n\n\t\t// Apache Cordova WebViewClient //\n\t\t//////////////////////////////////\n\t\ttry &#123;\n\t\t\tvar CordovaWebViewClient_Activity = Java.use('org.apache.cordova.CordovaWebViewClient');\n\t\t\tCordovaWebViewClient_Activity.onReceivedSslError.overload('android.webkit.WebView', 'android.webkit.SslErrorHandler', 'android.net.http.SslError').implementation = function (obj1, obj2, obj3) &#123;\n\t\t\t\tconsole.log('[+] Bypassing Apache Cordova WebViewClient');\n\t\t\t\tobj3.proceed();\n\t\t\t&#125;;\n\t\t&#125; catch (err) &#123;\n\t\t\tconsole.log('[-] Apache Cordova WebViewClient pinner not found');\n\t\t\t//console.log(err);\n\t\t&#125;\n\n\n\n\t\t// Boye AbstractVerifier //\n\t\t///////////////////////////\n\t\ttry &#123;\n\t\t\tvar boye_AbstractVerifier = Java.use('ch.boye.httpclientandroidlib.conn.ssl.AbstractVerifier');\n\t\t\tboye_AbstractVerifier.verify.implementation = function (host, ssl) &#123;\n\t\t\t\tconsole.log('[+] Bypassing Boye AbstractVerifier: ' + host);\n\t\t\t&#125;;\n\t\t&#125; catch (err) &#123;\n\t\t\tconsole.log('[-] Boye AbstractVerifier pinner not found');\n\t\t\t//console.log(err);\n\t\t&#125;\n\n\n\t&#125;);\n\n&#125;, 0);\n\n参考文章：\nAndroid HTTPS防抓包策略与对抗方法总结\nAndroid : 关于HTTPS、TLS/SSL认证以及客户端证书导入方法 - sheldon_blogs - 博客园 (cnblogs.com)\n(19条消息) Android+Nginx一步步配置https单向/双向认证请求_Dream Fly的专栏-CSDN博客\n","categories":["笔记"],"tags":["Android","笔记","HTTPS","总结","Hook","frida","SSL PINNING"]},{"title":"Note 《Deep learning for cyber security intrusion detection Approaches, datasets, and comparative study》","url":"/Note-Deep-learning-for-cyber-security-intrusion-detection-Approaches-datasets-and-comparative-study/","content":" Deep learning for cyber security intrusion detection Approaches, datasets, and comparative study\n Section 1.摘要\n本篇paper是一篇关于入侵检测的综述性文章，设计方法、数据集和研究综述。\n在这篇文章中，我们做了网络安全入侵检测深度学习方法的调查，使用的数据集，并进行比较研究。具体来说，我们提供了基于深度学习方法的入侵检测系统的综述。数据集在入侵检测中起着重要的作用，因此我们描述了35个有名的网络数据集（cyber datasets），并将这些数据集分为7类；network traffic-based dataset, electrical network-based dataset, internet traffic-based dataset, virtual private network-based dataset, android apps-based dataset, IoT traffic-based dataset, and internet-connected devices-based dataset. 我们分析了七种深度学习模型，包括递归神经网络、深度神经网络、受限玻尔兹曼机、深度信念网络、卷积神经网络、深度玻尔兹曼机和深度自动编码器（recurrent neural networks, deep neural networks, restricted Boltzmann machines, deep belief networks, convolu- tional neural networks, deep Boltzmann machines, and deep autoencoders）。对于每个模型，我们在两个新的真实流量数据集，即CSE-CIC-IDS2018数据集和Bot-IoT数据集下研究了两类分类(二分类和多分类)的性能。此外，我们使用最重要的性能指标，即准确性、误报率和检测率来评估几种方法的效率。\n主要贡献：\n\n回顾了使用机器学习技术的入侵检测系统\n分析了七个深度学习方法\n研究了每个深度学习模型在CSE-CIC-IDS2018 dataset &amp; Bot-IoT dataset 两个数据集上的表现\n比较了深度学习和四个传统机器学习方法（Naive Bayes, Artificial neural network, SVM, Random Forest）\n\n读这篇paper的主要原因就是它是使用深度学习方法构建入侵检测系统这类技术的方法、数据集等各项指标的研究综述，且是比较新的，2019年的研究2020年发表在《Journal of Information Security and Applications》\n\n[ ]  国内也有此类综述，还是2020年中国科学院信息工程研究所&amp;中国科学院大学网络空间安全学院的老师发表的：网络入侵检测技术综述 蹇诗婕 10.19363/J.cnki.cn10-1380/tn.2020.07.07\n\n Section 3.基于深度学习的入侵检测系统\n目前来说有十种用于入侵检测的深度学习技术：(1) deep neural network, (2) feed forward deep neural network, (3) recurrent neural network, (4) convolutional neural network, (5) restricted Boltzmann machine, (6) deep belief network, (7) deep auto-encoder, (8) deep migration learning, (9) self-taught learning, and (10) replicator neural network\n接下来针对每种方法各列举了几篇研究论文，简单叙述了各个研究的准确率。这里我不打算翻译一遍这些方法记下来只附一张表，想看的话可以文末连接看原文。我只想看看都有那些数据集。\n\n Section 4.公开数据集\n\n\n4.1 Network traffic-based dataset\n4.1.1 DARPA 1998 dataset\n该数据集基于网络流量和审计日志，最早于1998二月公开可用。training data包含七周的网络攻击流量，testing data包含两周。根据Sharafaldin等人的研究，该数据集不能代表真实世界的网络流量。\n4.1.2 KDD Cup 1999 dataset\nKDD CUP 99 数据集使用的是DARPA 1998 DataSet的原始数据，在DARPA 98数据集的基础上进行了预处理，提取出了以“连接”为单位的一条条记录。\n4.1.3 NSL-KDD dataset\n此数据集由Tavallaee等人提出，为了解决KDD CUP 99数据集固有的一些问题。与原始KDD数据集相比，（1）不包含冗余记录，（2）不包含重复的记录\n4.1.4 UNSW-NB15 dataset\n该数据集由四个工具创建：IXIAPerfect-Stormtool,Tcpdumptool,Argustool,andBro-IDStool. 这些工具用于创建攻击，包括DoS，漏洞利用，通用，侦察，Shellcode和蠕虫。\n4.1.5 DEFCON dataset\n该数据集有两个版本：DEFCON-8(2000)andDEFCON-10(2002). DEFCON-8 包含端口扫描和指针溢出，DEFCON-10 包含探测和非探测攻击(e.g.,badpacket,portsscan,portsweeps,etc.).\n4.1.6 CAIDAs dataset\n该数据集由Center of Applied Internet Data Analysis 发布，包含不同数据集：CAIDA DDOS, CAIDA Internet traces 2016, RSDos Attack Metadata(2018-09). 具体来说，CAIDA DDOS 包含一小时的DDos流量，数据集被分割为5min为单位的pcap文件。RSDos Attack Metadata(2018-09) 中包含随机spoofed denial-of-service 攻击流量。\n4.1.7 CDX dataset\n此数据集是由Homoliak等人在network warfare competition期间创建的，其中包含网络服务上的恶意和合法TCP通信。这些网络服务存在指针溢出的可利用漏洞，这些服务包括：Postfix Email FreeBSD, Apache Web Server Fedora 10, OpenFire Chat FreeBSD, and BIND DNS FreeBSD.\n4.1.8 KYOTO dataset\n该数据集基于真实的三年流量数据，这些数据是使用四个工具创建的：honeypots, darknet sensors, e-mail server and web crawler.\n4.1.9 TWENTE dataset\n该数据集是在6天的时间内收集的，包含14.2M流量和7.6M警告。TWENTE数据集使用三个IP协议（包括UDP，TCP和ICMP）表示一个子分类。\n4.1.10 CIC Dos dataset\n该数据集包含使用不同工具的4种不同类型的应用层DoS攻击。CICDoS数据集由Jazi等人提出。high-volume HTTP attacks, low-volume HTTP attack\n4.1.11 CICDS2017 dataset\n该数据集包含从2017年7月3日星期一到2017年7月7日星期五捕获的数据.CICIDS2017数据集由Sharafaldin等人提出。其中攻击手段包括：BruteForceSSH,DoS,Heartbleed,WebAttack,Infiltration,BotnetandDDoS,andBruteForceFTP.\n4.1.12 CSE-CIC-IDS2018dataset\n该数据集由Communications Security Establishment(CSE)&amp;the Canadian Institute(CIC)联合发布，包含7种攻击：Heartbleed,Brute-force,DoS,DDoS,Webattacks,Botnet,andinfiltration.\n4.1.13 ISCX dataset\n该数据集由Shiravi等人创建，七天的网络活动（包含恶意和正常流量）组成。恶意活动包括：（1）内网渗透（2）HTTP拒绝服务攻击（3）分布式拒绝服务攻击（4）爆破SSH。在ISCX dataset中有两种通用的profiles类别：(1)试图以明确的方式描述攻击场景的profiles (2)封装提取的特定实体的数学分布或行为的profiles\n4.1.14 ADFA2013 dataset\n该数据集由Creech和Hu提出，包含了攻击Ubuntu系统的向量和payload：password brute-force, add new super user, java based meterpreter, linux meterpreter payload, and C100 Webshell. 数据集结构包含三种数据类型，即(1)正常training data (2)正常validation data (3)attack data. 正常训练数据包含4373条trace。正常验证数据包含833条trace。攻击数据包含每个vector 的10次攻击。\n4.2 Electrical network-based dataset\n4.2.1 LBNL dataset\n该数据集是使用uPMU在劳伦斯·伯克利国家实验室电气网络收集到的。uPMU是微相量测量单元，它产生12个120赫兹的高精度值流，时间戳精确到100纳秒。该数据集可用于微电网同步以及负载和分布式发电的表征。\n4.2.2 ICS cyber attack dataset\n该数据集包含五个不同的数据集：(1)Power System Datasets, (2) Gas Pipeline Datasets, (3) Energy Management System Data, (4) New Gas Pipeline (5) Gas Pipeline and Water Storage Tank.\n电力系统包含37个场景，分为8个自然事件，1个无事件和28个攻击事件。有三类攻击，包括:(1)继电器设置改变，(2)远程跳闸命令注入，(3)数据注入。这些数据集可用于工业控制系统中的计算机安全入侵检测\n4.2.3 IEEE 300-bus power test system\n该数据集提供了电网的拓扑和电气结构，特别用于检测智能电网中的虚假数据注入攻击。系统有411个分支，平均度(&lt; k &gt;)为2.74。关于这个标准测试系统的更多细节，我们请读者参考Hines等人的工作。IEEE 300-bus电源测试系统已用于与网络攻击分类相关的多项工作\n4.3 Internet traffic-based dataset\n4.3.1 UMASS dataset\n该数据集包含两个不同的数据集：(1)strong flow correlation attacks (2)OneSwarm上的简单定时攻击\n前者是使用tor客户端在tor网络上访问top 50,0 0 0 Alexa websites。后者具体来说有三种独立的攻击，包括基于时间信息的攻击、基于查询转发的攻击和基于TCP吞吐量的攻击\n4.3.2 Tor-nonTor dataset\n该数据集由Lashkari等人提出，包含来自超过18种很具代表性应用程序的8种流量类型(VOIP, chat, audio-streaming, video-streaming, mail, P2P, browsing, and File Transfer)。\n4.3.3 URL dataset\n该数据集由Mamun等人提出，包含5种类型的url：1)Benign URLs 2)Spam URLs 3) Phishing URLs 4) Malware URLs 5) Defacement URLs.\n4.3.4 MAWI dataset\n该数据集包含数据包捕获形式的每日流量，该数据包捕获自日本和美国之间的跨太平洋链路。\n4.4 Virtual private network-based dataset\n4.4.1. VPN-nonVPN dataset\n该数据集由Draper-Gil提出，它捕获了普通会话和通过虚拟专用网络(VPN)的会话。具体来说，该数据集由标记的网络流量组成，包括浏览器(例如Firefox)、电子邮件(例如SMPTS)、聊天(例如Skype)、流媒体(例如Youtube)、文件传输(例如SFTP)、网络电话(例如Hangouts语音呼叫)和P2P(Uutrent)。\n4.5 Android apps-based dataset\n4.5.1 Android validation dataset\n数据集由72个原始应用组成，具有以下操作:替换图标、替换文件、插入垃圾代码、不同对齐、插入垃圾文件和替换字符串，Android validation dataset由Gonzalez等人提出，为了找寻上述这些android app之间的关系。他们提取了两个特征，即，(1)元信息(伴随每个.apk文件) (2) N -grams(表征.dex文件)。此外，Android validization数据集引入了以下定义，概述了应用程序之间的关系:双胞胎、兄弟姐妹、假兄弟姐妹、继兄弟姐妹、假继兄弟姐妹和表兄弟姐妹。原始基本集包含72个应用程序，而包含转换应用程序的完整集包含792个应用程序。\n4.5.2 Android adware dataset\n该数据集由190个应用程序生成，分为三类，包括广告软件(250个应用程序)、一般恶意软件(150个应用程序)和良性软件(1500个应用程序)。\n4.5.3. Android malware dataset\n该数据集名为CICAndMal2017，包含恶意软件和良性应用程序，这些样本来自谷歌应用市场2015-2017。CICAndMal2017数据集由Shiravi等人提出。\n4.6 IoT traffic-based dataset\n4.6.1. Bot-IoT dataset\n此数据集包含72,000,000以上的记录，包含的攻击手段有：DDoS, DoS, OS and Service Scan, Keylogging and Data exfiltration attacks. Bot-IoT数据集是由Koroniotis等人提出，在IoT安全领域这是一个全新的数据集。作者使用节点红工具模拟物联网设备的网络行为。为了连接机器到机器(M2M)的通信，数据集使用了MQTT协议，这是一种轻量级的通信协议。试验台中使用了五种物联网场景，即气象站、智能冰箱、运动激活灯、远程激活车库门和智能恒温器。\n4.7 Internet-connected devices-based dataset\n4.7.1 Botnet dataset\n该数据集由Beigi等人提出，分为训练数据集和测试数据集，分别包括7种和16种僵尸网络。僵尸网络类型在训练数据集中的分布包括：Neris, Rbot, Virut, NSIS, SMTP Spam, Zeus, and Zeus control (C&amp;C). 测试数据集中僵尸网络类型的分布包括：Neris, Rbot, Menti, Sogou… etc. 僵尸网络拓扑可以是集中式、分布式(例如P2P)或随机化的。使用的功能分为四组，即基于字节、基于数据包、基于时间和基于行为。\n论文链接：https://www.sciencedirect.com/science/article/abs/pii/S2214212619305046\n","categories":["论文阅读"],"tags":["论文","笔记","Intrusion-Detection","Deep-Learning"]},{"title":"【实战】某哩视频app的crack尝试","url":"/%E3%80%90%E5%AE%9E%E6%88%98%E3%80%91%E6%9F%90%E5%93%A9%E8%A7%86%E9%A2%91app%E7%9A%84crack%E5%B0%9D%E8%AF%95/","content":" 【实战】某哩视频app的crack尝试\n这个app已经被大哥们玩烂了，我这算是复现吧，随手记录一下，留作以后温习。\n目的：\n\n分析传输字段的生成方式和作用\n尝试破解部分VIP功能\n\n 从流量分析开始\nFiddler 抓包，前半部分还正常，从点开视频开始app在向localhost:1500请求，Fiddler 就会返回：502 Fiddler - Connection Failed 。我猜app解析视频的业务逻辑是向本地1500端口启动的服务发出类似以下请求，通过本地服务解析视频地址，或向服务器发出加密解析请求。\nGET http:&#x2F;&#x2F;localhost:1500&#x2F;video?mode&#x3D;online&amp;video_id&#x3D;100981&amp;quality&#x3D;qvga&amp;type&#x3D;.m3u8 HTTP&#x2F;1.1\nUser-Agent: com.xxxxxxxx.other.MyApplication&#x2F;2.3.1-2820 (Linux;Android 10) ExoPlayerLib&#x2F;2.11.1\nAccept-Encoding: gzip\nHost: localhost:1500\nConnection: Keep-Alive\n手机WIFI代理设置localhost不走代理就可以在代理下看视频了，但是这样你也抓不到上述请求包了。\napp启动过程中的流量：\n\n\n\n接口\n请求方式\n提交内容\n数据类型\n(猜测)用途\n\n\n\n\nhost_xxxxxxxx.txt\nGET\n仅有http头hash/lang/platform/time/userid/usertype/version\n密文（后续已破解）\n可能是返回能正常访问的主机群\n\n\n/speed.html\nGET\n同上\n明文&quot;ok&quot;\n测速\n\n\n/v1/initial\nGET\n同上\n明文\n新版本检测+首页弹窗+视频分类\n\n\n/v1/register/token\nPOST\ndevice_id/platform/key/universal_id/lang\n明文\n注册新用户，返回token和uid\n\n\n/v1/user/info\nPOST\ntoken/lang/download_amount\n明文\n查询并返回用户信息\n\n\n/v1/firstpurchase\nGET\n仅有http头\n明文\n买VIP悬浮广告\n\n\n\n第一次启动app时向服务器请求token样例：eyJ1c2VyX2lkIjo0NDk5NzQ3MjYsImxhc3Rsb2dpbiI6MTYxNDMxMTg0OX0.1ea8cfc740dc64f1b0a70adc0357be61.234d8d0043d2b0368285c605f974769d5eea56017ef0fcd116b613cf\n稍后再分析token\n 视频请求流量解密\n\n\n\n请求视频信息，返回json格式数据中video_url字段包含m3u8视频地址\n\n\n演员信息\n\n\n向服务器请求视频信息，返回数据是加密的，估计这就是m3u8的信息\n\n\n5.都是本地请求，猜测是请求解析具体m3u8视频的数据\n上图是具体视频数据信息可以看到是请求m3u8的切片ts文件。\n\n\n在media/240/xxxx.m3u8?token=…的响应报文中有一个X-VTag将是解密该报文的重要信息之一\na.b.i.a.h.a部分代码：\nString a4 = tVar.mo12446a(\"X-VTag\");\nif (a4 == null || (i = StringExt.m2783i(a4)) == null) &#123;\n    str = null;\n&#125; else &#123;\n    str = i.substring(8, 24);\n    C7509i.m26507a((Object) str, \"(this as java.lang.Strin…ing(startIndex, endIndex)\");\n&#125;\nResponseBody h0Var2 = a.f18837g;\nMediaType c = h0Var2 != null ? h0Var2.mo12175c() : null;\nif (!(str == null || c == null)) &#123;\n    ResponseBody a5 = ResponseBody.m17589a(c, EncodeUtility.abaababa(CipherClient.decodeKey(), str, a2));\n    C5325g0.C5326a aVar2 = new C5325g0.C5326a(a);\n    aVar2.f18849g = a5;\n    C5325g0 a6 = aVar2.mo12169a();\n    C7509i.m26507a((Object) a6, \"response.newBuilder().body(body).build()\");\n    return a6;\n&#125;\n去看一下StringExt.m2783i函数，虽然他虚晃你说他调用AES算法，但是你仔细看看就发现根本不符合AES的调用。再去看看AESEncryptor.HASH_ALGORITHM，就知道他应该是做md5散列的。所以综合这两个函数的分析得到：\n\n\n\n变量\n解析\n值\n\n\n\n\na4\nheaders中的X-VTag的值\n958893970\n\n\ni\nmd5(a4)\ne10adc3949ba59abbe56e057f20f883e\n\n\nstr\ni.substring(8, 24)\n49ba59abbe56e057\n\n\n\n接着看CipherClient.decodeKey()，你会发现这个类里面全是用常量生成的返回值：\npackage net.idik.lib.cipher.p620so;\n\n/* renamed from: net.idik.lib.cipher.so.CipherClient */\npublic final class CipherClient &#123;\n    public CipherClient() throws IllegalAccessException &#123;\n        throw new IllegalAccessException();\n    &#125;\n    public static final String apiHashKey() &#123;\n        return CipherCore.get(\"d708e111b5db90af74ef84ff4d5e647b\");\n    &#125;\n    public static final String decodeKey() &#123;\n        return CipherCore.get(\"aa01bdd83d0f12833ddaea2f2af22865\");\n    &#125;\n    public static final String encodeType() &#123;\n        return CipherCore.get(\"e82b6153b4ea2340333e2254c3553d03\");\n    &#125;\n    public static final String everIv() &#123;\n        return CipherCore.get(\"c5b287eb7ee64e90ed015bac470f4b6b\");\n    &#125;\n    ...\n&#125;\n追踪下去发现追到native程序里了：CipherClient.decodeKey()–&gt;CipherCore.get()–&gt;native CipherCore.getString()\nnative库为：cipher-lib\n不太想看native逻辑了，为了快速省事就用frida hook返回值看看：\nfunction main()&#123;\n    Java.perform(function()&#123;\n        var CipherCore &#x3D; Java.use(&quot;net.idik.lib.cipher.so.CipherCore&quot;)\n        var stringClass &#x3D; Java.use(&quot;java.lang.String&quot;)\n        var str &#x3D; stringClass.$new(&quot;aa01bdd83d0f12833ddaea2f2af22865&quot;)\n        var res &#x3D; CipherCore.get(str)\n        console.log(&quot;getString(&quot;+str+&quot;):&quot;+res)\n    &#125;)\n&#125;\nsetImmediate(main);\n\n通过上述方式我们可以把每个CipherClient方法的返回值都得到：\n\n\n\n函数\n常量\n获取到的结果\n\n\n\n\napiHashKey\nd708e111b5db90af74ef84ff4d5e647b\n666wInteriscommingyoUshouldNotpassmotherfuckeR=\n\n\nccToken\n810d903a88254b27c643e0bc471d406a\n2cfbf0f5d358406b96ab9fd1aa59ae89\n\n\ndecodeKey\naa01bdd83d0f12833ddaea2f2af22865\n6e561ccd4aade2fed458d4da61e76770\n\n\nencodeType\ne82b6153b4ea2340333e2254c3553d03\napp\n\n\neverIv\nc5b287eb7ee64e90ed015bac470f4b6b\nBakinsodaIgotbakinsoda\n\n\neverKey\n0d7cb519eb483a597549f7f466b189bc\niaMiNloveWithtHecoCo\n\n\nhostIv\n6cf9e96524081ac264dc31982d0be319\n5e8bf1f958f56644\n\n\nhostKey\n51cdba173d412fdecec3e78572cde731\nf332ae8214fcbb0d98f8626f123459b4\n\n\nimageDecodeIv\n9e1add49a87568f90c43e418e7370287\nE01EDE6331D37AFCC7BE05597D654D22\n\n\nimageDecodeKey\na8ae2831cbea74111bc5116ba81ec191\nB2F3842866F9583D1ECE61C4E055C255\n\n\nregisterKey\n951eeb6b19b70177fd25706aa620edcf\nggh%*KXOqk882jO&amp;Z3Sz43dQGTfD4y6SC&amp;9z\n\n\n\n我们再继续回去看a.b.i.a.h.a，现在就知道EncodeUtility.abaababa(CipherClient.decodeKey(), str, a2)应该是：\nEncodeUtility.abaababa(&quot;6e561ccd4aade2fed458d4da61e76770&quot;, &quot;49ba59abbe56e057&quot;, a2)\n猜测 a2 应该是密文\nEncodeUtility.abaababa 是 a.b.j.v3.a 的别名，阿巴阿巴是我不小心起的，忘记原来jadx给的命名是啥了。其定义如下\n/* renamed from: a */\npublic static String abaababa(String str, String str2, String str3) throws UnsupportedEncodingException, NoSuchAlgorithmException, NoSuchPaddingException, IllegalBlockSizeException, BadPaddingException, InvalidAlgorithmParameterException, InvalidKeyException &#123;\n\treturn m3078a(new IvParameterSpec(str2.getBytes(\"UTF-8\")), new SecretKeySpec(m3076a(str).getBytes(\"UTF-8\"), \"AES\"), str3);\n&#125;\n\nm3078a方法，看一下它的声明就知道，这个函数是AES解密，该函数中有声明AES模式：AESEncryptor.AES_MODE==&quot;AES/CBC/PKCS5Padding&quot; ，这里就不展示了。\nm3076a方法，同样看下声明也容易知道是MD5散列。\n现在基本就确定了abaababa方法最后一个参数就应该是密文，参数2是偏移，参数1的md5值是密钥，执行的是AES解密。\nmd5('6e561ccd4aade2fed458d4da61e76770')=ae52f7ffd2dd66ba5743bb180188b991\n综上所述：\n\nGET请求 /media/240/xxxxxx.m3u8?token=…向服务器请求视频信息，返回数据是是m3u8的信息，且经过AES加密的\nAES模式CBC，padding模式PKCS5Padding\nKEY：ae52f7ffd2dd66ba5743bb180188b991\nIV：49ba59abbe56e057\n\n\n解密成功。\n到这里视频请求信息的解密就完成了，我们来复盘一下思考下开发者是怎么考虑视频信息加密的：\n每个视频都有独特的x-vtag，md5(x-vtag).substring(8,24) 作为AES偏移量IV，app的java代码中的一个常量通过native 代码得到另一个常量作为AES的密钥KEY。KEY服务器和客户端都知道，x-vtag服务器明文发给客户端。服务端通过AES加密m3u8信息，客户端AES解密，获取m3u8信息。\n视频请求步骤：\n\n用户向服务器A请求某个视频的信息：/v1/video/info/&lt;视频id&gt;?token=... 得到JSON格式视频信息，包括演员，类别，视频TAG，封面图片连接，video_url 等信息\n用户向服务器B请求某个m3u8视频的媒体信息，就是使用上一步得到的video_url：/media/240/100981.m3u8?token=... 客户端得到AES加密的m3u8视频的媒体信息\nAPP解密上述信息得到视频的流媒体源，向本地请求：localhost:1500/video?mode=online&amp;video_id=100981&amp;quality=qvga&amp;type=.m3u8 这一步真正的意义现在还没弄懂\n根据m3u8解码得到的分片信息，向存储视频数据的服务器请求流媒体\n\n总的来说到这里就分析完了m3u8视频获取的流程和机密视频信息的方法了，但是要通过m3u8信息想获取视频你需要附加请求头。\n m3u8请求头\n发现在请求m3u8密文的时候会经常提示拒绝访问access denied\n看一下请求头中的参数\n\n\n\n参数\n值\n来源\n\n\n\n\nversion\n2.3.1\napp版本\n\n\nplatform\nAndroid\n安卓平台\n\n\ntime\n1614354694\n当前十位时间戳\n\n\nuserid\nYaoIcnqMTOWO\n每次不一，可能是随机值\n\n\nhash\ne6195dd2a9cf277f480f797222823542\n待探索\n\n\n\njadx中搜一下&quot;hash&quot;字符串，就能找到其定义，在a.b.a.v.f.e()中：\n\nString i = StringExt.m2783i(a + value + valueOf + str + str2);\n\n之前我们已经得出StringExt.m2783i是md5的操作。\n依次看a + value + valueOf + str + str2\na = version的值 = APP版本号\nvalue = platform的值 = 平台Android\nvalueOf = time的值 = 时间戳\nstr = userId的值 = 从\"abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789\"取随机字符\nstr2 = CipherClient.apiHashKey() = \"666wInteriscommingyoUshouldNotpassmotherfuckeR=\"\n所以i = md5(app版本 + 平台Android + 时间戳 + 随机userId + \"666wInteriscommingyoUshouldNotpassmotherfuckeR=\")\n上述案例中 i = md5(\"2.3.1Android1614354694YaoIcnqMTOWO666wInteriscommingyoUshouldNotpassmotherfuckeR=\") = \"e6195dd2a9cf277f480f797222823542\"\n另外，这个hash在其他报头也出现了，生成方式应该都是一样的。\n Host信息decrypt\n还记得最开始流量分析的时候，APP启动时有请求host_xxxxxxxx.txt这个文件吗，我们已经获得了hostIV和hostKey，可以AES解密该txt文件，其中包含各类业务的domain url：\n0xxxxxxx6dd4x26xxxxf0axxfx8&#123;\n    &quot;api&quot;: [ \n        &#123;\n            &quot;url&quot;: &quot;https:&#x2F;&#x2F;xxxxxxxxxx&quot;\n    ],\n    &quot;img&quot;: [\n        \n        &#123;\n            &quot;url&quot;: &quot;https:&#x2F;&#x2F;xxxxxxxxxxxx&quot;\n        &#125;, \n    ],\n    &quot;stream&quot;: [ \n        &#123;\n            &quot;url&quot;: &quot;https:&#x2F;&#x2F;xxxxxxxxxxxxx&quot;\n        &#125;,\n     ],\n    &quot;cash&quot;:[\n        &#123;\n            &quot;url&quot;: &quot;https:&#x2F;&#x2F;xxxxxxxxxx&quot;\n        &#125;,\n    ],\n    &quot;pwaredirect&quot;: [\n        &#123;\n            &quot;url&quot;: &quot;https:&#x2F;&#x2F;xxxxxxxx&quot;\n        &#125;\n    ]&#125;50xxxxxxxxxxxxxxxxxxxxxdc\n VIP?\n据大佬分析，修改两个函数就可以在本地实现vip，可以使用倍速和切换线路，但是高清播放这种功能大概是有云端验证所以没办法。具体来说需要修改：getExpiry()和getLevel()两个函数\n我使用frida做hook验证了，确实可以，脚本和命令如下：\nfrida -U -f com.xxxxxxxx --no-paus -l .\\hookvip.js\n//打印参数、返回值\nfunction main()&#123;\n    Java.perform(function()&#123;\n        Java.use(\"xxx.xxxxxxxx.model.response.ResponseUserInfo\").getExpiry.overload().implementation = function ()&#123;\n            console.log(\"[+]function getExpiry() was called\");\n            return 0x746a6480;\n        &#125;\n        Java.use(\"xxx.xxxxxxxx.model.response.ResponseUserInfo\").getLevel.overload().implementation = function ()&#123;\n            console.log(\"[+]function getLevel() was called\");\n            return 2;\n        &#125;\n    &#125;)\n&#125;\nsetImmediate(main);\n\n另外提一嘴，前段时间这个应用的web端程序，如果在请求token的时候修改提交的参数pwa-ckv=1，则会返回一个尊容vip的token。这个bug已经被修复了。\n\n Rigister请求字段生成\n多次抓注册请求报文：\n&#123;&quot;device_id&quot;:&quot;ce793c3e-e720-4e0d-9e56-fddcd033d862&quot;,&quot;platform&quot;:&quot;Android&quot;,&quot;key&quot;:&quot;3d5f7885e4d45c17c547a0f2bc9d499a3799ce69d616f8504091df86e55d360a&quot;,&quot;universal_id&quot;:&quot;884885A46B10364A5AF184395FF4DD93&quot;,&quot;lang&quot;:&quot;cn&quot;&#125;\ntoken&#x3D;eyJ1c2VyX2lkIjo0NDk5NzQ3MjYsImxhc3Rsb2dpbiI6MTYxNDY2MTAzN30.49f3f134c8f1d86478c5dd6cb0fbaeb8.7caa99b911ce29125339244676fd7c17a713f7117502dbc6948bc56c\n\n&#123;&quot;device_id&quot;:&quot;a591b897-0c35-4514-9a69-2c61ad40e812&quot;,&quot;platform&quot;:&quot;Android&quot;,&quot;key&quot;:&quot;631a809db87dae8c038ca63252cc3c67b0755a7c2a23ea180dca7deba768c3c0&quot;,&quot;universal_id&quot;:&quot;884885A46B10364A5AF184395FF4DD93&quot;,&quot;lang&quot;:&quot;cn&quot;&#125;\ntoken&#x3D;eyJ1c2VyX2lkIjo0NDk5NzQ3MjYsImxhc3Rsb2dpbiI6MTYxNDY2MzQ0MX0.aae1a8f7a517e7802e00b36832a98475.fe497a9da7ae1867dcc50f151305d2989d3e7e7ca670b0e076226d2f\n\n&#123;&quot;device_id&quot;:&quot;8b12ee47-199a-4f0d-9937-1509d84eac3f&quot;,&quot;platform&quot;:&quot;Android&quot;,&quot;key&quot;:&quot;fde31433a815946f4debbd4d961d3c23142b5b41a60367f9bab7055afc988de2&quot;,&quot;universal_id&quot;:&quot;884885A46B10364A5AF184395FF4DD93&quot;,&quot;lang&quot;:&quot;cn&quot;&#125;\ntoken&#x3D;eyJ1c2VyX2lkIjo0NDk5NzQ3MjYsImxhc3Rsb2dpbiI6MTYxNDY2MzQ1NH0.45689a362d7207a94dfd58ccebddfe66.979a33cbcf7f91cb671a830fc21c941c156a523b3b17efae97ce410b\n下面先来研究一下注册请求中各个字段的生成，具体代码应该是在 a.b.h.b.m.toString()\npublic String toString() &#123;\n    StringBuilder a = outline.m3774a(\"RegisterTokenBody(deviceId=\");\n    a.append(this.f2668a);\n    a.append(\", platform=\");\n    a.append(this.f2669b);\n    a.append(\", key=\");\n    a.append(this.f2670c);\n    a.append(\", universalId=\");\n    a.append(this.f2671d);\n    a.append(\", lang=\");\n    a.append(this.f2672e);\n    a.append(\", unicode=\");\n    a.append(this.f2673f);\n    a.append(\", utmSource=\");\n    a.append(this.f2674g);\n    a.append(\", utmMedium=\");\n    return outline.m3771a(a, this.f2675h, \")\");\n&#125;\n\n很容易可以找到deviceId、key、universalId的生成函数：a.b.h.a.e0中的a()、b()、c()\npublic final String a() &#123;\n    a a2 = g.a(g.f1311a, \"app_uuid\", \"\", null, null, 12);\n    boolean z2 = false;\n    i&lt;?> iVar = b[0];\n    m mVar = (m) a2;\n    if (((CharSequence) mVar.a((Object) null, iVar)).length() == 0) &#123;\n        z2 = true;\n    &#125;\n    if (!z2) &#123;\n        return (String) mVar.a((Object) null, iVar);\n    &#125;\n    String uuid = UUID.randomUUID().toString();//random???\n    z.t.c.i.a((Object) uuid, \"UUID.randomUUID().toString()\");//验证uuid非空\n    mVar.a(null, iVar, uuid);//存储uuid->/data/data/com.xxxxxxxx/shared_prefs/com.xxxxxxxx_preferences.xml\n    return (String) mVar.a((Object) null, iVar);\n&#125;\n\npublic final String b() &#123;\n    String a2 = a();//取device_id\n    if (a2 != null) &#123;\n        try &#123;\n            MessageDigest instance = MessageDigest.getInstance(\"SHA-256\");\n            String str = \"jav\" + a2 + \"jav\";//jav&lt;device_id>jav\n            Charset charset = StandardCharsets.UTF_8;\n            z.t.c.i.a((Object) charset, \"StandardCharsets.UTF_8\");\n            if (str != null) &#123;\n                byte[] bytes = str.getBytes(charset);\n                z.t.c.i.a((Object) bytes, \"(this as java.lang.String).getBytes(charset)\");\n                byte[] digest = instance.digest(bytes);//sha-256(jav&lt;device_id>jav)\n                z.t.c.i.a((Object) digest, \"hash\");//验证摘要信息非空\n                return f.a(digest);//摘要格式化为小写\n            &#125;\n            throw new k(\"null cannot be cast to non-null type java.lang.String\");\n        &#125; catch (Exception e) &#123;\n            e.printStackTrace();\n            return null;\n        &#125;\n    &#125; else &#123;\n        z.t.c.i.a(\"$this$toHashKey256\");\n        throw null;\n    &#125;\n&#125;\n@SuppressLint(&#123;\"HardwareIds\", \"DefaultLocale\"&#125;)\npublic final String c() &#123;\n    boolean z2;\n    String str;\n    String str2 = Build.SERIAL;//\n    if (str2 == null) &#123;\n        z2 = false;\n    &#125; else &#123;\n        z2 = str2.equalsIgnoreCase(\"unknown\");\n    &#125;\n    if (!z2) &#123;//1.如果Build.SERIAL取到的值不是unknow的话就可以返回\n        return str2;\n    &#125;\n    String str3 = null;\n    try &#123;//2.好像是从google Advertising Id中取值\n        AdvertisingIdClient.Info advertisingIdInfo = AdvertisingIdClient.getAdvertisingIdInfo(this.f1363a);\n        z.t.c.i.a((Object) advertisingIdInfo, \"AdvertisingIdClient.getAdvertisingIdInfo(context)\");\n        str = advertisingIdInfo.getId();\n    &#125; catch (Exception e) &#123;\n        e.printStackTrace();\n        str = null;\n    &#125;\n    if (str != null) &#123;\n        String i = f.i(str + \"com.xxxxxxxx\");\n        if (i != null) &#123;\n            String upperCase = i.toUpperCase();\n            z.t.c.i.a((Object) upperCase, \"(this as java.lang.String).toUpperCase()\");\n            return upperCase;\n        &#125;\n        throw new k(\"null cannot be cast to non-null type java.lang.String\");\n    &#125;\n    try &#123;//3.取Settings.Secure.getString(ContentResolver resolver, \"android_id\")\n        str3 = Settings.Secure.getString(this.f1363a.getContentResolver(), com.umeng.commonsdk.statistics.idtracking.b.f7127a);\n    &#125; catch (Exception e2) &#123;\n        e2.printStackTrace();\n    &#125;\n    if (str3 != null) &#123;\n        String i2 = f.i(str3 + \"com.xxxxxxxx\");\n        if (i2 != null) &#123;\n            String upperCase2 = i2.toUpperCase();\n            z.t.c.i.a((Object) upperCase2, \"(this as java.lang.String).toUpperCase()\");\n            return upperCase2;\n        &#125;\n        throw new k(\"null cannot be cast to non-null type java.lang.String\");\n    &#125;\n    String i3 = f.i(a());//取device_id\n    if (i3 != null) &#123;//4.md5(device_id)\n        String upperCase3 = i3.toUpperCase();\n        z.t.c.i.a((Object) upperCase3, \"(this as java.lang.String).toUpperCase()\");\n        return upperCase3;\n    &#125;\n    throw new k(\"null cannot be cast to non-null type java.lang.String\");\n&#125;\n\n看起来uuid（device_id）是获取random值，存储在shared_prefs/com.xxxxxxxx_preferences.xml中，而key=sha-256(“jav”+device_id+“jav”)\n关于universalId的生成，其实有几种不同的可能：\n\n\nandroid.os.Build.SERIAL：API level 9 添加的，在 Android O 中这个字段被置为&quot;unknown&quot;，需要使用android.os.Build.getSerial()方法来获取，且需要android.permission.READ_PHONE_STATE权限\n\n\nAdvertisingIdClient.getAdvertisingIdInfo(this.context).getId()\n\n\nSettings.Secure.getString(ContentResolver resolver, “android_id”)\n\n\nmd5(deviceID)：应该不是这个如果用这个方法，每次universalId应该随着device_id变化\n\n\n我通过frida hook主动调用尝试了几种universalId的获取方法\n关于1：\nfunction main()&#123;\n    Java.perform(function()&#123;\n        var build = Java.use(\"android.os.Build\")\n        var res = build.getSerial()\n        console.log(res)\n    &#125;)\n&#125;\nsetImmediate(main);\n//return:unknown\n关于2：\nfunction getContext() &#123;\n    return Java.use('android.app.ActivityThread').currentApplication().getApplicationContext();\n&#125; \nfunction main()&#123;\n    Java.perform(function()&#123;\n        var AdvertisingIdCliente = Java.use(\"com.google.android.gms.ads.identifier.AdvertisingIdClient\")\n        var advertisingIdCliente = AdvertisingIdCliente.getAdvertisingIdInfo(getContext())\n        var id = advertisingIdCliente.getId()\n        console.log(id)\n    &#125;)\n&#125;\nsetImmediate(main);\n//return:1764c41f-a2b5-4c87-aa34-ed53cc30aac3\n关于3：\nfunction getContext() &#123;\n    return Java.use('android.app.ActivityThread').currentApplication().getApplicationContext().getContentResolver();\n&#125;                                         \nfunction logAndroidId() &#123;\n    console.log('[+]', Java.use('android.provider.Settings$Secure').getString(getContext(), 'android_id'));\n&#125;\nJava.perform(logAndroidId)\n//return:00acc23791dcfe9d\n现在我们破案了：\n\n\n\n字段\n说明\n备注\n\n\n\n\ndevice_id\nrandom值\n生成后存储在：shared_prefs/com.xxxxxxxx_preferences.xml\n\n\nplatform\n“Android”\n固定明文\n\n\nkey\nsha-256(“jav”+device_id+“jav”)\n\n\n\nuniversal_id\nAdvertisingIdClient.getAdvertisingIdInfo(this.context).getId()\n\n\n\nlang\n“cn”\n固定明文\n\n\n\n备注e0.a()、e0.b()、e0.c()的frida主动调用代码：\nfunction main()&#123;\n    Java.perform(function()&#123;\n        var e0 = Java.use(\"a.b.h.a.e0\")\n        var inse = e0.$new()\n        var res = inse.c()\n        console.log(res)\n    &#125;)\n&#125;\nsetImmediate(main);\n//return:884885A46B10364A5AF184395FF4DD93\n\n另外，在注册请求的返回值中的token，这个字符串的中间一段是md5结果，其明文应该是9-10位的数字串，但具体生成方式只有服务端知道。\neyJ1c2VyX2lkIjo0NDk5NzQ3MjYsImxhc3Rsb2dpbiI6MTYxNDMxMTg0OX0.1ea8cfc740dc64f1b0a70adc0357be61.234...\nmd5(394171256) &#x3D;&#x3D; 1ea8cfc740dc64f1b0a70adc0357be61\neyJ1c2VyX2lkIjo0NDk5NzQ3MjYsImxhc3Rsb2dpbiI6MTYxNDY2MTAzN30.49f3f134c8f1d86478c5dd6cb0fbaeb8.7ca...\nmd5(1167890769) &#x3D;&#x3D; 49f3f134c8f1d86478c5dd6cb0fbaeb8\nref:\nhttps://www.52pojie.cn/thread-1375881-1-1.html\nhttps://cloud.tencent.com/developer/news/746067\nAndroid唯一标识：https://daemonyang.blogspot.com/2017/06/android.html\n","categories":["实战"],"tags":["Android","crack","实战"]},{"title":"Note 《Towards Dynamically Monitoring Android Applications on Non-rooted Devices in the Wild》","url":"/Note-Towards-Dynamically-Monitoring-Android-Applications-on-Non-rooted-Devices-in-the-Wild/","content":" Towards Dynamically Monitoring Android Applications on Non-rooted Devices in the Wild\n 主要解决什么问题\n目前动态分析技术需要在实验室环境中运行状态下触发代码级和系统级事件，以窥探app的敏感性为。这种严格的运行环境阻碍了此类动态分析技术的大规模部署。此外现存的Android应用程序动态分析利用输入生成器激发程序行为，这种方法并不能达到很好的代码覆盖率。\n做出的主要贡献：\n\n提出了一个动态分析系统UpDroid，它适用于公众使用的非root且未定制系统的设备。\n提出了几种基于设备上公共资源的状态变化来监控不同类型敏感行为的方法。\n提出使用机器学习技术-learning to rank，建立运行的app和检测到的事件之间的关系。此方法解决了未定制系统的设备上识别应用程序的问题。\n\n 使用的方法\nUpDroid通过监视设备上公共资源的变化来检测敏感事件，而不是访问需要root或干预系统的底层事件。为了识别触发检测的app，UpDroid将身份标识转为ranking problem，并采用learning to rank技术来解决该问题。\n Framework of UpDroid\nUpDroid的结构图如下，主要包含两部分：用户设备上的监控器，远端服务器上的分析器\n\n用户设备上的监控器有两个分别负责监控敏感信息（eg access camera）和收集运行时信息（eg cpu usage of running app）即上图Event Monitor 和 Runtime Info Monitor。\n要解决的重要问题是，在不渗透到app或system的情况下，无法识别为了检测而启动的app。为了识别目标app，用真实用户数据通过机器学习（算法：learning to rank）构建了一个判别模型。\n但是我觉得这个判别模型可用性也不会很好，一方面运行app的设备不同产生的信息也不同所以精准度不可能是百分百，另一方面，你必须有目标app的数据才可以，后期需要加入新的app，动态训练模型本身就是一个问题。\n EVENT MONITORING\n\nContent Observer\nContent Observer仅报告Content Provider是否变化，并非描述何种变化。受限于开销UpDroid仅检测四种常见的content provider：SMS, call log, contacts and calendar events\nFile Observer\nUpDroid使用File-Observer API监视外部存储中的文件和目录。Android官方提供了此API，以捕获对单个文件或目录的更改。此API进支持对单个文件和目录的监控所以本文递归的为每个要监控的文件和目录创建监控。注意，File Observer仅报告事件，但不报告更改的内容。\nInterrupt Observer\nAndroid继承了Linux的中断机制，中断表示CPU中断了正在运行的程序以处理外部硬件设备提出的中断请求的情况。\nInterrupt Observer具体做法是每100ms对/proc/interrupts取样一次，比较变化。由于大多数硬件设备都有一条对应的中断请求行，因此我们可以通过监控中断号的变化来推断硬件的运行状态。\n一个重要的问题是：在/proc/interrupts中显示的大多数设备名称都是用硬件型号名称或缩写表示的（且不同手机中使用的映射也不一样），因此很难识别。我们只能手动标识具体硬件和interrupts中的代号映射，本文分析了19种Android 设备。\nNetwork Observer\n借助TUN虚拟网络设备捕捉应用发出的TCP和UDP数据包。使用了MopEye的技术从以下proc文件/proc/net/tcp6|tcp|udp|udp6 中识别目标程序的数据包。\n NITIATOR IDENTIFYING\n尽管MopEye [35]为network event提供了一种智能的应用程序识别解决方案，但它不适用于其他事件，例如基于中断的事件。因此，UpDroid利用机器学习技术（learning to rank）来构建适用于所有事件的应用程序识别模型。该识别模型的结构图如下所示：\n\nApp Status Monitoring\n设备上的运行时信息监视器收集运行时信息（如CPU usage, memory usage）。这些信息将用作判断一个被检测到的事件是否是某个app导致的。UpDroid 使用ps命令获取app的运行时信息。但ps命令得到的是进程信息，而辨识一个事件是那个进程导致的是比较难的，所以我们把来自一个app的进程的运行时信息进行整合，得到了目标的真实运行时信息，关于整合的更多细节参考原文5.3节。\nData Collecting\n数据包含每个app在events发生时的events + runtime information。我们招募Android用户，帮助收集数据并实时标记应用程序。\nData Pre-processing\n为了得到用于最终模型训练的数据我们做了以下预处理工作：\n\n\n获取检测到event前后的app runtime information\n\n\n然后，我们结合了同一app不同进程的运行时信息，下表2是runtime information的结合规则（这就是上述提到的整合进程信息的方法）\nHere is an example of extracting feature f1 for an app. App a has two processes: p1 and p2. Event e is observed at time t. The process sampling provides the nearest process info logs at time t1 and t2, while t1≤t≤ t2. The f1 value of p1 is v1 at t1 and v2 at t2. The f1 value of p2 is u1 at t1 and u2 at t2. Hence, the processed feature f1 of app a for e is:\nf1a=AVG(v2−v1,u2−u1)f_{1a}=AVG(v_2−v_1,u_2−u_1)\nf1a​=AVG(v2​−v1​,u2​−u1​)\n\n\n最后，我们根据用户的回应，识别触发某事件的应用程序，并标记所有正在运行的应用程序。1标识app被selected（与触发事件有关），0标识app没有selected\n\n\n\nModelling and Precision\n上面说过了我们的问题转换为了排名问题，就是说运行中的app中那个最有可能是是invoke event的app，按照可能性排名。使用RankLib（该库包含几个流行的learning to rank算法）进行建模和测试。\n三分之二的数据作为训练资料，剩下的作为测试数据集。\n\ntop 123的意思是，在模型预测结果中排名前几的，也就是模型认为最有可能是invoke event的app，它们之中是否包含观察者（用户）人工指明的invoke event的app。\n在我看来，app识别器模型的结果还是不错的。\n 结果评估\n总的来说，实验结果表明，UpDroid可以成功检测到Android官方文档中标记为危险的26个权限中的15个权限。我们还将Up-Droid与API hooking技术进行了比较，该API hooking在理论上可以捕获所有敏感行为，但需要root和修改系统。结果表明，即使没有root和任何系统修改，UpDroid仍然可以实现API hooking技术的70％。\n下面是在各个方面UpDroid和API Hooking的比较\n权限覆盖率\n为了比较，我们使用了Google官方文档中声明的26个dangerous permission和44个normal permission。如果列表中的任何敏感API使用了特定权限，则我们认为API Hooking技术涵盖了此权限。此外，如果UpDroid可以捕获某权限所保护的一种行为，则我们认为UpDroid涵盖了此权限。两者对危险权限的覆盖情况的比较结果如下图所示：\n\nEvent Details Comparison\nAPI Hooking可以根据API的参数和返回值提供有关每个事件或行为的详细信息，而UpDroid则提供了揭露每个事件详细信息的不同方法。\n\n监控Content Provider，可以提供较general的信息，比API Hooking能提供的信息差一些。\n监控Interrupt，提供事件发生的时间和发生改变的方式（changing pattern）\n监控External Storage，比API Hooking能提供的信息差，前者只能提供谁改变的信息，后者可以提供读写内容。\n监控Network，UpDroid提供的信息比API Hooking更底层\n\nBehavior Outcome Comparison\nUpDroid更加强调应用程序行为的结果，而API Hooking则更多地强调应用程序行为的意图。\n 对我有什么启发\n一种新的在非root设备上动态监测的思路，涉及了Android系统运行原理和Linux运行机制。\n 这项研究的未来方向\n我感觉本文的方法还是由很多限制，并不能成为主流的app检测方式。\n论文链接：https://dlnext.acm.org/doi/abs/10.1145/3212480.3212504\n","categories":["论文阅读"],"tags":["Android","论文","笔记","动态检测"]},{"title":"Linux二进制分析笔记(ELF)","url":"/Linux%E4%BA%8C%E8%BF%9B%E5%88%B6%E5%88%86%E6%9E%90%E7%AC%94%E8%AE%B0(ELF)/","content":" Linux二进制分析笔记(ELF)\n不要读本文，建议只看参考文章，附一张我画的ELF文件思维导图：\n\nELF文件格式思维导图.xmind\n 关于Linux下gcc 编译 C 源文件时，生成的是Shared object file而不是Executable file\n最近在Debian下写C时，发现用readelf命令查看编译后的可执行文件类型时，发现文件类型是DYN (Shared object file)，而不是EXEC (Executable file)。\n-> % readelf -h a.out\nELF Header:\n  Magic:   7f 45 4c 46 02 01 01 00 00 00 00 00 00 00 00 00\n  Class:                             ELF64\n  Data:                              2's complement, little endian\n  Version:                           1 (current)\n  OS/ABI:                            UNIX - System V\n  ABI Version:                       0\n  Type:                              DYN (Shared object file)\n  Machine:                           Advanced Micro Devices X86-64\n  Version:                           0x1\n  Entry point address:               0x580\n  Start of program headers:          64 (bytes into file)\n  Start of section headers:          6656 (bytes into file)\n  Flags:                             0x0\n  Size of this header:               64 (bytes)\n  Size of program headers:           56 (bytes)\n  Number of program headers:         9\n  Size of section headers:           64 (bytes)\n  Number of section headers:         31\n  Section header string table index: 30\n多方查找，发现gcc默认加了--enable-default-pie选项（https://www.v2ex.com/amp/t/481562）\n-> % gcc -v\nUsing built-in specs.\nCOLLECT_GCC=gcc\nCOLLECT_LTO_WRAPPER=/usr/lib/gcc/x86_64-linux-gnu/6/lto-wrapper\nTarget: x86_64-linux-gnu\nConfigured with: ../src/configure -v --with-pkgversion='Debian 6.3.0-18+deb9u1' --with-bugurl=file:///usr/share/doc/gcc-6/README.Bugs --enable-languages=c,ada,c++,java,go,d,fortran,objc,obj-c++ --prefix=/usr --program-suffix=-6 --program-prefix=x86_64-linux-gnu- --enable-shared --enable-linker-build-id --libexecdir=/usr/lib --without-included-gettext --enable-threads=posix --libdir=/usr/lib --enable-nls --with-sysroot=/ --enable-clocale=gnu --enable-libstdcxx-debug --enable-libstdcxx-time=yes --with-default-libstdcxx-abi=new --enable-gnu-unique-object --disable-vtable-verify --enable-libmpx --enable-plugin --enable-default-pie --with-system-zlib --disable-browser-plugin --enable-java-awt=gtk --enable-gtk-cairo --with-java-home=/usr/lib/jvm/java-1.5.0-gcj-6-amd64/jre --enable-java-home --with-jvm-root-dir=/usr/lib/jvm/java-1.5.0-gcj-6-amd64 --with-jvm-jar-dir=/usr/lib/jvm-exports/java-1.5.0-gcj-6-amd64 --with-arch-directory=amd64 --with-ecj-jar=/usr/share/java/eclipse-ecj.jar --with-target-system-zlib --enable-objc-gc=auto --enable-multiarch --with-arch-32=i686 --with-abi=m64 --with-multilib-list=m32,m64,mx32 --enable-multilib --with-tune=generic --enable-checking=release --build=x86_64-linux-gnu --host=x86_64-linux-gnu --target=x86_64-linux-gnu\nThread model: posix\ngcc version 6.3.0 20170516 (Debian 6.3.0-18+deb9u1)\n关于gcc中pie选项可以参考这篇文章（https://blog.csdn.net/ivan240/article/details/5363395）\n\nPosition-Independent-Executable是Binutils,glibc和gcc的一个功能，能用来创建介于共享库和通常可执行代码之间的代码–能像共享库一样可重分配地址的程序，这种程序必须连接到Scrt1.o。标准的可执行程序需要固定的地址，并且只有被装载到这个地址时，程序才能正确执行。PIE能使程序像共享库一样在主存任何位置装载，这需要将程序编译成位置无关，并链接为ELF共享对象。\n\n\n引入PIE的原因是让程序能装载在随机的地址，通常情况下，内核都在固定的地址运行，如果能改用位置无关，那攻击者就很难借助系统中的可执行码实施攻击了。类似缓冲区溢出之类的攻击将无法实施。而且这种安全提升的代价很小\n\n因此，可以加上-no-pie 禁用掉该默认选项：\n-> % readelf -h a.out\nELF Header:\n  Magic:   7f 45 4c 46 02 01 01 00 00 00 00 00 00 00 00 00\n  Class:                             ELF64\n  Data:                              2's complement, little endian\n  Version:                           1 (current)\n  OS/ABI:                            UNIX - System V\n  ABI Version:                       0\n  Type:                              EXEC (Executable file)\n  Machine:                           Advanced Micro Devices X86-64\n  Version:                           0x1\n  Entry point address:               0x400450\n  Start of program headers:          64 (bytes into file)\n  Start of section headers:          6480 (bytes into file)\n  Flags:                             0x0\n  Size of this header:               64 (bytes)\n  Size of program headers:           56 (bytes)\n  Number of program headers:         9\n  Size of section headers:           64 (bytes)\n  Number of section headers:         30\n  Section header string table index: 29\n于是文件类型又变成了EXEC (Executable file)。\n Android APP中的so文件到底属于哪种类型的ELF文件？\n属于：Shared object file（共享目标文件），其ELF头中的e_type值为3\ne_type就是表示文件类型的当它的值：\n\n\n为1时表示的是可重定位类型(Relocatable file)的文件，文件后缀.o\ngcc -c ./elftest.c\n就会生成elftest.o\n\n\n为2时表示可执行类型(Executable file)的文件，一般无文件后缀\ngcc ./elftest.c -o elftest\n\n\n为3时表共享库(Shared object file)文件，文件后缀.so\ngcc -shared -fPIC ./elftest.c -o ./elftest.so\n-shared 生成共享目标文件，-fPIC 生成使用相对地址无关的目标代码\n\n\n 链接视图和执行视图\nELF文件有两种视图形式：链接视图和执行视图\n\n链接视图：\n可以理解为目标文件的内容视图。\n静态链接器（即编译后参与生成最终ELF过程的链接器，如ld ）会以链接视图解析ELF。编译时生成的 .o（目标文件）以及链接后的 .so （共享库）均可通过链接视图解析，链接视图可以没有段表（如目标文件不会有段表）。\n执行视图：\n可以理解为目标文件的内存视图。\n动态链接器（即加载器，如x86架构 linux下的 /lib/ld-linux.so.2或者安卓系统下的 /system/linker均为动态链接器）会以执行视图解析ELF并动态链接，执行视图可以没有节表。\n总结\n链接视图是以节（section）为单位，执行视图是以段（segment）为单位。链接视图就是在链接时用到的视图，而执行视图则是在执行时用到的视图。实际上，在链接阶段，我们可以忽略Program Header Table来处理此文件，在运行阶段可以忽略Section Header Table来处理此程序（所以很多加固手段删除了Section Header Table）。或者说链接视图是给链接编辑器（静态链接器）看的，执行视图是给程序解析器（动态连接器）看的。\n 如何解析一个SO文件的依赖库\nDYNAMIC段中的数据格式如下：\n\n一个动态段数据项大小为8byte。\nd_tag 为0x00000005时，该结构体为DT_STRTAB类型，指明了字符串表的地址，d_ptr指向.dymstr节的字符串表的地址，此表中包含符号名、库名等。\nd_tag 为0x00000001时，该结构体为DT_NEEDED类型，指明了一个所需的库的名字，使用d_val指向上述字符串表的中的偏移地址。也就是说：d_val指向的所需库的名字的字符串地址=字符串表的基地址+d_val\n 我们首先需要弄清楚的几个概念\n在已知ELF文件格式，通读《Understanding ELF》《Linux二进制分析》第一、二章的前提下，我们来回顾一些关键概念，捋清思路，帮助理解后面ELF文件加载过程中的链接和重定位到底是如何运转的。\n 链接编辑器\n链接编辑器所做的工作就是将所有的二进制文件链接起来融合成一个可执行程序，不管这些二进制文件是编译器生成的目标二进制文件还是共享库文件。Android NDK中/ndk/xx.xxx/toolchains/aarch64-linux-android-4.9/prebuilt/windows-x86_64/bin/aarch64-linux-android-ld.exe 就是链接编辑器。\n 动态链接器\n动态链接器是操作系统的一部分，负责按照可执行程序运行时的需要装入与链接共享库。装入是指把共享库在外部存储上的内容复制到内存，链接是指填充跳转表（jump table）与重定位指针。\n 重定位\n重定位就是将符号定义和符号引用进行连接的过程。可重定位文件需要包含描述如何修改节内容的相关信息，从而使得可执行文件和共享目标文件能够保存进程的程序镜像所需的正确信息。重定位条目就是我们上面说的相关信息。如.rel.plt节保存.plt节的动态链接过程中的重定位表。\n 动态链接符号表(.dynsym节)\n动态链接符号表包含所有外部引入的符号,表项结构同.symtab节的ElfN_Sym. 该节保存在text段中,节类型被标记为SHT_DYNSYM.\n 动态链接重定位表\n重定位表项结构为ElfN_Rela/ElfN_Rel，重定位表存储在.rel.*和.rela.*节中。\nr_offset指向需要进行重定位操作的位置。即它所指向的位置需要被动态链接器修改。重定位操作详细描述了如何对存放在r_offset中的代码或数据进行修改。\nr_info指定必须对其进行重定位的符号表索引以及要应用的重定位类型。即说明了重定位操作。\nr_addend指定常量加数，用于计算存储在可重定位字段中的值。这里是显式加数存储在重定位项中，隐式加数存储在重定位目标本身，也就是要被修改的位置里的。\n一个“重定位节(relocation section)”需要引用另外两个节：一个是符号表节，一个是被修改节（某些类型还需要GOT表）。在重定位节中，节头的sh_info和sh_link成员分别指明了引用关系。不同的目标文件中，重定位项的r_offset成员的含义略有不同。\n\n在重定位文件中，r_offset成员含有一个节偏移量。也就是说，重定位节本身描述的是如何修改文件中的另一个节的内容，重定位偏移量(r_offset)指向了另一个节中的一个存储单元地址。\n在可执行文件或共享目标文件中，r_offset含有的是符号定义在进程空间中的虚拟地址。可执行文件和共享目标文件是用于运行程序而不是构建程序的，所以对它们来说更有用的信息是运行期的内存虚拟地址，而不是某个符号定义在文件中的位置。\n\n关于重定位类型的更多介绍推荐参考：《Understanding_ELF.pdf》1.8 重定位\n 全局偏移表(Global Offset Table got)\n全局偏移量表用于把位置独立(Position Independent Code)的地址重定向到绝对地址。\n存在数据段，为什么要强调这点呢，因为GOT表是需要动态修改的，而现代操作系统不允许动态修改代码段。由于我们需要动态修改GOT帮助程序寻址，所以我们把GOT表节安置在数据段。\n如果一个程序要求直接访问符号的绝对地址，那么这个符号在全局偏移表中就必须有一个对应的项。动态链接器会在程序开始执行之前，处理好所有全局偏移表的重定位工作，这些和GOT表相关的重定位项类型为R_xxxx_GLOB_DAT。最后在程序执行的时候，可以保证所有这些符号都有正确的绝对地址。\n此外，全局偏移表中的前三项是保留的，第0项持有动态结构（.dynmic节）的地址，由符号_DYNAMIC引用。这样，其它程序，比如动态链接器就可以直接找到其动态结构，而不用借助重定位项。这对于动态链接器程序来说尤为重要，因为它必须在不依赖于其它程序重定位其内存镜像的情况下初始化自己。\n\ngot[0]：address of .dynamic section 也就是本ELF动态段(.dynamic段）的装载地址\ngot[1]：address of link_map object( 编译时填充0）也就是本ELF的link_map数据结构描述符地址，作用：link_map结构，结合.rel.plt段的偏移量，才能真正找到该elf的.rel.plt\ngot[2]：address of _dl_runtime_resolve function (编译时填充为0) 也就是_dl_runtime_resolve函数的地址，来得到真正的函数地址，回写到对应的got表位置中。\n\n全局偏移量表的格式和解析方法是依处理器而不同的，就Intel架构而言，符号_GLOBAL_OFFSET_TABLE_会被用来访问此表。\nextern  Elf32_Addr  _GLOBAL_OFFSET_TABLE_[];\nextern  Elf64_Addr  _GLOBAL_OFFSET_TABLE_[];\n符号_GLOBAL_OFFSET_TABLE_可能位于.got节中部，所以它也接受负的数组索引值。对于 32 位代码，符号类型是 Elf32_Addr 数组；对于 64 位代码，符号类型是 Elf64_Addr 数组。所以上述的第0项往往都在.got节的中间某个位置上。\n推荐参考：https://docs.oracle.com/cd/E26926_01/html/E25910/chapter6-74186.html\n 过程链接表(Procedure Linkage Table plt)\n过程链接表(Procedure Linkage Table)的作用是把位置独立的函数调用重定向到绝对地址。\n每个动态链接的可执行文件和共享目标文件都有一个 PLT，PLT 表的每一项都是一小段代码，对应于本运行模块要引用的一个全局函数。程序对某个函数的访问都被调整为对 PLT 入口的访问。\n过程链接表中的每一项都占用 3 个字（12 字节），并且表的最后一项后跟 nop 指令。\n链接编辑器不能解析函数在不同目标文件之间的跳转，那么，它就把对其它目标文件中函数的调用重定向到一个过程链接表项中去。\n在Intel架构中，过程链接表位于共享代码段中，但它们使用全局偏移表中的私有地址。动态链接器决定目标的绝对地址，并且会相应地修改全局偏移表的内存镜像。这样，动态连接器就可以在不牺牲位置无关性和代码的可共享性条件下，实现到绝对地址的重定位。可执行文件和共享目标文件有各自的过程链接表。\n重定位表与过程链接表关联。.dynmic节中的动态数组ElfN_Dyn中的 DT_JMP_REL 类型的项指定了第一个重定位项的位置。对于非保留的过程链接表的每一项，重定位表中都包含相同顺序的对应项。所有这些项的重定位类型均为 R_SPARC_JMP_SLOT。重定位项的偏移r_offset可指定关联的过程链接表项的第一个字节的地址。符号表索引会指向相应的符号。\n.PLT0:\n    pushl   got_plus_4\n    jmp     *got_plus_8\n    nop;    nop\n    nop;    nop\n.PLT1:\n    jmp     *name1_in_GOT\n    pushl   $offset\n    jmp     .PLT0@PC\n.PLT2:\n    jmp     *name2_in_GOT\n    pushl   $offset\n    jmp     .PLT0@PC\n.PLT0:\n    pushl   4(%ebx)\n    jmp     *8(%ebx)\n    nop;    nop\n    nop;    nop\n.PLT1:\n    jmp     *name1@GOT(%ebx)\n    pushl   $offset\n    jmp     .PLT0@PC\n.PLT2:\n    jmp     *name2@GOT(%ebx)\n    pushl   $offset\n    jmp     .PLT0@PC\n比较上面两图可知，在绝对地址代码和位置无关代码中，过程链接表中的指令使用的操作数寻址方式不同。但是它们给动态链接器的接口都是相同的。\n推荐参考：https://docs.oracle.com/cd/E26926_01/html/E25910/chapter6-1235.html\n ELF文件如何链接和重定位？\n 首先明确什么是链接和重定位\n链接：（个人理解）链接就是把一个目标文件中符号引用连接到其他目标文件中的符号定义。\n从编译/链接和运行的角度看，库分为动态链接和静态链接。相应的两种不同的ELF格式映像：\n1）静态链接，在装入/启动其运行时无需装入函数库映像、也无需进行动态连接。\n2）动态连接，需要在装入/启动其运行时同时装入函数库映像并进行动态链接。\nLinux内核既支持静态链接的ELF映像，也支持动态链接的ELF映像，GNU规定：\n1）把ELF映像的装入/启动在Linux内核中实现；\n2）把动态链接的实现放在用户空间（glibc），并为此提供一个称为”解释器”(ld-linux.so.2)的工具软件，而解释器的装入/启动也由内核负责。\n重定位：将符号定义和符号引用进行连接的过程。\n其实重定位可以算作连接的一个步骤，不过这里我们主要讲解这两部分，所以我们拆开做笔记。\n下面，我们以动态链接为主讲解一个ELF可执行程序的链接和重定位过程。\n 动态链接\n动态链接的步骤分为3步：动态链接器自举，装载共享对象，重定位和初始化。\nPT_INERP段说明了需要的动态链接器比如/lib64/ld-linux-x86-64.so.2\n动态链接器与普通共享对象一样被映射到了进程的地址空间，在系统开始运行程序之前，首先会把控制权交给动态链接器，由它完成所有的动态链接工作以后再把控制权交给程序，然后开始执行。\n动态链接器自举\n动态链接器本身也是一个共享对象，但有一些特殊性。首先，动态链接器本身不可以依赖于其他任何共享对象；其次动态链接器本身所需要的全局和静态变量的重定位工作由它本身完成。对于第一个条件，可以人为的控制在编写动态链接器时保证不使用任何系统库、运行库；对于第二个条件，动态链接器必须在启动时有一段非常精巧的代码可以完成这项艰巨的工作而同时又不能用到全局和静态变量。这种具有一定限制条件的启动代码往往被称为自举（Bootstrap）。\n动态链接器入口地址即是自举代码的入口，当操作系统将进程控制权交给动态链接器时，动态链接器的自举代码即开始执行。自举代码首先会找到它自己的GOT。而GOT的第一个入口保存的即是“.dynamic”段的偏移地址，由此找到了动态连接器本身的“.dynamic”段。通过“.dynamic”中的信息，自举代码便可以获得动态链接器本身的重定位表和符号表等，从而得到动态链接器本身的重定位入口，先将它们全部重定位。从这一步开始，动态链接器代码中才可以开始使用自己的全局变量和静态变量。实际上在动态链接器的自举代码中，除了不可以使用全局变量和静态变量之外，甚至不能调用函数，即动态链接器本身的函数也不能调用。因为使用PIC模式编译的共享对象，对于模块内部的函数调用也是采用跟模块外部函数调用一样的方式，即使用GOT/PLT的方式，所以在GOT/PLT没有被重定位之前，自举代码不可以使用任何全局变量，也不可以调用函数。\n装载共享对象\n完成基本自举以后，动态链接器将可执行文件和链接器本身的符号表都合并到一个符号表当中，称它为全局符号表（Global Symbol Table）。然后链接器开始寻找可执行文件所依赖的共享对象。在“.dynamic”段中，有一种类型的入口是DT_NEEDED，它所指出的是该可执行文件（或共享对象）所依赖的共享对象。由此，链接器可以列出可执行文件所需要的所有共享对象，并将这些共享对象的名字放入到一个装载集合中。然后链接器开始从集合里取一个所需要的共享对象的名字，找到相应的文件后打开该文件，读取相应的ELF文件头和“.dynamic”段，然后将它相应的代码段和数据段映射到进程空间中。如果这个ELF共享对象还依赖于其他共享对象，那么将所依赖的共享对象的名字放到装载集合中。如此循环直到所有依赖的共享对象都被装载进来为止，当然链接器可以有不同的装载顺序，如果把依赖关系看作一个图的话，那么这个装载过程就是一个图的遍历过程，链接器可能会使用深度优先或者广度优先或者其他的顺序来遍历整个图，这取决于链接器，比较常见的算法一般都是广度优先的。\n当一个新的共享对象被装载进来的时候，它的符号表会被合并到全局符号表中，所以当所有的共享对象都被装载进来的时候，全局符号表里面将包含进程中所有的动态链接所需要的符号。\n符号的优先级：\n一个共享对象里面的全局符号被另一个共享对象的同名全局符号覆盖的现象又被称为共享对象全局符号介入（Global Symbol Interpose）。关于全局符号介入这个问题，实际上Linux下的动态链接器是这样处理的：它定义了一个规则，那就是当一个符号需要被加入全局符号表时，如果相同的符号名已经存在，则后加入的符号被忽略。所以当程序使用大量共享对象时应该非常小心符号的重名问题。\n下面讲解最后一部分，重定位和初始化。\n 重定位\n重定位是解析ELF程序的最后一步，完成重定位一个ELF程序就可以执行了。\n当上面的步骤完成之后，链接器开始重新遍历可执行文件和每个共享对象的重定位表，将它们的GOT/PLT中的每个需要重定位的位置进行修正。因为此时动态链接器已经拥有了进程的全局符号表，所以这个修正过程也显得比较容易。\nELF使用PLT（Procedure Linkage Table）来实现延迟绑定，它使用了一些很精巧的指令序列来完成。在Glibc中，动态链接器完成绑定工作的函数叫_dl_runtime_resolve()，它必须知道绑定发生在哪个模块中的哪个函数，因此假设其函数原型为_dl_runtime_resolve(module, function)。 当调用某个外部模块的函数时，并不直接通过GOT跳转，而是通过一个叫作PLT项的结构来进行跳转，每个外部函数在PLT中都有一个相应的项，比如bar()函数在PLT中的项的地址称为bar@plt。来看看bar@plt的实现：\nbar@plt:\njmp *(bar@GOT)\npush n\npush moduleID\njump _dl_runtime_resolve\nbar@plt的第一条指令是通过GOT间接跳转的指令。bar@GOT表示GOT中保存bar()这个函数相应的项。如果链接器在初始化阶段已将bar()的地址填入该项，就跳转到bar()。但为了实现延迟绑定，链接器在初始化阶段并没有将bar()的地址填入到该项，而是将上面代码中第二条指令push n的地址填入到bar@GOT中，这个步骤不需要查找任何符号，所以代价很低。很明显，第一条指令的效果是跳转到第二条指令，相当于没有进行任何操作。第二条指令将一个数字n压入堆栈中，这个数字是bar这个符号引用在重定位表.rel.plt中的下标。接着又是一条push指令将模块的ID压入到堆栈，然后跳转到_dl_runtime_resolve。这实际上就是在实现_dl_runtime_resolve函数调用，它在进行一系列符号解析和重定位工作以后将bar()的真正地址填入到bar@GOT中。\n之后当我们再次调用bar@plt时，第一条jmp指令就能够跳转到真正的bar()函数中，bar()函数返回的时候会根据堆栈里面保存的EIP直接返回到调用者，而不会再继续执行bar@plt中第二条指令开始的那段代码，那段代码只会在符号未被解析时执行一次。\n上面描述的是PLT的基本原理，PLT真正的实现要比它的结构稍微复杂一些。ELF将GOT拆分成了两个表叫做“.got”和“.got.plt”。其中“.got”用来保存全局变量引用的地址，“.got.plt”用来保存函数引用的地址，即把外部函数的引用分离到“.got.plt”中。另外“.got.plt”的前三项是有特殊意义的：\n\n第一项保存的是“.dynamic”段的地址，这个段描述了本模块动态链接相关的信息\n第二项保存的是本模块的ID。\n第三项保存的是_dl_runtime_resolve()的地址。\n\n其中第二项和第三项由动态链接器在装载共享模块的时候负责将它们初始化。“.got.plt”的其余项分别对应每个外部函数的引用。如下图所示：\n\nPLT的结构也稍有不同，为了减少代码的重复，ELF把上面例子中的最后两条指令放到PLT中的第一项。并且规定每一项的长度是16个字节，刚好用来存放3条指令。实际的PLT基本结构代码如下：\nPLT0:\npush *(GOT + 4)\njump *(GOT + 8)\n...\nbar@plt:\njmp *(bar@GOT)\npush n\njump PLT0\nPLT在ELF文件中以独立的段存放，段名通常叫做“.plt”，因为它本身是一些地址无关的代码，所以可以跟代码段等一起合并成同一个可读可执行的“Segment”被装载入内存。\n\n共享对象中存在.plt,.rel.plt,.got,.got.plt 4个相关的段。\n\n重定位完成之后，如果某个共享对象有“.init”段，那么动态链接器会执行“.init”段中的代码，用以实现共享对象特有的初始化过程，比如最常见的，共享对象中的C的全局/静态对象的构造就需要通过“.init”来初始化。相应地，共享对象中还可能有“.finit”段，当进程退出时会执行“.finit”段中的代码，可以用来实现类似C全局对象析构之类的操作。\n如果进程的可执行文件也有“.init”段，那么动态链接器不会执行它，因为可执行文件中的“.init”段和“.finit”段由程序初始化部分代码负责执行。\n当完成了重定位和初始化之后，所有的准备工作就宣告完成了，所需要的共享对象也都已经装载并且链接完成了，这时候动态链接器就如释重负，将进程的控制权转交给程序的入口并且开始执行。\n 总结\n我发现现阶段我还是很难很好的总结ELF文件的解析过程，还需要一段时间的实践和学习，先挖个坑2021年之内一定要把这篇文章捋顺到通俗易懂。\n参考：\nelf(5) - Linux man page https://linux.die.net/man/5/elf\n《Understanding_ELF.pdf》 https://paper.seebug.org/papers/Archive/refs/elf/Understanding_ELF.pdf\nelf.h https://github.com/torvalds/linux/blob/master/include/uapi/linux/elf.h\n深入理解动态链接 https://www.jianshu.com/p/cdb5cfcb5056\nELF文件加载过程 https://zhuanlan.zhihu.com/p/287863861\nELF文件的加载过程(load_elf_binary函数详解)–Linux进程的管理与调度（十三） https://cloud.tencent.com/developer/article/1351964\n《Linux二进制分析》\nOracle Docs《链接程序和库指南》第 13 章程序装入和动态链接 https://docs.oracle.com/cd/E26926_01/html/E25910/glchi.html\nELF可执行文件在虚拟地址空间的分布：https://github.com/chenpengcong/blog/issues/14\n从PLT和GOT的角度理解程序的链接和重定位：https://zhuanlan.zhihu.com/p/130271689\n\n\n[ ]  链接、装载与库 — 动态链接：https://markrepo.github.io/kernel/2018/08/19/dynamic-link/\n\n\n[ ] 链接、装载与库 — 可执行文件的装载：https://markrepo.github.io/kernel/2018/08/17/load-and-process/\n\n\n","categories":["笔记"],"tags":["二进制","Linux","ELF"]},{"title":"【实战】某视频app的crack尝试","url":"/%E3%80%90%E5%AE%9E%E6%88%98%E3%80%91%E6%9F%90%E8%A7%86%E9%A2%91app%E7%9A%84crack%E5%B0%9D%E8%AF%95/","content":" 【实战】某视频app的crack尝试\nemmmm，是一个视频app，玩游戏的时候全局语音广告听到的，练练手。\napp版本1.3.0\nRedmi K30 5G(Android 10)\nHttpCanary\njadx 1.1.0\n 目标一：破解登录加密方式\n应该是检测了代理，使用代理则显示无网络链接。\n我使用HttpCanary抓包，拿到登录明文：\n主机Host:api.i69r7.com\nPOST &#x2F;uc&#x2F;app&#x2F;v2&#x2F;login HTTP&#x2F;1.1\nr: 4026847275\nclientType: android\nt: 1611839395333\nchannel: QD500\nsign: 4cf2e5b8a57a60a07a9f99b78ec8d6de0cb122a6\nX-JSL-API-AUTH: md5|1611841195|EsPrUIjgpp2Y|eccaa018eb2f0d3a3edd6b81d530b28f\ndeviceId: c5f0f2cc3b6f3a419b85452f32e15efc\ndeviceName: Redmi K30 5G\nsystemVersion: 10\nversion: 1.3.0\nX-YD-Req-Token: 1611841195|FqOrf01aRQ8T|92fcef7544cef6f57edd2a1a644b4c2c\nversionCode: 1300\nContent-Type: application&#x2F;json; charset&#x3D;utf-8\nContent-Length: 80\nHost: api.i69r7.com\nConnection: Keep-Alive\nAccept-Encoding: gzip\nUser-Agent: okhttp&#x2F;4.8.0\n\n&#123;&quot;captcha&quot;:&quot;&quot;,&quot;password&quot;:&quot;ax9aKqMKRCKe6bzanQjNWA\\u003d\\u003d&quot;,&quot;username&quot;:&quot;abaaba&quot;&#125;\n通过jadx反编译dex部分看到登录时的POST Data设置逻辑：\n\n用frida hook这个函数看看它传入了什么值\n\n显然，第一个值是密码，第二个是MIYOU_APP_LOGIN_KEY字符串\nsecretEncrypt 是native 方法，定义在digest-lib库中，看起来像是自己写的库\nemmmm，函数也不是动态注册的，所以一下就能找到函数：\n\n一波分析发现，应该是简单的加密：\nDES ECB模式 填充方式Pkcs7 无偏移量 密钥：MIYOU_APP_LOGIN_KEY\nonline tool：https://oktools.net/des\n另外，实际上密钥用MIYOU_AP就可以了，DES的密钥长度只有64位，而且，虽然表面上是64位的，实际只有其中的56位被实际用于算法，其余8位可以被用于奇偶校验，并在算法中被丢弃。\n但是这里很奇怪哦，native中的secretEncrypt方法居然调用了com/miu/lib_encipher/xcrypt/Encrypts中名为encryptDES，签名为(Ljava/lang/String;Ljava/lang/String;)Ljava/lang/String;的java 方法。\n我们模拟执行验证一下，看看它到底搞什么名堂\n AndroidNativeEmu 模拟执行\nAndroidNativeEmu项目介绍参考README，不赘述:https://github.com/AeonLucid/AndroidNativeEmu/\n确实符合上面的分析。这波啊是java-&gt;native-&gt;java 我用我自己加密，加密逻辑还是在java中佛了。encryptDES方法的两个字符串参数就是明文和密钥。证据如下：\n\n附代码：\nimport logging\nimport posixpath\nimport sys\n\nfrom unicorn import UcError, UC_HOOK_MEM_UNMAPPED\nfrom unicorn.arm_const import *\n\nfrom androidemu.emulator import Emulator\nfrom androidemu.java.java_class_def import JavaClassDef\nfrom androidemu.java.java_method_def import java_method_def\n\nfrom samples import debug_utils\n\n\n# Create java class.\nclass DigestUtils(metaclass=JavaClassDef, jvm_name='com/miu/lib_encipher/jni/DigestUtils'):\n\n    def __init__(self):\n        pass\n\n    @java_method_def(name='secretEncrypt', signature='(Ljava/lang/String;Ljava/lang/String;)Ljava/lang/String;', native=True)\n    def secretEncrypt(self, mu):\n        pass\n\n    def test(self):\n        pass\n\n\nclass Encrypts(metaclass=JavaClassDef, jvm_name='com/miu/lib_encipher/xcrypt/Encrypts'):\n\n    def __init__(self):\n        pass\n\n    @java_method_def(name='encryptDES', signature='(Ljava/lang/String;Ljava/lang/String;)Ljava/lang/String;', native=False,  args_list=[\"jstring\",\"jstring\"])\n    def encryptDES(self, *args, **kwargs):\n        print(\"[+]encryptDES:\",args[0].value, args[1].value)\n        return \"ch3nyeeeeeeeeeeeeee\"\n\n    def test(self):\n        pass\n\n# Configure logging\nlogging.basicConfig(\n    stream=sys.stdout,\n    level=logging.DEBUG,\n    format=\"%(asctime)s %(levelname)7s %(name)34s | %(message)s\"\n)\n\nlogger = logging.getLogger(__name__)\n\n# Initialize emulator\nemulator = Emulator(\n    vfp_inst_set=True,\n    vfs_root=posixpath.join(posixpath.dirname(__file__), \"vfs\")\n)\n\n# emulator.mu.hook_add(UC_HOOK_CODE, debug_utils.hook_code)\nemulator.mu.hook_add(UC_HOOK_MEM_UNMAPPED, debug_utils.hook_unmapped)\n\n# Register Java class.\nemulator.java_classloader.add_class(DigestUtils)\nemulator.java_classloader.add_class(Encrypts)\n\n# Load all libraries.\nemulator.load_library(\"example_binaries/libm.so\")\nemulator.load_library(\"example_binaries/libc.so\")\nemulator.load_library(\"example_binaries/libstdc++.so\")\nlib_module = emulator.load_library(\"example_binaries/libdigest-lib.so\")\n\n# Show loaded modules.\nlogger.info(\"Loaded modules:\")\n\nfor module in emulator.modules:\n    logger.info(\"=> 0x%08x - %s\" % (module.base, module.filename))\n\n\ntry:\n    # Run JNI_OnLoad.\n    # JNI_OnLoad will call 'RegisterNatives'.\n    emulator.call_symbol(lib_module, 'JNI_OnLoad', emulator.java_vm.address_ptr, 0x00)\n\n    # Do native stuff.\n    digest_utils = DigestUtils()\n    logger.info(\"Response from JNI call: %s\" % digest_utils.secretEncrypt(emulator,\"12345678\",\"MIYOU_APP_LOGIN_KEY\"))\n\n    # Dump natives found.\n    logger.info(\"Exited EMU.\")\n    logger.info(\"Native methods registered to DigestUtils:\")\n\n    for method in DigestUtils.jvm_methods.values():\n        if method.native:\n            logger.info(\"- [0x%08x] %s - %s\" % (method.native_addr, method.name, method.signature))\nexcept UcError as e:\n    print(\"Exit at %x\" % emulator.mu.reg_read(UC_ARM_REG_PC))\n    raise\n\n 尝试破解VIP高清视频\n这个app大概只提供两种清晰度，普通用户480P，VIP用户720P。\n尝试从播放页com.erkhioqq.irivjiamm.avod.play.videodetail.MovieDetailActivity 入手看看能不能hook住app查看vip状态的的函数，直接返回当前用户是vip，但是实际上没找到。\n抓取流量看到视频使用m3u8格式传输，其实抓到了链接：\nhttps://video.gdyjdt.com/files/202012/20201207/ZWAV202012020528281/400kb/hls/index.m3u8\n注意观察400kb是清晰度，对应480P。（其实一开始还没往这上面想，后来才发现，我就尝试了下：\nimport requests\n\nfor i in range(400,10000,100):\n    url = \"https://video.gdyjdt.com/files/202012/20201207/ZWAV202012020528281/&#123;&#125;kb/hls/index.m3u8\".format(str(i))\n    response = requests.head(url)\n    if response.status_code != 404:\n        print(\"[+]find:\",url)\n    else:\n        print(\"[-]try &#123;&#125; kb fail\".format(str(i)))\n1000kb大概就对应VIP那个清晰度，但是也很垃圾，还是记下番号到正规网站找片把，1080p它不香吗。\nAPP中大量使用了StringFog加密字符串，写了个解密脚本方便分析：\nDOWNLOADS_FILE_NAME = \"irivjiamm\"\n\nimport base64\nimport sys\ndef xor_func(str1,key):\n    lkey=len(key)\n    secret=[]\n    num=0\n    for each in str1:\n        if num>=lkey:\n            num=num%lkey\n\n        secret.append( chr( each^ord(key[num]) ) )\n        num+=1\n    return \"\".join(secret)\n\n\n\ndef decrypt(s):\n    return xor_func(base64.b64decode(s), DOWNLOADS_FILE_NAME)\n\n\ndef encrypt(s):\n    str = xor_func(bytes(s,encoding='utf-8'), DOWNLOADS_FILE_NAME).encode('utf-8')\n    return base64.b64encode(str)\n\n\n\nif __name__ == '__main__':\n    tips = \"need 2 parameter: d/e cipher/plain\"\n    args = []\n    if __file__ in sys.argv[0]:\n        args = sys.argv[1:]\n    else:\n        args = sys.argv\n    if len(args) != 2:\n        print(tips)\n        exit(-1)\n\n    if args[0] == 'd':\n        print(decrypt(args[1]))\n    elif args[0] == 'e':\n        print(encrypt(args[1]))\n后续还可以看几个函数，看看能不能再整点活\nMoiveDetailActivity.showMovieCache\nMoiveDetailActivity.download\nMoiveDetailActivity.handleDownload\n附件：\nbase.apk\n备用蓝奏云：https://wwe.lanzous.com/iWubulikvuf\n","categories":["实战"],"tags":["Android","crack","实战"]},{"title":"Note《Automatic Hot Patch Generation for Android Kernels》","url":"/Note-Automatic-Hot-Patch-Generation-for-Android-Kernels/","content":" Automatic Hot Patch Generation for Android Kernels\n 主要解决什么问题\nAndroid 版本升级覆盖缓慢，往往发布半年的最新Android系统设备覆盖率只有个位数，如此便使得旧版本的Android系统的内核漏洞无法修复，而OEM厂商又无意修补这些漏洞，因为内核的改动需要繁琐的测试保证更改不会影像现有功能和性能。因此，多数设备上搭载的旧版Android系统会保持漏洞存在很长一段时间。\n为了解决上述问题，已经有很多方案被制定出来，在所有可能的解决方案中，热补丁技术提供了一种在不中断程序正常功能的情况下修复漏洞的便捷方法。由于它可以在不重新启动设备的情况下修补内核漏洞，因此极大地改善了用户体验。\n根据正式发布的补丁程序编写热补丁程序是较难的工作。在二进制文件中，很难准确找到要修改的脆弱点。为了在适当的位置打上热补丁，安全专家需要了解官方补丁的语义并编写相应的热补丁。但是，这是一个耗时并且容易出错的过程。受限于快速的开发周期和有限的安全预算，人工开发热补丁往往是商业公司不可接受的。因此，需要开发一种自动解决方案以将正式补丁正确转换为热补丁。\n为此，本文提出了一种自动执行热补丁生成过程的解决方案。为了对漏洞修补程序有完整的了解，我们首先分析了2012年至2016年的大多数Android CVE，并根据其修补行为对其进行了分类。根据这些分析，我们开发了Vulmet，它可以通过程序分析提取官方补丁的语义来自动生成热补丁。Vulmet将在函数中找到合适的位置，构建并应用语义等效的热补丁来修复漏洞。为了测试Vulmet的有效性，我们为实际的Android CVE生成了热补丁。热补丁程序可以以很少的系统开销来修补漏洞。\n概述结论：\n为此，我们首先研究了从2012年到2016年从373个Android内核CVE中自动生成补丁的可行性（结论为可行）。然后，我们开发了一种自动热补丁生成工具，名为Vulmet，该工具通过从官方补丁中学习来生成语义保留热补丁。 Vulmetis的关键思想是使用最弱的前提推理（weakest precondition reasoning）将官方补丁所做的更改转换为hot patch约束。实验表明，Vulmet可以为55个现实世界的Android kernel CVE生成正确的热补丁。热补丁不影响内核的健壮性，并且性能开销较低。\nPS：\n注意1：本文实现的热补丁生成工具Vulmet，需要看官方补丁，从中学习语义，根据Vulmet被定义的约束条件和操作范围，生成可行的热补丁。（在我看来本文想法稍微有点鸡肋，既然发布了官方补丁，OEM厂商只要跟进就好了呀，只能说热补丁具有不需要重启的优势，这种优势在服务器系统补丁上会更明显吧😐）\n注意2：Vulmet并不能百分百根据官方补丁生成热补丁，只能实现满足下述Requirements和Operation Scopes的情况下，官方补丁自动生成热补丁。\n 使用的方法\n本节叙述Vulmet的实现过程：解释热补丁生成，android漏洞类型分析并定义操作范围，自动热补丁生成框架\n Section 2 Automatic Hot Patch Generation\nProblem Definition（定义热补丁生成）：\n给定一个易受攻击的function F及在F的L位置上打上官方的patch P ，我们希望在二进制代码中找到function F的一个合适位置L’来插入自动生成的热补丁P’，其语义与P相同。\nRequirements：\n\nRequirements 1：生成的热补丁应该保留和官方补丁一致的语义，确保其正确性\nRequirements 2：生成的热修补程序不应破坏系统，确保系统运行的健壮性\nRequirements 3：生成的热补丁应该产生较低的开销，确保系统效率\n\nOperation Scopes：\nOperation Rule 1：补丁只能放在函数的开始或结尾处，或函数调用的开始或结尾处\nOperation Rule 2：补丁可以读取内存的有效内容，但禁止修改\nOperation Rule 3：该补丁只能在一个函数范围内进行很小的更改，以尝试修复漏洞\n原文2.4节介绍了一个用本文定义的规则和操作范围修补CVE-2015-8940整数溢出漏洞的过程，大意就是官方补丁检查了几个变量的值，本文做法是在函数开始处检查上述值，由于这些值并不是一开始就知道的，所以通过一个algorithm（in Section4）自动确定这些变量之间的关系，以此生成了和官方语义相同的检查约束添加到函数开始，热补丁结束。详细过程请参考原文。\n Section 3 Patch Type Analysis\n在研究中，我们提供了漏洞补丁分类和分布情况以及从观察中的发现。本节，我们将讨论Vulmet能够支持的漏洞补丁的类型。\n我们的工作聚焦于热补丁生成，所以补丁分类应该反应补丁功能，而不是反应补丁修复漏洞的类型。目前来说针对后者的分类要原多于前者，我们遵从[36]的分类方案思想，形成我们的分类方案，如下图所示：\n\n3.2 Observation\n在漏洞补丁的研究过程中，我们获得了四个有趣的发现。\nObservation 1: 与程序更新相比，漏洞补丁的更改通常很小\nObservation 2: 大型漏洞补丁通常由几个小型独立补丁组成\nObservation 3: 补丁类型和漏洞类型并不是一定相关的，或者说是比较无关的\nObservation 4: 一些补丁同时包括安全补丁和非安全补丁\n3.3 Vulmet Work Scope\n\nSanity Testing 健壮性检查上面的栗子说过了，符合Requirements和Operation Scopes，Vulmet完全支持。\n对于Function Call类型，Vulmet可以进入被调用函数并该函数。如果此补丁的更改不涉及内存写入操作，则Vulmet可以支持该修补程序。\n对于Change of Variables和Change of Data Types都涉及内存写操作，因此不支持。\nVulmet不支持Redesign类型补丁，因为它极大地改变了原始函数的语义并违反了Operation Scopes 3。\n Section 4 Methodology\n\nPatch Filtering\nVulmet将通过对比存在漏洞的代码和已修补的代码之间的不同来提取官方补丁。然后，针对补丁中的每个语句，将其分为正常操作和禁止操作。禁止的操作包括变量或指针值的分配以及对内存修改函数的调用。如果正式补丁不包含禁止的操作，Vulmet会将其选择为生成热补丁的候选对象。否则，补丁将被过滤掉。\nInsertion Location Optimization\n根据第2.3节的规则1，函数头或尾是patch 可以hook的位置。因此，Vulmet只能hook 漏洞函数或者漏洞函数中调用的函数。这些可以hook的位置都是为可能的补丁插入位置。Vulmetis需要寻找最佳的插入位置。具体的寻找算法看原文吧，不赘述了。\nWeakest Precondition Reasoning\n在选择补丁插入点之后，下一步是通过计算和官方补丁的语义等效性的热补丁。在Vulmet中，此过程被重构为最弱的前提推理任务。在编程中，前提条件是在调用函数之前应为true的语句。而后置条件是一个陈述，如果函数完成并且所有先决条件都满足，则该陈述为真。表6展示了语义计算与最弱前提条件推理之间的关系。给定一个正式补丁，其语义可以转换为一个或多个后置条件。漏洞函数中的陈述将定义解决最弱前提的transformer。获得热补丁约束的过程等同于计算最弱的前提条件。产生的最弱前提条件是官方补丁的语义等价的热补丁。（这就是核心算法，看不懂的话就算了，知道是使用这个算法就可）\nBinary Hot Patch Generation\n最后一步是根据precondition constraints生成热补丁。Vulmet使用空函数作为模板，并将函数的输入参数的数量和类型设置为与原始目标函数相同。然后，将所有约束插入其中，并将功能编译为可以热修补到内核的二进制可执行文件。\n关于热补丁生成的细节，包括变量地址，指令集支持，可执行文件生成等细节问题就不多赘述了。\n 结果评估\n我们对Vulmet生成的热补丁的正确性，鲁棒性和效率进行了评估。\n正确性可以量化补丁修复漏洞的能力，健壮性可以量化补丁维护程序稳定性的能力，效率可以量化补丁引入的开销。\n我们设计了实验来测试这三个方面的补丁有效性。在实验中，所有补丁均在具有Android kernel版本7.1.1 r31 bullhead 构建的Android Open SourceProject（AOSP）平台Google Nexus 5X上进行了测试。\n5.1 Correctness Evaluation\n在本节中，我们评估生成的热补丁的正确性。实验包括三个部分。首先，我们使用真实的CVE漏洞测试补丁。其次，对于无法利用的漏洞，我们手动验证补丁的正确性。第三，我们手动编写热补丁，并将生成的热补丁与它们进行比较，以检查生成的补丁是否以与人类专家相同的方式解决了漏洞。\n\n手工收集了四个CVE攻击方法，用来攻击被Vulmet打过补丁的系统。结果表明，所有补丁均已成功阻止了来自攻击的攻击。对于CVE-2018-17182，该热补丁可以成功阻止漏洞利用，但无法阻止系统崩溃。这是因为该修补程序只能部分修复漏洞。\n\n人工验证59个自动生成的热补丁有55个是正常工作的，沃觉得算是相当不错了。\n\n与人类安全专家相比55个自动生成的热补丁只有一个语义不同，具体哪里不同参考原文。\n5.2 Robustness Evaluation\n为了构建测试环境，我们选择要使用Linux内核版本3.10构建的Android bullhead，然后回滚以生成具有许多未修补漏洞的内核。在此特定内核中，Vulmet管理21个漏洞补丁，可以将其转换为热补丁。\n然后我们将这些补丁应用到内核并运行安兔兔Benchmark和CF-bench，监控系统异常，比如崩溃和挂起。表11总结了该实验的结果。出于演示目的，我们选择5个CVE作为示例，并列出所有21个补丁的最终结果\n\n结果表明，所有热补丁都不会使程序崩溃或挂起。为了进一步检查现实情况中补丁的健壮性，我们从Google App Store中选择并安装了前100个Android应用程序。我们使用脚本来打开，加载和关闭已修补系统上的应用程序，并监视异常行为。结果是可以正确执行所有应用程序，这表明这些修补程序在实际情况下保持了良好的鲁棒性。总而言之，生成的补丁不会破坏补丁程序的正常功能\n5.3 Efficiency Evaluation\n我们在Google Nexus 5X设备上使用安兔兔基准测试修补前后的系统性能。我们将实验设置控制为相同，以便一次测试一个热补丁。每个实验重复10次，取平均分以避免噪声引起的变化。表12列出了具有5个单独CVE补丁的内核的性能以及应用所有21个补丁的总体性能。\n\n所有结果都在合理范围内，与原始内核结果相比，该范围略高或略低。因此，补丁使系统的开销较低。\n 缺点和不足（选）\n\nVulmet的前提之一是自动生成的热补丁无法修改内存和原始程序的内容。这就使得诸如变量类型修改，变量数值修改，函数过程重构等补丁类型无法自动生成。\nVulmet依靠官方补丁语义的精确总结来生成正确的热补丁。在实验中，由于Vulmet无法完全提取语义，因此某些生成的热补丁不完整。\n有些补丁过于复杂而无法分析。很难找到大补丁的精确语义。当前Vulmet只能处理，仅对一个函数过程进行修补的补丁。\n\n 对我有什么启发\nHot Patch 技术是我没有接触过的，主要借助此篇文章了解内核补丁技术、热补丁技术，希望能更加深入的了解内核级补丁修补和应用原理。\n 这项研究的未来方向\n可能会和人工智能结合的比较紧密吧，就如缺点2和3所述，补丁的生成依赖语义分析，如果语义分析器能借助人工智能技术的发展，强大到根据漏洞类型自动分析出某段代码的修补方案，这样的话，没有漏洞的世界就完成了。😃\n论文链接：https://www.usenix.org/conference/usenixsecurity20/presentation/xu\n参考文章：应用程序热补丁（一）：几行代码构造免重启修复补丁\n","categories":["论文阅读"],"tags":["Android","论文","笔记","Hot-Patch"]},{"title":"Note《FANS Fuzzing Android Native System Services via Automated Interface Analysis》","url":"/Note-FANS-Fuzzing-Android-Native-System-Services/","content":" FANS: Fuzzing Android Native System Services via Automated Interface Analysis\n 主要解决什么问题\n将Fuzzing技术应用到Android Native system services 面临的问题有：\n\nandroid native system services 通过特殊的程序间通信机制调用，即名为binder的机制，它通过服务特定的接口调用服务。因此Fuzzer 需要辨识所有接口，自动化地生成特定接口的测试用例\n有效的测试用例应该满足所有接口的接口模型\n测试用例也应该满足语义要求，包括变量依赖性和接口依赖性\n\nAndroid 系统中system services 要注册到 Service Manger。用户app 通过查询manger 获得访问目标services 的接口（封装在代理绑定器对象中 Proxy Binder object），然后通过形如IBinder::transact(code,data,reply,flags)的统一远程过程调用(RPC)接口，调用该接口提供的不同事务。code决定要调用的事务，data是事务的输入的序列化形式。因此，我们可以利用这个统一的IPC方法来测试所有的系统服务。为了彻底测试目标服务，我们可以首先找到所有接口和可用的事务，然后用满足该服务特定的格式和语义要求的输入数据来调用它们。具体而言，有三个挑战需要应对：\n\n多级接口识别\n接口模型提取\n生成语义正确的输入\n\npropose a generation-based fuzzing solution FANS to address the aforementioned challenges.\n在https://github.com/iromise/fans.开源了FANS的原型\n总结本文做出的贡献：\n\n我们系统地研究了Android native system services 中接口之间的依赖关系，挖掘了更深的多层接口。\n提出了一种自动提取输入接口模型和语义的解决方案。这种方法也可以应用于其他基于接口的程序。\n我们提出了一个解决方案，通过在序列化和反序列化pair中利用 变量名和类型知识 来推断事务间的依赖关系。\n我们实现了一个FANS原型来系统地模糊Android本地系统服务，并发现了30个独特的本地漏洞和138个独特的Java异常\n\n 使用的方法\n\n上图展示了我们的解决方案FANS的设计概述。首先，接口收集器(第3.3节)收集目标服务中的所有接口，包括顶级接口和多级接口。然后接口模型提取器(第3.4节)为这些接口中的每个候选事务提取输入和输出格式以及变量语义，即变量名和类型。提取器还收集与变量相关的结构、枚举和类型别名的定义。接下来，依赖推断器(第3.5节)推断接口依赖，以及事务内和事务间变量依赖。最后，基于上述信息，模糊引擎(第3.6节)随机生成事务，并调用相应的接口来模糊本地系统服务。fuzzer引擎还有一个管理器，负责同步主机和被测手机之间的数据。\n 3.3Interface Collector\n顶层或多层接口都有调度事务的onTransact方法。因此，我们可以利用这个特性来识别接口。不过，我们并不直接扫描AOSP代码库中的C/C文件来获取onTransact方法。相反，我们检查在AOSP编译命令中作为源出现的每个C/C文件，以便我们可以收集在编译期间由AIDL工具动态生成的接口，否则这些接口将被忽略。\n 3.4Interface Model Extractor\nDesign Choices\nRPC-centric testing：以RPC（remote procedure call）为中心的测试，攻击者必须通过IPC接口与目标service交互，并且只能产生有限数量的事件。为了减少误报，选择通过RPC接口测试目标服务。\nGeneration-based fuzzing：基于生成的fuzzing比mutation-based fuzzing 有更高的代码覆盖率，更好的语义，更少的漏报等优点。\nLearn input model from code：大意就是：基于生成的模糊测试器依赖于输入模型知识来生成有效的测试用例。FANS是通过分析Android 源码自动化构建输入模型。\nInterface Model Extractor\n\n从服务端代码中提取接口\n先将AIDL文件转为C++文件，然后使用AST提取接口模型\n\nTransaction Code Identification\n通过分析AST中的switch-case的case节点，就可以轻松的分析出接口的所有事务，并识别出相关的常量事务代码（constant transaction code）\nInput and Output Variable Extraction\n识别出事务代码以后，需要从向每个事务传递的参数data中提取输入的反序列化。此外，由于我们想推断事务的内部依赖，我们也需要提取事务的输出，输入是被序列化到reply的数据\n事务中使用的变量有三种可能的类别：\n\nSequential Variables 这种类型的变量没有任何前提条件。\nConditional Variables 这种类型的变量取决于一些条件。如果不满足这些条件，变量可能为空，或者不出现在数据中，甚至与满足条件时的类型不同\nLoop Variables 这种类型的变量在循环甚至嵌套循环中被反序列化\n\n这三类变量恰好对应程序中的三类语句，即顺序语句、条件语句和循环语句。因此，我们将主要在AST中处理这类语句。\nType Definition Extraction\n除了提取事务中的输入和输出变量，我们还提取类型定义。它有助于丰富变量语义，以便生成更好的输入。有三种类型需要分析:\n\nStructure-like Definition 这种类型包括联合和结构\nEnumeration Definition 提取所有给定的(常量)枚举值\nType Alias\n\n 3.5Dependency Inferer\n在提取接口模型后，我们推断出两种依赖关系:\n(1)接口依赖。即如何识别和生成多级接口。它还暗示了一个接口如何被其他接口使用。\n(2)变量依赖。事务中的变量之间存在依赖关系。以往的研究很少考虑这些依赖性。\n具体的依赖识别方法就不赘述了\n 3.6Fuzzer Engine\n首先，fuzzer管理器会将fuzzer程序的二进制文件、接口模型和依赖同步到手机上，并在手机上启动fuzzer。然后Fuzzer将生成一个测试用例，即一个事务及其相应的接口来模糊测试远程代码。同时，fuzzer管理器将定期同步手机上的崩溃日志。\n 结果评估\nExperimental Setup As shown in Fig Overview-of-FANS, we implement the first three components on Ubuntu 18.04 with i9-9900K CPU, 32 GB memory, 2.5 T SSD. As for test devices, we use the following Google’s Pixel series products: Pixel * 1, Pixel 2XL * 4, and Pixel 3XL * 1. We flash systems of these smartphones with AOSP build number PQ3A.190801.002, i.e., android-9.0.0_r46, which is a recent version supporting these devices when writing this paper. Although the Android release versions are the same, the source code can be slightly different for different Pixel models. For the following two sections (Section 5.1, Section 5.2), we report the experiment results carried out on Pixel 2XL.\nHow many interfaces have been found? What is the relationship between them? (Section 5.1)\n\n\nWhat does the extracted interface model look like? Is the model complete and precise? (Section 5.2)\n\n虽然不能做到完全精确和完整，但是足够好了\nHow effective is FANS in discovering vulnerabilities of Android native system services? (Section 5.3)\n为了评估FANS的有效性，我们在六部智能手机上断断续续运行了大约30天。我们已经从FANS报告的成千上万的崩溃中发现了30个独特的错误，下表列出了所有30个漏洞。除了在Android原生系统服务中发现的22个漏洞之外，在作为Android原生系统服务中的公共库的libcutils.so、libutils.so和libgui.so库中还有5个漏洞。此外，我们在Linux系统组件中发现了三个漏洞。例如，我们在iptables-restore中发现了一个堆栈溢出。这个程序是firewallconfiguration提供的用户空间程序。这些漏洞证明了FANS生成的输入可以在复杂的约束下驱动控制流进入深层路径。\n\n此外，虽然我们的目标是发现用C++实现的Android原生系统服务中的漏洞，但我们触发了138个Java异常，如FileNotFoundException、DateTimeException、NoSuchElementException和NullPointerException。这可以归因于Java应用程序有时依赖Android原生系统服务的事实。一些本地服务也调用Java方法。由于健壮性和稳定性对于Android原生系统服务很重要，所以这些Java异常应该不会发生。应该实施更严格的检查来解决这个问题。\n我们已经向谷歌报告了所有本地漏洞。其中20个被确认，18个被给予安卓身份证，其中三个与未披露的漏洞报告重复。到目前为止，谷歌已经给安卓ID 143895055和143899228分配了中等严重度。谷歌还将CVE-2019-2088分配给安卓标识143895055，并将在未来将我们放入他们的确认页面。正在提交Java异常。\n 对我有什么启发\n此前没了解过Fuzzing技术，借此文章进行学习。同时，也尝试从不同角度了解android 系统原理。文中提到的Native Services， Services Manager 等技术可以参考文末链接。\n论文链接：https://www.usenix.org/conference/usenixsecurity20/presentation/liu\n参考文章：\nService与Android系统设计（6）— Native Service https://blog.csdn.net/21cnbao/article/details/8087328\nService与Android系统设计（4）-- ServiceManager https://blog.csdn.net/21cnbao/article/details/8087304\nService与Android系统设计（2）-- Parcel https://blog.csdn.net/21cnbao/article/details/8086619\nAndroid系统服务(SystemService)简介 https://blog.csdn.net/geyunfei_/article/details/78851024\n","categories":["论文阅读"],"tags":["Android","论文","笔记","Fuzzing","System-Service"]},{"title":"ELMO, BERT, GPT","url":"/ELMO-BERT-GPT/","content":"\n ELMO, BERT, GPT\n这节课要讲的是关于如何让机器看懂人类的文字，也就是自然语言模型。今天要将的是到今天为止（2019年6月），这个方向上最较新的模型包括BERT 还有和BERT 很相似的也很知名的模型ELMO 和GPT 。\n Representation of Word\n先回顾一下，在计算机中怎么表示一个词。\n\n1-of-N Encoding\n最早的词表示方法。它就是one-hot encoding 没什么好说的，就是说如果词典中有N个词，就用N维向量表示每个词，向量中只有一个位置是1，其余位置都是0.\n但是这样的方式就失去的词意，比如说dog cat bird 都是动物，这些词的表示在数学上要是接近的。\nWord Class\n根据词的类型划分，但是这种方法还是太粗糙了，比如说dog cat 都是哺乳动物，bird是鸟类应该再细分，而dog cat 又不是同一科又可以再分，且cat 和car都是四个腿或许这两个词应该近一些。于是就有了Word Embedding\nWord Embedding\n有点像是soft 的word class，把词嵌入到高维空间中，具有相同属性，或者相似的词之间，距离近一些。Word Embedding的技术过去讲过，参考：https://www.youtube.com/watch?v=X7PH3NuYW0Q\n接下来进入没讲过的知识\n 一词多义\n\n\nhttps://arxiv.org/abs/1902.06006\n\n如上图所示，上面五个bank有三种意思。也就是说同一个词往往在不同的上下文中具有不一样的意思，就是说我们希望一个词可以有多个embedding 。以前要处理这件事，我们是让机器去查词典，看这个词有几种解释就给它设置几个embedding，然后通过语料库train 出这些向量。但是不同的词典对同一个词有不同的解释，比如说上图第五个blood bank（血库），有的词典认为这个bank就是银行的意思，有的词典认为这是区别于银行的第三种意思。事实上，有很多这种微妙的词汇，让人难以判断它应该设置几个embedding 。\n Contextualized Word Embedding\n\n我们希望机器能做到：\n\n\n每个token（不同上下文中的词）都有一个embedding （无论它们在词典解释中是否属于同一类）\n\n\nword token 的embedding 依赖于上下文\n\n\n如上图所示，三个bank是不同的token，它们的上下文是不一样的，未来将有不同的embedding （包括灰色的方块，每个token都会输出一个embedding）。这个技术叫做Contextualized Word Embedding 。\n那怎么做到Contextualized Word Embedding 呢，有一个实现的技术叫做：Embeddings from Language Model (ELMO)\n Embeddings from Language Model (ELMO)\n\n\nhttps://arxiv.org/abs/1802.05365\n\nELMO是一个RNN-based Language Model，训练的方法就是找一大堆的句子，也不需要做标注，然后做上图所示的训练，RNN忘记了的话去看以前的讲解吧。\nRNN-based Language Model 的训练过程就是不断学习预测下一个单词是什么。举例来说，你要训练模型输出“潮水退了就知道谁没穿裤子”，你教model，如果看到一个开始符号&lt;BOS&gt; ，就输出潮水，再给它潮水，就输出退了，再给它退了，就输出就…学完以后你就有Contextualized Word Embedding ，我们可以把RNN 的hidden layer 拿出来作为Embedding 。\n为什么说这个hidden layer 做Embedding 就是Contextualized 呢，因为RNN中每个输出都是结合前面所有的输入做出的。\n你可能会疑惑上图不是只看了单词的前文吗，怎么说是上下文呢？\n事实上，我们是如上图所示做了正反双向的训练，最终的word embedding 是把正向的RNN 得到的token embedding 和反向RNN 得到的token embedding 接起来作为最终的Contextualized Word Embedding 。\n现在出现了新的问题，通常来说RNN都是Deep的，如下图所示：\n\n中间的word embedding通常是多个，怎么解决呢？ELMO的解决方法很简单就是：我全部都要\n\n\n如上图所示，ELMO会将每个词输出多个embedding，这里我们假设LSTM叠两层。ELMO会用做weighted sum，weight是根据你做的下游任务训练出来的，下游任务就是说你用EMLO做SRL（Sematic Role Labeling 语义角色标注）、Coref（Coreference resolution 共指解析）、SNLI（Stanford Natural Language Inference 自然语言推理）、SQuAD（Stanford Question Answering Dataset） 还是SST-5（5分类情感分析数据集）。\n具体来说，你要先train好EMLO，得到每个token 对应的多个embedding，然后决定你要做什么task，然后在下游task 的model 中学习weight 的值。\n原始ELMO的paper 中给出了上图实验结果，Token是说没有做Contextualized Embedding之前的原始向量，LSTM-1、LSTM-2是EMLO的两层得到的embedding，然后根据下游5个task 学出来的weight 的比重情况。我们可以看出Coref 和SQuAD 这两个任务比较看重LSTM-1抽出的embedding，而其他task 都比较平均的看了三个输入。\n Bidirectional Encoder Representations from Transformers (BERT)\nBERT = Encoder of Transformer\n我们在transformer 一节中讲过Transformer 了，BERT 也有提及。\n如果后面讲的东西你没有听的很懂的话，你就记住BERT的用法就是吃一个句子，输出句中每个词的embedding 。\n\n这里老师做了一个提醒，由于中文的词太多，所以用词作为sequence的基本单位，可能导致输入向量过长（one-hot），所以也许使用字作为基本单位更好。\n Training of BERT\npaper上提及的BERT的训练方法有两种，Masked LM 和Next Sentence Prediction 。\nMasked LM\n\n基本思路：盖住输入的句子中15%的词，让BERT把词填回来。具体方法是，挖空的位置用符号[MASK]替换，然后输入到BERT，BERT给出输出向量，将挖空地方的输出向量，输入到一个简单的线性模型中，这个模型会分辨这个向量是那个词。所以说，BERT必须好好做embedding ，只有BERT抽出的向量能很好地表示被挖空的词，这个简单的线性模型才能找到到底是哪个词被Masked。\nNext Sentence Prediction\n\n基本思路：给BERT两个句子，然后判断这两个句子是不是应该接在一起。具体做法是，[SEP]符号告诉BERT交接的地方在哪里，[CLS]这个符号通常放在句子开头，它通过BERT得到的embedding 输入到简单的线性分类器中，分类器判断当前这个两个句子是不是应该接在一起。\n你可能会疑惑[CLS]难道不应该放在句末，让BERT看完整个句子再做判断吗？\n你仔细想想看，BERT里面一般用的是Transformer的Encoder，也就是说它做的是self-attention，我们之前说self-attention layer 不受位置的影响，它会看完整个句子，所以一个token放在句子的开头或者结尾是没有差别的。\n最后提一下上述两个方法中，Linear classifier 是和BERT一起训练的。上述两个方法在文献上是同时使用的，让BERT的输出去解这两个任务的时候会得到最好的训练效果。\n How to use BERT\n你当然可以把BERT当作一个抽embedding 的工具，抽出embedding 以后去做别的task，但是在文献中并不是这样，文献中是把BERT和down stream task 一起做训练😮。举了四个栗子：\n\n如上图，输入一个sentence 输出一个class ，有代表性的任务有情感分析，文章分类等。\n具体做法，以句子情感分析为例，你找一堆带有情感标签的句子，丢给BERT（BERT有trained model），再句子开头设一个判断情感的符号[CLS]，把这个符号通过BERT的输出丢给一个线性分类器做情感分类。线性分类器是随机初始化参数用你的训练资料train 的，这个过程你也可以堆BRET进行fine-tune ，或者就fix 住BRET的参数，都行。\n\n如上图，输入一个句子，输出每个单词的类别。有代表性的栗子，Slot filling。\n和上一个栗子类似，这里你就去train 每个线性分类器就好了，BERT也可以去fine-tune 。\n\n如上图，输入两个句子，输出自然语言推理的结果。举栗来说，给出一个前提一个假设，机器根据这个前提判断这个假设是T/F/unknown.\n做法如上图所示，你在开头设一个提问的符号，在把两个句子用[SEP]接起来，上面接一个线性分类器（3分类），同上的训练方法。\n\n那BERT解QA问题，而且明确是Extraction-based Question Answering ，这种QA问题是说，文中一定能找到问题的答案。\n解这种QA问题的model 如上图所示，模型输入Document D 有N个单词，Query Q有M个单词，模型输出答案在文中的起始位置和结束位置：s和e。举例来说，上图中第一个问题的答案是gravity，是Document 中第17个单词；第三个问题的答案是within a cloud，是Document 中第77到第79个单词。\n怎么用BERT解这个问题呢？\n\n输入如上图所示，Document 中每个词都会有一个向量表示，然后你再去learn 两个向量，上图红色和蓝色条，这两个向量的维度和BERT的输出向量相同，红色向量和Document 中的词汇做点积得到一堆数值，把这些数值做softmax 最大值的位置就是s，同样的蓝色的向量做相同的运算，得到e：\n\n如果e落在s的前面，有可能就是无法回答的问题。\n这个训练方法，你需要label很多data，每个问题的答案都需要给出在文中的位置。两个向量是白手起家learn出来的，BERT可能需要fine-tune 。\n BERT 屠榜\n现在BERT在NLP任务中几乎是屠榜的，几乎每个NLP任务都已经被BERT洗过了。\n\n Enhanced Representation through Knowledge Integration (ERNIE)\n\n\nhttps://arxiv.org/abs/1904.09223\nhttps://zhuanlan.zhihu.com/p/59436589\n\nERNIE是转为中文设计的NLP预训练模型，by 百度。细节可以看一下文章。\n What does BERT learn?\n\n\nhttps://arxiv.org/abs/1905.05950\nhttps://openreview.net/pdf?id=SJzSgnRcKX\n\n思考一下BERT每一层都在做什么，列出上述两个reference 给大家做参考。假如说我们的BERT有24层，down stream task有POS、Consts等等，我们来分析一下上述实验结果。这个实验把BERT的每一层的Contextualized Embedding 抽出来做weighted sum，就和EMLO的做法一样，ELMO就是把每层的Contextualized Embedding 抽出来做weighted sum，然后通过下游任务learn 出weight。\n这里也一样，假设我们是单纯用BERT做embedding ，用得到的词向量做下游任务。那BERT有24层，所以每个词都会抽出24个vector ，把这些vector 做weighted sum，weight 是根据下游任务learn出来的，看最后learn出的weight 的情况，就可以知道这个任务更需要那些层的vector 。\n上图中右侧蓝色的多个柱状图，代表通过不同任务learn 出的BERT各层的weight ，POS是做词性标注任务，会更依赖11-14层，Coref是做分析代词指代，会更依赖BERT高层的向量，而SRL语义角色标注就比较平均地依赖各层抽出的信息。\n Multilingual BERT\n\n\nhttps://arxiv.org/abs/1904.09077\n\nMultilingual BERT 用104种语言去训练。google 到wiki百科上爬了104种语言的百科给BERT学习，虽然BERT没看过这些语言之间的翻译，但是它看过104种语言的百科以后，它似乎自动学会了不同语言之间的对应关系。\n所以，如果你现在要用这个预训练好的BERT去做文章分类，你只要给他英文文章分类的label data set，它学完之后，竟然可以直接去做中文文章的分类 amazing😮。更多细节你可以参考上述reference\n Generative Pre-Training (GPT)\n\n\nhttps://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf\nSource of image: https://huaban.com/pins/1714071707/\n\n从上图你就可以看出，GPT非常巨大，其实它的卖点就是大。\nGPT目前有三个版本，GPT就是最开始版本，它的大小是平平无奇的，只是提出了一种NLP预训练模型架构，GPT-2是GPT的加大版本，GPT-3是上一个版本的加大版本。也就是中杯、大杯和超大杯。尤其是GPT-3在2020年引起了较大轰动，有人说它能为人工智能的热潮续命5年，关于GPT-3的文章已经有很多了，感兴趣可以自行查阅，本节主要探究GPT的架构。\n为什么用独角兽的形象，后面会解释。\n我们上面说BERT是Transformer 的Encoder ，GPT其实是Transformer 的Decoder 。\n\nGPT和一般的Language Model 做的事情一样，就是你给他一些词汇，它预测接下来的词汇。举例来说，如上图所示，把“潮水的”q拿出来做self-attention，然后做softmax 产生 α^\\hat{\\alpha}α^ ，再分别和v做相乘求和得到b，self-attention 可以有很多层（b是vector，上面还可以再接self-attention layer），通过很多层以后要预测“退了”这个词汇。\n\n预测出“退了”以后，把“退了”拿下来，做同样的计算，预测“就”这个词汇，如此往复。\n Result of GPT\n\nGPT-2是一个巨大的预训练模型，它可以在没有更多训练资料的情况下做以下任务：\n\nReading Comprehension\n\nBERT也可以做Reading Comprehension，但是BERT需要新的训练资料train 线性分类器，对BERT本身进行微调。而GPT可以在没有训练资料的情况下做这个任务。\n如上图所示，你就给GPT-2一段文章，给出一个问题，再写一个A:，他就会尝试做出回答。右侧是GPT-2在CoQA上的结果，最大的GPT-2可以和DrQA达到相同的效果，不要忘了GPT-2在这个任务上是zero-shot learning ，从来没有人教过它做QA 。\n\nSummarization\n\n给出一段文章加一个too long don’t read 的缩写&quot;TL;DR:&quot; 就会尝试总结这段文字。\n\nTranslation\n\n以上图所示的形式给出 一段英文=对应的法语，这样的栗子，然后机器就知道要给出第三句英文的法语翻译。\n其实后两个任务效果其实不是很好，Summarization就像是随机生成的句子一样。\n Visualization\n\n\nhttps://arxiv.org/abs/1904.02679\n\n有人分析了一下GTP-2的attention做的事情是什么。\n上图右侧的两列，GPT-2中左列词汇是下一层的结果，右列是前一层需要被attention的对象，我们可以观察到，She 是通过nurse attention 出来的，He是通过doctor attention 出来的，所以机器学到了某些词汇是和性别有关系的（虽然它大概不知道性别是什么）。\n上图左侧，是对不同层的不同head 做一下分析，你会发现一个现象，很多不同的词汇都要attend 到第一个词汇。一个可能的原因是，如果机器不知道应该attend 到哪里，或者说不需要attend 的时候就attend 在第一个词汇。如果真是这样的话，以后我们未来在做这种model 的时候可以设一个特别的词汇，当机器不知道要attend 到哪里的时候就attend 到这个特殊词汇上。这就是Visualization 可以告诉我们的事情。\n GPT-2 write novel\n\nGPT-2是OpenAI 做的，上图是OpenAI 用GPT-2 做了一个续写故事的栗子，他们给机器看第一段，后面都是机器脑部出来的。机器生成的段落中提到了独角兽和安第斯山，所以现在都拿独角兽和安第斯山来隐喻GPT：\n\nOpenAI 担心GPT-2最大的模型过于强大，可能会被用来产生假新闻这种坏事上，所以只发布了GPT-2的小模型。BERT是你能得到模型，GPT-2却是你得不到的，于是上图就很形象的表达了众多平民AI Master 的心情。\n有人用GPT-2的公开模型做了一个在线demo，大家可以去试试：\nhttps://app.inferkit.com/demo\n参考文章：\n\nAttention isn’t all you need！BERT的力量之源远不止注意力：https://zhuanlan.zhihu.com/p/58430637\n\n","categories":["李宏毅机器学习笔记"],"tags":["ML","note","Next-Step-of-ML"]},{"title":"Note《FIRMSCOPE Automatic Uncovering of Privilege-Escala Vulnerabilities in Pre-Installed Apps in Android Firmware》","url":"/Note-FIRMSCOPE-Automatic-Uncovering-of-Privilege-Escala/","content":" FIRMSCOPE: Automatic Uncovering of Privilege-Escalation Vulnerabilities in Pre-Installed Apps in Android Firmware\n 主要解决什么问题\nandroid 系统固件的预装软件中有很多app存在漏洞，而且这些app权限往往比较高，所以容易造成安全威胁。本文意在自动挖掘预装app中存在的权限提升漏洞。\n 使用的方法\n系统架构，workflow：\n\n 1.unpack firmware\n解压Android固件环节存在一个问题，各固件厂商的格式不一样。大部分厂商使用Android Sparse Image (SIMG) format 这种ext4格式的固件压缩方式，使用诸如e2tools这样的工具进行解压和挂载。但是，解压之后处理(unpack)就不是一致的了，有些厂商提供了第三方的unpack工具，比如华为的UApp格式，需要使用Splituapp进行unpack，HTC的RUU架构需要用HTC RUU DecryptTool，Sony 的.sin 架构需要用AnyXperia Dumper。一些厂商使用Sparse Data(SDAT)把image分块，分块文件可以使用Sdat2img重构回SIMG。\n对于没有可用的解包工具的厂商固件，采用启发式的方法搜索一致的SIMG/ext4 头，尝试unpack。通常，镜像文件会被额外的头填充，只要剥离这些额外的头，就能得到标准镜像文件。\n同时，努力寻找镜像文件中的build.prop和default.prop文件，其中可能包含了build fingerprint，os version，build configuration，exact make and model等信息。\n另外，我找到一篇参考文章：Android各厂商Rom包解压方式\n 2.Extracting and Disassembling Apps\n从unpack的image文件中找出所有DEX,ODEX,VDEX,OAT,JAR, 和APK格式文件，关于这些格式的更多信息可以参考here , 因为预装app的格式各不相同，文中将将所有提取出的app转换成APK的规范格式。对于预编译的OAT app，结合Oat2Dex和Baksmali，并参考之前提取出的framework files，把嵌入到ODEX、VDEX、OAT文件中的DEX classes提取出来，这一步输出汇编的DEX classes 或反汇编的smali。\n把所有Dalvik bytecode反汇编成smali 代码，然后转换为中间表示IL（intermediate Language）Jasmin [35] and Jimple[36]，我们还反编译应用程序的二进制的XML manifest文件，并提取有关应用程序及其所有声明的组件的元数据。具体来说，我们提取应用程序包的名称和版本信息，使用和声明的权限，以及所有导出的组件的全名和类型，以及所有指定访问权限的导出组件。\n 3.Static Taint Analysis\n它首先建立了过程间控制流图inter-procedural Control-Flow Graphs（ICFG）；然后，重建类层次结构并解析调用情况（第4.2.1节）；推断Def-Use链，并构建过程间数据流图inter-procedural Data-Flow Graph（IDFG）（第4.2.2节）；最后执行自定义的流敏感、上下文敏感、字段敏感和部分对象敏感的污点分析，以识别易受攻击的执行路径（第4.2.3节）。\nICFG和IDFG的部分跳过，来重点看看污点分析部分：（这部分先硬翻译一下😥）\n根据先前IDFG架构，污点跟踪问题就转变为图遍历问题，从一个污点源到一个污点汇聚点或到多个污点汇聚点，污点源和汇聚点都是多重图中的节点。在遍历过程中，我们应用验证规则集去修剪不会到达敏感点的路径。这些上下文验证规则对于效率和精确度非常有影响，因为先前的方法在实作上在成了不必要的复杂度扩大。例如，由于无法大规模地使用call-site stack提供上下文敏感性，因为为每个call site创建stack在计算上是不可实现的 ，特别是涉及虚拟调用时，每个call site需要维护多个stack。此外，通过 字段 的流必须始终保持流敏感和上下文敏感，但是由于明显过度的开销以及跟踪和将所有这些副本链接复制到其读写整个app所在位置的复杂性，所以我们不能在每次访问该字段时就复制与该字段相应的所有节点。因此，最终，我们的分析 实现了上下文敏感，流敏感和字段敏感以及部分对象敏感：\nContext-Sensitive：\n我们通过将每个call指令与其自身的返回伪寄存器配对，并在污点跟踪期间保持一个调用栈覆盖在每个标记为污点路径的上方，来确保上下文敏感。\nFlow- and Field-Sensitive：\n我们的IDFG构造对流敏感，因为我们考虑了语句顺序并跟踪每个程序点的流。因为我们跟踪每个类字段的流，所以我们的构造也对字段敏感。\nPartially-Insensitive：\n确保类型兼容性使我们的分析对同级和不相关类敏感，对对象敏感，对单定义方法不敏感，对子对象和其祖先之一具有定义的虚拟方法不敏感。\nPath-Insensitive：\nFIRMSCOPE对路径不敏感，因为它虽然根据控制流图流动信息，但信息流动与不相关的条件分支之间可能存在的条件依赖性无关。路径敏感性是一个已知的难题，在实践中没有绝对解决方案[39]。\nDetection Rules\n我们开发了一套规则引擎，用YAML文件写探测规则。具体来说，我们实施了规则和插件来检测以下提权漏洞：\n(i) command injection;\n(ii) arbitrary app installation/removal;\n(iii) code injection;\n(iv) factory reset of the device;\n(v) SMS injection, including accessing, sending, and manipulating text messages;\n(vi) device recording, including audio, video, and screen recording;\n(vii) log leakage to external storage or to other apps;\n(viii) AT Command injection;\n(ix) wireless settings modification;\n(x) system settings modification.\n 结果评估\n我们收集了从v4.0到v9.0的2,017个公开可用的Android固件映像（请参阅附录A以获得详细信息），总共涵盖了100多家Android供应商，其中包括全球排名前20的Android供应商。固件映像包含331,342个应用程序，具有15,144个唯一的软件包名称和39,541个唯一的软件包版本。该数据库的详细信息如表1所示。\n\n我们在三台服务器上部署了FIRMSCOPE，每台服务器在Intel Xeon E5-2630 v4 2.20GHz上运行64位Ubuntu 18.04，具有40个逻辑核心和150 GiB的RAM。我们使用GNU Parallel [41]实现了一个流水线来管理作业并在三个服务器上分配固件映像，并尽可能并行地分析多个应用程序，以保持最大80％的服务器负载而无内存交换。我们全面分析了每个固件映像，无论其某些应用程序是否可能出现在其他分析映像中。\n表2是研究结果摘要。我们发现了850个独特的特权升级漏洞（共3,483个），占分析固件的77％。命令注入漏洞排在最前面，影响了三分之一以上的固件映像。\n\n我们按弱点类别按供应商提供细分：\n\n总结：表4中显示了AOSP与供应商应用程序中已识别的漏洞的总数。大约92％的漏洞是由供应商引入的应用程序（375个唯一的软件包名称）中，而AOSP中只有8％的漏洞（18个唯一的AOSP软件包名称）。这些结果说明，类AOSP的镜像比供应商定制的镜像更安全。供应商的修改通常会带来无法预料的漏洞。\n\n 对我有什么启发\n这文章对我这个初学者来说有几方面的意义：\n\n更深入的接触到android 系统镜像\n对android run time(art), aot(ahead of time), oat(Ahead of time)file format, odex(Optimized Dalvik Executable file)的理解加深\n初次接触到taint analysis（污点分析技术）\nCFG、DFG的构建是以后要学习的内容\n\n 这项研究的未来方向\n其实我感觉它没有用很新的知识，就是把APP漏洞检测转到预装APP上做，多了一个拆包的过程，研究了一下AOSP-like的系统镜像和OEM厂商镜像中提权漏洞的类型和分布，并没有太多的创新点。\n文中构建的FIRMSCOPE系统使用静态检测技术，涉及的主要技术点包括过程间CFG、过程间DFG的构建，污点分析技术。\n未来，对于漏洞检测技术或许可以增加fuzzing、动态符号执行（Concoli）等动态分析技术。\n\nref:https://www.usenix.org/conference/usenixsecurity20/presentation/elsabagh\n\n","categories":["论文阅读"],"tags":["Android","论文","笔记","漏洞挖掘"]},{"title":"Android流量抓包环境配置","url":"/Android%E6%B5%81%E9%87%8F%E6%8A%93%E5%8C%85%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE/","content":" Android流量抓包环境配置\n 前言\n如果你的手机有magisk 推荐你直接按照步骤-PC端前半部分，安装用户证书，此后参考步骤-最终曲线救国一节就好。\n 环境：\nredmik305G MIUI12 Android 10（with Magisk）\nFiddler v5.0.20204.45441 for .NET 4.6.1Built: 2020年11月3日\n 步骤\n PC端\n安装Fiddler （注意是classic版本不是Fiddler Everywhere）\n配置Fiddler 可以参考Fiddler抓包（Android app） 前四节\n到这里Fiddler的证书是以用户证书的方式安装的，此时你的fiddler已经能收到很多流量了，但是很多APP都默认忽视用户证书，所以还是没办法抓，需要把证书装到system CA列表中。\n然后需要PC端导出Fiddler 证书，导出的证书是cer格式，根据部分APP无法代理抓包的原因及解决方法（flutter 抓包） 这篇文章的叙述，通过以下命令将cer证书转为pem格式：\nopenssl x509 -inform der -in FiddlerRoot.cer -out FiddlerRoot.pem\n再将pem证书重命名为Android system CA证书的格式：\nopenssl x509 -subject_hash_old -in FiddlerRoot.pem -noout\n或者\nopenssl x509 -inform der -subject_hash_old -in FiddlerRoot.cer -noout\n得到subject_hash_old值为269953fb\n重命名FiddlerRoot.pem为269953fb.0 ，后面的.&lt;number&gt;是为了如果有hash重复的证书则加1以区分证书。\n（好像cer格式的证书MIUI12 不认，疯狂卡死，报错，只能重启，装证书的时候可烦死我了😥）\n Android 端\n在修改系统文件之前最好先禁用系统验证：\nadb disable-verity\n将269953fb.0证书文件传到android设备的/system/etc/security/cacerts目录下，如果报只读文件系统的错误就remount一下：\nadb remount\n然后\nadb push ./269953fb.0 /system/etc/security/cacerts/\n这样完成就可以正常用fiddler抓包了。\n 如果你失败了\n但是如果你重启的话可能会发现证书无了，那你就和我遇到同样的问题了。\n如果你此时执行adb remount，就发现这个文件又回来了😥，我猜测是magisk 虚拟system文件系统每次重启都重置了。\n据说要固定文件系统更改要使用sync 命令：\nadb shell sync\n但是实测没有用，他可能是在真实的文件系统（without magisk）的情况下才好用吧😥\n不管是删除文件还是增加文件，似乎执行sync无效，只要一重启就撤销了修改，而remount以后就能看到修改，所以合理猜测，remount以后修改的文件是另外一个分区，每次remount 就挂载了那个分区，我加的证书就是放到那个分区了，而magisk默认启动时挂载的分区中我没有做修改。\n但是一重启证书就没了也不是办法，我们换一种方式，使用magisk模块，开机时自动将用户CA装载到System CA。\n 最终曲线救国\n使用magisk 模块把用户CA装载到system CA列表中，参考：安卓7.0+https抓包新姿势（无需Root） 具体过程就是：\n1.安装magisk（我有）\n2.安装模块：https://github.com/NVISO-BE/MagiskTrustUserCerts\n3.安装fiddler证书（先前以用户方式安装过了）\n4.reboot\n 后记\n我为了验证上面说的，MIUI对pem格式证书和cer证书格式的支持，删除了所有配置重新做了一遍，把自己搞崩了，现在只要一remount就疯狂卡死，报错。disable-verify也没用，我猜是我把remount的那个system分区搞坏了验证出问题了（但是我除了添加删除证书也没做别的呀555），难道以后都不能remount修改分区了吗，我现在感觉耶稣都救不了我😥😥😥。\n先这样吧，靠magisk模块，先用着。\n 更新-HttpCanary\nHttpCanary 是一款不错的 Android 手机端抓包软件，相比 Packet Capture 而言视图更加友好，更加方便复制数据，窗口模式抓包，还可以 json 格式化、实现注入拦截等操作。如果无 root 则需要安装平行空间来抓取 SSL/TLS 加密数据包。\n目前 Android 11 收紧了 APP 安装 CA 证书的权限，只能用户手动安装。而 HttpCanary 已经很长一段时间没有更新了，导致 Android 11 上面无法正常安装证书抓包。\n下面我们来解决证书安装问题，实现 Android 11 下 HttpCanary 的正常使用。\n具体的分析过程可以参考：https://www.52pojie.cn/thread-1367926-1-1.html\n具体来说HttpCanary 重写了 onActivityResult()，通过自动安装返回的 resultCode 判断了证书是否安装，如果安装成功便会在 cache 目录下写入一个无内容的 .jks。安装失败就无操作。而这个文件就是它判断是否安装了证书。因此，Android 11 上不能自动安装证书，就没有 .jks 文件，那么应用也就无法识别安装了证书，即使你早已手动安装。\n 获取证书\n较新的版本未安装证书是无法导出证书的，所以需要在 /data/data/com.guoshi.httpcanary/cache/ 目录下找到 HttpCanary.pem，将 HttpCanary.pem 复制到内部储存空间上，按照上述方法更名证书文件：\nopenssl x509 -subject_hash_old -in HttpCanary.pem -noout\n 安装证书\n安装为系统证书：将证书移到 /system/etc/security/cacerts/ 目录下设置好权限，或者自己弄一个 Magisk 模块替换。这个 Magisk 模块不难，就是模板压缩包里添加一个文件而已。\n 修改数据\n在 /data/data/com.guoshi.httpcanary/cache/ 目录下新建一个 HttpCanary.jks 无内容的文件，改好权限 600，就是改得跟旁边的文件一样的权限就行了。\n 其他改法\n说到底，就是安装证书后需要创建 .jks 文件来使得 APP 识别为已安装证书。至于安装为系统证书都可以自行安装。比如通过手机设置里的安装证书安装为用户证书，再在 Magisk 库里搜索安装 Move Certificates 模块将用户证书迁移为系统证书也是可以的。\n参考文章都不错：\n\nFiddler抓包（Android app） 第五节开始讲了一些Fiddler的使用方法，值得一看\n部分APP无法代理抓包的原因及解决方法（flutter 抓包） 讲了代理抓HTTPS流量的原理，值得一看\n\n","categories":["备忘"],"tags":["Android","流量抓包","教程","Fiddler"]},{"title":"区块链技术原理笔记","url":"/%E5%8C%BA%E5%9D%97%E9%93%BE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E7%AC%94%E8%AE%B0/","content":" ox0 前言\n2008年10月31日，一个名为Satoshi Nakamoto （中本聪）的ID 在cypherpunk 论坛发布了一篇名为《Bitcoin: A Peer-to-Peer Electronic Cash System》的文章，现在此文被看作比特币的白皮书，文中提出了&quot;区块链&quot;的概念。随后在2008年11月，中本聪发布了比特币的第一版代码。2009年1月，中本聪挖出了比特币的第一个区块——创世区块，比特币网络正式开始运行。\n现在我们知道这个叫做中本聪的神秘人，凭借这篇文章创造了万亿人民币的比特币市场。\n\n2010年12月12 日，中本聪在比特币论坛中发表了最后一篇文章，随后便不再公开露面，只通过电子邮件与比特币核心开发团队的少数人联系。2011年4月26日，在与比特币核心开发团队领导人加文·安德烈森联系过后，中本聪随即关闭了电子邮件，再也没有与任何人来往过。中本聪十分低调，在网络上可以追溯的行踪很少，他与任何人交流坚持使用PGP加密和Tor网络，而且他在论坛中的发言时间和说话方式也在刻意隐瞒自己的时区，伪装自己的表达习惯。由于他的代码逻辑十分精巧，设计区块链的具体数学细节也十分巧妙，所以有人怀疑中本聪是一个组织公用的ID。但是，时至今日也仍没有确切的证据证明某人或某组织就是中本聪。\n关于神秘人中本聪的故事就不过多叙述了，本文主要记述区块链的技术原理，涉及的要点有：\n\n区块链的技术原理\np2p交易原理\n比特币挖矿\nHD钱包？\n\n本文是我在了解区块链技术过程中的学习笔记，如果你也希望了解其中的技术原理，本文也许会对你有帮助。本文结合比特币讲解区块链技术的原理和应用方法，比特币只是区块链技术的应用之一，文中讨论的具体方法也比较原始，但就简单理解区块链技术来说足以。\n主要摘录整理廖雪峰老师的文章，所有参考文章附在文末。代码以廖雪峰老师给出的javascript 代码为例，每个代码块下方以引用的格式给出运行结果。\n 0x1 区块链原理\n区块链就是一个不断增长的全网总账本，区块链网络中的每个完全节点都拥有完整的区块链，并且，节点总是信任最长的区块链，伪造区块链需要拥有超过51%的全网算力。后面会解释为什么超过51%的全网算力就可以肆意妄为。\n区块链的一个重要特性就是不可篡改。为什么区块链不可篡改？我们先来看区块链的结构。\n区块链是由一个一个区块构成的有序链表，每一个区块都记录了一系列交易（现在一个比特币区块约记录2000~3000笔交易），并且，每个区块都指向前一个区块，从而形成一个链条：\n\n如果我们观察某一个区块，就可以看到，每个区块都有一个唯一的哈希标识，被称为区块哈希（下图中父区块哈希），同时，区块通过记录上一个区块的哈希来指向上一个区块：\n\n每一个区块还有一个Merkle哈希用来确保该区块的所有交易记录无法被篡改。\n区块链中的主要数据就是一系列交易，第一条交易通常是Coinbase交易，也就是矿工的挖矿奖励，后续交易都是用户的交易。\n 不可篡改性\n区块链的不可篡改特性是由哈希算法保证的。\n常用的哈希算法以及它们的输出长度如下：\n\n\n\n哈希算法\n输出长度(bit)\n输出长度(字节)\n\n\n\n\nMD5\n128 bit\n16 bytes\n\n\nRipeMD160\n160 bits\n20 bytes\n\n\nSHA-1\n160 bits\n20 bytes\n\n\nSHA-256\n256 bits\n32 bytes\n\n\nSHA-512\n512 bits\n64 bytes\n\n\n\n比特币使用的哈希算法有两种：SHA-256和RipeMD160\nSHA-256的理论碰撞概率是：尝试 21302^{130}2130 次随机输入，有99.8%的概率碰撞。注意 21302^{130}2130 是一个非常大的数字，大约是1361万亿亿亿亿。以现有的计算机的计算能力，是不可能在短期内破解的。\n比特币使用两种哈希算法，一种是对数据进行两次SHA-256计算，这种算法在比特币协议中通常被称为hash256或者dhash。\n另一种算法是先计算SHA-256，再计算RipeMD160，这种算法在比特币协议中通常被称为hash160。\nconst\n    bitcoin = require('bitcoinjs-lib'),\n    createHash = require('create-hash');\n\nfunction standardHash(name, data) &#123;\n    let h = createHash(name);\n    return h.update(data).digest();\n&#125;\n\nfunction hash160(data) &#123;\n    let h1 = standardHash('sha256', data);\n    let h2 = standardHash('ripemd160', h1);\n    return h2;\n&#125;\n\nfunction hash256(data) &#123;\n    let h1 = standardHash('sha256', data);\n    let h2 = standardHash('sha256', h1);\n    return h2;\n&#125;\n\nlet s = 'bitcoin is awesome';\nconsole.log('ripemd160 = ' + standardHash('ripemd160', s).toString('hex'));\nconsole.log('  hash160 = ' + hash160(s).toString('hex'));\nconsole.log('   sha256 = ' + standardHash('sha256', s).toString('hex'));\nconsole.log('  hash256 = ' + hash256(s).toString('hex'));\n\nripemd160 = 46c047bd035afb64dad2293cba29994a95b8b216\nhash160 = fe56649aa4f8fdb1edf6b88d2d41f3c1f72cf431\nsha256 = 23d4a09295be678b21a5f1dceae1f634a69c1b41775f680ebf8165266471401b\nhash256 = 1c78f53758ac96f43b99ed080f36327d2a823c4df4fa094e59b006d945bbb84d\n\n运行上述代码，观察对一个字符串进行SHA-256、RipeMD160、hash256和hash160的结果\n Merkle Hash\n在区块的头部，有一个Merkle Hash字段，它记录了本区块所有交易的Merkle Hash：\n\n注意到哈希值也可以看做数据，所以可以把a1和a2拼起来，a3和a4拼起来，再计算出两个哈希值b1和b2：\n       ┌───────────────┐               ┌───────────────┐\n       │b1&#x3D;dhash(a1+a2)│               │b2&#x3D;dhash(a3+a4)│\n       └───────────────┘               └───────────────┘\n               ▲                               ▲\n       ┌───────┴───────┐               ┌───────┴───────┐\n       │               │               │               │\n┌─────────────┐ ┌─────────────┐ ┌─────────────┐ ┌─────────────┐\n│a1&#x3D;dhash(tx1)│ │a2&#x3D;dhash(tx2)│ │a3&#x3D;dhash(tx3)│ │a4&#x3D;dhash(tx4)│\n└─────────────┘ └─────────────┘ └─────────────┘ └─────────────┘\n最后，把b1和b2这两个哈希值拼起来，计算出最终的哈希值，这个哈希就是Merkle Hash：\n                     ┌───────────────────┐\n                     │merkle&#x3D;dhash(b1+b2)│\n                     └───────────────────┘\n                               ▲\n               ┌───────────────┴───────────────┐\n               │                               │\n       ┌───────────────┐               ┌───────────────┐\n       │b1&#x3D;dhash(a1+a2)│               │b2&#x3D;dhash(a3+a4)│\n       └───────────────┘               └───────────────┘\n               ▲                               ▲\n       ┌───────┴───────┐               ┌───────┴───────┐\n       │               │               │               │\n┌─────────────┐ ┌─────────────┐ ┌─────────────┐ ┌─────────────┐\n│a1&#x3D;dhash(tx1)│ │a2&#x3D;dhash(tx2)│ │a3&#x3D;dhash(tx3)│ │a4&#x3D;dhash(tx4)│\n└─────────────┘ └─────────────┘ └─────────────┘ └─────────────┘\n如果交易的数量不恰好是4个怎么办？例如，只有3个交易时，第一个和第二个交易的哈希a1和a2可以拼起来算出b1，第三个交易只能算出一个哈希a3，这个时候，就把a3直接复制一份，算出b2，这样，我们也能最终计算出Merkle Hash：\n                     ┌───────────────────┐\n                     │merkle&#x3D;dhash(b1+b2)│\n                     └───────────────────┘\n                               ▲\n               ┌───────────────┴───────────────┐\n               │                               │\n       ┌───────────────┐               ┌───────────────┐\n       │b1&#x3D;dhash(a1+a2)│               │b2&#x3D;dhash(a3+a3)│\n       └───────────────┘               └───────────────┘\n               ▲                               ▲\n       ┌───────┴───────┐               ┌───────┴───────┐\n       │               │               │               │\n┌─────────────┐ ┌─────────────┐ ┌─────────────┐ ┌ ─ ─ ─ ─ ─ ─ ┐\n│a1&#x3D;dhash(tx1)│ │a2&#x3D;dhash(tx2)│ │a3&#x3D;dhash(tx3)│\n└─────────────┘ └─────────────┘ └─────────────┘ └ ─ ─ ─ ─ ─ ─ ┘\n如果有5个交易，我们可以看到，a5被复制了一份，以便计算出b3，随后b3也被复制了一份，以便计算出c2。总之，在每一层计算中，如果有单数，就把最后一份数据复制，最后一定能计算出Merkle Hash：\n                  ┌─────────┐\n                  │ merkle  │\n                  └─────────┘\n                       ▲\n           ┌───────────┴───────────┐\n           │                       │\n         ┌───┐                   ┌───┐\n         │c1 │                   │c2 │\n         └───┘                   └───┘\n           ▲                       ▲\n     ┌─────┴─────┐           ┌─────┴─────┐\n     │           │           │           │\n   ┌───┐       ┌───┐       ┌───┐       ┌ ─ ┐\n   │b1 │       │b2 │       │b3 │        b3\n   └───┘       └───┘       └───┘       └ ─ ┘\n     ▲           ▲           ▲\n  ┌──┴──┐     ┌──┴──┐     ┌──┴──┐\n  │     │     │     │     │     │\n┌───┐ ┌───┐ ┌───┐ ┌───┐ ┌───┐ ┌ ─ ┐\n│a1 │ │a2 │ │a3 │ │a4 │ │a5 │  a5\n└───┘ └───┘ └───┘ └───┘ └───┘ └ ─ ┘\n从Merkle Hash的计算方法可以得出结论：修改任意一个交易哪怕一个字节，或者交换两个交易的顺序，都会导致Merkle Hash验证失败，也就会导致这个区块本身是无效的，所以，Merkle Hash记录在区块头部，它的作用就是保证交易记录永远无法修改。\n Block Hash\n区块本身用Block Hash——也就是区块哈希来标识。但是，一个区块自己的区块哈希并没有记录在区块头部，而是通过计算区块头部的哈希得到的：\n\n区块头部的Prev Hash记录了上一个区块的Block Hash，这样，可以通过Prev Hash追踪到上一个区块。\n由于下一个区块的Prev Hash又会指向当前区块，这样，每个区块的Prev Hash都指向自己的上一个区块，这些区块串起来就形成了区块链。\n区块链的第一个区块（又称创世区块）并没有上一个区块，因此，它的Prev Hash被设置为00000000...000。\n如果一个恶意的攻击者修改了一个区块中的某个交易，那么Merkle Hash验证就不会通过。所以，他只能重新计算Merkle Hash，然后把区块头的Merkle Hash也修改了。这时，我们就会发现，这个区块本身的Block Hash就变了，所以，下一个区块指向它的链接就断掉了。\n\n由于比特币区块的哈希必须满足一个难度值，因此，攻击者必须先重新计算这个区块的Block Hash，然后，再把后续所有区块全部重新计算并且伪造出来，才能够修改整个区块链。\n在后面的挖矿中，我们会看到，修改一个区块的成本就已经非常非常高了，要修改后续所有区块，这个攻击者必须掌握全网51%以上的算力才行，所以，修改区块链的难度是非常非常大的，并且，由于正常的区块链在不断增长，同样一个区块，修改它的难度会随着时间的推移而不断增加。\n\n0x1 区块链原理总结：\n区块链依靠安全的哈希算法保证所有区块数据不可更改；\n交易数据依靠Merkle Hash确保无法修改，整个区块依靠Block Hash确保区块无法修改；\n保证修改区块链的难度非常巨大的机制：工作量证明机制（PoW-Proof of Work 即挖矿）在后面章节介绍\n\n 0x2 P2P交易原理\n比特币的交易是一种无需信任中介参与的P2P（Peer-to-peer）交易。\n传统的电子交易，交易双方必须通过银行这样的信任机构作为中介，这样可以保证交易的安全性，因为银行记录了交易双方的账户资金，能保证在一笔交易中，要么保证成功，要么交易无效，不存在一方到账而另一方没有付款的情况。但是在比特币这种去中心化的P2P网络中，并没有一个类似银行这样的信任机构存在，要想在两个节点之间达成交易，就必须实现一种在零信任的情况下安全交易的机制。\n一种创建交易的方法是：小明声称他给了小红一万块钱，只要能验证这个声明确实是小明作出的，并且小明真的有1万块钱，那么这笔交易就被认为是有效的：\n\n 数字签名\n如何验证这个声明确实是小明作出的呢？数字签名就可以验证这个声明是否是小明做的，并且，一旦验证通过，小明是无法抵赖的。在比特币交易中，付款方就是通过数字签名来证明自己拥有某一笔比特币，并且，要把这笔比特币转移给指定的收款方。使用签名是为了验证某个声明确实是由某个人做出的。例如，在付款合同中签名，可以通过验证笔迹的方式核对身份：\n\n而在计算机中，用密码学理论设计的数字签名算法比验证笔迹更加可信。使用数字签名时，每个人都可以自己生成一个秘钥对，这个秘钥对包含一个私钥和一个公钥：私钥被称为Secret Key或者Private Key，私钥必须严格保密，不能泄漏给其他人；公钥被称为Public Key，可以公开给任何人。这种使用一对公私密钥进行加密解密的方法就是非对称加密，早已经普遍应用于现实世界中，如果具备非对称加密的基础知识理解后面的内容会相当容易，推荐非对称加密算法RSA原理一、RSA原理二。\n当私钥持有人，例如，小明希望对某个消息签名的时候，他可以用自己的私钥对消息进行签名，然后，把消息、签名和自己的公钥发送出去：\n\n其他任何人都可以通过小明的公钥对这个签名进行验证，如果验证通过，可以肯定，该消息是小明发出的。这里的签名和验证就是指非对称加密中的加密和解密过程。\n\n这里要解释一下非对称加密应用的时候公钥和私钥到底谁负责加密谁负责解密？\n第一种用法：公钥加密，私钥解密。——用于加解密\n第二种用法：私钥签名，公钥验签。——用于签名\n有点混乱，不要去硬记，总结一下:\n你只要想：\n既然是加密，那肯定是不希望别人知道我的消息，所以只有我才能解密，所以可得出公钥负责加密，私钥负责解密；\n既然是签名，那肯定是不希望有人冒充我发消息，只有我才能发布这个签名，所以可得出私钥负责签名，公钥负责验证。\nref:https://blog.csdn.net/qq_23167527/article/details/80614454\n\n数字签名算法在电子商务、在线支付这些领域有非常重要的作用：\n首先，签名不可伪造，因为私钥只有签名人自己知道，所以其他人无法伪造签名。\n其次，消息不可篡改，如果原始消息被人篡改了，那么对签名进行验证将失败。\n最后，签名不可抵赖。如果对签名进行验证通过了，那么，该消息肯定是由签名人自己发出的，他不能抵赖自己曾经发过这一条消息。\n\n简单地说来，数字签名的三个作用：防伪造，防篡改，防抵赖。\n\n 签名和验证过程\n上面我们说了在p2p网络中数字签名的大致定义、作用和效果，我们来稍微具体讲一下区块链数字签名的签名过程和验证过程：\n签名\n对消息进行签名，实际上是对消息的哈希进行签名，这样可以使任意长度的消息在签名前先转换为固定长度的哈希数据。对哈希进行签名相当于保证了原始消息的不可伪造性。\n我们来看看使用ECDSA如何通过私钥对消息进行签名。关键代码是通过sign()方法签名，并获取一个ECSignature对象表示签名：\nconst bitcoin = require('bitcoinjs-lib');\nlet\n    message = 'a secret message!', // 原始消息\n    hash = bitcoin.crypto.sha256(message), // 消息哈希\n    wif = 'KwdMAjGmerYanjeui5SHS7JkmpZvVipYvB2LJGU1ZxJwYvP98617',\n    keyPair = bitcoin.ECPair.fromWIF(wif);\n// 用私钥签名:\nlet signature = keyPair.sign(hash).toDER(); // ECSignature对象\n// 打印签名:\nconsole.log('signature = ' + signature.toString('hex'));\n// 打印公钥以便验证签名:\nconsole.log('public key = ' + keyPair.getPublicKeyBuffer().toString('hex'));\n\nsignature = 304402205d0b6e817e01e22ba6ab19c0ab9cdbb2dbcd0612c5b8f990431dd0634f5a96530220188b989017ee7e830de581d4e0d46aa36bbe79537774d56cbe41993b3fd66686\npublic key = 02d0de0aaeaefad02b8bdc8a01a1b8b11c696bd3d66a2c5f10780d95b7df42645c\n\nECSignature对象可序列化为十六进制表示的字符串。\n验证\n在获得签名、原始消息和公钥的基础上，可以对签名进行验证。验证签名需要先构造一个不含私钥的ECPair，然后调用verify()方法验证签名：\nconst bitcoin = require('bitcoinjs-lib');\nlet signAsStr = '304402205d0b6e817e01e22ba6ab19c0'\n              + 'ab9cdbb2dbcd0612c5b8f990431dd063'\n              + '4f5a96530220188b989017ee7e830de5'\n              + '81d4e0d46aa36bbe79537774d56cbe41'\n              + '993b3fd66686'\n\nlet\n    signAsBuffer = Buffer.from(signAsStr, 'hex'),\n    signature = bitcoin.ECSignature.fromDER(signAsBuffer), // ECSignature对象\n    message = 'a secret message!', // 原始消息\n    hash = bitcoin.crypto.sha256(message), // 消息哈希\n    pubKeyAsStr = '02d0de0aaeaefad02b8bdc8a01a1b8b11c696bd3d66a2c5f10780d95b7df42645c',\n    pubKeyAsBuffer = Buffer.from(pubKeyAsStr, 'hex'),\n    pubKeyOnly = bitcoin.ECPair.fromPublicKeyBuffer(pubKeyAsBuffer); // 从public key构造ECPair\n\n// 验证签名:\nlet result = pubKeyOnly.verify(hash, signature);\nconsole.log('Verify result: ' + result);\n\nVerify result: true\n\n注意上述代码只引入了公钥，并没有引入私钥。\n修改signAsStr、message和pubKeyAsStr的任意一个变量的任意一个字节，再尝试验证签名，则不能通过。\n数字签名的验证实际上就是非对称加密中的解密过程，如果交易创造者使用私钥加密交易数据的hash得到的签名值（密文）能顺利被网络中其他人手中对应该私钥的公钥解密，即解密值==hash(交易数据)，就算是签名验证成功。数字签名算法，私钥和公钥的生成方式以及公钥和地址的对应关系我们在下一节数字签名算法中叙述。\n比特币对交易数据进行签名和对消息进行签名的原理是一样的，只是格式更加复杂。通过私钥可以对消息进行签名，签名可以保证消息防伪造，防篡改，防抵赖。对交易签名确保了只有持有私钥的人才能够花费对应地址的资金。\n 数字签名算法\n这一小节讲介绍比特币采用的：\n\n签名算法\n私钥和公钥的生成方式\n公钥和地址的对应关系。\n\n 签名算法\n常用的数字签名算法有：RSA算法，DSA算法和ECDSA算法。比特币采用的签名算法是椭圆曲线签名算法：ECDSA，使用的椭圆曲线是一个已经定义好的标准曲线secp256k1：\ny2=x3+7y^2 = x^3 + 7\ny2=x3+7\n这条曲线的图像长这样：\n\n比特币采用的ECDSA签名算法需要一个私钥和公钥组成的秘钥对：私钥本质上就是一个1～22561～2^{256}1～2256的随机数，公钥是由私钥根据ECDSA算法推算出来的，通过私钥可以很容易推算出公钥，所以不必保存公钥，但是，通过公钥无法反推私钥，只能暴力破解。\n 私钥\n比特币的私钥是一个随机的非常大的22562^{256}2256位整数。它的上限，确切地说，比22562^{256}2256要稍微小一点：\n\n0xFFFF FFFF FFFF FFFF FFFF FFFF FFFF FFFE BAAE DCE6 AF48 A03B BFD2 5E8C D036 4140\n\n而比特币的公钥是根据私钥推算出的两个256位整数。\n如果用银行卡作比较的话，比特币的公钥相当于银行卡卡号，它是两个256位整数：\n\n比特币的私钥相当于银行卡密码，它是一个256位整数：\n\n18E14A7B6A307F426A94F8114701E7C8E774E7F9A47E2C2035DB29A206321725\n\n银行卡的卡号由银行指定，银行卡的密码可以由用户随时修改。而比特币“卡”和银行卡的不同点在于：密码（实际上是私钥）由用户先确定下来，然后计算出“卡号”（实际上是公钥），即卡号是由密码通过ECDSA算法推导出来的，不能更换密码。\n由于比特币账本是全网公开的，所以，任何人都可以根据公钥查询余额，但是，不知道持卡人是谁。这就是比特币的匿名特性。\n\n如果丢失了私钥，就永远无法花费对应公钥的比特币！\n\n在比特币的P2P网络中不存在中央节点，私钥只有持有人自己知道，因此，丢失了私钥，对应的比特币就永远无法花费。如果私钥被盗，黑客就可以花费对应公钥的比特币，并且这是无法追回的。\n我们以JavaScript为例，演示如何创建比特币私钥。在JavaScript中，内置的Number类型使用56位表示整数和浮点数，最大可表示的整数最大只有9007199254740991。其他语言如Java一般也仅提供64位的整数类型。要表示一个256位的整数，只能使用数组来模拟。bitcoinjs使用bigi这个库来表示任意大小的整数。\n下面的代码演示了使用一个随机生成的数字，通过ECPair创建一个新的私钥后，表示私钥的整数就是字段d，我们把它打印出来：\nconst bitcoin = require('bitcoinjs-lib');\nlet keyPair = bitcoin.ECPair.makeRandom();\n// 打印私钥:\nconsole.log('private key = ' + keyPair.d);\n// 以十六进制打印:\nconsole.log('hex = ' + keyPair.d.toHex());\n// 补齐32位:\nconsole.log('hex = ' + keyPair.d.toHex(32));\n\nprivate key = 81014991225650123107594446292560463009575145732738301659501234189040494639354\nhex = b31cdc2d854be1d2047b0290d7b3039977bd29c036a62b63b2550c87d28cf8fa\nhex = b31cdc2d854be1d2047b0290d7b3039977bd29c036a62b63b2550c87d28cf8fa\n\n想要记住一个256位的整数是非常困难的，并且，如果记错了其中某些位，这个记错的整数仍然是一个有效的私钥，因此，比特币有一种对私钥进行编码的方式，这种编码方式就是带校验的Base58编码。\n对私钥进行Base58编码有两种方式，一种是非压缩的私钥格式，一种是压缩的私钥格式，它们分别对应非压缩的公钥格式和压缩的公钥格式。\n具体地来说，非压缩的私钥格式是指在32字节的私钥前添加一个0x80字节前缀，得到33字节的数据，对其计算4字节的校验码，附加到最后，一共得到37字节的数据：\n0x80           256bit             check\n┌─┬──────────────────────────────┬─────┐\n│1│              32              │  4  │\n└─┴──────────────────────────────┴─────┘\n计算校验码非常简单，对其进行两次SHA256，取开头4字节作为校验码。\n对这37字节的数据进行Base58编码，得到总是以5开头的字符串编码，这个字符串就是我们需要非常小心地保存的私钥地址，又称为钱包导入格式：WIF（Wallet Import Format）。可以使用wif这个库实现WIF编码：\nconst wif = require('wif');\n// 十六进制表示的私钥:\nlet privateKey = '0c28fca386c7a227600b2fe50b7cae11ec86d3bf1fbe471be89827e19d72aa1d';\n// 对私钥编码:\nlet encoded = wif.encode(\n        0x80, // 0x80前缀\n        Buffer.from(privateKey, 'hex'), // 转换为字节\n        false // 非压缩格式\n);\nconsole.log(encoded);\n\n5HueCGU8rMjxEXxiPuD5BDku4MkFqeZyd4dZ1jvhTVqvbTLvyTJ\n\n（如果你去验证了这个算法发现5HueC…解不出来0c28…，或者800c28…aa1d507a5b8d编码结果不是5HueC…，是因为你用的base58工具把你拼接的38字节数据看作字符串解析的，而实际上应该是看作16进制数值解析。私钥地址总是以5开头就是因为十六进制私钥以0x80开头base58结果总是5）\n另一种压缩格式的私钥编码方式，与非压缩格式不同的是，压缩的私钥格式会在32字节的私钥前后各添加一个0x80字节前缀和0x01字节后缀，共34字节的数据，对其计算4字节的校验码，附加到最后，一共得到38字节的数据：\n0x80           256bit           0x01 check\n┌─┬──────────────────────────────┬─┬─────┐\n│1│              32              │1│  4  │\n└─┴──────────────────────────────┴─┴─────┘\n对这38字节的数据进行Base58编码，得到总是以K或L开头的字符串编码。通过代码实现压缩格式的WIF编码如下：\nconst wif = require('wif');\n// 十六进制表示的私钥:\nlet privateKey = '0c28fca386c7a227600b2fe50b7cae11ec86d3bf1fbe471be89827e19d72aa1d';\n// 对私钥编码:\nlet encoded = wif.encode(\n        0x80, // 0x80前缀\n        Buffer.from(privateKey, 'hex'), // 转换为字节\n        true // 压缩格式\n);\nconsole.log(encoded);\n\nKwdMAjGmerYanjeui5SHS7JkmpZvVipYvB2LJGU1ZxJwYvP98617\n\n目前，非压缩的格式几乎已经不使用了。bitcoinjs提供的ECPair总是使用压缩格式的私钥表示：\nconst\n    bitcoin = require('bitcoinjs-lib'),\n    BigInteger = require('bigi');\nlet\n    priv = '0c28fca386c7a227600b2fe50b7cae11ec86d3bf1fbe471be89827e19d72aa1d',\n    d = BigInteger.fromBuffer(Buffer.from(priv, 'hex')),\n    keyPair = new bitcoin.ECPair(d);\n// 打印WIF格式的私钥:\nconsole.log(keyPair.toWIF());\n\nKwdMAjGmerYanjeui5SHS7JkmpZvVipYvB2LJGU1ZxJwYvP98617\n\n\n私钥小结：\n比特币的私钥本质上就是一个256位整数，对私钥进行WIF格式编码可以得到一个带校验的字符串。\n使用非压缩格式的WIF是以5开头的字符串，使用压缩格式的WIF是以K或L开头的字符串。\n\n 公钥\n比特币的公钥是根据私钥计算出来的。\n私钥本质上是一个256位整数，记作k。根据比特币采用的ECDSA算法，可以推导出两个256位整数，记作(x, y)，这两个256位整数即为非压缩格式的公钥。\n由于ECC曲线的特点，根据非压缩格式的公钥(x, y)的x实际上也可推算出y，但需要知道y的奇偶性，因此，可以根据(x, y)推算出x'，作为压缩格式的公钥。\n压缩格式的公钥实际上是，根据y的奇偶性在x前面添加02或03前缀，y为偶数时添加02，否则添加03，这样，得到一个1+32=33字节的压缩格式的公钥数据，记作x'，这个284位的整数作为压缩格式的公钥。\n注意压缩格式的公钥和非压缩格式的公钥是可以互相转换的，但均不可反向推导出私钥。\n非压缩格式的公钥目前已很少使用，原因是非压缩格式的公钥签名脚本数据会更长。\n我们来看看如何根据私钥推算出公钥：\nconst bitcoin = require('bitcoinjs-lib');\nlet\n    wif = 'KwdMAjGmerYanjeui5SHS7JkmpZvVipYvB2LJGU1ZxJwYvP98617',\n    ecPair = bitcoin.ECPair.fromWIF(wif); // 导入私钥\n// 计算公钥:\nlet pubKey = ecPair.getPublicKeyBuffer(); // 返回Buffer对象\nconsole.log(pubKey.toString('hex')); // 02或03开头的压缩公钥\n\n02d0de0aaeaefad02b8bdc8a01a1b8b11c696bd3d66a2c5f10780d95b7df42645c\n\n构造出ECPair对象后，即可通过getPublicKeyBuffer()以Buffer对象返回公钥数据。\n 地址\n要特别注意，比特币的地址并不是公钥，而是公钥的哈希，即从公钥能推导出地址，但从地址不能反推公钥，因为哈希函数是单向函数。\n以压缩格式的公钥为例，从公钥计算地址的方法是，首先对1+32=33字节的公钥数据进行Hash160（即先计算SHA256，再计算RipeMD160），得到20字节的哈希。然后，添加0x00前缀，得到1+20=21字节数据，再计算4字节校验码，拼在一起，总计得到1+20+4=25字节数据：\n0x00      hash160         check\n┌─┬──────────────────────┬─────┐\n│1│          20          │  4  │\n└─┴──────────────────────┴─────┘\n对上述25字节数据进行Base58编码，得到总是以1开头的字符串，该字符串即为比特币地址。使用JavaScript实现公钥到地址的编码如下：\nconst bitcoin = require('bitcoinjs-lib');\nlet\n    publicKey = '02d0de0aaeaefad02b8bdc8a01a1b8b11c696bd3d66a2c5f10780d95b7df42645c',\n    ecPair = bitcoin.ECPair.fromPublicKeyBuffer(Buffer.from(publicKey, 'hex')); // 导入公钥\n// 计算地址:\nlet address = ecPair.getAddress();\nconsole.log(address); // 1开头的地址\n\n1LoVGDgRs9hTfTNJNuXKSpywcbdvwRXpmK\n\n计算地址的时候，不必知道私钥，可以直接从公钥计算地址，即通过ECPair.fromPublicKeyBuffer构造一个不带私钥的ECPair即可计算出地址。\n要注意，对非压缩格式的公钥和压缩格式的公钥进行哈希编码得到的地址，都是以1开头的，因此，从地址本身并无法区分出使用的是压缩格式还是非压缩格式的公钥。\n以1开头的字符串地址即为比特币收款地址，可以安全地公开给任何人。\n仅提供地址并不能让其他人得知公钥。通常来说，公开公钥并没有安全风险。实际上，如果某个地址上有对应的资金，要花费该资金，就需要提供公钥。如果某个地址的资金被花费过至少一次，该地址的公钥实际上就公开了。\n私钥、公钥以及地址的推导关系如下：\n┌───────────┐      ┌───────────┐\n│Private Key│─────▶│Public Key │\n└───────────┘      └───────────┘\n      ▲                  │\n      │                  │\n      ▼                  ▼\n┌───────────┐      ┌───────────┐\n│    WIF    │      │  Address  │\n└───────────┘      └───────────┘\n\n对数字签名这一节做一个总结：\n1.数字签名应用于p2p交易过程，目的是对交易发起者（付款人）进行验证。数字签名的三个作用：防伪造，防篡改，防抵赖。\n2.比特币使用的数字签名算法是ECDSA\n3.比特币私钥本质上是一个256位随机整数，对私钥进行WIF格式编码可以得到一个带校验的字符串。私钥由持有者自行保管，一旦丢失不可找回，钱包再也无法找回\n4.比特币公钥根据私钥计算的来，算法是ECDSA（即非对称加密私钥计算公钥），本质上是两个256位整数，公钥有压缩和非压缩两种表示方法，可互相转换\n5.比特币的地址是公钥哈希的编码，并不是公钥本身，通过公钥可推导出地址；通过地址不可推导出公钥，通过公钥不可推导出私钥。\n\n 比特币钱包\n比特币钱包实际上就是帮助用户管理私钥的软件。因为比特币的钱包是给普通用户使用的，它有几种分类：\n\n本地钱包：是把私钥保存在本地计算机硬盘上的钱包软件，如Electrum；\n手机钱包：和本地钱包类似，但可以直接在手机上运行，如Bitpay；\n在线钱包：是把私钥委托给第三方在线服务商保存；\n纸钱包：是指把私钥打印出来保存在纸上；\n脑钱包：是指把私钥记在自己脑袋里。\n\n对大多数普通用户来说，想要记住私钥非常困难，所以强烈不建议使用脑钱包。\n作为用户，可以生成任意数量的私钥-公钥对，公钥是接收别人转账的地址，而私钥是花费比特币的唯一手段，钱包程序可以帮助用户管理私钥-公钥对。\n 交易\n我们再来看记录在区块链上的交易。每个区块都记录了至少一笔交易，一笔交易就是把一定金额的比特币从一个输入转移到一个输出：\n\n例如，小明把两个比特币转移给小红，这笔交易的输入是小明，输出就是小红。实际记录的是双方的公钥地址。\n如果小明有50个比特币，他要转给小红两个比特币，那么剩下的48个比特币应该记录在哪？比特币协议规定一个输出必须一次性花完，所以，小明给小红的两个比特币的交易必须表示成：\n\n小明给小红2个比特币，同时小明又给自己48个比特币，这48个比特币就是找零。所以，一个交易中，一个输入可以对应多个输出。\n当小红有两笔收入时，一笔2.0，一笔1.5，她想给小白转3.5比特币时，就不能单用一笔输出，她必须把两笔钱合起来再花掉，这种情况就是一个交易对应多个输入和1个输出：\n\n果存在找零，这笔交易就既包含多个输入也包含多个输出：\n\n在实际的交易中，输入比输出要稍微大一点点，这个差额就是隐含的交易费用，交易费用会算入当前区块的矿工收入中作为矿工奖励的一部分：\n\n计算出的交易费用：\n交易费用 &#x3D; 输入 - 输出 &#x3D; (2.0 + 1.5) - (2.99 + 0.49) &#x3D; 3.5 - 3.48 &#x3D; 0.02\n比特币实际的交易记录是由一系列交易构成，每一个交易都包含一个或多个输入，以及一个或多个输出。未花费的输出被称为UTXO（Unspent Transaction Ouptut）。\n当我们要简单验证某个交易的时候，例如，对于交易f36abd，它记录的输入是3f96ab，索引号是1（索引号从0开始，0表示第一个输出，1表示第二个输出，以此类推），我们就根据3f96ab找到前面已发生的交易，再根据索引号找到对应的输出是0.5个比特币，所以，交易f36abd的输入总计是0.5个比特币，输出分别是0.4个比特币和0.09个比特币，隐含的交易费用是0.01个比特币：\n\n再来解释一下上图，防止大家看不懂。上图中每一行都是一次交易，一次交易中input 必须来自过去发生的交易的output （直到追溯到Coinbase交易，即挖矿者创建的铸币交易），UTXO 的值索引过去的某一笔交易中的某个输出地址和钱数，比如说，f36abd此次交易的input 是交易3f96ab中输出给1mPvuPA的0.5个比特币，此次交易输出给16Gr9nB和1vg47TL各0.4，0.09个比特币，总输出比输入少了0.01个比特币，这些钱就是交易费用，给矿工了。\n\n交易小结\n比特币使用数字签名保证零信任的可靠P2P交易：\n\n私钥是花费比特币的唯一手段；\n钱包软件是用来帮助用户管理私钥；\n所有交易被记录在区块链中，可以通过公钥查询所有交易信息。\n\n\n 0x3 挖矿原理\n在比特币的P2P网络中，有一类节点，它们时刻不停地进行计算，试图把新的交易打包成新的区块并附加到区块链上，这类节点就是矿工。因为每打包一个新的区块，打包该区块的矿工就可以获得一笔比特币作为奖励。所以，打包新区块就被称为挖矿。\n比特币的挖矿原理就是一种工作量证明机制。工作量证明POW是英文Proof of Work的缩写。\n在讨论POW之前，我们先思考一个问题：在一个新区块中，凭什么是小明得到50个币的奖励，而不是小红或者小军？\n\n当小明成功地打包了一个区块后，除了用户的交易，小明会在第一笔交易记录里写上一笔“挖矿”奖励的交易，从而给自己的地址添加50个比特币。为什么比特币的P2P网络会承认小明打包的区块，并且认可小明得到的区块奖励呢？\n因为比特币的挖矿使用了工作量证明机制，小明的区块被认可，是因为他在打包区块的时候，做了一定的工作，而P2P网络的其他节点可以验证小明的工作量。\n 工作量证明\n比特币的工作量证明需要归结为计算机计算，也就是数学问题。工作量证明问题转为如何构造一个数学问题来实现工作量证明。\n在比特币网络中，矿工的挖矿也是一种工作量证明，要让计算机实现工作量证明，必须找到一种工作量算法，让计算机无法在短时间内算出来。这种算法就是哈希算法。\n通过改变区块头部的一个nonce字段的值，计算机可以计算出不同的区块哈希值：\n\n直到计算出某个特定的哈希值的时候，计算结束。这个哈希和其他的哈希相比，它的特点是前面有好几个0：\nhash256(block data, nonce&#x3D;0) &#x3D; 291656f37cdcf493c4bb7b926e46fee5c14f9b76aff28f9d00f5cca0e54f376f\nhash256(block data, nonce&#x3D;1) &#x3D; f7b2c15c4de7f482edee9e8db7287a6c5def1c99354108ef33947f34d891ea8d\nhash256(block data, nonce&#x3D;2) &#x3D; b6eebc5faa4c44d9f5232631f39ddf4211443d819208da110229b644d2a99e12\nhash256(block data, nonce&#x3D;3) &#x3D; 00aeaaf01166a93a2217fe01021395b066dd3a81daffcd16626c308c644c5246\nhash256(block data, nonce&#x3D;4) &#x3D; 26d33671119c9180594a91a2f1f0eb08bdd0b595e3724050acb68703dc99f9b5\nhash256(block data, nonce&#x3D;5) &#x3D; 4e8a3dcab619a7ce5c68e8f4abdc49f98de1a71e58f0ce9a0d95e024cce7c81a\nhash256(block data, nonce&#x3D;6) &#x3D; 185f634d50b17eba93b260a911ba6dbe9427b72f74f8248774930c0d8588c193\nhash256(block data, nonce&#x3D;7) &#x3D; 09b19f3d32e3e5771bddc5f0e1ee3c1bac1ba4a85e7b2cc30833a120e41272ed\n...\nhash256(block data, nonce&#x3D;124709132) &#x3D; 00000000fba7277ef31c8ecd1f3fef071cf993485fe5eab08e4f7647f47be95c\n比特币挖矿的工作量证明原理就是，不断尝试计算区块的哈希，直到计算出一个特定的哈希值，它比难度值要小。这里的小是说哈希值看作数值比目标小，举例来说如果目标的是前5位是零，那你找到的哈希值需要小于0x00000fffffff…这就是比特币挖矿的工作量证明方法。\n比特币使用的SHA-256算法可以看作对随机输入产生随机输出，这个随机很重要，就是说hash值的每一位都是随机的。例如，我们对字符串Hello再加上一个数字计算两次SHA-256，根据数字的不同，得到的哈希是完全无规律的256位随机数：\nhash256(&quot;Hello?&quot;) &#x3D; ????????????????????????????????????????????????????????????????\n大约计算16次，我们可以在得到的哈希中找到首位是0的哈希值，因为首位是0出现的概率是1/16：\nhash256(&quot;Hello1&quot;) &#x3D; ffb7a43d629d363026b3309586233ab7ffc1054c4f56f43a92f0054870e7ddc9\nhash256(&quot;Hello2&quot;) &#x3D; e085bf19353eb3bd1021661a17cee97181b0b369d8e16c10ffb7b01287a77173\n...\nhash256(&quot;Hello15&quot;) &#x3D; 0442e1c38b810f5d3c022fc2820b1d7999149460b83dc680abdebc9c7bd65cae\n如果我们要找出前两位是0的哈希值，理论上需要计算256次，因为00出现的概率是16^2=256，实际计算44次：\nhash256(&quot;Hello44&quot;) &#x3D; 00e477f95283a544ffac7a8efc7decb887f5c073e0f3b43b3797b5dafabb49b5\n如果我们要找出前6位是0的哈希值，理论上需要计算16^6=1677万次，实际计算1558万次：\nhash256(&quot;Hello15583041&quot;) &#x3D; 0000009becc5cf8c9e6ba81b1968575a1d15a93112d3bd67f4546f6172ef7e76\n每增加一个0，理论计算量将增加16倍。\n对于比特币挖矿来说，就是先给定一个难度值，然后不断变换nonce，计算Block Hash，直到找到一个比给定难度值低的Block Hash，就算成功挖矿。\n我们用简化的方法来说明难度，例如，必须计算出连续17个0开头的哈希值，矿工先确定Prev Hash，Merkle Hash，Timestamp，bits，然后，不断变化nonce来计算哈希，直到找出连续17个0开头的哈希值。我们可以大致推算一下，17个十六进制的0相当于计算了1617次，大约需要计算2.9万亿亿次。\n17个0 &#x3D; 16^17 &#x3D; 295147905179352825856 &#x3D; 2.9万亿亿次\n实际的难度是根据bits由一个公式计算出来，比特币协议要求计算出的区块的哈希值比难度值要小，这个区块才算有效。例如：\nDifficulty &#x3D; 402937298\n           &#x3D; 0x180455d2\n           &#x3D; 0x0455d2 * 2^(8 * (0x18 - 3))\n           &#x3D; 106299667504289830835845558415962632664710558339861315584\n           &#x3D; 0x00000000000000000455d2000000000000000000000000000000000000000000\n上述计算过程，0x180455d2是压缩的难度标记，末3字节作为底，前面0x18作为幂，做上述运算得到难度值。\n注意，难度值越小，说明哈希值前面的0越多，计算难度越大。\n 难度调整\n比特币网络的难度值是不断变化的，它的难度值保证大约每10分钟产生一个区块，而难度值在每2015个区块调整一次：如果区块平均生成时间小于10分钟，说明全网算力增加，难度值也会增加，如果区块平均生成时间大于10分钟，说明全网算力减少，难度值也会减少。因此，难度值随着全网算力的增减会动态调整。\n关于比特币挖矿难度与收益计算可以参考：https://zhuanlan.zhihu.com/p/28805231\n\n比特币设计时本来打算每2016个区块调整一次难度，也就是两周一次，但是由于第一版代码的一个bug，实际调整周期是2015个区块。\n\n根据比特币每个区块的难度值和产出时间，就可以推算出整个比特币网络的全网算力。\n比特币网络的全网算力一直在迅速增加。目前，全网算力已经超过了100EH/每秒，也就是大约每秒钟计算1万亿亿次哈希：\n\n所以比特币的工作量证明被通俗地称之为挖矿。在同一时间，所有矿工都在努力计算下一个区块的哈希。而挖矿难度取决于全网总算力的百分比。举个例子，假设小明拥有全网总算力的百分之一，那么他挖到下一个区块的可能性就是1%，或者说，每挖出100个区块，大约有1个就是小明挖的。\n由于目前全网算力超过了100EH/s，而单机CPU算力不过几M，GPU算力也不过1G，所以，单机挖矿的成功率几乎等于0。比特币挖矿已经从早期的CPU、GPU发展到专用的ASIC芯片构建的矿池挖矿。\n\n当某个矿工成功找到特定哈希的新区块后，他会立刻向全网广播该区块。其他矿工在收到新区块后，会对新区块进行验证，如果有效，就把它添加到区块链的尾部。同时说明，在本轮工作量证明的竞争中，这个矿工胜出，而其他矿工都失败了。失败的矿工会抛弃自己当前正在计算还没有算完的区块，转而开始计算下一个区块，进行下一轮工作量证明的竞争。\n为什么区块可以安全广播？因为Merkle Hash锁定了该区块的所有交易，而该区块的第一个Coinbase交易输出地址是该矿工地址。每个矿工在挖矿时产生的区块数据都是不同的，所以无法窃取别人的工作量。\n比特币总量被限制为约2100万个比特币，初始挖矿奖励为每个区块50个比特币，以后每4年减半。\n 共识算法\n如果两个矿工在同一时间各自找到了有效区块，注意，这两个区块是不同的，因为coinbase交易不同，所以Merkle Hash不同，区块哈希也不同。但它们只要符合难度值，就都是有效的。这个时候，网络上的其他矿工应该接收哪个区块并添加到区块链的末尾呢？答案是，都有可能。\n通常，矿工接收先收到的有效区块，由于P2P网络广播的顺序是不确定的，不同的矿工先收到的区块是有可能的不同的。这个时候，我们说区块发生了分叉：\n\n在分叉的情况下，有的矿工在绿色的分叉上继续挖矿，有的矿工在蓝色的分叉上继续挖矿：\n\n但是最终，总有一个分叉首先挖到后续区块，这个时候，由于比特币网络采用最长分叉的共识算法，绿色分叉胜出，蓝色分叉被废弃，整个网络上的所有矿工又会继续在最长的链上继续挖矿。\n由于区块链虽然最终会保持数据一致，但是，一个交易可能被打包到一个后续被孤立的区块中。所以，要确认一个交易被永久记录到区块链中，需要对交易进行确认。如果后续的区块被追加到区块链上，实际上就会对原有的交易进行确认，因为链越长，修改的难度越大。一般来说，经过6个区块确认的交易几乎是不可能被修改的。\n\n\n0x3 挖矿原理小结\n比特币挖矿是一种带经济激励的工作量证明机制\n工作量证明保证了修改区块链需要极高的成本，从而使得区块链的不可篡改特性得到保护\n比特币的网络安全实际上就是依靠强大的算力保障的\n比特币网络的难度值是不断变化的，它的难度值保证大约每10分钟产生一个区块\n最长分叉共识算法保证了区块链网络中只有一条合法链存在\n\n 0x4 更多细节\n通过上面的叙述，相信你已经理解了区块链的基本原理和比特币的交易方式。本节讲笼统的叙述一些细节上的概念和原理，不再展开感兴趣的话可以去廖雪峰老师的网站上了解细节。\n 可编程支付原理\n比特币的所有交易的信息都被记录在比特币的区块链中，任何用户都可以通过公钥查询到某个交易的输入和输出金额。当某个用户希望花费一个输出时，例如，小明想要把某个公钥地址的输出支付给小红，他就需要使用自己的私钥对这笔交易进行签名，而矿工验证这笔交易的签名是有效的之后，就会把这笔交易打包到区块中，从而使得这笔交易被确认。\n但比特币的支付实际上并不是直接支付到对方的地址，而是一个脚本，这个脚本的意思是：谁能够提供另外一个脚本，让这两个脚本能顺利执行通过，谁就能花掉这笔钱。\n比特币的脚本通过不同的指令还可以实现更灵活的功能。例如，多重签名可以让一笔交易只有在多数人同意的情况下才能够进行。最常见的多重签名脚本可以提供3个签名，只要任意两个签名被验证成功，这笔交易就可以成功。\n支付的本质\n从比特币支付的脚本可以看出，比特币支付的本质是由程序触发的数字资产转移。这种支付方式无需信任中介的参与，可以在零信任的基础上完成数字资产的交易，这也是为什么数字货币又被称为可编程的货币。\n由此催生出了智能合约：当一个预先编好的条件被触发时，智能合约可以自动执行相应的程序，自动完成数字资产的转移。保险、贷款等金融活动在将来都可以以智能合约的形式执行。智能合约以程序来替代传统的纸质文件条款，并由计算机强制执行，将具有更高的更低的信任成本和运营成本。\n 多重签名\n由比特币的签名机制可知，如果丢失了私钥，没有任何办法可以花费对应地址的资金。\n这样就使得因为丢失私钥导致资金丢失的风险会很高。为了避免一个私钥的丢失导致地址的资金丢失，比特币引入了多重签名机制，可以实现分散风险的功能。\n具体来说，就是假设N个人分别持有N个私钥，只要其中M个人同意签名就可以动用某个“联合地址”的资金。\n\n以3开头的地址就是比特币的多重签名地址，但从地址本身无法得知签名所需的M/N。\n\n UTXO模型\n比特币的区块链由一个个区块串联构成，而每个区块又包含一个或多个交易。\n如果我们观察任何一个交易，它总是由若干个输入（Input）和若干个输出（Output）构成，一个Input指向的是前面区块的某个Output，只有Coinbase交易（矿工奖励的铸币交易）没有输入，只有凭空输出。所以，任何交易，总是可以由Input溯源到Coinbase交易。\n还没有被下一个交易花费的Output被称为UTXO：Unspent TX Output，即未花费交易输出。给定任何一个区块，计算当前所有的UXTO金额之和，等同于自创世区块到给定区块的挖矿奖励之和。\n钱包的当前余额总是钱包地址关联的所有UTXO金额之和。\n如果刚装了一个新钱包，导入了一组私钥，在钱包扫描完整个比特币区块之前，是无法得知当前管理的地址余额的。\n那么，给定一个地址，要查询该地址的余额，难道要从头扫描几百GB的区块链数据？\n当然不是。\n要做到瞬时查询，我们知道，使用关系数据库的主键进行查询，由于用了索引，速度极快。\n因此，对区块链进行查询之前，首先要扫描整个区块链，重建一个类似关系数据库的地址-余额映射表。这个表的结构如下：\n\n\n\naddress\nbalance\nlastUpdatedAtBlock\n\n\n\n\naddress-1\n50.0\n0\n\n\n\n一开始，这是一个空表。每当扫描一个区块的所有交易后，某些地址的余额增加，另一些地址的余额减少，两者之差恰好为区块奖励：\n\n\n\naddress\nbalance\nlastUpdatedAtBlock\n\n\n\n\naddress-1\n50.0\n0\n\n\naddress-2\n40.0\n3\n\n\naddress-3\n50.0\n3\n\n\naddress-4\n10.0\n3\n\n\n\n这样，扫描完所有区块后，我们就得到了整个区块链所有地址的完整余额记录，查询的时候，并不是从区块链查询，而是从本地数据库查询。大多数钱包程序使用LevelDB来存储这些信息，手机钱包程序则是请求服务器，由服务器查询数据库后返回结果。\n总而言之，重建整个地址-余额数据库需要扫描整个区块链，并按每个交易依次更新记录，即可得到当前状态，每当新的块加入区块链，数据库也同时更新。此后，查询余额只需要从数据库查询即可。\n 0x5 Q&amp;A\n\n\nQ：区块链的安全性由哪些因素决定？\n我认为区块链的安全性主要由以下几部分决定：\n数字签名安全性：采取的ECDSA或其他非对称加密算法的安全性\n工作量证明机制安全性：比如比特币网络中，hash算法的防止碰撞的强度\n区块链网络安全性：存在足够多的“矿工”节点，保证区块不断更新\n\n\nQ：区块链中每个区块都包含那些元素，这些元素是怎么得来的，作用分别是什么？\n以比特币网络为例主要包括：\n父区块hash：=hash(父区块)\nMerkle根：按序hash区块体中的记录\n难度目标：工作量证明机制的目标，计算方法参考0x3-工作量证明\n时间戳：做工作量证明之前估计一个值就可以，要比上一个区块大\nnonce：需要不断变化，计算区块hash，以证明工作量\n区块大小：如名\n区块体：交易记录\n\n\nQ：p2p交易如比特币交易，每个块能存储多少笔交易？\n目前一个比特币区块中约有2000-3000笔交易。我的理解是：具体多少是由比特币网络规定的保证约10分钟打包一个区块，这10分钟产生的交易数量决定的。\n\n\nQ：挖矿的工作量整明，实际上是怎么做的？\n实际上就是通过变换区块头中的nonce的值，计算整个区块的hash值，要求hash值前n位是0，如果计算出来了，你就获得了这个区块的打包权力，就可以向区块链网络发出广播，你挖了一个新区块，这个区块中有一笔交易是给你的钱包添加一笔挖矿奖励，来源就是是挖矿。\nn是难度的象征，比特币网络每挖出2015个区块进行一次难度调整，维持10分钟一个区块的难度。\n\n\n参考文章：\n\n廖雪峰区块链教程：https://www.liaoxuefeng.com/wiki/1207298049439968\nSatoshi Nakamoto：Bitcoin: A Peer-to-Peer Electronic Cash System https://bitcoin.org/bitcoin.pdf\nhttps://www.bitmain.com/\n阮一峰数字签名是什么：http://www.ruanyifeng.com/blog/2011/08/what_is_a_digital_signature.html\n比特币挖矿难度与收益计算：https://zhuanlan.zhihu.com/p/28805231\nhttps://www.528btc.com/college/58842.html\n\n","categories":["笔记"],"tags":["区块链","比特币"]},{"title":"Transformer","url":"/Transformer/","content":"\n Transformer\n\nTransformer 现在有一个非常有名的应用就是BERT，这一节还不会讲到BERT，我们先讲Transformer。BERT就是无监督train 的Transformer。Transformer是一个seq2seq model，之前讲的课程中教过seq2seq model 所以这里就假设大家都懂这个东西。Transformer它特别的地方就是在seq2seq model中大量使用了&quot;self attention&quot;这种layer。接下来我们要讲的就是&quot;self attention&quot;这种layer 它具体是在做什么。\n Sequence\n一般遇到处理sequence 的问题最常想到的model 就是RNN，无论是单向还是双向总之RNN就是比较适合处理输入是sequence 的问题。\n\nRNN的架构就如上图左侧所示（BiDirectional），它的输入和输出都是一个vector 。在单向RNN中在输出b4的时候a1-a4它都看过了，在输出b3的时候a1-a3它都看过了，在双向RNN中，在输出任意一个b的时候，所有输入它都要看过，所以它的问题是不好做平行运算的，因为他计算后一个输入的时候要依赖前一个输入。\n为了解决这个问题，有人提出了用CNN替代RNN的想法，如上图右侧所示。每个三角形代表一个filter，它就吃输入的一小段，输出一个数值，filter扫过输入产生一排输出，即上图一排红色的点，还会有其他的filter 扫过输入产生一排输出，即上图黄色的点…这些filter 可以平行计算。那你就会说了，CNN它没有考虑所有输入，RNN是看过所有输入才计算出输出，而CNN只看了一部分呀。只看一层的话确实是这样，但是我们可以叠很多层CNN（如蓝色三角示例），这样上层的CNN就会看到更多的输入。\n总而言之，CNN确实可以平行计算了，但是它需要叠很多层才能看到全部输入的信息，那有没有更好的办法呢，接下来就介绍Self Attention 。\n Self Attention\nSelf Attention 就是想要取代RNN 做的事情，并且能克服RNN不能平行计算的缺点。下面的内容如果你看不下去，你就只要记得Self Attention 能做到和RNN 一摸一样的事情，它也是吃一个sequence 吐出一个sequence ，输出sequence 的每个元素都是看过所有输入计算出来的，而且是平行计算出来的。\n\n关键就是你可以在你要做的事情上用Self Attention 取代RNN 。\n\n\n最早提出self-attention 替换RNN 的paper（by google）：Attention Is All You Need\nhttps://arxiv.org/abs/1706.03762\n\n输入是 x1x_1x1​ 到 x4x_4x4​ 的sequence ，输入x通过一个matrix W 做embedding 得到sequence a 。（注意 xix_ixi​ 和 aia^iai 应该都是vector，列向量） 然后每个 aia^iai 都乘上三个不同的matrix 得到三个输出向量q k v （列向量）。\nq代表query 它是要去match 别人的\nk代表key 它是要被match的\nv代表information 就是要被抽取出来的信息\n接下来我们就拿每个q对每个k做attention ：\n\nattention 怎么做呢？（根据google论文中所述这里讲解的attention function是类似于dot-proudct attention）self-attention layer 它要做的就是拿 q1q^1q1 和 k1k^1k1 做attention，得到 α1,1\\alpha_{1,1}α1,1​ ，如上图所示同样的做法得到 α1,2\\alpha_{1,2}α1,2​ ，α1,3\\alpha_{1,3}α1,3​ ，α1,4\\alpha_{1,4}α1,4​ 。attention 的具体公式就是上图所示的，对于 α1,i\\alpha_{1,i}α1,i​ attention就是指把 q1q^1q1 点积 kik^iki 然后除以根号d，d是q和k的维度。（α\\alphaα 是数值）\n为什么除以根号d，而不是除以其他东西呢？在google 的Self-attention 的paper 中3.2.1节attention公式下有解释，你可以自己去看看，大意好像就是为了避免dot-product attention将softmax函数推入梯度极小的区域，我也解释不太好😥。老师也没有实践这个除以根号d对结果有多大影响。\n另外，你还需要知道attention function显然不止一种，paper中另外有提到addictive attention，不过没有实践替换attention function 会对结果有多大影响。\n\n然后，你要把上述得到的 α\\alphaα 通过一个softmax layer 得到 α^\\hat\\alphaα^ ，softmax layer做的事情就是上图公式所示，相当于做一个normalization 。\n\n得到 α^\\hat\\alphaα^ 以后要把每个 α^\\hat\\alphaα^ 和对应的v 相乘，再求和就得到 b1b^1b1 。我们上面说self-attention 也是输入一个sequence 输出一个sequence，这个 b1b^1b1 就是输出的第一个element 。之后我们只要用同样的办法求出 b2b^2b2 ， b3b^3b3 ， b4b^4b4  就结束了。这个过程中每个output element 的计算都是独立的，都不依赖其他的output element 所以可以并行计算。\n我们再看一下上图中self-attention layer 的结构，对于 b1b^1b1 来说它看过了整个input sequence ，而且我们也可以让它看部分的input sequence，只要把不希望它看的部分产生的 α\\alphaα 的值设为0就可以了。所以使用self-attention layer 对于output sequence 中的element 来说它可以看任意多个任意位置的input sequence 中的element 。\n\nb2b^2b2 的计算也是一样的，如上图所示，q2q^2q2 和 kik^iki 做attention 得到对应的 α2,i\\alpha_{2,i}α2,i​ 然后做softmax 得到 α^2,i\\hat\\alpha_{2,i}α^2,i​ 再乘 viv^ivi 做summation 就得到 b2b^2b2 。以此类推平行计算所有b：\n\n如果你觉得上面的讲解比较乱，看不懂，你可以只记住self-attention layer 的输入输出就好。\n接下来我们要具体讲解一下attention 中的一连串矩阵运算，为什么是容易被平行计算，为什么是容易被加速的。\n Why Could Speed Up\n\n首先来看列向量q、k、v的计算方法，根据上面说过的，如上图右上角所示，我们可以把列向量 aia^iai 排在一起，形成一个矩阵 III 。将四个q、k、v的计算转换成矩阵运算。\n\n再来看数值 α\\alphaα 的计算，以 α1,i\\alpha_{1,i}α1,i​ 为例，我们可以把k和q的内积转换为 kTk^TkT 与 qqq 的积，进一步可以把四个 α1,i\\alpha_{1,i}α1,i​ 的运算转换为上图右下角的矩阵乘向量。更进一步，对于 α2,i\\alpha_{2,i}α2,i​ 也是一样的，所以得到了下述公式：\n\nmatrix A 中每个元素都是我们要求的值，A可以转换为matrix 乘积 KTQK^T QKTQ 。再把A中每一列元素做一下softmax 就能得到 A^\\hat AA^ 。\n\n接着，把列向量 v1,v2,v3,v4v^1,v^2,v^3,v^4v1,v2,v3,v4 分别乘以 α1,1^,α1,2^,α1,3^,α1,4^\\hat{\\alpha_{1,1}},\\hat{\\alpha_{1,2}},\\hat{\\alpha_{1,3}},\\hat{\\alpha_{1,4}}α1,1​^​,α1,2​^​,α1,3​^​,α1,4​^​ 再加起来就得到 b1b^1b1 ，对于其他的b也是一样的，就这样结束。\n我们再来总的看一下从input III 到output OOO 经历了哪些步骤：\n\n Multi-head Self-attention\n接下来讲一个Self-attention 的变形 (2 heads as example)\n\n在这种方法中q、k、v都会有多个，生乘的输出也会有多组，这里以2 head 举例所以画了两个，具体的做法可以是你把 qiq^iqi 乘一个matrix Wq,1W^{q,1}Wq,1 得到 qi,1q^{i,1}qi,1 ，乘一个matrix Wq,2W^{q,2}Wq,2 得到 qi,2q^{i,2}qi,2 。然后你在做attention 的时候就是每个head 生乘的q、k、v只在本组内做运算，举例来说如上图所示，如果你要计算head 1组的 bi,1b^{i,1}bi,1 你就用  aia^iai 生成的 qi,1,ki,1,vi,1q^{i,1},k^{i,1},v^{i,1}qi,1,ki,1,vi,1 和 aja^jaj 生成的同head组的 qj,1,kj,1,vj,1q^{j,1},k^{j,1},v^{j,1}qj,1,kj,1,vj,1 做计算。对于head 2组的 bi,2b^{i,2}bi,2：\n\n输出的两组b会concatenate 起来，如果你不希望b的维度增加，你可以用下图所示的方法做一下降维：\n\nmulti-head的好处是，不同的head 可以考虑不同的事情。比如说有的head 只需要近期的信息，所以只看local 的信息，有的head 需要长时间以前的信息，看global 的信息。\n Positional Encoding\n\nself-attention layer 它没有考虑input sequence 的顺序，上面我们说了，对于self-attention 的输出可以看任意数量任意位置的input，所以对它来说没有顺序的概念，也就是说你今天输入&quot;我吃饭了&quot;和&quot;饭吃我了&quot;对它来说可能是完全一样的，我们显然不希望是这样。\nPositional Encoding就是要解决这个问题，做法就是在列向量 aia^iai 上加上一个标识位置的列向量 eie^iei 这个 eie^iei 在原始paper 中是人设定的，位置1就是 e1e^1e1 位置2就是 e2e^2e2… 你可能会问为什么是加，为什么不concatenate起来，你把一个向量加上去那原来的信息不就变了吗，concatenate不是会更好吗。\n为了让你理解这个做法，这里老师用另一种方式来讲解，假如说我们在x都后面concatenate 一个表示位置的one-hot向量p，然后再做本来做的那个乘矩阵W转换为a的步骤，如果我们把W拆分成 WI,WPW^I,W^PWI,WP 来看待，根据线性代数的知识，这个公式就可以拆分成上图所示的两两相乘再相加样子， WIxiW^I x^iWIxi 就是 aia^iai ， WPpiW^P p^iWPpi 就是 eie^iei 。这样是不是就好理解了。\n让人难以理解的是你当然可以learn WPW^PWP ，在self-attention 的paper 中有提到以前用CNN做seq2seq model 的时候就有人试过learn WPW^PWP 了，结果没有更好，他们的 WPW^PWP 是用一个奇怪的公式产生出来的，它就长这个样子：\n\n Seq2seq with Attention\n上面讲的是在Seq2seq model 中我们可以用self-attention 取代RNN ，接下来我们就来讲在Seq2seq model 中self-attention 是怎么应用的。\n\n\nReview: https://www.youtube.com/watch?v=ZjfjPzXw6og&amp;feature=youtu.be\n\n\n\nsource:https://ai.googleblog.com/2017/08/transformer-novel-neural-network.html\n\n这是google 用self-attention 做的翻译模型的gif演示。首先每个step 会把input sequence 两两之间做attention，然后弹出一排一排的小点是告诉你这些step 是平行运算的，每一层都做self-attention，接着做Decoding ，会对input 的encoder做attention，在decode 第二个word 的时候它就不只对input 做attention 还会对之前产生的东西做attention 。\n Transformer\n\n上图是一个seq2seq model ，做的任务是中英互译，左半部是encoder 右半部是decoder，我们以中翻英为例，输入&quot;机器学习&quot;，encoder 进行encoding 然后输入给deocder ，decoder 吃进去产生输出，machine 然后把machine 当作输入再吃进去，在产生learning ，如此运行知道输出句末标志结束。\n下面来介绍一下self-attention 在这个模型中的具体运作方式：\n\n\nLayer Norm: https://arxiv.org/abs/1607.06450\nBatch Norm: https://www.youtube.com/watch?v=BZh1ltr5Rkg\n\n先看encoder，input 经过embedding 和 positional encoding 以后进入灰色的block，这部分会重复N次。在这个灰色的block中，先是一个multi-head attention layer；下一个layer是Add&amp;Norm，意思是说把multi-head attention layer的input和output加起来，再把得到的b′b&#x27;b′做Layer Normalization；（如果要进一步了解Layer Norm可以参考上述文献）\n我们之前讲过Batch Norm，Batch Norm是说我们希望一个Batch的data中每个维度的mean=0，variance=1；Layer Norm是不需要考虑Batch 的，举例来说给一笔Data 我们希望它所有维度的mean=0，variance=1。Layer Norm一般会搭配RNN使用，transformer 很像RNN，这可能就是Layer Norm用在这里的原因。\n回到decoder，再往后，就如图说是，进入一个前馈神经网络，然后在跟一个Add&amp;Norm。\n再看Deocder，Deocder的输入是前一个step 产生的output，前面的处理都是一样的，灰色的block会重复N次，其中上来就是一个Masked Multi-head Attention，这个Masked意思是在做self-attention的时候decoder 只会attend已经产生的sequence，这也是很合理嘛，毕竟还没有产生出来的东西你怎么做self-attention。接着是一个Multi-head Attention 这个layer attend之前encoder 的输出，接着还有一个Add&amp;Norm，再后面的东西也是和encoder 一样的不再赘述。\n Attention Visualization\n\n\nhttps://arxiv.org/abs/1706.03762\n\n这是google的paper中最终版本得到的结果，所有单词两两之间都会有attention，颜色越深表示attention的weight越大。\n\n\nThe encoder self-attention distribution for the word “it” from the 5th to the 6th layer of a Transformer trained on English to French translation (one of eight attention heads).\nhttps://ai.googleblog.com/2017/08/transformer-novel-neural-network.html\n\n上图是这个意思，左边输入的句子是这只动物没走过街道，因为它太累了，机器学到了it指代的是animal，这两个词之间的attention很深，而当我们把之后的tired换成wide，句子变成这只动物没走过街道，因为它太宽了，机器就学到it是指代街道。amazing\n Multi-head Attention\n\n上如图所示是两组q、k、v，也就是两个head 做出的结果，显然上面就偏向于attend全局信息，下面就偏向于attend局部信息。\n Example Appplication\n\nTransformer 可以用在哪里呢？基本上原来用seq2seq model 做的任务都可以换成transformer，现在这些任务基本上已经被洗过一轮了，都被做干了。\n\nhttps://arxiv.org/abs/1801.10198\n\n比较惊人的是做Summarizer，这篇文章是google 做的，他们的input是一堆文章，然后写出这堆文章的总结和摘要，要求有wiki的风格，所以输出也是一篇文章。比如说你搜索台湾大学，把google出来的文章都作为input，机器就会写一个台湾大学的wiki。这个任务的训练资料是很多的，出现transformer之前，大概是做不起来的。\n Universal Transformer\n\n\nhttps://ai.googleblog.com/2018/08/moving-beyond-translation-with.html\n\n出现Transformer后提出的，时间上换成transformer，纵向深度上仍然用RNN，具体细节可以参考上述链接。\n Self-Attention GAN\n\n\nhttps://arxiv.org/abs/1805.08318\n\ntransformer 最早提出来是用在文字上，现在它也可以被用在影像上，举例来说，有一个Self-attention GAN ，你在处理影像的时候可以让每个pixel 都去attention 其他的pixel ，所以你在处理影像的时候可以考虑到比较global 的信息。\n","categories":["李宏毅机器学习笔记"],"tags":["ML","note","Next-Step-of-ML"]},{"title":"符号执行技术笔记","url":"/%E7%AC%A6%E5%8F%B7%E6%89%A7%E8%A1%8C%E6%8A%80%E6%9C%AF%E7%AC%94%E8%AE%B0/","content":" 符号执行技术笔记\n此前没有接触过符号执行，所以本文是在我学习过程中对一些好的文章的摘录，算是方便我日后回顾学习，特别感谢r1ce和K0rz3n的文章，所有参考连接将放在文末。如果我后续研究到符号执行技术相关的内容大概会补充进来。文末有一些我读文章时候的疑问，读完本文再看，可能有助于你的理解。\n 0x1 通俗地解释符号执行\nWiki中的定义是：在计算机科学中，符号执行技术指的是通过程序分析的方法，确定哪些输入向量会对应导致程序的执行结果为某个向量的方法(绕)。通俗的说，如果把一个程序比作DOTA英雄，英雄的最终属性值为程序的输出（包括攻击力、防御力、血槽、蓝槽），英雄的武器出装为程序的输入（出A杖还是BKB）。那么符号执行技术的任务就是，给定了一个英雄的最终属性值，分析出该英雄可以通过哪些出装方式达到这种最终属性值效果。\n可以发现，符号执行技术是一种白盒的静态分析技术。即分析程序可能的输入需要能够获取到目标源代码的支持。同时，它是静态的，因为并没有实际的执行程序本身，而是分析程序的执行路径。如果把上述英雄的最终属性值替换成程序形成的bug状态，比如，存在数组越界复制的状态，那么，我们就能够利用此技术挖掘漏洞的输入向量了。\n可以发现，符号执行技术是一种白盒的静态分析技术。即，分析程序可能的输入需要能够获取到目标源代码的支持。同时，它是静态的，因为并没有实际的执行程序本身，而是分析程序的执行路径。如果把上述英雄的最终属性值替换成程序形成的bug状态，比如，存在数组越界复制的状态，那么，我们就能够利用此技术挖掘漏洞的输入向量了。\n这里再举一个简单的例子，让大家有深刻的理解。\n以下面的源代码为例子：\nint m&#x3D;M, n&#x3D;N, q&#x3D;Q; \nint x1&#x3D;0,x2&#x3D;0,x3&#x3D;0;\nif(m!&#x3D;0)\n&#123;\n    x1&#x3D;-2;\n&#125;\nif(n&lt;12)\n&#123;\n    if(!m &amp;&amp; q)\n    &#123;\n        x2&#x3D;1;\n    &#125;\n    x3&#x3D;2;\n&#125;\nassert(x1+x2+x3!&#x3D;3) &#x2F;&#x2F; if &#x3D;3 then program crash\n上述代码是一个简单的c语言分支结构代码，它的输入是M,N,Q三个变量；输出是x1,x2,x3的三个变量的和。我们这里设置的条件是想看看什么样的输入向量&lt;M,N,Q&gt;的情况下，得到的三个输出变量的和等于3.\n那么我们通过下面的树形结构来看看所有的情况：\n\n上面的分析图把所有可能的情况都列举出来了，其中，叶子节点显示的数值表示当前输入情况下，可以得到的数值。（比如，如果英雄出装是M^(N&lt;12)，那么最终的属性值R=0）。其中M^(N&lt;12)表达的是，M是非零值且N要小于12，Q为任意值的情况下，得到R=0。可以发现，当条件为~M^(N&lt;5)^Q时，得到了最终结果等于3.即，我们通过这种方式逆向发现了输入向量。如果把结果条件更改为漏洞条件，理论上也是能够进行漏洞挖掘了。\n对于如何根据最终得到的结果求解输入向量，已经有很多现成的数学工具可以使用。上述问题其实可以规约成约束规划的求解问题（更详细的介绍看这里：Constraint_programming ）。比较著名的工具比如SMT（Satisfiability Modulo Theory，可满足性模理论）和SAT。\n但是在实际的漏洞分析过程中，目标程序可能更加复杂，没有我们上面的例子这么简单。实际的程序中，可能包含了与外设交互的系统函数，而这些系统函数的输入输出并不会直接赋值到符号中，从而阻断了此类问题的求解。\n比如下面的这个包含了文件读写的例子：\nint main(int argc, char* argv[])\n&#123;\n    FILE *fop &#x3D; fopen(&quot;test.txt&quot;);\n    ...\n    if(argc &gt; 3)\n    &#123;\n        fputs(&quot;Too many parameters, exit.&quot;, fop);\n    &#125;\n    else\n    &#123;\n        fputs(&quot;Ok, we will run normally.&quot;, fop);\n    &#125;\n    ...\n    output &#x3D; fgets(..., fop);\n    assert(!strcmp(output, &quot;Ok, we will run normally.&quot;)); &#x2F;&#x2F; if str is &quot;OK,...&quot; then program crash\n    return 0;\n&#125;\n上述示例代码中，想要发现什么情况下会得到输出”Ok, we will run normally.”这个字符串。通过一系列的执行到if语句，此时，根据程序输入的参数个数将会产生两个分支。分支语句中将执行系统的文件写操作。**在传统的符号执行过程中，此类函数如果继续沿着系统函数的调用传递下去的话，符号数值的传递将会丢失。**而在之后的output = fgets(…, fop);这行代码中，符号从外部获得的数值也将无法正常的赋值到output中。因此，符号执行无法求解上述问题，因为在调用系统函数与外设交互的时候，符号数值的赋值过程被截断了。\n\n为了解决这个问题，最经典的项目就是基于LLVM的KLEE（klee)它把一系列的与外设有关的系统函数给重新写了一下，使得符号数值的传递能够继续下去。从比较简化的角度来说，就是把上面的fputs函数修改成，字符串赋值到某个变量中，比如可以是上面的fop里面。再把fgets函数修改成从某个变量获取内容，比如可以是把fop的地址给output。这样，就能够把符号数值的传递给续上。当然，这里举的例子是比较简单的例子，实际在重写函数的时候，会要处理更复杂的情况。在KLEE中，它重新对40个系统调用进行了建模，比如open, read, write, stat, lseek, ftruncate, ioctl。感兴趣的读者可以进一步阅读他们发表在OSDI2008年的论文（KLEE-OSDI08)他们的文章深入浅出，非常适合学习。\n 0x2 从公式原理上理解符号执行\n符号执行的关键思想就是，把输入变为符号值，那么程序计算的输出值就是一个符号输入值的函数。这个符号化的过程在上一篇AEG文章中已有简要阐述，简而言之，就是一个程序执行的路径通常是true和false条件的序列，这些条件是在分支语句处产生的。在序列的第i位置如果值是true，那么意味着第i个条件语句走的是then这个分支；反之如果是false就意味着程序执行走的是else分支。\n那么，如何形式化地表示符号执行的过程呢？程序的所有执行路径可以表示为树，叫做执行树。接下来我们就以一个例子来阐述通过符号执行遍历程序执行树的过程。\nint twice(int v)&#123;\n\treturn 2*v;\n&#125;\n\nvoid testme(int x, int y)&#123;\n\tz &#x3D; twice(y); \n\tif(z &#x3D;&#x3D; x)&#123;\n\t\tif(x &gt; y+10)\n\t\t\tERROR;\n\t\t&#125;\n\t&#125;\n&#125;\n\n&#x2F;* simple driver exercising testme() with sym inputs *&#x2F;\nint main()&#123;\n    x &#x3D; sym_input();\n    y &#x3D; sym_input();\n    testme(x, y);\n    return 0;\n&#125;\n\n代码中，testme()函数有3条执行路径，组成了上图中的执行树。直观上来看，我们只要给出三组输入就可以遍历这三个路径，即图中圆圈中的x和y取值。符号执行的目标就是能够生成这样的输入集合，在给定的时间内探索所有的路径。\n为了形式化地完成这个任务，符号执行会在全局维护两个变量。其一是符号状态 σ\\sigmaσ ，它表示的是一个从变量到符号表达式的映射。其二是符号化路径约束PC（或者叫路径条件），这是一个==无量词的一阶公式==，用来表示路径条件。在符号执行的开始，符号状态 σ\\sigmaσ 会先初始化为一个空的映射，而符号化路径约束PC初始化为true。 σ\\sigmaσ 和PC在符号执行的过程中会不断更新。在符号执行结束时，PC就会用约束求解器进行求解，以生成实际的输入值。这个实际的输入值如果用程序执行，就会走符号执行过程中探索的那条路径，即此时PC的公式所表示的路径。\n我们以上图的例子来阐述这个过程。当符号执行开始时，符号状态 σ\\sigmaσ 为空，符号路径约束PC为true。当我们遇到一个读语句，形式为var=sym_input()，即接收程序输入，符号执行就会在符号状态 σ\\sigmaσ 中加入一个映射 var→svar \\rightarrow svar→s ，这里s就是一个新的未约束的符号值。上述代码中，main()函数的前两行会得到结果 σ={x→x0,y→y0}\\sigma=\\lbrace x \\rightarrow x_0, y \\rightarrow y_0\\rbraceσ={x→x0​,y→y0​} ，其中 x0x_0x0​ 和 y0y_0y0​ 是两个初始的未约束的符号化值。\n当我们遇到一个赋值语句，形式为v=e，符号执行就会将符号状态 σ\\sigmaσ 更新，加入一个v到 σ(e)\\sigma(e)σ(e) 的映射，其中 σ(e)\\sigma(e)σ(e) 就是在当前符号化状态计算e得到的表达式。例如，在代码执行完第6行时， σ={x→x0,y→y0,z→2y0}\\sigma=\\lbrace x \\rightarrow x_0, y \\rightarrow y_0, z \\rightarrow 2y_0\\rbraceσ={x→x0​,y→y0​,z→2y0​} 。\n当我们遇到条件语句if(e) S1 else S2，PC会有两个不同更新。首先是PC更新为 PC∧σ(e)PC \\wedge \\sigma(e)PC∧σ(e) ，这就表示then 分支；然后是建立一个新的路径约束PC’，初始化为 PC∧¬σ(e)PC \\wedge \\neg \\sigma(e)PC∧¬σ(e) ，这就表示else分支。如果PC是可满足的，即可以给符号分配一些实际值，那么程序执行就会走then 分支，此时的状态为：符号状态 σ\\sigmaσ 和符号路径约束PC。同样，如果PC’是可满足的，那么会建立另一个符号执行实例，其符号状态为 σ\\sigmaσ ，符号路径约束为PC’，走else分支（请注意，与具体执行不同，符号执行技术可以同时使用两个分支，从而产生两个执行路径）。如果PC和PC’都不能满足，那么执行就会在对应路径终止。例如，第7行建立了两个不同的符号执行实例，路径约束分别是 x0=2y0x_0=2y_0x0​=2y0​ 和 x0≠2y0x_0 \\not= 2y_0x0​​=2y0​ 。在第8行，又建立了两个符号执行实例，路径约束分别是 (x0=2y0)∧(x0&gt;y0+10)(x_0=2y_0) \\wedge (x_0 &gt; y_0 + 10)(x0​=2y0​)∧(x0​&gt;y0​+10) 和 (x0=2y0)∧(x0≤y0+10)(x_0=2y_0) \\wedge (x_0 \\leq y_0 + 10)(x0​=2y0​)∧(x0​≤y0​+10) .\n如果符号执行遇到了exit语句或者错误（指的是程序崩溃、违反断言等），符号执行的当前实例会终止，利用约束求解器对当前符号路径约束赋一个可满足的值，而可满足的赋值就构成了测试输入：如果程序执行这些实际输入值，就会在同样的路径结束。例如，在上图例子中，经过符号执行的计算会得到三个测试输入: {x=0,y=1}\\lbrace x=0,y=1\\rbrace{x=0,y=1}, {x=2,y=1}\\lbrace x=2,y=1\\rbrace{x=2,y=1}, {x=30,y=15}\\lbrace x=30,y=15\\rbrace{x=30,y=15} .\nvoid testme_inf() &#123;          \n    int sum &#x3D; 0;\n    int N &#x3D; sym_input();      \n    while (N &gt; 0) &#123;\n        sum &#x3D; sum + N\n        N &#x3D; sym_input();\n    &#125;\n&#125;\n当我们遇到了循环和递归应该怎么办呢？如果循环或递归的终止条件是符号化的，包含循环和递归的符号执行会导致无限数量的路径。比如上面代码中这个例子，这段代码就有无数条执行路径，每条路径的可能性有两种：要么是任意数量的true加上一个false结尾，要么是无穷多数量的true。我们形式化地表示包含n个true条件和1个false条件的路径，其符号化约束如下：\n(∧i∈[1,n]Ni&gt;0)∧(Nn+1≤10)(\\wedge_{i \\in [1,n]}N_i &gt; 0) \\wedge (N_{n+1} \\leq 10)\n(∧i∈[1,n]​Ni​&gt;0)∧(Nn+1​≤10)\n其中每个 NiN_iNi​ 都是一个新的符号化值，执行结尾的符号状态是：\n{N→Nn+1,sum→∑i∈[1,n]Ni}\\Big\\lbrace N \\rightarrow N_{n+1}, sum \\rightarrow \\sum_{i \\in [1,n]} N_i \\Big\\rbrace\n{N→Nn+1​,sum→i∈[1,n]∑​Ni​}\n其实这就是符号执行面临的问题之一，即如何处理循环中的无限多路径。在实际中，有一些方法可以应对，比如对搜索加入限制，要么是限制搜索时间的长短，要么是限制路径数量、循环迭代次数、探索深度等等。\n还需要考虑到的一个问题就是，如果符号路径约束包含不能由求解器高效求解的公式怎么办？比如说，如果原本的代码发生变化，把twice函数替换为下述语句：\nint twice(int v) &#123;\n    return (v*v) % 50;\n&#125;\n那么符号执行就会产生路径约束 x0≠(y0y0)mod50x_0 \\neq (y_0y_0)mod50x0​​=(y0​y0​)mod50 以及  x0=(y0y0)mod50x_0 = (y_0y_0)mod50x0​=(y0​y0​)mod50 。我们做另外一个假设，如果twice是一个我们得不到源码的函数，也就是我们不知道这个函数有什么功能，那么符号执行会产生路径约束 x0≠twice(y0)x_0 \\neq twice(y_0)x0​​=twice(y0​) 以及  x0=twice(y0)x_0 = twice(y_0)x0​=twice(y0​) ，其中twice是一个未解释的函数。这两种情况下，约束求解器都是不能求解这样的约束的，所以符号执行不能产生输入。\n其实我们上述介绍的内容，应该属于纯粹的静态符号执行的范畴。我们提出的两个问题，是导致静态符号执行不能够实用的原因之一。符号执行的概念早在1975年[4]就提出了，但是符号执行技术真正得到实用，却是在一种方式提出之后，即混合实际执行和符号执行，称为concolic execution，是真正意义上的动态符号执行。我们会在后面的小节中介绍。\n 0x3 动态符号执行 Concolic Execution\n最早将实际执行(concrete execution)和符号执行(symbolic execution)结合起来的是2005年发表的DART[5]，全称为“Directed Automated Random Testing”(或称concolic testing)，以及2005年发表的CUTE[6]，即“A concolic unit testing engine for C”。\n**Concolic执行维护一个实际状态(concrete state)和一个符号化状态(symbolic state)：实际状态将所有变量映射到实际值，符号状态只映射那些有非实际值的变量。Concolic执行首先用一些给定的或者随机的输入来执行程序，收集执行过程中条件语句对输入的符号化约束，然后使用约束求解器去推理输入的变化，从而将下一次程序的执行导向另一条执行路径。**简单地说来，就是在已有实际输入得到的路径上，对分支路径条件进行取反，就可以让执行走向另外一条路径。这个过程会不断地重复，加上系统化或启发式的路径选择算法，直到所有的路径都被探索，或者用户定义的覆盖目标达到，或者时间开销超过预计。\n我们依旧以上面那个程序的例子来说明。Concolic执行会先产生一些随机输入，例如{x=22, y=7}，然后同时实际地和符号化地执行程序。这个实际执行会走到第7行的else分支，符号化执行会在实际执行的路径上生成一个路径约束：x0≠2y0x_0 \\neq 2y_0x0​​=2y0​ 。然后Concolic执行会将路径约束的连接词取反，得到新的路径约束 x0=2y0x_0 = 2y_0x0​=2y0​ ，约束求解得到一个新的测试输入{x=2, y=1}，这个新输入就会让执行走向一条不同的路径。之后，Concolic执行会在这个新的测试输入上再同时进行实际的和符号化的执行，执行会取与此前路径不同的分支，即第7行的then分支和第8行的else分支，这时产生的约束就是 (x0=2y0)∧(x0≤y0+10)(x_0=2y_0) \\wedge (x_0 \\leq y_0 + 10)(x0​=2y0​)∧(x0​≤y0​+10) ，生成新的测试输入让程序执行没有被执行过的路径。再探索新的路径，就需要将上述新加入的约束条件取反，也就是 (x0=2y0)∧(x0&gt;y0+10)(x_0=2y_0) \\wedge (x_0 &gt; y_0 + 10)(x0​=2y0​)∧(x0​&gt;y0​+10) ，通过求解约束得到测试输入{x=30, y=15}，程序会在这个输入上遇到ERROR语句。如此一来，我们就完成了所有3条路径的探索。\nPS：注意在这个搜索过程中，其实Concolic执行使用了深度优先的搜索策略。也就是说每次产生新的输入Concolic执行都将执行完整个程序，直到程序结束，然后才返回上一个没有测试的路径约束条件，取反约束条件，得到新的输入，进入新的路径。\n这上述过程中，我们从一个实际输入{x=22, y=7}出发，得到第一个约束条件 x0≠2y0x_0 \\neq 2y_0x0​​=2y0​ ，第一次取反得到 x0=2y0x_0 = 2y_0x0​=2y0​ ，从而得到测试输入{x=2, y=1}和新约束 (x0=2y0)∧(x0≤y0+10)(x_0=2y_0) \\wedge (x_0 \\leq y_0 + 10)(x0​=2y0​)∧(x0​≤y0​+10) ；第二次取反得到 (x0=2y0)∧(x0&gt;y0+10)(x_0=2y_0) \\wedge (x_0 &gt; y_0 + 10)(x0​=2y0​)∧(x0​&gt;y0​+10) ，从而求解出测试输入{x=30, y=15}。\nCristian Cadar在2006年发表EXE，以及2008年发表EXE的改进版本KLEE，对上述Concolic执行的方法做了进一步优化。其创新点主要是在实际状态和符号状态之间进行区分，称之为执行生成的测试（Execution-Generated Testing），简称EGT。这个方法在每次运算前动态检查值是不是都是实际的，如果都是实际的值，那么运算就原样执行，否则，如果至少有一个值是符号化的，运算就会通过更新当前路径的条件符号化地进行。例如，对于我们的例子程序，第17行把y=sym_input()改变成y=10，那么第6行就会用实际参数10去调用函数twice，并实际执行。然后第7行变成if(20==x)，符号执行若走then路径，则加入约束x=20；对条件进行取反就可以走else路径，约束是x≠20。在then路径上，第8行变成if(x&gt;20)，那么该if的then路径就不能走了，因为此时有约束x=20。简言之，EGT本质上还是将实际执行与符号执行相结合，通过路径取反探索所有可能路径。\n正是因为concolic执行的出现，让传统静态符号执行遇到的很多问题能够得到解决——那些符号执行不好处理的部分、求解器无法求解的部分，用实际值替换就好了。使用实际值，可以让因外部代码交互和约束求解超时造成的不精确大大降低，但付出的代价就是，会有丢失路径的缺陷，牺牲了路径探索的完全性。我们举一个例子来说明这一点。假设我们原始例子程序做了改动，即把twice函数的定义改为返回(v*v)%50。假设执行从随机输入{x=22, y=7}开始，if(x==z)条件取反生成路径约束 x0≠(y0y0)mod50x_0 \\neq (y_0y_0)mod50x0​​=(y0​y0​)mod50 。因为约束求解器无法求解非线性约束，所以concolic执行的应对方法是，把符号值用实际值替换，此处会把 y0y_0y0​ 的值替换为7，这就将程序约束简化为 x0≠49x_0 \\neq 49x0​​=49 。通过求解这个约束，可以得到输入{x=49, y=7}，走到一个此前没有走到的路径。传统静态符号执行是无法做到这一步的。但是，在这个例子中，我们无法生成路径true-false的输入，即约束 (x0≠(y0y0)mod50)∧(x0≤y0+10)(x_0 \\neq (y_0y_0)mod50) \\wedge (x_0 \\leq y_0 + 10)(x0​​=(y0​y0​)mod50)∧(x0​≤y0​+10)  ，因为 y0y_0y0​ 的值已经实际化了，这就造成了丢失路径的问题，造成不完全性。\n然而总的来说，Concolic执行的方法是非常实用的，有效解决了遇到不支持的运算以及应用与外界交互的问题。比如调用库函数和系统调用的情况下，因为库和系统调用无法插桩，所以这些函数相关的返回值会被实际化，以此获得简化的程序约束。\n 0x4 面临挑战&amp;解决方案\n符号执行曾经遇到过很多问题，使其难以应用在真实的程序分析中。经过研究者的不懈努力，这些问题多多少少得到了解决，由此也产生了一大批优秀的学术论文。这一部分将简单介绍其中的一些关键挑战以及对应的解决方案。\n 1.路径爆炸(Path Explosion)\n由于在每一个条件分支都会产生两个不同约束，符号执行要探索的执行路径依分支数指数增长。在时间和资源有限的情况下，应该对最相关的路径进行探索，这就涉及到了路径选择的问题。通过路径选择的方法缓解指数爆炸问题，主要有两种方法：\n1）使用启发式函数对路径进行搜索，目的是先探索最值得探索的路径；\n2）使用一些可靠的程序分析技术减少路径探索的复杂性。\n启发式搜索是一种路径搜索策略，比深度优先或者宽度优先要更先进一些。大多数启发式的主要目标在于获得较高的语句和分支的覆盖率，不过也有可能用于其他优化目的。最简单的启发式大概是随机探索的启发式，即在两边都可行的符号化分支随机选择走哪一边。还有一个方法是，使用静态控制流图（CFG）来指导路径选择，尽量选择与未覆盖指令最接近的路径。另一个方法是符号执行与进化搜索相结合，其fitness function用来指导输入空间的搜索，其关键就在于fitness function的定义，例如利用从动态或静态分析中得到的实际状态(concrete state)信息或者符号信息来提升fitness function。\n用程序分析和软件验证的思路去减少路径探索的复杂性，也是一种缓解路径爆炸问题的方式。一个简单的方法是，通过静态融合减少需要探索的路径，具体说来就是使用select表达式直接传递给约束求解器，但实际上是将路径选择的复杂性传递给了求解器，对求解器提出了更高的要求。还有一种思路是重用，即通过缓存等方式存储函数摘要，可以将底层函数的计算结果重用到高级函数中，不需要重复计算，减小分析的复杂性。还有一种方法是剪枝冗余路径，RWset技术的关键思路就是，如果程序路径与此前探索过的路径在同样符号约束的情况下到达相同的程序点，那么这条路径就会从该点继续执行，所以可以被丢弃。\n 2.约束求解(Constraint Solving.)\n符号执行在2005年之后的突然重新流行，一大部分原因是因为求解器能力的提升，能够求解复杂的路径约束。但是约束求解在某种程度上依然是符号执行的关键瓶颈之一，也就是说符号执行所需求的约束求解能力超出了当前约束求解器的能力。所以，实现约束求解优化就变得十分重要。这里主要介绍两种优化方法：不相关约束消除，增量求解。\n a.不相关约束消除(Irrelevant constraint elimination)\n在符号执行的约束生成过程中，尤其是在concolic执行过程中，通常会通过条件取反的方式增加约束，一个已知路径约束的分支谓词会取反，然后由此产生的约束集会检查可满足性以识别另一条路径是否可行。一个很重要的现象是，一个程序分支通常只依赖一小部分程序变量，所以我们可以尝试从当前路径条件中移除与决定当前分支结果不相关的约束。例如，当前的路径条件是 (x+y&gt;10)∧(z&gt;0)∧(y&lt;12)∧(z−x=0)(x+y&gt;10) \\wedge (z&gt;0) \\wedge (y&lt;12) \\wedge (z−x=0)(x+y&gt;10)∧(z&gt;0)∧(y&lt;12)∧(z−x=0) 假如我们想对y&lt;12这个分支条件探索新路径，可以通过约束 (x+y&gt;10)∧(z&gt;0)∧¬(y&lt;12)(x+y&gt;10) \\wedge (z&gt;0) \\wedge \\neg (y&lt;12)(x+y&gt;10)∧(z&gt;0)∧¬(y&lt;12) 求解新的输入， ¬(y&lt;12)\\neg (y&lt;12)¬(y&lt;12) 是取反的条件分支，我们可以去掉对z的约束，是因为z对 ¬(y&lt;12)\\neg (y&lt;12)¬(y&lt;12) 的分支是不会有影响的。减小的约束集会给出x和y的新值，我们用此前执行的z值就可以生成新输入了。如果更形式化地说，算法会计算在取反条件所依赖的所有约束的传递闭包。\n（其实沃觉得是有影响的，x=z，z&gt;0，那不就是x&gt;0吗，如果去掉x-z=0，相当于去掉x&gt;0，x会通过第一项约束条件影响y啊，不是吗？所以这个栗子可能不恰当，这里理解不相关约束消除的概念就行了，如有不同意见欢迎指正）\n b.增量求解(Incremental solving)\n本质上也是利用重用的思想。符号执行中生成的约束集有一个重要特性，就是表示为程序源代码中的静态分支的固定集合。所以，很多路径有相似的约束集，可以有相似的解决方案。通过重用以前相似请求的结果，可以利用这种特性来提升约束求解的速度，这种方法在CUTE和KLEE中都有实现。举个例子来说明，在KLEE中，所有的请求结果都保存在缓存中，该缓存将约束集映射到实际变量赋值。例如，缓存中的一个映射可能是：\n(x+y&lt;10)∧(x&gt;5)⇒{x=6,y=3}(x+y&lt;10) \\wedge (x&gt;5) \\Rightarrow \\lbrace x=6,y=3 \\rbrace\n(x+y&lt;10)∧(x&gt;5)⇒{x=6,y=3}\n使用这些映射，KLEE可以迅速解答一些相似的请求类型，包括已经缓存的约束集的子集和超集。比如对于请求(x+y&lt;10)∧(x&gt;5)∧(y≥0)(x+y&lt;10) \\wedge (x&gt;5) \\wedge (y \\geq 0)(x+y&lt;10)∧(x&gt;5)∧(y≥0) ，KLEE可以迅速检查{x=6,y=3}是一个可行的答案。这样就可以让求解过程加快很多。\n 3.内存建模(Memory Modeling)\n程序语句转换为符号约束的精度对符号执行能实现的覆盖率以及约束求解的可伸缩性有很大影响。例如，使用数学整数(actual mathematical integers)去近似去代替固定宽度的整数变量，这样的内存模型可能会更有效率，但另一方面，根据诸如算术溢出之类的极端情况，可能导致代码分析不精确-这可能会导致符号执行遗漏路径或探索不可行的路径。\n另外一个问题是指针。在访问内存的时候，内存地址（指针）用来引用一个内存单元，当这个地址的引用来自于用户输入时，内存地址就成为了一个表达式。当符号化执行时，我们必须决定什么时候将这个内存的引用进行实际化。一个可靠的策略是，考虑为从任何可能满足赋值的加载，但这个可能值的空间很大，如果实际化不够精确，会造成代码分析的不精确。\n还有一个是别名问题，即地址别名导致两个内存运算引用同一个地址，比较好的方法是进行别名分析，事先推理两个引用是否指向相同的地址，但这个步骤要静态分析完成。KLEE使用了别名分析和让SMT考虑别名问题的混合方法。而DART和CUTE压根没解决这个问题，只处理线性约束的公式，不能处理一般的符号化引用。\n符号化跳转也是一个问题，主要是switch这样的语句，常用跳转表实现，跳转的目标是一个表达式而不是实际值。以往的工作用三种处理方法。\n1）使用concolic执行中的实际化策略，一旦跳转目标在实际执行中被执行，就可以将符号执行转向这个实际路径，但缺陷是实际化导致很难探索完全的状态空间，只能探索已知的跳转目标。\n2）使用SMT求解器。当我们到达符号跳转时，假设路径谓词为 Π\\PiΠ ，跳转到e，我们可以让SMT求解器找到符合 Π∧e\\Pi \\wedge eΠ∧e 的答案。但是这种方案相比其他方案效率会低很多。\n3）使用静态分析，推理整个程序，定位可能的跳转目标。\n实际中，源代码的间接跳转分析主要是指针分析。二进制的跳转静态分析推理在跳转目标表达式中哪些值可能被引用。例如，函数指针表通常实现为可能的跳转目标表。\n 4.处理并发(Handling Concurrency)\n大型程序通常是并发的。因为这种程序的内在特性，动态符号执行系统可以被用来高效地测试并发程序，包括复杂数据输入的应用、分布式系统以及GPGPU程序。\n 0x5 发展脉络(至2017)\n本节仍然摘录自r1ce的文章，主要是以时间的顺序梳理符号执行技术的发展脉络，同时对值得关注的项目和论文做一个小小总结和推荐。\n符号执行最初提出是在70年代中期，主要描述的是静态符合执行的原理，到了2005年左右突然开始重新流行，是因为引入了一些新的技术让符号执行更加实用。Concolic执行的提出让符号执行真正成为可实用的程序分析技术，并且大量用于软件测试、逆向工程等领域。在2005年作用涌现出很多工作，如DART[5]、CUTE[6]、EGT/EXE[7]、CREST[8]等等，但真正值得关注和细读的，应该是2008年Cristian Cadar开发的KLEE[3]。KLEE可以说是源代码符号执行的经典作品，又是开源的，后来的许多优秀的符号执行工具都是建立在KLEE的基础上，因此我认为研究符号执行，KLEE的文章是必读的。\n基于二进制的符号执行工具则是2009年EPFL的George Candea团队开发的S2E[9]最为著名，其开创了选择符号执行这种方式。2012年CMU的David Brumley团队提出的Mayhem[10]则采用了混合offline和online的执行方式。2008年UC Berkeldy的Dawn Song团队提出的BitBlaze[11]二进制分析平台中的Rudder模块使用了online的执行方式，也值得一看。总之，基于二进制的符号执行工作了解这三个就足够了。其中，S2E有开源的一个版本，非常值得仔细研究。最近比较火的angr[12]，是一个基于Python实现的二进制分析平台，完全开源且还在不断更新，其中也实现了多种不同的符号执行策略。\n在优化技术上，近几年的两个工作比较值得一看其一是2014年David Brumley团队提出的路径融合方法，叫做Veritesting[13]，是比较重要的工作之一，angr中也实现了这种符号执行方式。另一个是2015年Stanford的Dawson Engler（这可是Cristian Cadar的老师）团队提出的Under-Constrained Symbolic Execution[14]。\n另外，近年流行的符号执行与fuzzing技术相结合以提升挖掘漏洞效率，其实早在DART和2012年微软的SAGE[15]工作中就已经有用到这种思想，但这两年真正火起来是2016年UCSB的Shellphish团队发表的Driller[16]论文，称作符号辅助的fuzzing（symbolic-assisted fuzzing），也非常值得一看。\n 0x6 总结\n符号执行已是一种有效的程序测试技术，它提供了一种自动生成&quot;触发软件错误的输入&quot;的方法，这些错误从底层程序崩溃到高层语义特性不等。符号执行技术可以用于测试例的自动生成，也可以用于源代码安全性的检测。这两项工作的成效都十分依赖于约束求解器的性能，同时还受硬件设备处理能力的影响。\n虽然需要更多的研究来提高符号执行的代码覆盖率，测试精度，分析效率等等指标，但是事实证明，现有工具可以有效地测试和发现各种软件中的错误，从低级网络和操作系统代码到高级应用程序代码，不一而足。\n从实用性的角度来说：\n（2018年）现有的符号执行工具，在开源方面，主要还是基于KLEE项目的。可见对于KLEE项目的深入理解，将有助于我们打造更加高效的工具。有了高效的工具，就能够使得我们一边学习理论，一遍验证，从而走上高速公路。Inception工具是就ARM架构，而对于路由器中常使用的MIPS架构，现在应该还尚未有类似的符号执行工具发布（如果已经由类似工具，欢迎读者留言）。其中，基于IDA的脚本工具bugscam，经过揭秘路由器0DAY漏洞的作者修改之后，也能够支持分析MIPS架构的漏洞了。然而，其误报率非常之高，该工具报告的漏洞基本都不可用。因此，如何基于上述符号执行的思想，结合IDA工具中强大的反汇编能力，开发也具有符号执行功能的MIPS架构漏洞分析工具，相信也是非常有价值的。\n 0x7 To Learn More\n摘自r1ce的文章：\n对于符号执行入门，有两篇文章可以参考。其一是2010年David Brumley团队在S&amp;P会议上发表的《All You Ever Wanted to Know about Dynamic Taint Analysis and Forward Symbolic Execution (but Might Have Been Afraid to Ask)》[1]。这篇文章同时介绍了动态污点分析和前向符号执行的基本概念，作者构造了一种简化的中间语言，用来形式化地描述这两种程序分析技术的根本原理。其二是2011年Cristian Cadar发表在ACM通讯上的一篇短文《Symbolic execution for software testing: three decades later》[2]，以较为通俗的语言和简单的例子阐述了符号执行的基本原理，并介绍了符号执行技术的发展历程和面临挑战。\n其实这两篇文章的作者都是二进制分析领域大名鼎鼎的人物，卡内基梅隆的David Brumley是AEG的提出者，其带领团队是DARPA CGC比赛的第一名；英国帝国理工的Cristian Cadar则是符号执行引擎KLEE[3]的作者——KLEE在符号执行领域的地位不言而喻。这两篇文章各有千秋：前者更加学术化一些，用中间语言进行的形式化描述有些晦涩难懂，但对于进一步研究符号执行引擎的源码很有帮助；后者则更通俗一些，有助于初学者的理解，且对于符号执行的发展脉络有更多的介绍。\nQ&amp;A:\nA:如何通俗易懂的总结符号执行技术？\nQ:符号执行是一种白盒静态代码分析技术，用符号表示程序的输入，根据分析程序语义得到的约束条件，去求解能得到你所希望的程序输出所需要的输入，的一种方法。更容易理解的说法可以参开0x1第一段。\nQ:路径约束PC到底长什么样子？\nA:一长串约束条件，比如0x2中的示例： (∧i∈[1,n]Ni&gt;0)∧(Nn+1≤10)(\\wedge_{i \\in [1,n]}N_i &gt; 0) \\wedge (N_{n+1} \\leq 10)(∧i∈[1,n]​Ni​&gt;0)∧(Nn+1​≤10)\nQ:约束求解器是怎么运作的？\nA:\nQ:Concolic执行的出现，让传统静态符号执行遇到的很多问题能够得到解决（解决了哪些问题？怎么解决的？）\nA:Concolic执行的方法有效解决了遇到不支持约束求解的运算以及应用与外界交互的问题；具体方法就是，那些传统静态符号执行不好处理的部分、求解器无法求解的部分，用实际值替换就好了\nQ:Concolic执行和传统静态符号执行哪里不同？\nA:我的理解是，1.传统静态符号执行使用符号表示变量，比如符号状态 σ\\sigmaσ 中存在变量x的映射x→x0x \\rightarrow x_0x→x0​，2.使用约束求解器判定哪条分支可行，并根据预先设计的路径调度策略实现对该过程所有路径的遍历分析，最后输出每条可执行路径的分析结果，即每条路径对应的输入和输出；\n而Concolic执行来说，1.它使用具体数值作为输入来模拟执行程序代码，2.从当前路径的分支语句的谓词中搜集所有符号约束。然后修改该符号约束内容构造出一条新的可行的路径约束，并用约束求解器求解出一个可行的新的具体输入，接着符号执行引擎对新输入值进行一轮新的分析。通过使用这种输入迭代产生变种输入的方法分析程序。\n参考文章：\n\nK0rz3n：https://www.k0rz3n.com/2019/02/28/简单理解符号执行技术/\nr1ce：https://zhuanlan.zhihu.com/p/26927127\n符号执行——从入门到上高速：https://www.anquanke.com/post/id/157928\nhttp://pwn4.fun/2017/03/20/符号执行基础/\n[2]* Symbolic execution for software testing: three decades later：https://people.eecs.berkeley.edu/~ksen/papers/cacm13.pdf\n[1] Schwartz E J, Avgerinos T, Brumley D. All You Ever Wanted to Know about Dynamic Taint Analysis and Forward Symbolic Execution (but Might Have Been Afraid to Ask) [C]// Security &amp; Privacy. DBLP, 2010:317-331.\n[2] Cadar C, Sen K. Symbolic execution for software testing: three decades later[M]. ACM, 2013.\n[3] C. Cadar, D. Dunbar, and D. Engler. KLEE: Unassisted and Automatic Generation of High-Coverage Tests for Complex Systems Programs. In Proceedings of the 8th USENIX Symposium on Operating Systems Design and Implementation (OSDI’08), volume 8, pages 209–224, 2008.\n[4] R. S. Boyer, B. Elspas, and K. N. Levitt. SELECT – a formal system for testing and debugging programs by symbolic execution. SIGPLAN Not., 10:234–245, 1975.\n[5] P. Godefroid, N. Klarlund, and K. Sen. DART: Directed Automated Random Testing. In PLDI’05, June 2005.\n[6] K. Sen, D. Marinov, and G. Agha. CUTE: A concolic unit testing engine for C. In ESEC/FSE’05, Sep 2005.\n[7] C. Cadar, V. Ganesh, P. M. Pawlowski, D. L. Dill, and D. R. Engler. EXE: Automatically Generating Inputs of Death. In Proceedings of the 13th ACM Conference on Computer and Communications Security, pages 322–335, 2006.\n[8] J. Burnim and K.Sen,“Heuristics for scalable dynamic test generation,” in Proc. 23rd IEEE/ACM Int. Conf. Autom. Software Engin., 2008, pp. 443–446.\n[9] V. Chipounov, V. Georgescu, C. Zamfir, and G. Candea. Selective Symbolic Execution. In Proceedings of the 5th Workshop on Hot Topics in System Dependability, 2009.\n[10] S. K. Cha, T. Avgerinos, A. Rebert, and D. Brumley. Unleashing Mayhem on Binary Code. In Proceedings of the IEEE Symposium on Security and Privacy, pages 380–394, 2012.\n[11] Song D, Brumley D, Yin H, et al. BitBlaze: A New Approach to Computer Security via Binary Analysis[C]// Information Systems Security, International Conference, Iciss 2008, Hyderabad, India, December 16-20, 2008. Proceedings. DBLP, 2008:1-25.\n[12] Yan S, Wang R, Salls C, et al. SOK: (State of) The Art of War: Offensive Techniques in Binary Analysis[C]// Security and Privacy. IEEE, 2016:138-157.\n[13] T. Avgerinos, A. Rebert, S. K. Cha, and D. Brumley. Enhancing Symbolic Execution with Veritesting. pages 1083–1094, 2014.\n[14] D. a. Ramos and D. Engler. Under-Constrained Symbolic Execution: Correctness Checking for Real Code. In Proceedings of the 24th USENIX Security Symposium, pages 49–64, 2015.\n[15] P. Godefroid, M. Y. Levin, and D. Molnar. SAGE: Whitebox Fuzzing for Security Testing. ACM Queue, 10(1):20, 2012.\n[16] N. Stephens, J. Grosen, C. Salls, A. Dutcher, R. Wang, J. Corbetta, Y. Shoshitaishvili, C. Kruegel, and G. Vigna. Driller: Augmenting Fuzzing Through Selective Symbolic Execution. In Proceedings of the Network and Distributed System Security Symposium, 2016.\n\n","categories":["笔记"],"tags":["总结","学习","符号执行"]},{"title":"Flow-based Generative Model","url":"/Flow-based-Generative-Model/","content":" Flow-based Generative Model\n本节要讲一个新的生成模型，一个新的Generative 技术，它和GAN的目的相同，但是不如GAN有名，而且实际上也没有胜过GAN，但是这个方法有比较新颖的思想所以还是讲一讲。这个方法叫做Flow-based Generative Model，这里的Flow就是流的意思，后面会解释为什么说它是流。\n\n\nLink: https://youtu.be/YNUek8ioAJk\nLink: https://youtu.be/8zomhgKrsmQ\n\n上图是从以前的课程中截取出来的，如果你有看过以前的课程（链接如上）你就能知道这里的三个方法的具体细节。我们之前说Generative Model 有三种，第一种Component-by-component ，上课的时候我们以生成宝可梦图片为例子，以像素为component 一个像素一个像素的生成，这种方法更常见的名字是Autoregressive Model。我们也讲过Variational Autoencoder（VAE），和Generative Adversarial Network（GAN），上述三种是我们之前见过的生成模型，今天我们要介绍第四种Flow-based Generative Model 。\n 过去的Generative Models 的缺点\n\n我们之前介绍的生成模型都有各自的问题，如上图所示。\nAuto-regressive Model 是一个一个component 地生成的，所以component 的最佳生成顺序是难以求解的。比如说生成图像的例子中，我们就是一个一个pixel 地，从左上角开始一行一行地生成。当然，还有一些任务的生成目标就是有顺序的，比如说生成语音，后面的component 依赖前面的component ，这样的目标比较适合用Auto-regressive Model ，但是现在这个模型太慢了，如果你要生成一秒像是人类说的语音你需要做90分钟合成，这显然是不实用的。\nVariational Auto-encoder 介绍这个模型的时候我们证明了，VAE optimize的对象是likelihood 的lower bound，它不是去maximize 我们要它maximize 的likelihood 而是去maximize likelihood 的lower bound，我们不知道这个lower bound 和真正想要maximize 的对象的差距到底有多大。Flow-base generative model 会解决这个问题，它并不会用approximation，并不是optimize lower bound 而是optimize probability 的本体，后面我们会介绍flow 是怎么做到这件事的。\nGenerative Adversarial Network 是现在做生成得到的目标质量最好的模型。但是我们都知道GAN是很难train的，因为你的Generative 和Discriminator 目标是不一致的，很容易train崩掉。\n Generator 的内涵\n\n我们抛开具体模型，先来看看Generator 的定义。Generator 是一个网络，该网络定义了一个可能性分布pGp_GpG​ 。怎么理解这句话呢，看上图，有一个Generator GGG 输入一个 zzz 输出一个 xxx ，如果现在是在做人脸生成，  xxx 是一张人脸图片，可以看成一个高维向量其中每一个element 就是图片的一个pixel， zzz 我们通常假设它是从一个简单的空间中sample 出来的，比如说Normal Distribution，虽然 zzz 是从一个简单的分布中sample 出来的，但是通过 GGG 以后 xxx 可能会形成一个非常复杂的分布，这个Distribution 我们用 PGP_GPG​ 来表示。对我们而言我们会希望通过 GGG 得到的Distribution 能和real data 也就是真实人脸数据集的Distribution PdataP_dataPd​ata 尽可能相同。\n那怎么能让 PGP_GPG​ 和 PdataP_dataPd​ata 尽可能相同呢，常见的做法是我们训练G的时候训练目标定为maximize likelihood ，讲的具体一点就是从人脸图片数据集中sample 出m笔data $\\lbrace x1,x2,…,x^m\\rbrace $ ，然后你就是希望这m张图片是从 PGP_GPG​ 这个分布中sample 出来的概率越大越好。就是maximize 上图中左下角的公式，这就是我们熟知的maximize likelihood。如果你难以理解为什么要让产生这些data 的概率越大越好的话，老师提供了一个更直观的解释：maximize likelihood 等同于minimize  PGP_GPG​ 和 PdataP_dataPd​ata 的K-L变换。Ref: https://youtu.be/DMA4MrNieWo\n总而言之就是让 PGP_GPG​ 和 PdataP_dataPd​ata 这两个分布尽可能相同。\nFlow 这个模型有什么厉害的地方呢？Generator 是个一个网络，所以 PGP_GPG​ 显然非常复杂，一般我们不知道怎么optimize objective function，而Flow 可以直接optimize objective function，可以直接maximize likelihood。\n Flow 的数学基础\nmath warning\n\n这可能也是Flow-based Generative Model 不太出名的一个原因，其他生成模型都比较容易理解，而此模型用到了比较多的数学知识。你要知道上图中的三个概念：Jacobian、Determinant、Change of Variable Theorem 。\n Jacobian Matrix\n\n把 fff 看作生成模型，zzz 就是输入，xxx 就是生成的输出。这里我们的栗子中输入输出的维度是相同的，实际做的时候往往是不同的。Jacobian Matrix 记为 JfJ_fJf​ ，就定义为输出和输入向量中element 两两组合做偏微分组成的矩阵，如上图左下角所示，输出作为列，输入作为行， JfJ_fJf​ 每个element 都是输出对输入做偏微分。 Jf−1J_{f^{-1}}Jf−1​ 的定义同上只是变成了输入对输出的偏微分。\n举一个栗子，如上图右侧，自己看一下就好，就不赘述了。需要强调一下的是 JfJf−1J_f J_{f^{-1}}Jf​Jf−1​ 得到单位矩阵，function 之间有inverse的关系，得到的Jacobian Matrix 也有inverse的关系。\n Determinant\n\nDeterminant ：方阵的行列式是一个标量，它提供有关方阵的信息。你只要记得上图左下角行列式的性质，矩阵的det和矩阵逆的det互为倒数。\n\n矩阵的行列式的绝对值可以表示行列式每一行表示的向量围成的空间的&quot;面积&quot;（3维空间中是体积，更高维空间中也是类似的概念）。或者你可以想象它是矩阵所表示的线性变换前后的“面积”放缩比例。\n💖关于行列式的含义你可以参考3Blue1Brown的线性代数的本质05行列式 。\n Change of Variable Theorem\nDeterminant 就是为了解释Change of Variable Theorem\n\n假设我们有一个分布 π(z)\\pi(z)π(z) ，zzz 带入 fff 会得到 xxx ， xxx 形成了分布 p(x)p(x)p(x) ，我们现在想知道 π(z)\\pi(z)π(z) 和  p(x)p(x)p(x) 之间的关系。如果我们可以写出两者之间的关系，就可以分析一个Generator 。\n两者之间的关系能写出来吗？是可以的。我们现在要问的问题是：假设说分布 π(z)\\pi(z)π(z) 上有一个 z′z&#x27;z′ 对应 π(z′)\\pi(z&#x27;)π(z′) ， z′z&#x27;z′ 通过 fff 得到 x′x&#x27;x′ 对应 p(x′)p(x&#x27;)p(x′) ，现在想知道 π(z′)\\pi(z&#x27;)π(z′) 和 p(x′)p(x&#x27;)p(x′) 之间的关系是什么？\n举一个简单的栗子来说明上述问题：\n\n假设说两个分布如上图所示，我们知道probability density function 的积分等于1，所以两者的高度分别为1和12\\frac{1}{2}21​，假设 xxx 和 zzz 的关系是 x=2z+1x=2z+1x=2z+1 ，底就变成原来的两倍，高度就变成原来的二分之一。现在从 π(z′)\\pi(z&#x27;)π(z′) 到 p(x′)p(x&#x27;)p(x′) 中间的关系很直觉就是二分之一的关系。\n那我们再来看一个更general 的case：\n\n两个分布如上图浅蓝色和浅绿色线所示，现在我们不知道这两者的Distribution，假设我们知道 xxx 和 zzz 的关系即知道 fff ，那我们就可以求出 π(z′)\\pi(z&#x27;)π(z′) 和 p(x′)p(x&#x27;)p(x′) 之间的关系，这就是Change of Variable Theorem 的概念。\n我们把 z′z&#x27;z′ 加一个 ΔzΔzΔz 映射到 x′+Δxx&#x27;+Δxx′+Δx ，此时上图中蓝色方块和绿色方块需要有同样的面积，所以有上图右侧所示公式。\np(x′)=π(z′)∣dzdx∣p(x&#x27;) = \\pi(z&#x27;)\\left|\\frac{dz}{dx}\\right|\np(x′)=π(z′)∣∣∣∣​dxdz​∣∣∣∣​\n这个绝对值是因为有时会出现上图左下角的情况，通过 fff 变换后，形成了交叉。\n我们在举一个 xxx 和 zzz 都是二维的栗子：\n\n我们在 zzz 的分布中取一个小小的矩形区域高是 Δz2Δz_2Δz2​ 宽是 Δz1Δz_1Δz1​ ，它通过一个function fff 投影到 xxx 的分布中变成一个菱形。我们用向量 [Δx11,Δx21][Δx_{11}, Δx_{21}][Δx11​,Δx21​] 表示菱形的右下边，用向量[Δx22,Δx12][Δx_{22}, Δx_{12}][Δx22​,Δx12​] 表示菱形的左上边。根据上面Determinant 一节中提到的知识点，我们可以用这两个向量组成的矩阵的determinant 表示该矩阵的面积。π(z′)\\pi(z&#x27;)π(z′) 作为正方形为底的三维形体的高， p(x′)p(x&#x27;)p(x′) 作为以绿色菱形为底的三维形体的高，这两个形体的积分值相等，所以得到了如上图所示的公式。\n另外再讲一下四个Δ值的意思：\nΔx11Δx_{11}Δx11​是z1z_1z1​（维度上）改变的时候x1x_1x1​（维度上）的变化量\nΔx21Δx_{21}Δx21​是z1z_1z1​改变的时候x2x_2x2​的变化量\nΔx12Δx_{12}Δx12​是z2z_2z2​改变的时候x1x_1x1​的变化量\nΔx22Δx_{22}Δx22​是z2z_2z2​改变的时候x2x_2x2​的变化量\n 整理公式得出结论\n\n接下来就是做一波整理，相信大家都看得懂（看不懂就重学一下线代😜），就不赘述了。\n最终的结论就是上图左下角的公式：\n总而言之， π(z′)\\pi(z&#x27;)π(z′) 乘上 ∣det(Jf−1)∣\\left| det(J_{f^{-1}}) \\right|∣∣​det(Jf−1​)∣∣​ 就得到 p(x′)p(x&#x27;)p(x′) ，这就是 π(z′)\\pi(z&#x27;)π(z′) 和 p(x′)p(x&#x27;)p(x′) 的关系。\n如果你没看太懂数学推导，你就记住这个结论就好， π(z′)\\pi(z&#x27;)π(z′) 和 p(x′)p(x&#x27;)p(x′) 之间就是差一个 ∣det(Jf−1)∣\\left| det(J_{f^{-1}}) \\right|∣∣​det(Jf−1​)∣∣​ 。\nend of math warning\n Flow-based Model\n接下来我们就正式进入Flow的部分，我们先回到Generation 上。\n\n一个Generator 是怎么训练出来的呢，Generator 训练的目标就是要maximize PG(xi)P_G(x^i)PG​(xi) ，xix^ixi 是从real data 中sample 出来的一笔data。那 PG(xi)P_G(x^i)PG​(xi) 长什么样子呢，有了前面的Change of Variable Theorem 的公式我们就可以把 PG(xi)P_G(x^i)PG​(xi) 写出来了：\nPG(xi)=π(zi)∣det(JG−1)∣P_G(x^i) = \\pi(z^i)\\left|det(J_{G^{-1}})\\right|\nPG​(xi)=π(zi)∣det(JG−1​)∣\n而 ziz^izi 就是把xix^ixi 带入GGG inverse 以后的结果。\n我们在给上式加一个log，把公式右侧的乘转为加。现在我们知道 log(PG(xi))log(P_G(x^i))log(PG​(xi)) 长什么样子了，我们只要能maximize 这个式子我们就可以train Generator 就结束了，但是我们要想maximize 这个式子需要有一些前提：\n\n要知道如何计算 det(JG)det(J_G)det(JG​)\n要知道 G−1G^{-1}G−1\n\n要做Generator 的Jacobian 其实还比较好做，你只要知道如何计算∂x/∂z\\partial{x}/\\partial{z}∂x/∂z ，但是问题是这个Jacobian Matrix 可能会很大，比如说 xxx 和 zzz 都有1000维，那你的Jacobian Matrix 就是一个1000*1000的矩阵，你要算它的det，计算量是非常大的，所以我们要好好设计Generator ，让它的Jacobian Matrix 的det 好算一些。\n另一个前提是：我们要把 ziz^izi 变成 xix^ixi ，就要知道G−1G^{-1}G−1 。我们同时也要让det(JG−1)det(J_{G^{-1}})det(JG−1​) 变得容易算。为了确保G是invertible，我们设计的Generator 的时候 zzz 和 xxx 的维度是相同的。 zzz 和 xxx 的维度相同不能保证G一定是invertible，但是如果不相同G一定不是invertible。\n所以，到这里我们就能看出来，Flow-based Model 和GAN 不一样，GAN是低维生成高维，但是Flow这个技术中输入输出的维度必须相同，这一点是有点奇怪的。另外，为了保证G是可逆的，我们要限制G的架构，这样的做法必然会限制整个生成模型的能力。\n\n既然一个G不够强，你就多加一些。最终maximize的目标就是上图最下面的公式。\nG∗=argmaxG∑imlog(pK(xi))G^* = arg\\mathop{max}\\limits_{G}\\sum_{i}^{m}log(p_K(x^i))\nG∗=argGmax​i∑m​log(pK​(xi))\n 实作上怎么做Flow-based Model\n\n我们先考虑一个G，在公式中其实只有出现 G−1G^{-1}G−1 ，所以实际上我们在训练的时候我们训练的是 G−1G^{-1}G−1 ，在生成的时候把 G−1G^{-1}G−1 倒过来用 GGG 生成目标。在训练的时候，从real data 中sample 一些数据出来，输入 G−1G^{-1}G−1 中得到 ziz^izi 。\n看一下上图中的公式，我们的目标是maximize 它，这个公式有两项，先看第一项。因为 π\\piπ 是一个normal distribution，要这一项的值最大只要让 G−1(xi)G^{-1}(x^i)G−1(xi) 是零向量就好了，但是显然这么做会出大问题，如果 ziz^izi 变0，JG−1J_{G^{-1}}JG−1​ 将是一个零矩阵，则 det(JG−1)=0det(J_{G^{-1}}) = 0det(JG−1​)=0 。此时第二项，就是负无穷。\n为了maximize log(PG(xi))log(P_G(x^i))log(PG​(xi)) ，如上图所示，一方面前一项要让 G−1(xi)G^{-1}(x^i)G−1(xi) 向原点靠拢，一方面第二项又会对 G−1G^{-1}G−1 做一些限制，让它不会把所有的 zzz mapping 到零向量。\n这就是我们在实作Flow-based Model 时所做的事情。\n Coupling Layer\ncoupling layer 是一个实用的G，NICE 和Real NVP 这两个Flow-based Model 都用到了coupling layer ，接下来将介绍这个coupling layer 的具体做法。\n\n\nNICE\nhttps://arxiv.org/abs/1410.8516\nReal NVP\nhttps://arxiv.org/abs/1605.08803\n\n如上图所示，输入和输出都是D维向量，然后把输入输出的向量拆成两部分，前d维为一组，后D-d维为一组。对于上面的第一部分直接通过copy z得到x。对于第二部分，我们将第一部分通过两个变换F和H（这两个function 可以任意设计，多复杂都可以）得到向量β和γ，再将第二部分和β做内积再加上γ，就得到第二部分的x。\n接下来要讲怎么取coupling layer 的inverse。就是说在不知道z，只有后面的x，这样的情况下怎么把z找出来呢？\n\n对于第一部分直接把x copy过去就可以了，对于第二部分，我们就把刚才得到的z的第一部分通过F和H得到β和γ，然后将x的第二部分通过 xi−γiβi\\frac{x_i-γ_i}{β_i}βi​xi​−γi​​ 就可以算出z ，结束。\n\n不只要能算G的inverse，还要算 JGJ_GJG​ ，计算方法就如上图所示。\n左上角显然是单位矩阵，因为x和z是copy的。\n右上角是浅蓝色对应深绿色的部分，上图右上角可以明显看出两者没有关系，偏微分结果为0.\n左下角我们并不关心了，因为现在矩阵的左上角是单位矩阵，右上角是零矩阵，整个矩阵的det就是右下角矩阵的det。\n右下角矩阵是Diagonal ，因为你想想看 zd+1z_{d+1}zd+1​ 只和 xd+1x_{d+1}xd+1​ 有关系，改行其他值都是0，同样的 zDz_{D}zD​ 只和 xDx_{D}xD​ 有关系，所以整个右下角矩阵就是一个对角矩阵。\n所以这这个矩阵的det：\ndet(JG)=∂xd+1∂zd+1∂xd+2∂zd+2...∂xD∂zD=βd+1βd+2...βDdet(J_G) =\\frac{\\partial{x_{d+1}}}{\\partial{z_{d+1}}}\\frac{\\partial{x_{d+2}}}{\\partial{z_{d+2}}}...\\frac{\\partial{x_D}}{\\partial{z_D}} \\\\\\\\\n=β_{d+1}β_{d+2}...β_{D}\ndet(JG​)=∂zd+1​∂xd+1​​∂zd+2​∂xd+2​​...∂zD​∂xD​​=βd+1​βd+2​...βD​\n stacking\n\n如果你是简单的把coupling layer 叠在一起，那你会发现最后生成的东西有一部分是noise，因为上面是这部分向量是直接一路copy到输出的。\n所以我们做一些手脚，把其中一些coupling layer 做一下反转，如上图下侧所示。\n\n我们再讲的更具体一点，如果说我们在做图像生成，有两种拆分向量的做法：一种是把横轴纵轴切分成多条，横轴纵轴的index之和是奇数就copy偶数就不copy做transform 。或者是，一张image 通常由rgb三个channel，你就其中某几个channel 做copy 某几个channel 做transform，每次copy 和transform 的channel 可能是不一样，你有很多层coupling layer 嘛。当然，你也可以把这两种做法混在一起用，有时用第一种拆分方法，有时候用第二种拆分方法。\n 1x1 Convolution\n\n\nGLOW\nhttps://arxiv.org/abs/1807.03039\n\n讲道理，Flow-based Generative Model 其实很早就被提出来了，上面说的coupling layer 做Generator 的方法中提到的NICE 和Real NVP 这两个Flow-based Model 都是14、15年左右被提出来了，算是很古老的东西了，为什么重提旧事呢，就是因为GLOW这个技术是去年（指2018年）被提出来了，这个技术的结果还是蛮惊人的。虽然，这个方法的结果还是没有打败GAN，但是你会惊叹&quot;我靠，不用GAN也可以做到这种程度啊！&quot;\nGLOW除了Coupling Layer，还有个神奇的layer 叫做1x1 Convolution，举例来说我们现在要产生图片，我们就用一个3x3的W矩阵，将z一个像素一个像素地转换为x 。W是learn 出来的，它很可能能够做到的事情是shuffle channel，如上图所示的栗子，就是把z的channel 做了交换。\n如果W是invertible ，那就很容易算出 W−1W^{-1}W−1 ，但是W是learn 出来的，这样的话它一定是invertible 的吗，文献中作者在initial 的时候就用了一个invertible matrix，他也许是期待initial invertible matrix，最后的结果也能是invertible matrix。但是实际上invertible matrix 是比较容易出现的，你随机设置一个matrix 大概率就是invertible ，你想想看只有det是0才是非invertible，所以实作上也没太大问题，老师讲如果你看到有文献解释或者考虑这件事情了记得call他，也记得call我，方便我在这里做一些补充。\n\n我们来举一个栗子，如上图所示，我们把一个像素 [x1,x2,x3][x_1,x_2,x_3][x1​,x2​,x3​] 乘上W，做一个 转换，求这个W的Jacobian Matrix 的公式如上图所示，你自己细算一下就会发现，JfJ_fJf​ 就是W。\n\n所以现在全部的z和x，做变换使用的Generator （也就是很dxd个W）的Jacobian Matrix 就是上图所示的对角矩阵，因为只有对角线上的元素对应的蓝色pixel 和绿色pixel 才是有关系的，其它地方的偏微分都是0。\n这个矩阵的det是什么呢，如果你线代够好的话，就知道这个值就是 (det(W))d×d(det(W))^{d \\times d}(det(W))d×d ，而W是3x3的，det很好算，所以大矩阵的det也就很好算出来了，结束。\n Demo of OpenAI (GLOW)\nFlow-based Model 有一个很知名的结果就是GLOW这个Model，OpenAI有做一个GLOW Model demo 的网站：\n\nhttps://openai.com/blog/glow/\n\n你可以做人脸的结合：\n\n可以把脸做种种变形，比如说让人笑起来：\n\n你需要先收集一堆笑的人脸、一堆不笑的人脸，把这两个集合的图片的z求出来分别取平均，然后相减的到 zsmilez_{smile}zsmile​ ，zsmilez_{smile}zsmile​ 就是从不笑到笑移动多少距离。然后，你只要往不笑的脸上加上一个 zsmilez_{smile}zsmile​ 通过G，就可以得到笑的脸了。\n To Learn More …\n\nGLOW 现在做的最多的就是语音合成。在语音合成任务上，不知道为什么用GAN做的结果都不是很好，Auto-regressive Model 就是一个一个sample 产生出来，结果是很好，但是计算太慢不太实用，所以现在都在用GLOW ，就留给大家自学了。\n","categories":["李宏毅机器学习笔记"],"tags":["ML","note","Next-Step-of-ML"]},{"title":"GAN(Quick Review)","url":"/GAN(Quick-Review)/","content":" GAN(Quick Review)\n本节的目的就是帮大家复习一下GAN，如果你对GAN不是特别感兴趣，你就可以通过这节课了解本课程需要用到的GAN的知识。如果听完以后对GAN产生了浓厚的兴趣，你可以去看2018年的GAN的课程视频：https://youtube.com/playlist?list=PLJV_el3uVTsMq6JEFPW35BCiOQTsoqwNw\n Three Categories of GAN\n我们把GAN分为以下三种：\n\n 1.Typical GAN\n第一种经典的GAN就是训练一个Generator，这个生成器吃一个向量，生成一个目标对象。比如说，你要做二次元人脸图片的生成，你就collect 一大堆的动漫人脸图片，然后喂给Generator 去训练，Generator 就会学会生成二次元人脸的图像。\n那怎么训练这个Generator 呢，模型的架构是什么样子的呢？\n\n在经典的GAN方法中，我们会有一个Generator 和一个Discriminator ，这两者是交替的训练出来的，怎么交替训练后面叙述。我们继续以生成二次元人脸图像为例，Generator 就是的input 是一个低维向量，output 是一个高维向量。Discriminator 输入就是Generator 生成的图片，输出是一个分数，这个分数就是说输入有多像二次元人脸，如果很想的话就给出高分，否则就给出低分。这就是经典GAN的基本架构。\n Algorithm\n现在来讲一下Generator 和Discriminator 是怎么被训练出来的。这里先明确G和D都是神经网络，这两者的具体架构根据具体的任务而定，比如说你要做图像生成那你可能就会用CNN，如果你要让Generator 通过一些词生成文章，你可能就会用RNN…我们今天不做过多讨论这方面的内容，而是关注于G和D是怎么交替训练的。\n\n首先，G和D是迭代的训练的，在每个training iteration 中有两个步骤：\nstep1：固定住G，只更新D的参数。也就是说通过当前iteration分配的一个mini batch data 进行Discriminator 的训练。你可以把这个训练看作是分类问题，也可以看作回归问题，相当于如果是动漫人物头像就输出1，否则输出0。这就达到了我们的目的，训练一个Discriminator 它看到好的图片就给一个高分，看到差的就给低分。另外提一下，Database 中都是我们收集的正类样本，而此时Generator 产生样本就可以作为负类。\n\nstep2：固定住D，只更新G。这个步骤你可以把G和D两个网络连起来看成一个大的网络，这个网络输入就是低维vector，输出就是一个分数，中间某个维度拿出来就是一张图片，这整个网络仍然用backpropagation更新参数，只不过是固定住后面D的参数。这一步训练目的是为了让Generator 努力骗过Discriminator ，这样G生成的图片就会更像是我们收集的数据集中的样本。\n你也可以这样考虑，我们原本train 一个网络是去optimize 人定的objective function，或者是minimize 人定的loss function，但是现在我们训练的Discriminator 就相当于是让机器学了一个objective function或者说loss function，让G朝着这个方向努力。\n\n实际在训练的时候就是如上图所示，train D、train G、train D、train G…交替进行。在网络上比较好的栗子：\n\nYour browser does not support the video tag.\n\nhttps://crypko.ai/#/\n GAN is hard to train…\n众所周知，GAN这个技术是比较难train起来的，实际上有很多人提出了更好的训练GAN的方法，WGAN，improve WGAN…如果你有兴趣就去看文章开头链接的课程吧。\n 2.Conditional GAN\n上面我们说的是用GAN的技术随机生成目标，更多的情况下我们是希望控制机器产生的东西。比如说我们输入一端文字，让机器根据这段文字生成一张图像，比如下图所示，如果我们输入“Girl with red hair”机器就会生成一张夏娜的图片。\n\n这种根据某个特定输入产生对应的输出的Generator 叫做Conditional GAN 。\n我们以文字产生影像为例子，假如说你能收集到文字和图像对应的数据：\n\n接下来你就可以套用传统的监督学习方法，直接训练一个网络输入一段文字，输出一张图片。但是这样训练方法往往不理想，举一个直观的栗子，如上图所示，如果文字是火车，对应火车的图片有火车的各个方向，还有各种不同的火车，这样机器在考虑火车的特征的时候就会把所有的已知特征取平均，这样的话结果就会垮掉。\n\n所以我们就需要新的技术，比如说Conditional GAN，我们会用这个技术做给出文字生成对应影像的任务。Conditional GAN 也是需要label data 的也就是说同样是监督学习，但是它和传统的监督学习的学习目标是不太一样的（事实上Conditional GAN 也可以在无监督的情况下做，这个我们会在后面讲到）。怎么个不一样呢，如上图所示，Generator 的输入是一段描述，输出对应图片，它不去看输入输出是否对应而是看Discriminator 给出的得分。关于Discriminator 我们不仅要输入Generator 生成的图片，还要输入。原因是这样，如果D只吃一张图片的话，那G只要生成出像真实世界的图片就可以骗过D了，这样的话G就会变得很懒惰，就学不起来。如上图所示，Generator 忽视了input condition。\n\n所以我们要给Discriminator 多输入一段描述，Discriminator 输出的分数包含两个含义，一个是x到底有多像真实世界的影像，一个是c和x有多match。训练D的时候要用到如上图所示的输入，text-image pair 需要包含文字匹配真实图像，文字匹配G生成影像，文字不匹配真实图像这些情况，这是第一次实作Conditional GAN容易忽略的点。\n Conditional GAN - Sound-to-image\n其实上述使用Conditional GAN 根据文字生成影像的应用已经满坑满谷了，其实只要你有label data 你都可以尝试使用Conditional GAN 来试试，这里老师实作了一个根据声音生成图像的栗子。\n\n从video中提取音轨和图像，就得到了一段声音和图像的对应关系，就可以作为data set 使用上述的Conditional GAN 方法进行训练。\n\n\nThe images are generated by Chia-Hung Wan and Shun-Po Chuang.\nhttps://wjohn1483.github.io/audio_to_scene/index.html\n\n上图老师实作的结果，第一行是一段类似电视雪花的声音，是识别结果是小溪，然后调大声音，识别结果渐渐变成了瀑布。第二行是类似螺旋桨轰鸣声，识别结果是海上的快艇，随着声音增大，快艇周围的水花逐渐增大。这两个结果其实是有carry kick 的，第三行是一些音乐，识别结果就烂掉了，但总体来说我觉得还不错，有兴趣的话可以去上面那个链接看看。👀\n Conditional GAN - Image-to-label\n我们反向思考，可以做根据图像生成对应标签的模型，把Conditional GAN 用在multi-label image classification 任务上：\n\n所谓multi-label 就是说目标的类别不止一个，比如上图所示，示例图片中有person、sports ball、baseball bat、baseball glove，这张图片属于上述所有类别，或者说这张图片拥有这些属性。我们可以把一张图片拥有多个类别想象成一个生成问题，就是说给出一张图片生成它可能拥有的类别或者说属性。\n你就把这个问题当作一般的Conditional GAN 做下去就可以了，图片作为condition 输入，类别作为Generator 的输出。\n\n上图是实验的结果，用F1F_1F1​分数来评价，分数越高表示分类准确率越好，使用了两个corpus：MS-COCO和NUS-WIDE，几个不同架构的模型的表现都是加上GAN就会变得更好。\n Talking Head\n这里还有一个效果更好的，根据一张人脸照片，一个人脸line mark，去产生另外一张人脸。能做你拿一张蒙娜丽莎的图片，再拿一张人脸line mark，就可以让蒙娜丽莎摆出人脸line mark的表情。\n\n\nhttps://arxiv.org/abs/1905.08233\n\n直观一点，效果就是：\n\n\n 3.Unsupervised Conditional GAN\n最后，我们要讲的是无监督的GAN，上面讲的Conditional GAN 是需要输入输出的对应关系的，实际上我们是有机会能在不知道输入输出的对应关系的情况下，教会机器把输入转成输出。这种技术最常见的应用场景是image style transformation ：\n\n Cycle GAN\n\n\nhttps://arxiv.org/abs/1703.10593\nhttps://junyanz.github.io/CycleGAN/\nBerkeley AI Research (BAIR) laboratory, UC Berkeley\n\n这里我们以Cycle GAN 为例，如上图所示，这个GAN的架构和Conditional GAN 好像没什么不同。G就是将输入转换成一个同样大小的输出但是风格产生变化，D就是将G的输出和梵高的画作做对比，相似性较高的就给出高分，较低就给出低分。我们希望用这样的方法使得G产生的output是带有梵高风格的画作。\n\n但是这样做G有可能就学会只输出梵高的某一幅画或者某几幅画来骗过D，所以这样是不行的。我们要做改进：\n\n通过加一个G将前一个G生成的图片还原回原图片的方式，限制G通过产生固定输出骗过D的情况出现。这种方法叫做cycle consistency 所以这种GAN叫做cycle GAN。\n\ncycle GAN 可以是双向的：刚才讲的是先X→YX \\rightarrow YX→Y 再Y→XY \\rightarrow XY→X，你现在可以先Y→XY \\rightarrow XY→X再X→YX \\rightarrow YX→Y ，你同样要一个X domain 的discriminator 看看Y→XY \\rightarrow XY→X的结果想不想是真实的X domain 的图片，用另外一个X→YX \\rightarrow YX→Y的Generator 把输出转回输入，让两者越接近越好。\n同样的技术也可以用在其他任务上，举例来说，假设现在X domain Y domain 是两个人的声音，那你就可以用Cycle GAN 做音色转换。再比如说，X domain Y domain 是不同风格的文字，那你就可以用Cycle GAN 做文字风格转换。再比如说，X domain Y domain 是正面和负面的句子，我们就可以用同样的训练的想法将负面的句子转成正面的句子。\n\n Discrete Issue\n需要提醒一下，如果直接把影像的技术套用到文字上是会有一些问题的。\n\n上图就是Cycle GAN 的架构，大概率会使用seq2seq的模型来处理文字，而中间生成的positive 的句子作为一个hidden layer的话，就不可以微分了，因为这个句子他是一个token，是离散的值，所以没办法用backpropagation更新参数。之前做的图像的任务G生成的图像是连续值，是可微分的。那怎么办呢，其实在文献上是有各种而样的办法的，这里就不展开了：\n\n\n\nGumbel-softmax\n\n[Matt J. Kusner, et al, arXiv, 2016]\n\n\n\nContinuous Input for Discriminator\n\n[Sai Rajeswar, et al., arXiv, 2017][Ofir Press, et al., ICML workshop, 2017][Zhen Xu, et al., EMNLP, 2017][Alex Lamb, et al., NIPS, 2016][Yizhe Zhang, et al., ICML, 2017]\n\n\n\n“Reinforcement Learning”\n\n[Yu, et al., AAAI, 2017][Li, et al., EMNLP, 2017][Tong Che, et al, arXiv, 2017][Jiaxian Guo, et al., AAAI, 2018][Kevin Lin, et al, NIPS, 2017][William Fedus, et al., ICLR, 2018]\n\n\n\n\n\n\n这是一个实作的结果。\n Speech Recognition\n\n上述的Unsupervised GAN的技术其实不只能用于风格迁移、正负面转换这些场景，还可以有很多其他的应用，比如说一个很有意义的就是语音识别。\n在语音识别任务中，监督学习需要大量的语音注释。然而，大多数语言资源不足。我们这么考虑，一个domain是语音，一个domain是文字，而无标签的语音和文字的数据在网络上是由很多很多的，所以有没有可能用Unsupervised GAN的技术在无标签的情况下就让机器学会语音辨识，细节其实比较复杂，这里就跳过，感兴趣就去看文章开头的课程。\n Experimental Result\n\n这里讲一下结果，上图李宏毅老师的实验室做的结果。TIMIT Benchmark Corpus 上进行测试，在Nonmatched PER上达到了33.1% 是一个很低的错误率了。Nonmatched是说训练数据中音频和文字是不对应的，并不是说文字是根据音频人工写出来的，而是不对应的。PER（phoneme error rate）你可以理解为音表错误率。\n\n可以看到在准确率上，这个无监督的GAN训练出来的模型可以和三十年内的有监督学习的匹敌。\n","categories":["李宏毅机器学习笔记"],"tags":["ML","note","Next-Step-of-ML"]},{"title":"污点分析技术笔记","url":"/%E6%B1%A1%E7%82%B9%E5%88%86%E6%9E%90%E6%8A%80%E6%9C%AF%E7%AC%94%E8%AE%B0/","content":" 污点分析技术\n同符号执行一样,污点分析也是我们分析代码漏洞,检测攻击方式的重要手段,在漏洞自动化扫描或者检测工具中有着十分广泛的应用,本文主要参考(copy)王蕾等人2016年的二进制程序动态污点分析技术研究综述[1]和K0rz3n大佬的简单理解污点分析技术这篇文章做一些个人学习总结,文末附上参考的论文和文章链接.\n 0x1 污点分析基本原理\n 1.污点分析定义\n污点分析可以抽象成一个三元组&lt;sources,sinks,sanitizers&gt;的形式,其中,source 即污点源,代表直接引入不受信任的数据或者机密数据到系统中;sink 即污点汇聚点,代表直接产生安全敏感操作(违反数据完整性)或者泄露隐私数据到外界(违反数据保密性);sanitizer 即无害处理,代表通过数据加密或者移除危害操作等手段使数据传播不再对软件系统的信息安全产生危害.污点分析就是分析程序中由污点源引入的数据是否能够不经无害处理,而直接传播到污点汇聚点.如果不能,说明系统是信息流安全的;否则,说明系统产生了隐私数据泄露或危险数据操作等安全问题.\n在漏洞分析中,使用污点分析技术将所感兴趣的数据(通常来自程序的外部输入,假定所有输入都是危险的)标记为污点数据,然后通过跟踪和污点数据相关的信息的流向,可以知道它们是否会影响某些关键的程序操作,进而挖掘程序漏洞.即将程序是否存在某种漏洞的问题转化为污点信息是否会被 Sink 点上的操作所使用的问题.\n污点分析的处理过程可以分成 3 个阶段(如图 2 所示):\n(1) 识别污点源和汇聚点;(根据所分析的系统的不同使用定制的识别策略)\n(2) 污点传播分析;(利用特定的规则跟踪分析污点信息在程序中的传播过程)\n(3) 漏洞检测、无害处理\n\n 2.识别污点源和汇聚点\n识别污点源和污点汇聚点是污点分析的前提.目前,在不同的应用程序中识别污点源和汇聚点的方法各不相同.缺乏通用方法的原因一方面来自系统模型、编程语言之间的差异.另一方面,污点分析关注的安全漏洞类型不同,也会导致对污点源和污点汇聚点的收集方法迥异.表 1 所示为在 Web 应用程序漏洞检测中的污点源示例[29],它们是 Web 框架中关键对象的属性.\n\n现有的识别污点源和汇聚点的方法可以大致分成 3 类:\n(1)使用启发式的策略进行标记,例如把来自程序外部输入的数据统称为“污点”数据,保守地认为这些数据有可能包含恶意的攻击数据(如 PHP Aspis);\n(2)根据具体应用程序调用的 API 或者重要的数据类型,手工标记源和汇聚点(如 DroidSafe[12]);\n(3)使用统计或机器学习技术自动地识别和标记污点源及汇聚点.[11]\n 3.污点传播分析\n污点传播分析就是分析污点标记数据在程序中的传播途径.按照分析过程中关注的程序依赖关系的不同, 可以将污点传播分析分为显式流分析和隐式流分析.\n (1)显示流分析\n污点传播分析中的显式流分析就是分析污点标记如何随程序中变量之间的数据依赖关系传播.\n\n以图 3 所 示的程序为例,变量 a 和 b 被预定义的污点源函数 source 标记为污点源.假设 a 和 b 被赋予的污点标记分别为taint_a 和 taint_b.由于第 5 行的变量 x 直接数据依赖于变量 a,第 6 行的变量 y 直接数据依赖于变量 b,显式流分析会分别将污点标记 taint_a 和 taint_b 传播给第 5 行的变量 x 和第 6 行的变量 y.又由于 x 和 y 分别可以到达第 7 行和第 8 行的污点汇聚点(用预定义的污点汇聚点函数 sink 标识),图 3 所示的代码存在信息泄漏的问题.我们将在后面具体介绍目前污点传播分析中显式流分析面临的主要挑战和解决方法.\n (2)隐式流分析\n污点传播分析中的隐式流分析是分析污点标记如何随程序中变量之间的控制依赖关系传播,也就是分析污点标记如何从条件指令传播到其所控制的语句.\n\n在图 4 所示的程序中,变量 X 是被污点标记的字符串类型变量,变量 Y 和变量 X 之间并没有直接或间接的数据依赖关系(显式流关系),但 X 上的污点标记可以经过控制依赖隐式地传播到 Y.\n具体来说,由第 4 行的循环条件控制的外层循环顺序地取出 X 中的每一个字符,转化成整型后赋给变量 x,再由第 7 行的循环条件控制的内层循环以累加的方式将 x 的值赋给 y,最后由外层循环将 y 逐一传给 Y.最终,第 12 行的 Y 值和 X 值相同,程序存在信息泄漏问题.但是,如果不进行隐式流污点传播分析,第 12 行 的变量 Y 将不会被赋予污点标记,程序的信息泄漏问题被掩盖.\n隐式流污点传播一直以来都是一个重要的问题,和显式流一样,如果不被正确处理,会使污点分析的结果不精确.由于对隐式流污点传播处理不当导致本应被标记的变量没有被标记的问题称为欠污染(under-taint)问题.相反地,由于污点标记的数量过多而导致污点变量大量扩散的问题称为过污染(over-taint)问题.目前,针对隐式流问题的研究重点是尽量减少欠污染和过污染的情况.我们将在后面具体介绍现有技术是如何解决上述问题的.\n 4.无害处理\n污点数据在传播的过程中可能会经过无害处理模块,无害处理模块是指污点数据经过该模块的处理后,数据本身不再携带敏感信息或者针对该数据的操作不会再对系统产生危害.换言之,带污点标记的数据在经过无害处理模块后,污点标记可以被移除.正确地使用无害处理可以降低系统中污点标记的数量,提高污点分析的效率,并且避免由于污点扩散导致的分析结果不精确的问题.\n在应用过程中,为了防止敏感数据被泄露(保护保密性),通常会对敏感数据进行加密处理.此时,加密库函数应该被识别成无害处理模块.这一方面是由于库函数中使用了大量的加密算法,导致攻击者很难有效地计算出密码的可能范围;另一方面是加密后的数据不再具有威胁性,继续传播污点标记没有意义.\n此外,为了防止外界数据因为携带危险操作而对系统关键区域产生危害(保护完整性),通常会对输入的数据进行验证.此时,输入验证(input validation)模块应当被识别成无害处理模块.例如,为了防止代码注入漏洞,PHP 提供的 htmlentities 函数可以将特殊含义的 HTML 字符串转化成HTML实体(例如,将’&lt;’转化成’&amp;lt;’).输入字符串经过上述转化后不会再携带可能产生危害的代码,可以安全地 发送给用户使用.除了语言自带的输入验证函数外,一些系统还提供了额外的输入验证工具,比如ScriptGard[50],CSAS[51],XSS Auditor[]52,Bek[53].这些工具也应被识别成无害处理模块.\n综上,目前对污点源、污点汇聚点以及无害处理模块的识别通常根据系统或漏洞类型使用定制的方法.由于这些方法都比较直接,本文将不再进行更深入的探讨.下一节将重点介绍污点传播中的关键技术.\n总结一下,使用污点分析检测程序漏洞的工作原理如下图所示：\n\n\n基于数据流的污点分析(显示流分析).在不考虑隐式信息流的情况下,可以将污点分析看做针对污点数据的数据流分析.根据污点传播规则跟踪污点信息或者标记路径上的变量污染情况,进而检查污点信息是否影响敏感操作.\n基于依赖关系的污点分析(隐式流分析).考虑隐式信息流,在分析过程中,根据程序中的语句或者指令之间的依赖关系,检查 Sink 点处敏感操作是否依赖于 Source 点处接收污点信息的操作.\n\n 0x2 污点传播分析的关键技术\n污点传播分析是当前污点分析领域的研究重点.与程序分析技术相结合,可以获得更加高效、精确的污点分析结果.根据分析过程中是否需要运行程序,可以将污点传播分析分为静态污点分析和动态污点分析.本节主要介绍如何使用动/静态程序分析技术来解决污点传播中的显式流分析和隐式流分析问题.\n笔者认为显式流分析和隐式流分析是从两种不同的角度(数据流和控制流)来观察污点传播,所以在实作的时候是应该两种分析方式同时应用的,这样才能全面的分析到污点的传播情况,静态和动态方法则可以取其一,也可以结合使用.\n 1.污点传播中的显式流分析\n (1)静态分析技术\n静态污点传播分析(简称静态污点分析)是指在不运行且不修改代码的前提下,通过分析程序变量间的数据依赖关系来检测数据能否从污点源传播到污点汇聚点.\n静态污点分析的对象一般是程序的源码或中间表示.可以将对污点传播中显式流的静态分析问题转化为对程序中静态数据依赖的分析:\n(1)首先,根据程序中的函数调用关系构建调用图(call graph,简称CG);\n(2)然后,在函数内或者函数间根据不同的程序特性进行具体的数据流传播分析.常见的显式流污点传播方式包括直接赋值传播、通过函数(过程)调用传播以及通过别名(指针)传播.\n以图 5 所示的 Java 程序为例:\n\n第 3 行的变量 b 为初始的污点标记变量,程序第 4 行将一个包含变量 b 的算术表达式的计算结果直接赋给变量 c.由于变量 c 和变量 b 之间具有直接的赋值关系,污点标记可直接从赋值语句右部的变量传播到左部,也就是上述 3种显式流污点传播方式中的直接赋值传播.\n接下来,变量 c 被作为实参传递给程序第 5 行的函数 foo,c 上的污点标记也通过函数调用传播到 foo 的形参 z,z 的污点标记又通过直接赋值传播到程序第 8 行的 x.f.由于 foo 的另外两个参数对象 x 和 y 都是对对象 a 的引用,二者之间存在别名,因此,x.f的污点标记可以通过别名传播到第 9 行的污点汇聚点,程序存在泄漏问题.\n目前,利用数据流分析解决显式污点传播分析中的直接赋值传播和函数调用传播已经相当成熟,研究的重点是如何为别名传播的分析提供更精确、高效的解决方案.由于精确度越高(上下文敏感、流敏感、域敏感、对象敏感等)的程序静态分析技术往往伴随着越大的时空开销,追求全敏感且高效的别名分析难度较大.又由于静态污点传播分析关注的是从污点源到污点汇聚点之间的数据流关系,分析对象并非完整的程序,而是确定的入口和出口之间的程序片段.这就意味着可以尝试采用按需(on-demand)定制的别名分析方法来解决显式流态污点分析中的别名传播问题.文献[10]使用按需的上下文敏感的别名分析的污点分析方法来检测Java应用程序漏洞.TAJ[25]工具使用了混合切片结合对象敏感的别名分析来进行 Java Web 应用上的污点分析.Andromeda[61]工具使用了按需的对象敏感别名分析技术解决对象的访问路径(access path)问题,FlowDroid[13]工具提出一种按需的别名分析,从而提供上下文敏感、流敏感、域敏感、对象敏感的污点分析,用以解决 Android的隐私泄露问题.\n (2)动态分析技术\n动态污点传播分析(简称动态污点分析)是指在程序运行过程中,通过实时监控程序的污点数据在系统程序中的传播来检测数据能否从污点源传播到污点汇聚点.动态污点传播分析首先需要为污点数据扩展一个污点标记(tainted tag)的标签并将其存储在存储单元(内存、寄存器、缓存等)中,然后根据指令类型和指令操作数设计相应的传播逻辑传播污点标记.\n动态污点传播分析按照实现层次被分为基于硬件、基于软件以及混合型的污点传播分析这3类.\n 1.硬件\n基于硬件的污点传播分析需要定制的硬件支持,一般需要在原有体系结构上为寄存器或者内存扩展一个标记位,用来存储污点标记,代表的系统有 Minos[62],Raksha[63]等.\n 2.基于软件\n基于软件的污点传播分析通过修改程序的二进制代码来进行污点标记位的存储与传播,代表的系统有 TaintEraser[64],TaintDroid[19]等.\n基于软件的污点传播的优点在于不必更改处理器等底层的硬件,并且可以支持更高的语义逻辑的安全策略(利用其更贴近源程序层次的特点),但缺点是使用插桩(instrumentation 在保证被测程序原有逻辑完整性的基础上在程序中插入一些探针)或代码重写(code rewriting)修改程序往往会给分析系统带来巨大的开销.相反地,基于硬件的污点传播分析虽然可以利用定制硬件降低开销,但通常不能支持更高的语义逻辑的安全策略,并且需要对处理器结构进行重新设计.\n 3.混合型\n混合型的污点分析是对上述两类方法的折中,即,通过尽可能少的硬件结构改动以保证更高的语义逻辑的安全策略,代表的系统有 Flexitaint[65],PIFT[19]等.\n目前,针对动态污点传播分析的研究工作关注的首要问题是如何设计有效的污点传播逻辑,以确保精确的污点传播分析.\nTaintCheck[67]利用插桩工具Valgrind[68]对其中间表示Ucode插桩并提供移动指令、算术指令以及除移动和算术外其他指令的3类传播逻辑实现对x86程序的动态污点分析.Privacy Scope[69],Dytan[70]和Libdft[71]以插桩工具PinTool为基础,实现针对x86程序的动态污点分析,并解决了一系列x86指令污点传播逻辑的问题.TaintDroid[19]提供了一套基于Android Dalvik虚拟机的DEX格式[73]的污点传播分析方法.由于DEX的指令包含多数常用的指令和具有面向对象特性的指令,普适性高,这里以TaintDroid中污点传播方法为示例,介绍动态污点传播的逻辑.\nDEX支持的变量类型有5种:本地变量、方法参数、类静态域、类实例域和数组.TaintDroid用υX\\upsilon_XυX​代表本地变量和方法参数,fXf_XfX​代表类的静态域,υY(fX)\\upsilon_Y(f_X)υY​(fX​)代表实例域,其中,υY\\upsilon_YυY​是具体实例的变量引用.υX[⋅]\\upsilon_X[⋅]υX​[⋅]代表数组,其中,υX\\upsilon_XυX​表示数组的对象引用.同时,TaintDroid使用虚拟污点映射函数τ(⋅)\\tau(⋅)τ(⋅)来辅助污点传播,对于变量υ\\upsilonυ, τ(υ)\\tau(\\upsilon)τ(υ)返回变量的污点标记ttt. τ(υ)\\tau(\\upsilon)τ(υ)可以被赋值给其他的变量.符号←代表将位于符号右部的变量的污点标记传播给左部的变量.具体的污点传播逻辑规则见表2.\n\n如果你上面一段没看懂,没关系,你只要知道TaintDroid使用的动态污点传播分析的传播逻辑就是上表所示.\n举例来说,对于指令move-op υA\\upsilon_AυA​ υB\\upsilon_BυB​ 污点传播规则就是将B的标记传播给A,也就是说如果B被标记为污点则A也应该被标记为污点.\n 2.污点传播中的隐式流分析\n污点传播分析中的隐式流分析就是分析污点数据如何通过控制依赖进行传播,如果忽略了对隐式流污点传播的分析,则会导致欠污染的情况;如果对隐式流分析不当,那么除了欠污染之外,还可能出现过污染的情况.与显式流分析类似,隐式流分析技术同样也可以分为静态分析和动态分析两类.\n (1)静态分析技术\n静态隐式流分析面临的核心问题是精度与效率不可兼得的问题.精确的隐式流污点传播分析需要分析每一个分支控制条件是否需要传播污点标记.路径敏感的数据流分析往往会产生路径爆炸问题,导致开销难以接受.为了降低开销,一种简单的静态传播(标记)分支语句的污点标记方法是将控制依赖于它的语句全部进行污点标记,但该方法会导致一些并不携带隐私数据的变量被标记,导致过污染情况的发生.过污染会引起污点的大量扩散,最终导致用户得到的报告中信息过多,难以使用.\n (2)动态分析技术\n动态隐式流分析关注的首要问题是如何确定污点控制条件下需要标记的语句的范围.由于动态执行轨迹并不能反映出被执行的指令之间的控制依赖关系,目前的研究多采用离线的静态分析辅助判断动态污点传播中的隐式流标记范围.Clause等人[70]提出,利用离线静态分析得到的控制流图节点间的后支配(post-dominate)关系来解决动态污点传播中的隐式流标记问题.\n例如,如图 6(a)所示,程序第 3 行的分支语句被标记为污点源,当document.cookie 的值为 abc 时,会发生污点数据泄露.根据基于后支配关系的标记算法,会对该示例第 4 行语句的指令目的地,即 x 的值进行污点标记.(ps:因为根据该分支控制下的语句的执行结果可以判定污染源document.cookie的值,造成污点数据泄露)\n\n动态分析面临的第 2 个问题是由于部分泄漏(partially leaked)导致的漏报.部分泄漏是指污点信息通过动态未执行部分进行传播并泄漏.Vogt等人[29]发现,只动态地标记分支条件下的语句会发生这种情况.\n仍以图 6(a)中的程序为例:当第 3 行的控制条件被执行时,对应的 x 会被标记.此时,x 的值为 true,而 y 值没有变化,仍然为 false.在后续执行过程中,由于第 9行的污点汇聚点不可达,而第 12 行的汇聚点可达,动态分析没有检测到污点数据泄漏.但攻击者由第 11 行 y 等于 false 的条件能够反推出程序执行了第 3 行的分支条件,程序实际上存在信息泄漏的问题.这个信息泄露是由第 6 行未被执行到的 y 的赋值语句所触发的.因此,y 应该被动态污点传播分析所标记.为了解决部分泄漏问题,Vogt等人在传统的动态污点分析基础上增加了离线的静态分析,以跟踪动态执行过程中的控制依赖关系,对污点分支控制范围内的所有赋值语句中的变量都进行标记.具体到图 6(a)所示的例子,就是第 4 行和第 6 行中的变量均会被污点标记.但是,Vogt 等人的方法仍然会产生过污染的情况.\n动态分析需要解决的第 3 个问题是如何选择合适的污点标记分支进行污点传播.鉴于单纯地将所有包含污点标记的分支进行传播会导致过污染的情况,可以根据信息泄漏范围的不同,定量地设计污点标记分支的选择策略.\n以图 6(b)所示的程序为例,第 2 行的变量 a 为初始的污点标记变量.第 5 行、第 7 行、第 9 行均为以 a作为源操作数的污点标记的分支.如果传播策略为只要分支指令中包含污点标记就对其进行传播,那么第 5 行、第 7 行、第 9 行将分别被传播给第 6 行、第 8 行、第 10 行,并最终传播到第 12 行的污点汇聚点.如果对这段程序进行深入分析会发现,3个分支条件所提供的信息值(所能泄露的信息范围)并不相同,分别是 a 等于 10、a大于 10 且小于或等于 13(将 w 值代入计算)以及 a 小于 10.对于 a 等于 10 的情况,攻击者可以根据第 12 行泄漏的 x 的值直接还原出污点源处 a 的值(这类分支也被称为能够保存完整信息的分支);对于 a 大于 10 且小于或等于 13 的情况,攻击者也只需要尝试 3 次就可以还原信息;而对于 a 小于 10 的情况,攻击者所获得的不确定性较大,成功还原信息的几率显著低于前两种,对该分支进行污点传播的实际意义不大.\nBao等人[76]只将严格控制依赖(strict control dependence)识别成需要污点传播的分支,其中,严格控制依赖即分支条件表达式的两端具有常数差异的分支.但是,Bao 的方法只适用于能够在编译阶段计算出常数差异的分支.\nKang 等人[77]提出的 DTA++ 工具使用基于离线执行踪迹(trace)的符号执行的方法来寻找进行污点传播的分支,但该方法只关注信息被完整保存的分支,即图 6(b)中第 5 行的 a==10 会被选择污点传播,但是信息仍然能够通过另一个范围(第 7 行的分支)而泄露.\nCox 等人[78]提出的 SpanDex 的主要思想是:动态地获得控制分支中污点数据的范围,根据数据的更改以及数据间的依赖关系构建一个基于操作的有向无环图(OP-DAG),再结合一个在线的约束求解器(CSP solver)确定隐式流中传播的隐私数据值的范围,通过预先设定的阈值,选择是否对数据进行污点传播.该方法对图 6(b)所示例子中的第 5 行、第 7行的分支进行污点传播.但是目前,该方法只能求解密码字符的范围,暂不支持对复杂操作(位、除法、数组等操作)的求解.\n 0x3 污点分析方法实现\n为了更进一步理解污点分析技术的实现,加入这一节,主要摘录自CTF-All-In-One[2]\n 1.静态污点分析技术\n静态污点分析系统首先对程序代码进行解析,获得程序代码的中间表示,然后在中间表示的基础上对程序代码进行控制流分析等辅助分析,以获得需要的控制流图、调用图等.在辅助分析的过程中,系统可以利用污点分析规则在中间表示上识别程序中的 Source 点和 Sink 点.最后检测系统根据污点分析规则,利用静态污点分析检查程序是否存在污点类型的漏洞.\n (1)基于数据流的污点分析\n在基于数据流的污点分析中,常常需要一些辅助分析技术,例如别名分析、取值分析等,来提高分析精度.辅助分析和污点分析交替进行,通常沿着程序路径的方向分析污点信息的流向,检查 Source 点处程序接收的污点信息是否会影响到 Sink 点处的敏感操作.\n过程内的分析中,按照一定的顺序分析过程内的每一条语句或者指令,进而分析污点信息的流向.\n\n记录污点信息.在静态分析层面,程序变量的污染情况为主要关注对象.为记录污染信息,通常为变量添加一个污染标签.最简单的就是一个布尔型变量,表示变量是否被污染.更复杂的标签还可以记录变量的污染信息来自哪些 Source 点,甚至精确到 Source 点接收数据的哪一部分.当然也可以不使用污染标签,这时我们通过对变量进行跟踪的方式达到分析污点信息流向的目的.例如使用栈或者队列来记录被污染的变量.\n程序语句的分析.在确定如何记录污染信息后,将对程序语句进行静态分析.通常我们主要关注赋值语句、控制转移语句以及过程调用语句三类.\n\n赋值语句.\n\n对于简单的赋值语句,形如 a = b 这样的,记录语句左端的变量和右端的变量具有相同的污染状态.程序中的常量通常认为是未污染的,如果一个变量被赋值为常量,在不考虑隐式信息流的情况下,认为变量的状态在赋值后是未污染的.\n对于形如 a = b + c 这样带有二元操作的赋值语句,通常规定如果右端的操作数只要有一个是被污染的,则左端的变量是污染的(除非右端计算结果为常量).\n对于和数组元素相关的赋值,如果可以通过静态分析确定数组下标的取值或者取值范围,那么就可以精确地判断数组中哪个或哪些元素是污染的.但通常静态分析不能确定一个变量是污染的,那么就简单地认为整个数组都是污染的.\n对于包含字段或者包含指针操作的赋值语句,常常需要用到指向分析的分析结果.\n\n\n控制转移语句.\n\n在分析条件控制转移语句时,首先考虑语句中的路径条件可能是包含对污点数据的限制,在实际分析中常常需要识别这种限制污点数据的条件,以判断这些限制条件是否足够包含程序不会受到攻击.如果得出路径条件的限制是足够的,那么可以将相应的变量标记为未污染的.\n对于循环语句,通常规定循环变量的取值范围不能受到输入的影响.例如在语句 for (i = 1; i &lt; k; i++)&#123;&#125; 中,可以规定循环的上界 k 不能是污染的.\n\n\n过程调用语句.\n\n可以使用过程间的分析或者直接应用过程摘要进行分析.污点分析所使用的过程摘要主要描述怎样改变与该过程相关的变量的污染状态,以及对哪些变量的污染状态进行检测.这些变量可以是过程使用的参数、参数的字段或者过程的返回值等.例如在语句 flag = obj.method(str); 中,str 是污染的,那么通过过程间的分析,将变量 obj 的字段 str 标记为污染的,而记录方法的返回值的变量 flag 标记为未污染的.\n在实际的过程间分析中,可以对已经分析过的过程构建过程摘要.例如前面的语句,其过程摘要描述为：方法 method 的参数污染状态决定其接收对象的实例域 str 的污染状态,并且它的返回值是未受污染的.那么下一次分析需要时,就可以直接应用摘要进行分析.\n\n\n\n\n代码的遍历.一般情况下,常常使用流敏感的方式或者路径敏感的方式进行遍历,并分析过程中的代码.如果使用流敏感的方式,可以通过对不同路径上的分析结果进行汇集,以发现程序中的数据净化规则.如果使用路径敏感的分析方式,则需要关注路径条件,如果路径条件中涉及对污染变量取值的限制,可认为路径条件对污染数据进行了净化,还可以将分析路径条件对污染数据的限制进行记录,如果在一条程序路径上,这些限制足够保证数据不会被攻击者利用,就可以将相应的变量标记为未污染的.\n\n过程间的分析与数据流过程间分析类似,使用自底向上的分析方法,分析调用图中的每一个过程,进而对程序进行整体的分析.\n (2)基于依赖关系的污点分析\n在基于依赖关系的污点分析中,首先利用程序的中间表示、控制流图和过程调用图构造程序完整的或者局部的程序的依赖关系.在分析程序依赖关系后,根据污点分析规则,检测 Sink 点处敏感操作是否依赖于 Source 点.\n分析程序依赖关系的过程可以看做是构建程序依赖图的过程.程序依赖图是一个有向图.它的节点是程序语句,它的有向边表示程序语句之间的依赖关系.程序依赖图的有向边常常包括数据依赖边和控制依赖边.在构建有一定规模的程序的依赖图时,需要按需地构建程序依赖关系,并且优先考虑和污点信息相关的程序代码.\n 静态污点分析实例分析\n在使用污点分析方法检测程序漏洞时,污点数据相关的程序漏洞是主要关注对象,如 SQL 注入漏洞、命令注入漏洞和跨站脚本漏洞等.\n下面是一个存在 SQL 注入漏洞 ASP 程序的例子：\n&lt;%\n    Set pwd &#x3D; &quot;bar&quot;\n    Set sql1 &#x3D; &quot;SELECT companyname FROM &quot; &amp; Request.Cookies(&quot;hello&quot;)\n    Set sql2 &#x3D; Request.QueryString(&quot;foo&quot;)\n    MySqlStuff pwd, sql1, sql2\n    Sub MySqlStuff(password, cmd1, cmd2)\n    Set conn &#x3D; Server.CreateObject(&quot;ADODB.Connection&quot;)\n    conn.Provider &#x3D; &quot;Microsoft.Jet.OLEDB.4.0&quot;\n    conn.Open &quot;c:&#x2F;webdata&#x2F;foo.mdb&quot;, &quot;foo&quot;, password\n    Set rs &#x3D; conn.Execute(cmd2)\n    Set rs &#x3D; Server.CreateObject(&quot;ADODB.recordset&quot;)\n    rs.Open cmd1, conn\n    End Sub\n%&gt;\n首先对这段代码表示为一种三地址码的形式,例如第 3 行可以表示为：\na &#x3D; &quot;SELECT companyname FROM &quot;\nb &#x3D; &quot;hello&quot;\nparam0 Request\nparam1 b\ncallCookies\nreturn c\nsql1 &#x3D; a &amp; c\n解析完毕后,需要对程序代码进行控制流分析,这里只包含了一个调用关系(第 5 行).\n具体的分析过程如下:\n\n调用 Request.Cookies(“hello”) 的返回结果是污染的,所以变量 sql1 也是污染的.\n调用 Request.QueryString(“foo”) 的返回结果 sql2 是污染的.\n函数 MySqlStuff 被调用,它的参数 sql1,sql2 都是污染的.分了分析函数的处理过程,根据第 6 行函数的声明,标记其参数 cmd1,cmd2 是污染的.\n第 10 行是程序的 Sink 点,函数 conn.Execute 执行 SQL 操作,其参数 cmd2 是污染的,进而发现污染数据从 Source 点传播到 Sink 点.因此,认为程序存在 SQL 注入漏洞\n\n 2.动态污点分析\n动态污点分析是在程序运行的基础上,对数据流或控制流进行监控,从而实现对数据在内存中的显式传播、数据误用等进行跟踪和检测.动态污点分析与静态污点分析的唯一区别在于静态污点分析技术在检测时并不真正运行程序,而是通过模拟程序的执行过程来传播污点标记,而动态污点分析技术需要运行程序,同时实时传播并检测污点标记.\n动态污点分析技术可分为三个部分：\n\n污点数据标记：程序攻击面是程序接受输入数据的接口集,一般由程序入口点和外部函数调用组成.在污点分析中,来自外部的输入数据会被标记为污点数据.根据输入数据来源的不同,可分为三类：网络输入、文件输入和输入设备输入.\n污点动态跟踪：在污点数据标记的基础上,对进程进行指令粒度的动态跟踪分析,分析每一条指令的效果,直至覆盖整个程序的运行过程,跟踪数据流的传播.\n\n动态污点跟踪通常基于以下三种机制\n\n动态代码插桩：可以跟踪单个进程的污点数据流动,通过在被分析程序中插入分析代码,跟踪污点信息流在进程中的流动方向.\n全系统模拟：利用全系统模拟技术,分析模拟系统中每条指令的污点信息扩散路径,可以跟踪污点数据在操作系统内的流动.\n虚拟机监视器：通过在虚拟机监视器中增加分析污点信息流的功能,跟踪污点数据在整个客户机中各个虚拟机之间的流动.\n\n\n污点动态跟踪通常需要影子内存(shadow memory)来映射实际内存的污染情况,从而记录内存区域和寄存器是否是被污染的.对每条语句进行分析的过程中,污点跟踪攻击根据影子内存判断是否存在污点信息的传播,从而对污点信息进行传播并将传播结果保存于影子内存中,进而追踪污点数据的流向.\n一般情况下,数据移动类和算数类指令都将造成显示的信息流传播.为了跟踪污点数据的显示传播,需要在每个数据移动指令和算数指令执行前做监控,当指令的结果被其中一个操作数污染后,把结果数据对应的影子内存设置为一个指针,指向源污染点操作数指向的数据结构.\n\n\n污点误用检查：在正确标记污点数据并对污点数据的传播进行实时跟踪后,就需要对攻击做出正确的检测即检测污点数据是否有非法使用的情况.\n\n动态污点分析的优缺点：\n\n优点：误报率较低,检测结果的可信度较高.\n缺点：\n\n漏报率较高：由于程序动态运行时的代码覆盖率决定的.\n平台相关性较高：特定的动态污点分析工具只能够解决在特定平台上运行的程序.\n资源消耗大：包括空间上和时间上.\n\n\n\n 动态污点分析的方法实现\n (1)污点数据标记\n污点数据通常主要是指软件系统所接受的外部输入数据,在计算机中,这些数据可能以内存临时数据的形式存储,也可能以文件的形式存储.当程序需要使用这些数据时,一般通过函数或系统调用来进行数据访问和处理,因此只需要对这些关键函数进行监控,即可得到程序读取或输出了什么污点信息.另外对于网络输入,也需要对网络操作函数进行监控.\n识别出污点数据后,需要对污点进行标记.污点生命周期是指在该生命周期的时间范围内,污点被定义为有效.污点生命周期开始于污点创建时刻,生成污点标记,结束于污点删除时刻,清除污点标记.\n\n污点创建\n\n将来自于非可靠来源的数据分配给某寄存器或内存操作数时\n将已经标记为污点的数据通过运算分配给某寄存器或内存操作数时\n\n\n污点删除\n\n将非污点数据指派给存放污点的寄存器或内存操作数时\n将污点数据指派给存放污点的寄存器或内存地址时,此时会删除原污点,并创建新污点\n一些会清除污点痕迹的算数运算或逻辑运算操作时\n\n\n\n (2)污点动态跟踪\n当污点数据从一个位置传递到另一个位置时,则认为产生了污点传播.污点传播规则：\n\n\n\n指令类型\n传播规则\n举例说明\n\n\n\n\n拷贝或移动指令\nT(a)&lt;-T(b)\nmov a, b\n\n\n算数运算指令\nT(a)&lt;-T(b)\nadd a, b\n\n\n堆栈操作指令\nT(esp)&lt;-T(a)\npush a\n\n\n拷贝或移动类函数调用指令\nT(dst)&lt;-T(src)\ncall memcpy\n\n\n清零指令\nT(a)&lt;-false\nxor a, a\n\n\n\n注：T(x) 的取值分为 true 和 false 两种,取值为 true 时表示 x 为污点,否则 x 不是污点.\n对于污点信息流,通过污点跟踪和函数监控,已经能够进行污点信息流流动方向的分析.但由于缺少对象级的信息,仅靠指令级的信息流动并不能完全给出要分析的软件的确切行为.因此,需要在函数监控的基础上进行视图重建,如获取文件对象和套接字对象的详细信息,以方便进一步的分析工作.\n污点动态跟踪的实现通常使用：\n\n影子内存：真实内存中污点数据的镜像,用于存放程序执行的当前时刻所有的有效污点.\n污点传播树：用于表示污点的传播关系.\n污点处理指令链：用于按时间顺序存储与污点数据处理相关的所有指令.\n\n当遇到会引起污点传播的指令时,首先对指令中的每个操作数都通过污点快速映射查找影子内存中是否存在与之对应的影子污点从而确定其是否为污点数据,然后根据污点传播规则得到该指令引起的污点传播结果,并将传播产生的新污点添加到影子内存和污点传播树中,同时将失效污点对应的影子污点删除.同时由于一条指令是否涉及污点数据的处理,需要在污点分析过程中动态确定,因此需要在污点处理指令链中记录污点数据的指令信息.\n (3)污点误用检查\n污点敏感点,即 Sink 点,是污点数据有可能被误用的指令或系统调用点,主要分为：\n\n跳转地址：检查污点数据是否用于跳转对象,如返回地址、函数指针、函数指针偏移等.具体操作是在每个跳转类指令(如call、ret、jmp等)执行前进行监控分析,保证跳转对象不是污点数据所在的内存地址.\n格式化字符串：检查污点数据是否用作printf系列函数的格式化字符串参数.\n系统调用参数：检查特殊系统调用的特殊参数是否为污点数据.\n标志位：跟踪标志位是否被感染,及被感染的标志位是否用于改变程序控制流.\n地址：检查数据移动类指令的地址是否被感染.\n\n根据sink 出现的误用情况,确定污点的漏洞类型.即在进行污点误用检查时,通常需要根据一些漏洞模式来进行检查,首先需要明确常见漏洞在二进制代码上的表现形式,然后将其提炼成漏洞模式,以更有效地指导自动化的安全分析.\n 动态污点分析的实例分析\n下面我们来看一个使用动态污点分析的方法检测缓冲区溢出漏洞的例子.\nvoid fun(char *str)\n&#123;\n    char temp[15];\n    printf(\"in strncpy, source: %s\\n\", str);\n    strncpy(temp, str, strlen(str));        // Sink 点\n&#125;\nint main(int argc, char *argv[])\n&#123;\n    char source[30];\n    gets(source);                           // Source 点\n    if (strlen(source) &lt; 30)\n        fun(source);\n    else\n        printf(\"too long string, %s\\n\", source);\n    return 0;\n&#125;\n漏洞很明显, 调用 strncpy 函数存在缓冲区溢出.\n程序接受外部输入字符串的二进制代码如下：\n0x08048609 &lt;+51&gt;:    lea    eax,[ebp-0x2a]\n0x0804860c &lt;+54&gt;:    push   eax\n0x0804860d &lt;+55&gt;:    call   0x8048400 &lt;gets@plt&gt;\n...\n0x0804862c &lt;+86&gt;:    lea    eax,[ebp-0x2a]\n0x0804862f &lt;+89&gt;:    push   eax\n0x08048630 &lt;+90&gt;:    call   0x8048566 &lt;fun&gt;\n程序调用 strncpy 函数的二进制代码如下：\n0x080485a1 &lt;+59&gt;:    push   DWORD PTR [ebp-0x2c]\n0x080485a4 &lt;+62&gt;:    call   0x8048420 &lt;strlen@plt&gt;\n0x080485a9 &lt;+67&gt;:    add    esp,0x10\n0x080485ac &lt;+70&gt;:    sub    esp,0x4\n0x080485af &lt;+73&gt;:    push   eax\n0x080485b0 &lt;+74&gt;:    push   DWORD PTR [ebp-0x2c]\n0x080485b3 &lt;+77&gt;:    lea    eax,[ebp-0x1b]\n0x080485b6 &lt;+80&gt;:    push   eax\n0x080485b7 &lt;+81&gt;:    call   0x8048440 &lt;strncpy@plt&gt;\n首先,在扫描该程序的二进制代码时,能够扫描到 call &lt;gets@plt&gt;,该函数会读入外部输入,即程序的攻击面.确定了攻击面后,我们将分析污染源数据并进行标记,即将 [ebp-0x2a] 数组(即源程序中的source)标记为污点数据.程序继续执行,该污染标记会随着该值的传播而一直传递.在进入 fun() 函数时,该污染标记通过形参实参的映射传递到参数 str 上.然后运行到 Sink 点函数 strncpy().该函数的第二个参数即 str 和 第三个参数 strlen(str) 都是污点数据.最后在执行 strncpy() 函数时,若设定了相应的漏洞规则(目标数组小于源数组),则漏洞规则将被触发,检测出缓冲区溢出漏洞.\n 0x4 污点分析在实际应用中的关键技术\n污点分析被广泛地应用在系统隐私数据泄露、安全漏洞等问题的检测中.在实际应用过程中,由于系统框架、语言特性等方面的差异,通用的污点分析技术往往难以适用.比如:系统框架的高度模块化以及各模块之间复杂的调用关系导致污点源到汇聚点的传播路径变得复杂、庞大,采用通用的污点分析技术可能面临开销难以接受的问题;通用的污点分析技术对新的语言特性支持有限等.为此,需要针对不同的应用场景,对通用的污点分析技术进行扩展或定制.\n本节以两个代表性的应用场景——智能手机的隐私泄漏检测和Web应用安全漏洞检测为切入点,总结近10年来污点分析技术在上述领域的应用实践过程中所面临的问题和关键解决技术.\n 1.检测智能手机隐私泄露\n针对Android的污点传播分析也围绕组件展开,按照传播可能通过的模块的不同,分为组件内污点传播、组件间污点传播、组件与库函数之间的污点传播这3类(如图7所示).接下来将分别介绍针对这3类传播问题的静态和动态污点传播分析技术.\n\n 1.静态污点分析\n (1)组件内污点传播分析\n组件内部污点分析面临的主要问题是如何构建完整的分析模型.不同于传统的C/C++程序(有唯一的Main函数入口),Android应用程序存在有多个入口函数的情况.这个情况源于Android应用程序复杂的运行生命周期(例如onCreate,onStart,onResume,onPause等)以及程序中大量存在的回调函数和异步函数调用.由于任何的程序入口都有可能是隐私数据的来源,在静态的污点分析开始之前必须构建完整的应用程序模型,以确保程序中每一种可能的执行路径都会被静态污点传播分析覆盖到.\nLeakMiner[14]和CHEX[15]尝试使用增量的方法构建系统调用图.\nArzt等人设计的FlowDroid[13]提出了一种更系统的构建Android程序完整分析模型的方法:首先,通过XML配置文件提取与Android生命周期相关的入口函数,将这些方法作为节点,并根据Android生命周期构建调用图(如图8所示);其次,对于生命周期内的回调函数,在该调用图的基础上增加不透明谓词节点(即图8中菱形的P节点);然后,增量式地将回调函数加入这个函数调用图;最后,将调用图上所有的执行入口连接到一个虚假的Main函数上.FlowDroid中的一次合法的执行,就是对调用图进行的一次遍历.\n\nGordon等人[12]提出的DroidSafe使用Android设备实现(Android device implementation)来构建Android的完整分析模型.Android设备实现是对Android运行环境的一个简单模拟,它使用Java语言,结合Android Open Source Porject(AOSP),实现了与原Android接口语义等价的模型,并使用精确分析存根(accurate analysis stub)将AOSP代码之外的函数加入到模型中.\n (2)组件间污点传播分析\n即使正确分析了组件内的数据流关系,污点数据仍然可能通过组件间的数据流来传递,从而造成信息泄露.如上图7左侧所示,即使保证了对组件A内部污点传播的精确分析,组件A仍然可能通过调用方法startActivityforResult()将信息传递给组件B,再通过组件B产生泄露.因此,针对Android应用的污点分析还需要分析出组件间所有可能的数据流信息.组件间通信是通过组件发送Intent消息对象完成的.Intent按照参数字段是否包含目标组件名称分为显式Intent和隐式Intent.如图9所示:显式Intent对象使用一个包含目标组件名称的参数显式地指定通信的下一个组件;隐式Intent使用action,category等域隐式地让Android系统通过Intent Filter自动选择一个组件调用.目前,解决该问题的主要思想是利用Intent参数信息分析组件间的数据流.\n\n解决组件间数据流的前提是解析Intent的目的地,解析Intent目的地包括解析显式Intent的目的地和隐式Intent的目的地.由于显式Intent的目的地可以直接通过初始化Intent的地址字符串参数获得,目前,解析显式Intent目的地的常用方法是使用字符串分析工具(例如JSA[84])提取Intent中字符串参数的信息.\n解析隐式Intent目的地的主要方法是分析配置文件信息与Intent Filter注册器之间的映射关系,建立发送Intent组件和接受Intent组件之间的配对关系.在解析出Intent目的地之后,问题的重点转移到如何提高组件间数据流分析的精度上.\nKlieber等人[17]尝试在已经建立好的组件内污点分析的基础上,结合推导规则来分析组件间数据流.在分析之前,需要收集组件内部的污点源和汇聚点以及组件内Intent的发送目的地标签等信息.表4和表5给出了推导规则的前提定义和具体的推导规则,其中,一次完整的分析是指根据已知组件内部的信息src→sinksrc→sinksrc→sink以及推导规则识别所有src′→sink′src′→sink′src′→sink′的流集合.\nOcteau等人[18]尝试使用现有的程序分析方法提高组件间数据流分析的精度,他们将组件间数据流分析问题转化成IDE(interprocedural  distributive environment)问题[58]进行求解.DroidSafe设计了一种对象敏感的别名分析技术,在此基础上提供的精度优化方法包括:提取Intent的目的地的字符串参数、将Intent目的地的初始化函数嵌入到目的组件当中以提高别名分析的精度,同时,增加处理Android Service的支持.\n\n\n (3)组件与库函数之间的污点传播分析\n组件与库函数之间的污点传播分析面临的主要问题包括对Android库函数自身庞大的代码量的分析以及组件和某些库函数使用的实现语言不同(Android组件通常用Java实现,而本地库则采用C/C++代码编写)这两方面.\n目前的一类方法是使用手动定制来解决上述问题.比如,FlowDroid[13]尝试手动地分析库函数的语义,根据参数与返回值之间的关系为其提供显式传播规则.对于没有制定规则的库函数,FlowDroid使用保守的策略,即:只要库函数的参数中包含污点数据参数,就对函数的返回值进行污点标记.DroidSafe[12]使用精确分析存根实现了3176个本地代码库,这些存根是使用Java代码手动实现的,且与原本地代码的语义等价.对本地代码的调用被传递到对应的存根代码上进行分析.此类工作需要在人工理解程序语义的基础上加以实现.\n与之相对的是自动推导的方法.StubDroid[85]首次提出了用自动推导解决库函数传播的问题,该方法将产生供分析的摘要文件(summary file),将推导的污点数据流存储到摘要文件中,并提供了相应的部署策略.StubDroid的分析单元是一个API方法,将该方法内部的所有访问路径(包括方法参数、方法this引用和所有静态可见域)标记成源,将方法的返回值作为汇聚点.基于此,问题被转化成方法内部的污点分析问题. StubDroid利用现有的静态污点分析工具FlowDroid自动地处理库内部的污点数据流,最终得到的摘要文件包含API方法中的访问路径源是否能够传播到返回值的信息.另外,StubDroid还能够解决由于别名和访问路径带导致库函数传播中的精确度下降的问题,最后的摘要文件进一步被应用到FlowDroid框架,完成对整个app的分析.与FlowDroid类似,StudDroid同样无法对本地库函数进行自动分析.\n 2.动态污点分析\nAndroid系统中的动态污点同样需要分析组件内污点传播、组件间污点传播以及组件代码与本地库之间的污点传播.动态污点分析面临的主要挑战是系统信息除了在系统内部通过DEX指令传播以外,还会经过其他的通道,如本地库、网络、文件等.\nTaintDroid[19]首先提出了面向Android平台上的动态污点分析工具,之后的工具大多是基于它的优化或者应用扩展(Appsplaygroud[20],VetDroid[21],BayesDroid[22],AppFence[23]).本节重点介绍TaintDoid上的3种污点传播处理：\n1.组件内的污点传播主要是在Dalvik虚拟机DEX指令的变量级别传播,详见第2.1.2节动态分析技术中的关于TaintDroid的介绍.\n2.组件间的污点传播利用了Binder机制.Android底层使用Binder机制完成IPC调用,数据被存储在包(parcels)对象结构中.TaintDroid的污点传播方法是对包对象的结构进行重新设计,使之附带与污点相关的信息.TaintDroid提供了两种包结构扩展方法:\n(a) 使用一个标签变量将污点信息存储到包中,然后通过网络进行传输.当接受者接收到这个结构时,将其中包含的标签变量提取出来并继续传输;\n(b) 在方法(a)的基础上进行改进,提出了基于污点标签向量的传播技术.\n3.组件与本地库函数间的污点传播包括污点数据通过本地库代码或文件进行传播.TaintDroid通过设计后置条件对本地代码的函数进行污点传播.后置条件为:\n(a) 所有本地代码访问的外部变量都会被标记上污点标签;\n(b) 根据预定义规则将被赋值的函数返回值也标记上污点标签.\nTaintDroid使用了人工插桩与启发式相结合的方法来提供这些后置条件.TaintDroid解决污点数据通过文件进行传播的方法是以文件为单位添加污点标签,当文件被写入或者被读出到缓冲区时进行污点传播.如果一个被标记的数据被写入一个文件,就需要这个文件进行标记.该文件在后续的操作中将作为污点源,如果有对该文件的读取操作,那么读出的数据也需要被标记.\n 2.Java Web 框架上的污点分析技术\n不记录了,有需要的话可以读[1]原文4.2节\n 0x5 总结\n污点分析作为信息流分析的一种实践技术,被广泛应用于互联网及移动终端平台上应用程序的信息安全保障中.本文介绍了污点分析的基本原理和通用技术,并针对近年来污点分析在解决实际应用程序安全问题时遇到的问题和关键解决技术进行了分析综述.不同于基于安全类型系统的信息流分析技术,污点分析可以不改变程序现有的编程模型或语言特性,并提供精确信息流传播跟踪.在实际应用过程中,污点分析还需要借助传统的程序分析技术的支持,例如静态分析中的数据流分析、动态分析中的代码重写等技术.另外,结合测试用例生成技术、符号执行技术以及虚拟机技术,也会给污点分析带来更多行之有效的解决方案.\n如果文中有笔误，欢迎指正。😁\n 0x-1 参考\n\n[1] 王蕾,李丰,李炼,冯晓兵.污点分析技术的原理和实践应用.软件学报,2017,28(4):860−882.http://www.jos.org.cn/1000-9825/5190.htm\n[2] https://github.com/firmianay/CTF-All-In-One/blob/master/doc/5.5_taint_analysis.md\n[3] 宋铮，王永剑，金波，等.二进制程序动态污点分析技术研究综述[J].信息网络安全，2016（3）：77-83.\n[10] Livshits  VB,  Lam  MS.  Finding  security  vulnerabilities  in  Java  applications  with  static  analysis.  In:  Proc.  of  the  Conf.  on  Usenix Security Symp. USENIX Association, 2005. 262−266. https://www.usenix.org/legacy/event/sec05/tech/full_papers/livshits/livshits_html/\n[11] Rasthofer S, Arzt S, Bodden E. A machine-learning approach for classifying and categorizing android sources and sinks. In: Proc. of the Network and Distributed System Security Symp. (NDSS). 2014. [doi: 10.14722/ndss.2014.23039]\n[12] Gordon MI, Kim D, Perkins JH, Gilham L, Nguyen N, Rinard MC. Information flow analysis of Android applications in DroidSafe. In: Proc. of the NDSS 2015. 2015. [doi: 10.14722/ndss.2015.23089]\n[13] Arzt S, Rasthofer S, Fritz C, Bodden E, Bartel A, Klein J, Le Traon Y, Octeau D, McDaniel P. Flowdroid: Precise context, flow, field,  object-sensitive  and  lifecycle-aware  taint  analysis  for  Android  apps.  ACM  SIGPLAN  Notices,  2014,49(6):259−269.  [doi:  10.1145/2594291.2594299]\n[14] Yang  Z,  Yang  M.  Leakminer:  Detect  information  leakage  on  Android  with  static  taint  analysis.  In:  Proc.  of  the  Software  Engineering. IEEE, 2012. 101−104. [doi: 10.1109/WCSE.2012.26]\n[15] Lu L, Li Z, Wu Z, Lee W, Jiang G. Chex: Statically vetting Android apps for component hijacking vulnerabilities. In: Proc. of the 2012 ACM Conf. on Computer and Communications Security. ACM Press, 2012. 229−240. [doi: 10.1145/2382196.2382223]\n[19] Enck  W,  Gilbert  P,  Han  S,  Tendulkar  V,  Chun  BG,  Cox  LP,  Jung  J,  McDaniel  P,  Sheth  AN.  TaintDroid:  An  information-flow  tracking  system  for  realtime  privacy  monitoring  on  smartphones.  ACM  Trans.  on  Computer  Systems,  2014,32(2):393−407.  [doi:  10.1145/2619091]\n[25] Tripp O, Pistoia M, Fink SJ, Sridharan M, Weisman O. TAJ: Effective taint analysis of Web applications. ACM SIGPLAN Notices, 2009,44(6):87−97. [doi: 10.1145/1542476.1542486]\n[26] Papagiannis I, Migliavacca M, Pietzuch P. PHP ASPIS: Using partial taint tracking to protect against injection attacks. In: Proc. of the Usenix Conf. on Web Application Development. USENIX Association, 2011. 2. https://www.usenix.org/conference/webapps11/php-aspis-using-partial-taint-tracking-protect-against-injection-attacks\n[29] Vogt  P,  Nentwich  F,  Jovanovic  N,  Kirda  E,  Kruegel  C,  Vigna  G.  Cross  site  scripting  prevention  with  dynamic  data  tainting  and  static analysis. In: Proc. of the NDSS 2007. 2007. 12. http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.72.4505\n[50] Saxena P, Molnar D, Livshits B. SCRIPTGARD: Automatic context-sensitive sanitization for large-scale legacy Web applications. In: Proc. of the 18th ACM Conf. on Computer and Communications Security. ACM Press, 2011. 601−614. [doi: 10.1145/2046707. 2046776]\n[51] Samuel M, Saxena P, Song D. Context-Sensitive auto-sanitization in Web templating languages using type qualifiers. In: Proc. ofthe 18th ACM Conf. on Computer and Communications Security. ACM Press, 2011. 587−600. [doi: 10.1145/2046707.2046775]\n[52] Bates D, Barth A, Jackson C. Regular expressions considered harmful in client-side XSS filters. In: Proc. of the 19th Int’l Conf. on World Wide Web. ACM Press, 2010. 91−100. [doi: 10.1145/1772690.1772701]\n[53] Hooimeijer  P,  Livshits  B,  Molnar  D,  Saxena  P,  Veanes  M.  Fast  and  precise  sanitizer  analysis  with  BEK.  In:  Proc.  of  the  20th  USENIX Conf. on Security. USENIX Association, 2011. 1−1. http://dl.acm.org/citation.cfm?id=2028068\n[61] Tripp O, Pistoia M, Cousot P, Cousot R, Guarnieri S. Andromeda: Accurate and scalable security analysis of Web applications. In: Proc. of the Int’l Conf. on Fundamental Approaches to Software Engineering. Berlin, Heidelberg: Springer-Verlag, 2013. 210−225. [doi: 10.1007/978-3-642-37057-1_15]\n[62] Crandall JR, Chong FT. Minos: Control data attack prevention orthogonal to memory model. In: Proc. of the 37th Int’l Symp. on Microarchitecture (MICRO-37). IEEE, 2004. 221−232. [doi: 10.1109/MICRO.2004.26]\n[63] Dalton  M,  Kannan  H,  Kozyrakis  C.  Raksha:  A  flexible  information  flow  architecture  for  software  security.  ACM  SIGARCH  Computer Architecture News, 2007,35(2):482−493. [doi: 10.1145/1273440.1250722]\n[64] Zhu DY, Jung J, Song D, Kohno T, Wetherall D. Tainteraser: Protecting sensitive data leaks using applicationlevel taint tracking. ACM SIGOPS Operating Systems Review, 2011,45(1):142−154. [doi: 10.1145/1945023.1945039]\n[65] Venkataramani G, Doudalis I, Solihin Y, Prvulovic M. Flexitaint: A programmable accelerator for dynamic taint propagation. In: Proc. of the 2008 IEEE 14th Int’l Symp. on High Performance Computer Architecture. IEEE, 2008. 173−184. [doi: 10.1109/HPCA. 2008.4658637]\n[67] Newsome J, Song D. Dynamic taint analysis for automatic detection, analysis, and signature generation of exploits on commodity software. In: Proc. of the Network and Distributed System Security Symp. 2005. 720−724.\n[68] Nethercote  N,  Seward  J.  Valgrind:  A  program  supervision  framework.  Electronic  Notes  in  Theoretical  Computer  Science,  2003,  89(2):44−66. [doi: 10.1016/S1571-0661(04)81042-9]\n[69] Zhu Y, Jung J, Song D, Kohno T, Wetherall D. Privacy scope: A precise information flow tracking system for finding application leaks. Technical Report, EECS-2009-145, Berkeley: University of California, 2009.\n[70] Clause  J,  Li  W,  Orso  A.  DYTAN:  A  generic  dynamic  taint  analysis  framework.  In:  Proc.  of  the  2007  Int’l  Symp.  on  Software  Testing and Analysis. ACM Press, 2007. 196−206. [doi: 10.1145/1273463.1273490]\n[71] Kemerlis  VP,  Portokalidis  G,  Jee  K,  Keromytis  AD.  libdft:  Practical  dynamic  data  flow  tracking  for  commodity  systems.  ACM  SIGPLAN Notices, 2012,47(7):121−132. [doi: 10.1145/2365864.2151042]\n[76] Bao T, Zheng Y, Lin Z, Zhang X, Xu D. Strict control dependence and its effect on dynamic information flow analyses. In: Proc. of the 19th Int’l Symp. on Software Testing and Analysis. ACM Press, 2010. 13−24. [doi: 10.1145/1831708.1831711]\n[77] Kang MG, McCamant S, Poosankam P, Song D. DTA++: Dynamic taint analysis with targeted control-flow propagation. In: Proc. of the Network and Distributed System Security Symp. (NDSS 2011). San Diego, 2011.\n[78] Cox LP, Gilbert P, Lawler G, Pistol V, Razeen A, Wu B, Cheemalapati S. Spandex: Secure password tracking for Android. In: Proc.of the 23rd USENIX Security Symp. (USENIX Security 2014). 2014. 481−494. https://www.usenix.org/node/184402\n[84] Christensen  AS,  Møller  A,  Schwartzbach  MI.  Precise  analysis  of  string  expressions.  In:  Proc.  of  the  Int’l  Static  Analysis  Symp.Berlin, Heidelberg: Springer-Verlag, 2003. 1−18. [doi: 10.1007/3-540-44898-5_1]\n[85] Arzt S, Bodden E. StubDroid: Automatic inference of precise data-flow summaries for the Android framework. In: Proc. of the 38th Int’l Conf. on Software Engineering. ACM Press, 2016. 725−735. [doi: 10.1145/2884781.2884816]\n\n","categories":["笔记"],"tags":["总结","学习","污点分析"]},{"title":"Network Compression","url":"/Network-Compression/","content":" Network Compression\n做网络压缩的原因是这样：我们未来希望把network 放到很多的设备上使用，这些设备上存储空间有限，计算能力有限，所以我们希望能把网络做压缩，而尽可能小的损失其准确度，以适应这些设备。\n\n Outline\n先列一下本节的大纲：\n\n\nNetwork Pruning\n\n\nKnowledge Distillation\n\n\nParameter Quantization\n\n\nArchitecture Design\n\n\nDynamic Computation\n\n\n另外，我们不会讨论硬件加速和优化相关的内容。\n Network Pruning\nNetwork Pruning（网络修剪）就是把一个大的network 把一些neuron 去掉，以达到network compression 的目的。我们之所以能做到这件事，是因为我们相信我们通常训练出来的神经网络是over-parameterized，也就是说网络中的很多参数是没有用的。就是我们不需要那么多参数就能解出当前的问题，但是我们给了网络过多的参数。如果你去分析训出来的network 中的参数，你会发现很多的neuron 的output 总是0，有些weight 是非常接近0的，这些参数是没有作用的。我们就把这些没用的东西剪掉。\n这个概念是非常古老的，在90s就已经有了这样的想法：\n\n\nOptimal Brain Damage 意思是最优脑损伤，你可以在直觉上的这样考虑，机器做Network Pruning 就好像人类的大脑发育中的一个现象：如上图所示人类在出生时候脑中神经连接是比较少的在发育的过程中经历了增加又减少的过程。\n weight/neuron pruning\n\n做weight pruning 的过程如上图所示，我们先要有一个训练好的network ，然后评估每个参数（neuron或weight）的重要性。那怎么评估参数重要性呢，已经有很多的评估方法被提出来了。举例来说，对于weight 的重要性我们可以之间看他的数值大小，如果他的值很接近零就可能是不重要的weight，如果它的绝对值很大就可能是重要的参数，所以你可以通过计算weight的L1，L2的数值来看它是不是重要的；对于neuron 的重要性，你给network 一批数据，如果这个neuron 的输出几乎都是零的话，这个neuron 就可能是不重要的。\n接下来，你就根据重要新排序weight 或者neuron 然后去除不重要的，你就做好的裁剪的动作。然后，你要做一次fine-tune，去修补你做裁剪的时候损失的准确度，也就是说要修补你做裁剪对模型造成的损伤。接着，你就看看当前这个模型的大小和准确度你是不是满意，如果满意的话网络压缩就结束，否则你就回去步骤二重新评估重要性，以此类推。\n这里有个一点需要注意，通常来说你在每次prune 的时候都是裁剪掉一点点，这样迭代多次，而不要一次prune 太多以至于造成没办法修补的损伤。\n关于network pruning 的做法就介绍到这里，接下来我考虑一个问题：\n Why Pruning?\n我们为什么要做network pruning ，我是说我们为什么不直接train 一个更小的网络？\n\n\n小的神经网络比较难训\n\n\n大的神经网络更容易优化？ https://www.youtube.com/watch?v=_VuWvQUMQVk\n\n\nLottery Ticket Hypothesis（大乐透假设）\n\nhttps://arxiv.org/abs/1803.03635\n\n\n\n第一点没什么好说的，众所周知小的神经网络比较难训。\n大的神经网络更容易优化，原因就是大的神经网络比较不容易卡在local minima、saddle point 这些点上。这也许就是为什么小的network 比较难train 的原因。所以说，我们通常选择train 好一个大的network 再做pruning 。\n关于大乐透假设，我们来解释一下：\n Lottery Ticket Hypothesis\n\n如上图所示，我们将train 好的一个大network 进行pruning 得到一个小的network 如右上角所示，我们随机初始化这个小网络的参数，重新训练这个小的网络，发现train 不起来。我们再把上述紫色的pruned 的network 设为最初始的参数，重新训练小网络，发现train 起来了。\n根据这个现象，作者提出了大乐透假说，就是说train network 就像买乐透一样，不同的random initialized parameters 得到不同的初始神经网络，有的train 得起来有的则不行。一个巨大的network 是由很多小的network 组成，这些小的network 就有的能train 起来有的不行，而大的network 中只要有一个小的network 能train 起来，整个大的network 就train 起来。所以你可以直觉上这样想，大的network 就相当于你一次买了很多乐透，增加中奖几率。然后你再把大的network 做pruning 找出那个能train 起来的小的network ，这个小的network 最开始初始的参数是能使它被train 起来的，所以我们看到了上图所述的现象。\n下面是一个与上述对立的看法。\n Rethinking the Value of Network Pruning\n\nhttps://arxiv.org/abs/1810.05270\n\n这篇文章主要就是讲小的network 也是train 的起来的。\n\n这个文章的实验中的随机初始化参数就是真的随机初始化参数，而不是从原始模型的初始化参数copy 过来。\n上图是实验结果，我们可以对比Unpruned 和Fine-tuned 两列，似乎就是pruned network 也是train 的起来的。\n所以说，上述这两篇paper 的结论就是矛盾对立的，两篇文章都是发表在ICLR ，且是open review 的，网络上有reviewer 问到两篇paper 的对立观点，他们的作者也都对此做了一些解释，有兴趣可以自行搜索。\n Network Pruning-Practical Issue\nNetwork Pruning 有一些实作上的问题是我们需要注意的，我们上面说你可以衡量weight 或者neuron 的重要性，然后prune 掉不重要的，那weight 和neuron 两者prune哪个比较好呢。\n如果我们prune weight：\n\n你prune 掉不重要的weight 后，你得到的网络架构是不规则的，所谓不规则是说，在同一层中有的neuron 吃两个input 有的吃四个input，这样的网络的算法程序你比较难实现，就算你真的实现了这种算法，你也不好用GPU 加速矩阵运算。\n所以实作上你做weight pruning 的话你就会把weight 设零，而不是拿掉weight，但是这么做你并没有实际上丢掉weight，模型的大小是没有变的，所以这不能达到我们的network compression 的目的。\n\n\nhttps://arxiv.org/pdf/1608.03665.pdf\n\n紫色线是我们prune 的量，几乎都在95%以上。我们可以看到prune 以后，集中模型的速度大部分都是有所下降的。所以得不偿失，你以为prune 以后会更快，但实际上变得更慢了。\n所以说prune neuron 是比较好实作也比较好加速的：\n\n Knowledge Distillation\n\n\nKnowledge Distillation\nhttps://arxiv.org/pdf/1503.02531.pdf\nDo Deep Nets Really Need to be Deep?\nhttps://arxiv.org/pdf/1312.6184.pdf\n\n如上图所示，Knowledge Distillation 就是用一个小的network 去学习大的network 的行为。我们不是较Student Net 正确的结果是什么，而是告诉他当前输入可能是什么，举例来说，当输入是图片1的时候，Student Net 去学习Teacher Net 的输出，它会学到当前图片有0.7的可能性是1，有0.2的可能性是7，有0.1的可能性是9。在这个过程中，Student Net 不仅会学习到当前输入的图片可能是什么，它还会学习到1和7和9是相似的，所以这样的学习方式是可以学习到更丰富的信息的。所以，有可能即使Student Net 没见过图片7，它只见过图片1和9，但是这么学完以后它是可以触类旁通的认出7的。\n\nKnowledge Distillation 的一个用处是用一个小的Student Net 模拟一个巨大的ensemble 的Teacher Net 。通常来说ensemble 的方法可以让你的模型的准确度更上一层楼，但是这么做是牺牲了算力和空间的，此时我们就可以用Knowledge Distillation 的方法，用一个小的network 模拟大的ensemble network 达到相近的准确度。\n Temperature\n在Knowledge Distillation 的实作上有一个技巧叫做Temperature：\n\nTemperature 就是如上图所示，我们在做classification 的network 的最后会有一个softmax layer ，softmax layer 就是会把network output的值取Exponential 然后做一个normalize。我们通常会在做softmax 之前对network 的output 做除上temperature 的动作，temperature 通常是一个大于1的值。\n为什么这样做呢，我们举个例子。首先，我们知道Knowledge Distillation 之所以会有用是因为大的network 的输出是可能性，而不是one-hot 的向量，如果是后者那就失去了不同class 之间的相似性信息，所以为了让不同的label 之间的分数拉近一点，我们就除上temperature。本来的x通过softmax layer 得到的y的后两个维度都接近0，而x除上temperature 后通过softmax layer 得到的y的各个维度之间的分数就被拉近了。\n但是，在实际上Knowledge Distillation 没有特别有用。🤣\n Parameter Quantization\n参数量化，这一节将在参数上做一点文章。\n\nUsing less bits to represent a value\n\n这没什么好说的，就是去掉一些参数的精度，来换取存储空间的下降。\n\nWeight clustering\n\n\n如上图所示，Weight clustering 就是将相近的参数聚簇，然后用一个映射表来存储，以降低参数占用的内存空间。一个cluster 中的参数可以取均值作为这个cluster 的值。这样做你也是会损失一些进度，但是换来了很好的模型压缩率。\n\nRepresent frequent clusters by less bits, represent rare clusters by more bits\n\n更进一步，你就可以把常见的clusters 用比较短的coding 表示，比较罕见的clusters 用比较长的coding 表示，以进一步提高模型压缩率。比如使用哈夫曼编码。\n Binary Weights\nParameter Quantization 这种思想和方法的极致就是你可不可以只用±1来表示一个weight。其实文献上有一些尝试是可以直接train binary  weight 的network，最早的一篇paper 就是下面这个Binary Connect 。\n\n\nBinary Connect: https://arxiv.org/abs/1511.00363\nBinary Network:\nhttps://arxiv.org/abs/1602.02830\nXNOR-net:\nhttps://arxiv.org/abs/1603.05279\n\nBinary Weights 的精神就是你的参数是二元化的用±1来表示。上图灰色的点代表参数空间，每一个点都可以看作是一个binary weight 的network，这个network 中所有参数都是二元化的都是+1或者-1。\n然后你就初始一组参数，这组参数可以是real value 的，你就现根据当前network 的参数找一个最接近的binary weight 的network 去计算gradient ，根据这个gradient 更新当前network 的参数，然后再去根据更新后的network 找一个最接近的binary weight 的network 去计算gradient ，根据这个gradient 更新当前network 的参数，以此类推。\n这个Binary Connect 根据文献上的结果看起来还不错：\n\n\nhttps://arxiv.org/abs/1511.00363\n\n从上面来看，Binary Connect 居然还比原来的network 的结果还要好，为什么呢？你可以这样想，Binary Connect 可以看作在做regularization ，它限制参数的值只能是±1。只是这个方法还是没有做Dropout 更好就是了。\n Architecture Design\n调整network 的架构设计让它变得只需要较少的参数，以实现network compression 。这也许是现在实作上最有效的做法。\n先来看看fully connected network ：\n Fully Connected Network\n Low rank approximation\n\n如上图左上角，我们有一个network ，其中M、N两层中间有参数W，这两层分别各有M、N个neuron，然后我们在这两层中间加一个neuron比较少的linear hidden layer K，你仔细想想看，这样做参数其实是变少了。\n原先有M×NM\\times NM×N那么多参数，然后变成M×K+K×NM\\times K+K\\times NM×K+K×N 个参数。如果我们调控好K的值，就可以做到减少参数的效果。\n但是这个trick 会对network 有一定的限制，这也是不可避免的。\n CNN-network compression\n Review: Standard CNN\n\n如果我们的input 有两个channel ，我们的filter 就要对应有两个，如上图中间所示，我们通常有多个filter 比如4个的话，这样我们得到的output 就是四个，如图右边所示。所以这个CNN 的filter 的参数个数是72个，后面我们要做network compression 看看能从72减少到多少。\n Depthwise Separable Convolution\n\nDepthwise Separable Convolution 是把convolution拆成两个步骤：\n\nstep1 Depthwise Convolution\n\n\nfilter 数量=输出的channel\n每个filter 都只处理一个channel\nfilter 是k*k 的矩阵\n不同的channel 之间互相没有影响的\n\n这样每个filter 就不再考虑其他channel ，卷积得到输出是两层\n\n\nstep2 Pointwise Convolution\n\n\n第二步骤，是说每个filter 都只用一个value ，用这样的filter 去处理第一步得到的两层输出，这一步的输出就和经典的CNN 的卷积输出相同了。\n综合这两步，filter 的参数总量是24。\n下面来解释一下，这个拆解的步骤和原来的CNN 有什么样的关系。\n\n上图上侧是一般的CNN 的卷积过程。\n下侧是Depthwise Separable Convolution ，观察卷积过程，第一步将得到中间产物两层的输出，然后经过第二步的每个filter 得到最终输出的每一层。这个过程你可以这样考虑，第一步使用的filter 处理了9个input ，然后把输出结果丢给第二步的filter 处理这两个output，产生一个output。这个过程和经典CNN 中的卷积做的事情类似，经典CNN 中使用一个节点处理18个input 产生一个输出，而我们现在通过叠加两层处理过程，用更少的参数做到了经典CNN 一层处理过程做到的事。\n接下来算一下这个方法理论上的模型压缩程度：\n\n计算过程如上图所示，最后结果就是从(k×k×I)×O(k\\times k\\times I)\\times O(k×k×I)×O ==&gt;k×k×I+I×Ok\\times k\\times I + I\\times Ok×k×I+I×O\n To learn more ……\n\n\n\nSqueezeNet\n\nhttps://arxiv.org/abs/1602.07360\n\n\n\nMobileNet\n\nhttps://arxiv.org/abs/1704.04861\n\n\n\nShuffleNet\n\nhttps://arxiv.org/abs/1707.01083\n\n\n\nXception\n\nhttps://arxiv.org/abs/1610.02357\n\n\n\n\n Dynamic Computation\n\n我们希望模型能调整它所的计算能力，再资源充足的时候努力做到最好，在资源紧张的时候降低计算精度以提高持续服务的时间。比如在手机快没电的时候语音助手的运行功耗的调整。\n这边来介绍一些可能的解决方法：\n\n\n训练多个模型，根据设备的情况选择适当的网络\n\n但是这样的方法比较吃存储空间，所以不太好。\n\n训练中间层也可以做分类器的网络\n\n这个做法是说，正常情况下通过整个模型的计算输出结果，但是在资源紧张的情况下把模型的中间某些层的结果直接拿出来通过一个简单的计算就可以得到结果，以此自由调整network的运算量。\n如果你直接这么做结果往往是不太好的，因为你在train 整个network 的时候前面的layer 往往是学习识别很简单的信息，后面的layer 才能综合这些信息做判断。\n上图左下角是中有一个实验结果，纵轴就是准确率，横轴从左到右就是从模型的前到后抽出中间结果。显而易见，越深的地方抽出的中间结果才越能准确的做好任务。\n还有一个问题是，如果你在中间加一些classifier ，这些分类器是和整个network 一起train 的，这些classifier 的训练会伤害到整个network 的功能布局。原来network 的前几层要抽出一些基础信息，但是你现在强加的classifier 要求前几层同时能够综合这些信息，就导致前几层不能把所有的注意力都用在抽基础信息上。实验结果如上图右下角，你在比较浅的地方加classifier 整个network 的表现就会暴跌，在靠后的位置加classifier 对network 的影响就会小一些。\n\nhttps://arxiv.org/abs/1703.09844\n\n有没有方法能解决这些问题呢？有的：\n\n\nhttps://arxiv.org/abs/1703.09844\n\n你可以自行查阅这篇paper 。\n","categories":["李宏毅机器学习笔记"],"tags":["ML","note","Next-Step-of-ML"]},{"title":"More about Auto-Encoder","url":"/More-about-Auto-Encoder/","content":" More about Auto-Encoder\n这节课要讲的是在以前的课程上没有讲的， 近年来的热门的Auto-Encoder 技术。\n Review\n\nAuto-Encoder 就是有一个Encoder 的network ，有个Decoder 的network ，中间会产出一个vector ，你可以叫它Embedding，Latent Representation，Latent Code等。Decoder 会根据vector 产出一个输出，我们希望这个输出和原始的输入越接近越好，以此作为loss 更新模型参数。\n今天要讲的内容分两个部分：\n\n为什么一定要Minimize reconstruction error ，有没有其他的做法呢\n怎么让Encoder 输出的vector 更容易被解读\n\n接下来我们要先讲第一个问题，这里是通过引入Discriminator ，来替代minimize Reconstruction Error 。详情且看下面的描述。\n What is good embedding?\n我们回想一下为什么要做embedding 呢，我们是希望通过这个embedding 来代表原来的输入。举例来说，我们看到耳机就能想到三玖，而不会想到一花。🤣\n\n Beyond Reconstruction\nEncoder 吃进一个输入，就产生一个对应的代表该输入的embedding：\n\n那我们怎么知道这个产生的embedding 的代表性好不好呢，我们训一个Discriminator ，input 是一个张图片和embedding ，output 就是这一对输入是不是对应的，你可以想象这是一个二分类器。如上图左上方所示。\n这个Discriminator 的训练资料就是很多（图片+embedding）的二元组，这些二元组有的是对应的有的是故意用不对应的组合，并且打上标签。如上图右下角所示。\n然后就要定一个loss function LDL_DLD​，对于这个Discriminator 最简单的就是用Binary cross entropy ，没什么好说的。\n接下来就是训练这个Discriminator 去minimize loss function ，去找一组最好的Discriminator 的参数 ϕ\\phiϕ 。找到的最好的 ϕ\\phiϕ 达到的最低的loss 我们把此时的损失函数值叫做 LD∗{L_D}^*LD​∗ 。\n如果说这个 LD∗{L_D}^*LD​∗ 很小，那意味着现在这个Encoder 是很好的，Discriminator 很容易就能知道embedding 之间是不同的，可以轻易的分辨出各个embedding 应该对应的原始输入；如果说这个 LD∗{L_D}^*LD​∗ 很大，那就意味着现在这个Encoder 就是比较差的，这个Encoder 得到的embedding 都比较相似，Discriminator 难以分辨这些embedding 之间的区别。\n如果形象点用颜色来说，好的Encoder 得到的embedding 应该会是如上图颜色各异的，而差的Encoder 得到的embedding 可能就是下面这样颜色相近的：\n\n接下来我们要做的事，就是根据 LD∗{L_D}^*LD​∗ 调整Encoder 的参数 θ\\thetaθ ，使得Encoder 得到的embedding 在通过Discriminator 时候能被很好的分辨，得到较低的 LD∗{L_D}^*LD​∗ 。\n\n上述的这个方法，被用在Deep InfoMax(DIM) 这篇paper 中。这个Discriminator 可以做不同的设计，比如用不同的loss 计算方法等，结果会不太一样。\n在训练的时候，我们一起train Encoder 和Discriminator ，这件事就好像我们在train 一般的Auto-Encoder 时，一起train Encoder 和Decoder 去minimize Reconstruction Error 一样。\n这就是引入Discriminator ，来替代minimize Reconstruction Error 的做法。\n Typical auto-encoder is a special case\n经典的auto-encoder 可以看作上述的方法的一个特例。怎么说呢\n\n你可以把Discriminator 看作它根据vector 做了一个Decoder 做的事情，然后把输出和原始输入算一个Reconstruction Error 分数。\n Sequential Data\n如果你的数据是序列性的，比如说文章，这时你就可以做更多的变化。\n\n Skip thought\n\nhttps://papers.nips.cc/paper/5950-skip-thought-vectors.pdf\n\n如上图所示，Skip thought 就是根据当前输入的的句子预测上下文。这件事和训 WordEmbedding 是很像的，WordEmbedding 是说把文章中出现在同样或者相似上下文的单词语义应该是一样的，所以用相同或者相似的Embedding 来表示。Skip thought 的思想也是这样，只是扩展到句子，如果两个句子的上下文是相同或者相似的，那这两个句子应该是语义相近的。比如说，A提问这个东西有多贵，B回答10块，A提问这个东西要多少钱，B回答10块。Skip thought 就会知道有多贵和要多少钱的语义是相似的。\nSkip thought 不仅要训练Encoder（做embedding） 还要训练Decoder （做预测上下文），让机器产生预测结果，这是比较耗时的。\n Quick thought\n\nhttps://arxiv.org/pdf/1803.02893.pdf\n\n这是Skip thought 的升级版，速度比较快。我就只learn encoder 不去learn decode 了。\n我们现在把每个句子都同过encoder 得到各自的embedding ，每个句子要跟它的下个句子的embedding 越近越好。如果是其他的句子，那它们的embedding 就要和当前句子的embedding 越远越好。\n真正实作的时候就是，有个classifier 它吃当前句子embedding 、下个句子embedding 和一些随机sample 的句子的embedding ，然后给出哪个句子是当前句子的下一句。如上图所示。\nclassifer 和encoder 是共同训练的。文章中的classifier 是很简单的，它就是算当前句子embedding 和其他句子embedding 之间的内积，内积越大就越相信这个句子是当前句子的下一个。\n Contrastive Predictive Coding (CPC)\n\n\nhttps://arxiv.org/pdf/1807.03748.pdf\n\n看图就大概能明白它在做什么，输入是一段声音，这段声音分成小段，通过encoder 得到对应的embedding ，希望得到的embedding 能去预测接下来的同一个encoder 会output 的embedding 。就和上面Quick thought 的概念有点像。\n More interpretable embedding\n接下来我们进入下一个主题，怎么让embedding 更具解释性。\n\n Feature Disentangle\nDisentangle 这个词的中文意思是&quot;解开&quot;，大概就是下图中一团东西缠在一起想要解开的那个解开的意思。\n\n我们encoding 的对象包含各式各样的信息，比如说一段声音信号，包含语者的语义、语者的语调、环境噪音等等的信息，文字也是一样，有语义的信息、文法的信息等，图片也是，内容的信息、图片风格的信息等。\n以语音信息为例，encoder 得到的embedding 可能包括语者的语义信息、语调信息、环境噪声等很多信息，但是我们不知道这个向量中那些维度对应那些信息。现在，我们希望decoder 可以告诉我们那些维度是语义信息、哪些维度是语者的信息等等。\n\n这个想法，可以通过两个思路来做。第一种，如上图上半部分所示，我们希望encoder 输出的embedding 有一部分代表语音信息，一部分代表语者信息。第二种，做个变形如上图下半部分所示，我们搞两个encoder 一个提取出语义信息的embedding 一个提取出语者信息的embedding ，把两个embedding 拼在一起放入Decoder 才能还原输入。\n这里是做了简化，假设只有两个信息，实际上有更多的。\n那这样的话能做到的什么呢？\n Feature Disentangle-Voice Conversion\n一看就知道李老师也是老变声器了。🤣\n\n上图很好理解了，我们如上图所示做训练，我们用语音信息训练这个auto-encoder 模型，让encoder output 的embedding 可以把语者和语义信息分开来。然后：\n\n我们做如上图所示的embedding 拼接，丢给decoder 它就能输出男生说&quot;How are you?&quot;的音频。😮\n你可能会说这有什么用？这当然有用，比如用新垣结衣的声音劝你读博士，你可能就会满口答应下来。🤣\n\n那要怎么做才能让encoder 把不同的信息分开到不同的维度上呢？下面介绍几种做法。\n Feature Disentangle-Adversarial Training\n用GAN的思想\n\n如上图所示，做法是这样的，我们训练一个语者辨识的classifier ，Encoder 想办法去骗过speaker classifier 。比如说把embedding 的前100维给speaker classifier 做辨识，当Encoder 尝试骗过speaker classifier 的时候，Encoder 就可能会把有关语者的信息藏在embedding 后面的维度中。以此，来做到把语者和语义信息分开到不同的维度上。\n所以，从GAN的角度来说，speaker classifier 就是Discriminator ，Encoder 就是Generator，speaker classifier 和Encoder 是迭代train的，就是你先train speaker classifier 再train Encoder ，如此交替往复。\n Feature Disentangle-Designed Network Architecture\n\n我们还可以直接修改Encoder 的架构，如上图所示，直接让不同的Encoder 输出对应的信息，滤掉不需要的信息。举例来说，有一种神经网络的layer 叫instance normalization ，这种layer 我们就不展开讲了，它能做到的就是移除global information ，global information 就是所有样本都具有的信息。那当我们把同一个人说的话都输入网络，网络种instance normalization layer 就可能会滤掉语者信息。\n但是这样是不够的，这样就算我们保证了Encoder1 只包含了语义信息，我们也不能保证Encoder2 只包含语者信息。所以我们要在Decoder 上加一个adaptive instance normalization layer ，如下图所示：\n\nEncoder1 的embedding 直接input 给Deocder ，Encoder2 的embedding input 到AdaIN 这个layer ，AdaIN 会调整输出的global 的information ，也就是说如果Encoder2 在其输出的embedding 中放了语义信息，这个embedding 就会很大程度上改变Deocder 的输出。用这种方法，使Encoder2 输出的embedding 尽量不包含语义信息。\n用这种改变Encoder 架构的方法来实现，将不同信息分离到不同维度上。\n这里老师演示了他的学生Ju-chieh Chou 用Adversarial Training 得到的结果，可以参考：\n\nhttps://jjery2243542.github.io/voice_conversion_demo/\n\n Discrete Representation\n接下来我们要讲的是，过去我们在训Auto-Encoder 的时候得到的向量都是连续值，这个向量具体表示什么，你可能自己也不是很清楚。现在我们考虑Encoder 能不能输出离散的embedding ，这样我们就更容易解读这个embedding 的含义，也更容易做分群，比如说Encoder 输出是1的图片是一类，输出是2的图片是一类。所以，Encoder 如果能输出Discrete 的embedding ，那解读起来会更容易。\n\n\nhttps://arxiv.org/pdf/1611.01144.pdf\n\n举例来说，我们就让embedding 是One-hot embedding，如上图所示，我们就在Encoder 输出embedding 后面加点东西，它做到的事情就是把整个embedding 中最大的一维设为1，其他都是设0就好了。\n如果你不想要one-hot vector ，那也可以让embedding 转为binary vector ，如上图下半部分所示，某个维度的值大于0.5就设1，否则就设0。\n你可能会说那这个东西没法微分啊，但是实际上还是有一些技巧可以做的，这里就不展开了，可以自行查阅论文。\n Vector Quantized Variational Auto-encoder (VQVAE)\n上述的做法在文献上有个非常知名的做法叫做VQVAE\n\n\nhttps://arxiv.org/abs/1711.00937\nhttps://arxiv.org/pdf/1901.08810.pdf\n\nVQVAE 的做法是这样的，有一个Codebook 其中包含多个vector，这里假设只有5个好了，这些vector 是从数据中学出来的。输入一张图片给Encoder 输出一个数值上是连续的embedding ，那这个embedding 和Codebook 中的vector 算相似度，取相似度最高的作为Encoder 的输入，然后取minimize reconstruction error，结束。\n你会说，你算完相似度然后取最相似的vector 这个步骤相当于是在做Discrete，但是这不是没法微分吗，实际上是可以做的，有一些技巧，不展开了，自行读文献吧。\n还有一个重要的事情是，如果你用VQVAE 或者其他的Discrete embedding的方法，那你就能做到让Deocder 得到的Discrete embedding 只包含语义信息而不包含语者信息和环境噪声等，也就是说只有有关文字的信息才会被存下来。原因是这样的，你想想看这些Discrete embedding 就是容易存Discrete 的信息，而声音信息、环境噪声都是连续的，但文字信息是一个一个的token，是离散的，所以说文字信息被保留下来，其他信息被滤掉了。\n Sequence as Embedding\n我们上面说让embedding 变成离散的会更容解读，那我们甚至可以让中间表示不再是一个向量，让它用word sequence 表示：\n\n\nhttps://arxiv.org/abs/1810.02851\n\n假如说我们的Encoder 的输入对象是document，我们可以learn 一个seq2seq2seq 的model ，Encoder 做文章压缩，得到中间的word sequence ，Decoder 根据中间词序列做文章还原。这样的话我们直觉上会觉得，中间的word sequence 就是对文章的summary 。但是事实上如果我们直接这么train 下去中间的结果是不可读的。因为，Encoder 和Decoder 都是机器，他们会自己的暗语，说一些只有他们才能懂的word sequence 。比如说，台湾大学，中结果可能不是&quot;台大&quot;，而是是&quot;湾学&quot;，反正只要Deocder 能正确解回台湾大学就可以了。\n那怎么让Encoder 输出的sequence 让人能看懂呢，我们就用GAN 的技术。\n\n我们再那一个Discriminator 来，它来判断一个句子是不是人写的，让Encoder 努力学习骗过Discriminator 。这样就能让中间的word sequence 变得人类可读。\n你可能又说，这里又不能微分啊，中间的latent representation 是一个word sequence ，也是Discrete 的，Encoder Decoder 整个network 合起来不能微分啊。确实不能微分，所以实作的时候是用reinforcement learning 硬train Encoder 和Deocder 的。\n下面是一些实验结果：\n\n\n Tree as Embedding\n\n\nhttps://arxiv.org/abs/1806.07832\nhttps://arxiv.org/abs/1904.03746\n\n另外还有一些比较新的研究成果，供大家参考。\n Conclusion\n\n总结一下就是讲了：\n\n除了reconstruction error 以外有没有别的做法\n\nUsing Discriminator\nSequential Data\n\n\n有没有比较好的解释embedding 的方法\n\nFeature Disentangle\nDiscrete and Structured\n\n\n\n","categories":["李宏毅机器学习笔记"],"tags":["ML","note","Next-Step-of-ML"]},{"title":"Meta Learning-Metric-based","url":"/Meta-Learning-Metric-based/","content":" Meta Learning-Metric-based Approach\n加下来我们就要实践我们之前提到的疯狂的想法：直接学一个function，输入训练数据和对应的标签，以及测试数据，直接输出测试数据的预测结果。也就是说这个模型把训练和预测一起做了。\n\n虽然这个想法听起很不错，好像挺难的，但是实际上现实生活中有在使用这样的技术，举例来说：手机的人脸验证\n\n我们在使用手机人脸解锁的时候需要录制人脸信息，这个过程中我们转动头部，就是手机在收集资料，收集到的资料就是作为few-shot learning 的训练资料。另外，语音解锁Speaker Verification 也是一样的技术，只要换一下输入资料和network 的架构。\n这里需要注意Face Verification 和Face Recognition 是不一样的，前者是说给你一张人脸，判定是否是指定的人脸，比如人脸验证来解锁设备；后者是辨别一个人脸是人脸集合里面谁，比如公司人脸签到打卡。\n下面我们就以Face Verification 为例，讲一下Metric-based Meta Learning\n Training Tasks &amp; Testing Tasks\n\n训练任务集中的任务都是人脸辨识数据，每个任务的测试集就是某个人的面部数据，测试集就是按标准（如手机录制人脸）收集的人脸数据，如果这个人和训练集相同就打一个Yes 标签，否则就打一个No 标签。测试任务和训练任务类似。总的来说，network 就是吃训练的人脸和测试的人脸，它会告诉你Yes or No 。\n Siamese Network\n实际上是怎么做的呢，使用的技术是Siamese Network（孪生网络）。\n\nSiamese Network 的结构如上图所示，两个网络往往是共享参数的，根据需要有时候也可以不共享，假如说你现在觉得Training data 和Testing data 在形态上有比较大的区别，那你就可以不共享两个网路的参数。\n从两个CNN 中抽出两个embedding ，然后计算这两个embedding 的相似度，比如说计算conference similarity 或者Euclidean Distance ，你得到一个数值score ，这个数值大就代表Network 的输出是Yes ，如果数值小就代表输出是No 。\n Siamese Network - Intuitive Explanation\n接下来从直觉上来解释一下孪生网络。\n\n如上图所示，你可以把Siamese Network 看成一个二分类器，他就是吃进去两张人脸比较一下相似度，然后告诉我们Yes or No 。这样解释会比从Meta Learning 的角度来解释更容易理解。\n\n如上图所示，Siamese Network 做的事情就是把人脸投影到一个空间上，在这个空间上只要是同一个人的脸，不管他是往哪边看，不管机器看到的是他的哪一侧脸，都能被投影到这个空间的同一个位置上。\n那你就会想了，这种图片降维的方法，这和Auto-Encoder 有什么区别呢，他比Auto-Encoder 好在哪？\n你想你在做Auto-Encoder 的时候network 不知道你要解的任务是什么，它会尽可能记住图片中所有的信息，但是它不知道什么样的信息是重要的什么样的信息是不重要的。上图右侧，如果用Auto-Encoder 它可能会认为一花（左下）和三玖（右上）是比较接近的，因为他们的背景相似，这好吗，这不好。在Siamese Network 中，因为你要求network 把一花（左下）和三玖（右上）拉远，把三玖（右上）和三玖（右下）拉近，它可能会学会更加注意头发颜色的信息要忽略背景的信息。\n To learn more…\n\nWhat kind of distance should we use?\n\nSphereFace: Deep Hypersphere Embedding for Face Recognition\nAdditive Margin Softmax for Face Verification\nArcFace: Additive Angular Margin Loss for Deep Face Recognition\n\n\nTriplet loss（三元是指：Data 可以包括训练人脸，正确的测试人脸，错误的测试人脸）\n\nDeep Metric Learning using Triplet Network\nFaceNet: A Unified Embedding for Face Recognition and Clustering\n\n\n\n N-way Few/One-shot Learning\n刚才的栗子中，训练资料都只有一张，机器只要回答Yes or No 。那现在如果是一个分类的问题呢？现在我们打算把同样的概念用在5-way 1-shot 的任务上该怎么办呢？\n\n5-way 1-shot 就是说5个类别，每个类别中只有1个样本。就比如说上图，《五等分花嫁》中的五姐妹，要训一个模型分辨一个人脸是其中的谁，而训练资料是每个人只有一个样本。我们期待做到的事情是，Network 就把这五张带标签的训练图片外加一张测试图片都吃进去，然后模型就会告诉我们测试图片的分辨结果。\n那模型的架构要怎么设计呢，比如说一个经典的设计：\n Prototypical Network\n这是一个经典的做法：\n\n\nhttps://arxiv.org/abs/1703.05175\n\n这个方法和Siamese Network 非常相似，只不过从input 一张training data 扩展到input 多张training data 。\n来解释一下这个方法，如上图所示，把每张图片丢到同一个CNN 中算出一个embedding 用橙色条表示，然后把测试图片的embedding 和所有训练图片的embedding 分别算出一个相似度 sis_isi​ 。黄色的方块表示计算相似度。接下来，取一个softmax ，这样就可以和正确的标签做cross entropy ，去minimize cross entropy，这就和一般的分类问题的loss function 相同的，就可以根据这个loss 做一次gradient descent ，因为是1-shot 所以只能做一次参数更新。\n？？？？这里有问题呀，如果testing data 是已知标签的数据，用来做了1-shot，那真正的要预测的数据呢？？？？？\n如果你看明白了，请务必教教我🙏🙏🙏发邮件@sud0su@163.com或者加我Q@2948335218\n那如果是Few-shot 呢，怎么用Prototypical Network 解决呢。如右上角，我们把每个类别的几个图片用CNN 抽出的embedding 做average 来代表这个类别就好了。进来一个Testing Data 我们就看它和哪个class 的average 值更接近，就算作哪一个class 。\n Matching Network\nMatching Network 和Prototypical Network 最不同的地方是，Matching Network 认为也许Training data 中的图片互相之间也是有关系的，所以用Bidirectional LSTM 处理Training data，把Training data 通过一个Bidirectional LSTM 也会得到对应的embedding ，然后的做法就和Prototypical Network 是一样的。\n\n\nhttps://arxiv.org/abs/1606.04080\n\n事实上是Matching Network 先被提出来的，然后人们觉得这个方法有点问题，问题出在Bidirectional LSTM 上，就是说如果输入Training data 的顺序发生变化，那得到的embedding 就变了，整个network 的辨识结果就可能发生变化，这是不合理的。\n Relation Network\n\n\nhttps://arxiv.org/abs/1711.06025\n\n这个方法和上面讲过的很相似，只是说我们之前通过人定的相似度计算方法计算每一类图片和测试图片的相似度，而Relation Network 是希望用另外的模型 gϕg_\\phigϕ​ 来计算相似度。\n具体做法就是先通过一个 fϕf_\\phifϕ​ 计算每个类别的以及测试数据的embedding ，然后把测试数据的embedding 接在所有类别embedding 后面丢入 gϕg_\\phigϕ​ 计算相似度分数。\n Few-shot learning for Imaginary Data\n我们在做Few-Shot Learning 的时候的难点就是训练数据量太少了，那能不能让机器自己生成一些数据提供给训练使用呢。这就是Few-shot learning for Imaginary Data 的思想。\n\n\nhttps://arxiv.org/abs/1801.05401\n\nLearn 一个Generator GGG ，怎么Learn 出这个Generator 我们先不管，你给Generator 一个图片，他就会生成更多图片，比如说你给他三玖面无表情的样子，他就会YY出三玖卖萌的样子、害羞的样子、生气的样子等等。然后把生成的图片丢到Network 中做训练，结束。\n实际上，真正做训练的时候Generator 和Network 是一起训的，这就是Few-shot learning for Imaginary Data 的意思。具体的做法，这里不展开了。\n Meta Learning-Train+Test as RNN\n我们在讲Siamese Network 的时候说，你可以把Siamese Network 或其他Metric-based 的方法想成是Meta Learning ，但其实你是可以从其他更容易理解的角度来考虑这些方法。总的来说，我们就是要找一个function，这个function 可以做的到就是吃训练数据和测试数据，然后就可以吐出测试数据的预测结果。我们实际上用的Siamese Network 或者Prototypical Network 、Matching Network 等等的方法多可以看作我们为了实现这个目的做模型架构的变形。\n现在我们想问问，有没有可能直接用常规的network 做出这件事？有的。\n\n用LSTM 把训练数据和测试数据吃进去，在最后输出测试数据的判别结果。训练图片通过一个CNN 得到一个embedding ，这个embedding 和这个图片的label（one-hot vector）做concatenate（拼接）丢入LSTM 中，Testing data 我们不知道label 怎么办，我们就用0 vector 来表示，然后同样丢入LSTM ，得到output 结束。这个方法用常规的LSTM 是train 不起来的，我们需要修改LSTM 的架构，有两个方法：\n\n具体方法我们就不展开讲了，放出参考链接：\n\n\nOne-shot Learning with Memory-Augmented Neural Networks\nhttps://arxiv.org/abs/1605.06065\nA Simple Neural Attentive Meta-Learner\nhttps://arxiv.org/abs/1707.03141\n\nSNAIL 你看看他的架构图就能了解他在做什么，如上图右侧所示，和我们上面刚说过想法的是一样的，输入一堆训练数据给RNN 然后给他一个测试数据它输出预测结果，唯一不同的东西就是，它不是一个单纯的RNN ，它里面有在做回顾这件事，它在input 第二笔数据的时候会回去看第一笔数据，在input 第三笔数据的时候会回去看第一第二笔数据…在input 测试数据额时候会回去看所有输入的训练数据。\n所以你会发现这件事是不是和prototypical network 和matching network 很相似呢，matching network 就是计算input 的图片和过去看过的图片的相似度，看谁最像，就拿那张最像的图片的label 当作network 的输出。SNAIL 的回顾过去看过的数据的做法就和matching network 的计算相似度的做法很像。\n所以说，你虽然想用更通用的方法做到一个模型直接给出测试数据预测结果这件事，然后你发现你要改network 的架构，改完起了个名字叫SNAIL 但是他的思想变得和原本专门为这做到这件事设计的特殊的方法如matching network 几乎一样了，有点殊途同归的意思。\n Experiment\n\n总之SNAIL 和其他方法相比都是最好的，没什么好说的了。\n","categories":["李宏毅机器学习笔记"],"tags":["ML","note","Next-Step-of-ML"]},{"title":"Meta Learning-Gradient Descent as LSTM","url":"/Meta-Learning-Gradient-Descent-as-LSTM/","content":" Meta Learning - Gradient Descent as LSTM\n上节课讲了MAML 和Reptile ，我们说Meta Learning 就是要让机器自己learn 出一个learning 的algorithm。今天我们要讲怎么把我们熟悉的learning algorithm ：Gradient Descent ，当作一个LSTM 来看待，你直接把这个LSTM train下去，你就train 出了Gradient Descent 这样的Algorithm 。（也就是说我现在要把学习算法，即参数的更新算法当作未知数，用Meta Learning 训练出来）\n\n上周我们讲的MAML 和Reptile 都是在Initial Parameters 上做文章，用Meta Learning 训练出一组好的初始化参数，现在我们希望能更进一步，通过Meta Learning 训练出一个好的参数update 算法，上图黄色方块。\n我们可以把整个Meta Learning 的算法看作RNN，它和RNN 有点像的，同样都是每次吃一个batch 的data ，RNN 中的memory 可以类比到Meta Learning 中的参数 θ\\thetaθ 。\n把这个Meta Learning 的算法看作RNN 的思想主要出自两篇paper ：\n\nOptimization as a Model for Few-Shot Learning | OpenReview\nSachin Ravi, Hugo Larochelle\n[1606.04474] Learning to learn by gradient descent by gradient descent (arxiv.org)\nMarcin Andrychowicz, Misha Denil, Sergio Gomez, Matthew W. Hoffman, David Pfau, Tom Schaul, Brendan Shillingford, Nando de Freitas\n\n第二篇文章的题目非常有趣，也说明了此篇文章的中心：让机器学习用梯度下降学习这件事，使用的方法就是梯度下降。\n Review: RNN\n从与之前略微不同的角度快速回顾一下RNN。\n\nRNN就是一个function f，这个函数吃h,x 吐出 h’,y  ，每个step 会有一个x（训练样本数据）作为input，还有一个初始的memory 的值 h0h_0h0​ 作为input，这个初始参数有时候是人手动设置的，有时候是可以让模型learn 出来的，然后输出一个y和一个 h1h^1h1 。到下一个step，它吃上一个step 得到的 h1h^1h1 和新的x，也是同样的输出。需要注意的是，h的维度都是一致的，这样同一个f 才能吃前一个step 得到h 。这个过程不断重复，就是RNN。\n所以，无论多长的input/output sequence 我们只需要一个函数f 就可以运算，无论你的输入再怎么多，模型的参数量不会变化，这就是RNN 厉害的地方，所以它特别擅长处理input 是一个sequence 的状态。（比如说自然语言处理中input 是一个长句子，用word vector 组成的很长的sequence）\n我们如今用的一般都是RNN 的变形LSTM，而且我们现在说使用RNN 基本上就是在指使用LSTM 的技术。那LSTM 相比于RNN 有什么特别的地方呢。\n\n如上图，LSTM（右）相比于RNN ，把input 的h 拆解成两部分，一部分仍然叫做 hhh ，一部分我们叫做 ccc 。为什么要这样分呢，你可以想象是因为 ccc 和 hhh 扮演了不同的角色。\n\nccc 变化较慢，通常就是把某个向量加到上一个 ct−1c^{t-1}ct−1 上就得到了新的 ctc^tct ，这个 ctc^tct 就是LSTM 中memory cell 存储的值，由于这个值变化很慢，所以LSTM 可以记住时间比较久的数据\nhhh 变化较快， ht−1h^{t-1}ht−1 和 hth^tht 的变化是很大的\n\n Review: LSTM\n我们接下来看看LSTM 的做法和结构：\n\nct−1c^{t-1}ct−1 是memory 记忆单元，把x和h拼在一起乘上一个权重矩阵W，再通过一个tanh 函数得到input z，z是一个向量。同样的x和h拼接后乘上对应的权重矩阵得到对应向量input gate ziz^izi ，forget gate zfz^fzf ，output  gate zoz^ozo ，接下来：\n\nzf⋅ct−1z^f \\cdot c^{t-1}zf⋅ct−1 决定是否保留上个memory， zi⋅zz^i \\cdot zzi⋅z 决定是否把现在的input 存到memory；\n通过 zo⋅tanh(ct)z^o \\cdot tanh(c^t)zo⋅tanh(ct) 得到新的 hth^tht ；\nW′W&#x27;W′ 乘上新的 hth^tht ，再通过一个sigmoid function 得到当前step 的output yty^tyt ；\n重复上述步骤，就是LSTM 的运作方式：\n\n好，讲了这么多，它和Gradient Descent 到底有什么样的关系呢？\n LSTM similar to gradient descent based algorithm\n我们把梯度下降参数θ更新公式和LSTM 的memory c更新公式都列出来，如下图所示：\n\n我们知道在gradient descent 中我们在每个step 中，把旧的参数减去，learning rate 乘梯度，作为更新后的新参数，如上图所示，此式，和LSTM 中memory 单元 ccc 有些相似，我们就把 ccc 替换成 θ\\thetaθ 看看：\n\n接下来我们再做一些变换。输入ht−1h^{t-1}ht−1 来自上一个step，xtx^txt 来自外界输入，我们就把ht−1h^{t-1}ht−1 xtx^txt 换成$-\\nabla_\\theta l $ 。然后我们假设从input 到z 的公式中乘的matrix 是单位矩阵，所以z 就等于$-\\nabla_\\theta l $ 。再然后，我们把zfz^fzf 定位全1的列向量，ziz^izi 定位全为learning rate 的列向量，此时LSTM 的memory ccc 的更新公式变得和Gradient Descent 一摸一样：\n\n所以你可以说Gradient Descent 就是LSTM 的简化版，LSTM中input gate 和forget gate是通过机器学出来的，而在梯度下降中input gate 和forget gate 都是人设的，input gate 永远都是学习率，forget gate 永远都是不可以忘记。😮\n现在，我们考虑能不能让机器自己学习gradient descent 中的input gate 和forget gate 呢？\n另外，input的部分刚才假设只有gradient 的值，实作上可以拿更多其他的数据作为input，比如常见的做法，可以把 ct−1c^{t-1}ct−1 在现在这个step算出来的loss 作为输入来control 这个LSTM的input gate 和forget gate 的值。\n\n如果们可以让机器自动的学input gate 和forget gate 的值意味着什么，意味着我们可以拥有动态的learning rate，每一个step 中learning rate 都是不一样的而不是一个不变的值。而 zfz^fzf 就像一个正则项，它做的事情是把前一个step 算出来的参数缩小。我们以前做的L2 regularization 又叫做Weight Decade ，为什么叫Weight Decade，因为如果你把它微分的式子拿出来看，每个step 都会把原来的参数稍微变小，现在这个zfz^fzf 就扮演了像是Weight Decade 的角色。但是我们现在不是直接告诉机器要做多少Weight Decade 而是要让机器学出来，它应该做多少Weight Decade 。\n LSTM for Gradient Descent\n我们来看看一般的LSTM和for Gradient Descent 的LSTM：\n\nTypical LSTM 就是input x ，output c 和 h，每个step 会output 一个y ，希望y 和label 越接近越好。\nGradient Descent 的LSTM是这样：我们先sample 一个初始参数θ ，然后sample 一个batch 的data ，根据这一组data 算出一个gradient ∇θl\\nabla_\\theta l∇θ​l ，把负的gradient input 到LSTM 中进行训练，这个LSTM 的参数过去是人设死的，我们现在让参数在Meta Learning 的架构下被硬learn 出来。上述的这个update 参数的公式就是：\nθt=zf⋅θt−1+zi⋅−∇θl\\theta^t = z^f \\cdot \\theta^{t-1} + z^i \\cdot -\\nabla_\\theta l\nθt=zf⋅θt−1+zi⋅−∇θ​l\nzfz^fzf ziz^izi 以前是人设死的，现在LSTM 可以自动把它学出来。\n现在就可以output 新的参数θ1\\theta^1θ1 ，接着就是做一样的事情：再sample 一组数据，算出梯度作为新的input，放到LSTM 中就得到output θ2\\theta^2θ2 ，以此类推，不断重复这个步骤。最后得到一组参数θ3θ^3θ3（这里假设只update 3次，实际上要update 更多次），拿这组参数去应用到Testing data 上算一下loss ： l(θ3)l(θ^3)l(θ3) ，这个loss 就是我们要minimize 的目标，然后你就要用gradient descent 调LSTM 的参数，去minimize 最后的loss 。\n\n到这里可能比较懵了，我在这里写一下我的理解不一定对，欢迎指正。看完下面的[Experimental Results](#Experimental Results) 一节可以回来再看一遍这个解释：\n一般来说我们使用network 作为模型，其中会有很多参数θ，这些参数每一个都会拿到这个LSTM 中做如上述训练，一方面在train LSTM 中的参数，一方面在train 每一个参数θ。当network中所有θ都经过一轮上述的LSTM 的训练以后，得到的一组参数放回network 中，用testing data 计算loss of θ，据此用梯度下降回调LSTM 参数。如此往复，去minimize loss，最后就得到了一组比较好的参数，使得network 能在testing data 上取得比较好的成绩，这个过程中LSTM 担任了以前使用的梯度下降来update 参数的角色，而且LSTM 中的zfz^fzf ziz^izi 还是动态变化的，可能要比经典梯度下降效果好。\n\n这里有一些需要注意的地方。在一般的LSTM 中c 和x 是独立的，LSTM 的memory 存储的值不会影响到下一次的输入，但是Gradient Descent LSTM 中参数θ会影响到下一个step 中算出的gradient 的值，如上图虚线所示。所以说在Gradient Descent LSTM 中现在的参数会影响到未来看到的梯度。所以当你做back propagation 的时候，理论上你的error signal 除了走实线的一条路，它还可以走θ到−∇θl-\\nabla_\\theta l−∇θ​l 虚线这一条路，可以通过gradient 这条路更新参数。但是这样做会很麻烦，和一般的LSTM 不太一样了，一般的LSTM c 和x 是没有关系的，现在这里确实有关系，为了让它和一般的LSTM 更像，为了少改一些code ，我们就假设没有虚线那条路，结束。现在的文献上其实也是这么做的。\n另外，在LSTM input 的地方memory 中的初始值可以通过训练直接被learn 出来，所以在LSTM中也可以做到和MAML相同的事，可以把初始的参数跟着LSTM一起学出来。\n Real Implementation\nLSTM 的memory 就是要训练的network 的参数，这些参数动辄就是十万百万级别的，难道要开十万百万个cell 吗？平常我们开上千个cell 就会train 很久，所以这样是train不起来的。在实际的实现上，我们做了一个非常大的简化：我们所learn 的LSTM 只有一个cell 而已，它只处理一个参数，所有的参数都公用一个LSTM。所以就算你有百万个参数，都是使用这同一个LSTM 来处理。\n\n也就是说如上图所示，现在你learn 好一个LSTM以后，它是直接被用在所有的参数上，虽然这个LSTM 一次只处理一个参数，但是同样的LSTM 被用在所有的参数上。θ1θ^1θ1 使用的LSTM 和θ2θ^2θ2 使用的LSTM 是同一个处理方式也相同。那你可能会说，θ1θ^1θ1 和 θ2θ^2θ2 用的处理方式一样，会不会算出同样的值呢？会不，因为他们的初始参数是不同的，而且他们的gradient 也是不一样的。在初始参数和算出来的gradient 不同的情况下，就算你用的LSTM的参数是一样的，就是说你update 参数的规则是一样的， 最终算出来的也是不一样的 θ3θ^3θ3 。\n这就是实作上真正implement LSTM Gradient Descent 的方法。\n这么做有什么好处：\n\n在模型规模上问题上比较容易实现\n在经典的gradient descent 中，所有的参数也都是使用相同的规则，所以这里使用相同的LSTM ，就是使用相同的更新规则是合理的\n训练和测试的模型架构可以是不一样的，而之前讲的MAML 需要保证训练任务和测试任务使用的model architecture 相同\n\n Experimental Results\n\n\nhttps://openreview.net/forum?id=rJY0-Kcll&amp;noteId=ryq49XyLg\n\n我们来看一个文献上的实验结果，这是做在few-shot learning 的task上。横轴是update 的次数，每次train 会update 10次，左侧是forget gate zfz^fzf 的变化，不同的红色线就是不同的task 中forget gate 的变化，可以看出zfz^fzf 的值多数时候都保持在1附近，也就是说LSTM 有learn 到θt−1θ^{t-1}θt−1 是很重要的东西，没事就不要给他忘掉，只做一个小小的weight decade，这和我们做regularization 时候的思想相同，只做一个小小的weight decade 防止overfitting 。\n右侧是input gate ziz^izi 的变化，红线是不同的task，可以看出它的变化有点复杂，但是至少我们知道，它不是一成不变的固定值，它是有学到一些东西的，是动态变化的，放到经典梯度下降中来说就是learning rate 是动态变化的。\n LSTM for Gradient Descent (v2)\n只有刚才的架构还不够，我们还可以更进一步。想想看，过去我们在用经典梯度下降更新参数的时候我们不仅会考虑当前step 的梯度，我们还会考虑过去的梯度，比如RMSProp、Momentum 等。\n\n\n在刚才的架构中，我们没有让机器去记住过去的gradient ，所以我们可以做更进一步的延伸。我们在过去的架构上再加一层LSTM，如下图所示：\n\n蓝色的一层LSTM 是原先的算learning rate、做weight decade 的LSTM，我们再加入一层LSTM ，让算出来的gradient −∇θl-\\nabla_\\theta l−∇θ​l 先通过这个LSTM ，把这个LSTM 吐出来的东西input 到原先的LSTM 中，我们希望绿色的这一层能做到记住以前算过的gradient 这件事。这样，可能就可以做到Momentum 可以做的的事情。\n上述的这个方法，是老师自己想象的，在learning to learn by gradient descent by gradient descent 这篇paper 中上图中蓝色的LSTM 使用的是一般的梯度下降算法，而在另一篇paper 中只有上面没有下面，而老师觉得这样结合起来才是实现，能考虑过去的gradient 的gradient descent 算法的完全体。\n Experimental Result 2\nlearning to learn by gradient descent by gradient descent 这篇paper 的实验结果。\n\n\nhttps://arxiv.org/abs/1606.04474\n\n第一个实验图，是做在toy example 上，它可以制造一大堆训练任务，然后测试在测试任务上，然后发现，LSTM 来当作gradient descent 的方法要好过人设计的梯度下降方法。\n第二张图把这个技术应用带MNIST 上，这个实验是训练任务测试任务都是MNIST。\n第三张图是说虽然训练和测试任务都是相同的dataset也是相同的，但是train 和test 的时候network 的架构是不一样的。 在train 的时候network 是只有一层，该层只有20个neuron。这张图是training 的结果。\n第四张图是上述改变network 架构后在testing 的结果，testing 的时候network 只有一层该层40个neuron。从图上看还是做的起来，而且比一般的gradient descent 方法要好很多。\n第五张图是上述改变network 架构后在testing 的结果，testing 的时候network 有两层。从图上看还是做的起来，而且比一般的gradient descent 方法要好很多。\n第六张图是上述改变network 激活函数后在testing 的结果，training 的时候激活函数是sigmoid 而testing 的时候改成ReLU。从图上看做不起来，崩掉了，training 和testing 的network 的激活函数不一样的时候，LSTM 没办法跨model 应用。\n","categories":["李宏毅机器学习笔记"],"tags":["ML","note","Next-Step-of-ML"]},{"title":"Meta Learning-MAML","url":"/Meta-Learning-MAML/","content":" Meta Learning-MAML\nMeta learning 总体来说就是让机器学习如何学习。\n\n如上图，我们希望机器在学过一些任务以后，它学会如何去学习更有效率，也就是说它会成为一个更优秀的学习者，因为它学会了学习的技巧。举例来说，我们教机器学会了语音辨识、图像识别等模型以后，它就可以在文本分类任务上做的更快更好，虽然说语音辨识、图像识别和文本分类没什么直接的关系，但是我们希望机器从先前的任务学习中学会了学习的技巧。\n讲到这里，你可能会觉得Meta Learning 和 Life-Long Learning 有点像，确实很像，但是LLL 的着眼点是用同一个模型apply 到不同的任务上，让一个模型可以不断地学会新的任务，而Meta Learning 中不同的任务有不同的模型，我们的着眼点是机器可以从过去的学习中学到学习的方法，让它在以后的学习中更快更好。\n我们先来看一下传统的ML 的做法：\n\n我们过去学过的ML ，通常来说就是定义一个学习算法，然后用训练数据train ，吐出一组参数（或者说一个参数已定的函数式），也就是得到了模型，这个模型可以告诉我们测试数据应该对应的结果。比如我们做猫狗分类，train 完以后，给模型一个猫的照片，它就会告诉我们这是一只猫。\n我们把学习算法记为 FFF ，这个学习算法吃training data 然后吐出目标模型 f∗f^*f∗ ，形式化记作：\nf∗=F(Dtrain)f^* = F(D_{train})\nf∗=F(Dtrain​)\nMeta Learning 就是要让机器自动的找一个可以吃training data 吐出函数 f∗f^*f∗ 的函数 FFF 。\n总结一下：\n\nMachine Learning 和Meta Learning 都是让机器找一个function ，只不过要找的function 是不一样的。\n我们知道Machine Learning 一共分三步（如下图），Meta Learning 也是一样的，你只要把Function fff 换成**学习的算法 FFF **这就是Meta Learning 的步骤：\n\n\n我们先定义一组Learning 的Algorithm 我们不知道哪一个算法是比较好的，\n\n\n然后定义一个Learning Algorithm 的Loss ，它会告诉你某个算法的好坏，\n\n\n最后，去train 一发找出哪个Learning Algorithm比较好。\n\n\n所以接下来我们将分三部分来讲Meta Learning 的具体过程。\n\n Three Step of Meta-Learning\n Define a set of learning algorithm\n什么是一组learning algorithm 呢？\n\n如上图所示，灰色框中的，包括网络（模型）架构，初始化参数的方法，更新参数的方法，学习率等要素构成的整个process ，可以被称为一个learning algorithm 。在训练的过程中有很多要素都是人设计的，当我们选择不同的设计的时候就相当于得到了不同的learning algorithm 。现在，我们考虑能不能让机器自己学出某一环节，或者全部process 的设计。比如说，我们用不同的初始化方法得到不同的初始化参数以后，保持训练方法其他部分的相同，且用相同的数据来训练模型，最后都会得到不同的模型，那我们就考虑能不能让机器自己学会初始化参数，直接得到最好的一组初始化参数，用于训练。\n我们就希望通过Meta Learning 学习到初始化参数这件事，好，现在我们有了一组learning algorithm ，其中各个算法只有初始化参数的方法未知，是希望机器通过学习得出来的。\n那现在我们怎么衡量一个learning algorithm 的好坏呢？\n Define the goodness of a function F\n\n我们需要很多个task，每个task都有training set 和testing set，然后就把learning algorithm 应用到每个task上，用training set 训练，用testing set 测试，得到每一个task 的loss lil^ili，对于一个learning algorithm FFF 的整体loss 就可以用每个task 的loss 进行求和。\nL(F)=∑n=1NlnL(F) = \\sum\\limits_{n=1}^{N}l^n\nL(F)=n=1∑N​ln\n从这里我们就能看出，meta learning 和传统的machine learning 在训练资料上是有些不同的：\n\n做meta learning 的话你可能需要准备成百上千个task，每个task 都有自己的training set 和testing set 。这里为了区分，我们把meta learning的训练集叫做Training Tasks，测试集叫做Testing Tasks，其中中每个task 的训练集叫做Support set ，测试集叫做 Query set 。\nPS：这里有学生提问，根据老师的回答，大意就是，training tasks 和testing tasks可以是不同类型的任务，比如全部用影像辨识任务训练然后用语音辨识任务测试也是可以做得起来的。这取决于你的meta learning 的algorithm 是什么样的，有的训练和测试任务的类型可以是不一样的有些是需要一样的。\n讲到这里你可能觉得比较抽象，后面会讲到实际的栗子，你可能就理解了meta learning 的实际运作方法。Meta learning 有很多方法，加下来会讲几个比较常见的算法，本节课会讲到一个最有名的叫做MAML ，以及MAML 的变形叫做Reptile 。\n Find the best function F∗F^*F∗\n定好了loss function 以后我们就要找一个最好的F* ，这个F∗F^*F∗可以使所有的training tasks 的loss 之和最小，形式化的写作下图下面的公式（具体计算方法后面再讲）：\n\n现在我们就有了meta learning 的algorithm ，我们可以用testing tasks 测试这个F∗F^*F∗，把测试任务的训练集放丢入F∗F^*F∗，就会得到一个f∗f^*f∗ ，再用测试任务的测试集去计算这个f∗f^*f∗ 的loss ，这个loss 就是整个meta learning algorithm 的loss ，来衡量这个方法的好坏。\n Omniglot Corpus\n Introduction\n\nhttps://github.com/brendenlake/omniglot\n\n\n这是一个corpus，这里面有一大堆奇怪的符号，总共有1623个不同的符号，每个符号有20个不同的范例。上图下侧就是那些符号，右上角是一个符号的20个范例。\n Few-shot Classification\n\nN-ways K-shot classification 的意思是，分N个类别，每个类别有K个样例。\n所以，20 ways 1shot 就是说分20类，每类只有一个样例。这个任务的数据集就例如上图中间的20张support set 和1张query set 。\n\n把符号集分为训练符号集和测试符号集\n\n从训练符号集中随机抽N个符号，从这N个符号的范例中各随机抽K个样本，这就组成了一个训练任务training task 。\n从测试符号集中随机抽N个符号，从这N个符号的范例中各随机抽K个样本，这就组成了一个测试任务testing task 。\n\n\n\n Techniques Today\n\n这两个大概是最近（2019年）比较火的吧，Reptile 可以参考一下openai的这篇文章。\n\nReptile: A Scalable Meta-Learning Algorithm (openai.com)\n\n接下来先将MAML（详细） ，再讲Reptile（简略）。\n MAML\n\nMAML要做的就是学一个初始化的参数。过去你在做初始化参数的时候你可能要从一个distribution 中sample 出来，现在我们希望通过学习，机器可以自己给出一组最好的初始化参数。\n做法就如上图所示，我们先拿一组初始化参数 ϕ\\phiϕ 去各个training task 做训练，在第n个task 上得到的最终参数记作 θ^n\\hat{θ}^nθ^n ，而ln(θ^n)l^n(\\hat{θ}^n)ln(θ^n) 代表第n个task 在其testing set 上的loss，此时整个MAML 算法的Loss 记作：\nL(ϕ)=∑n=1Nln(θ^n)L(\\phi) = \\sum\\limits_{n=1}^{N}l^n(\\hat{θ}^n)\nL(ϕ)=n=1∑N​ln(θ^n)\n这里提示一下，MAML就是属于需要所有任务的网络架构相同的meta learning algorithm，因为其中所有的function 要共用相同的初始化参数 ϕ\\phiϕ 。\n那怎么minimize L(ϕ)L(\\phi)L(ϕ) 呢？\n答案就是Gradient Descent，你只要能得到 L(ϕ)L(\\phi)L(ϕ) 对 ϕ\\phiϕ 的梯度，那就可以更新 ϕ\\phiϕ 了，结束。😮\nϕ=ϕ−η∇ϕL(ϕ)\\phi = \\phi - \\eta \\nabla_{\\phi}L(\\phi)\nϕ=ϕ−η∇ϕ​L(ϕ)\n这里我们先假装已经会算这个梯度了，先把这个梯度更新参数的思路理解就好，具体的更新算法在[Warning of Math](#Warning of Math)中会将到，我们先来康一下MAML 和 Model Pre-training 在Loss Function 上的区别。\n MAML v.s. Model Pre-training\n\n通过上图我们仔细对比两个损失函数，可以看出，MAML是根据训练完的参数 θ^n\\hat{θ}^nθ^n 计算损失，计算的是训练好的参数在各个task 的训练集上的损失；而预训练模型的损失函数则是根据当前初始化的参数计算损失，计算的是当前参数在要应用pre-training 的task 上的损失。\n再举一个形象一点的栗子：\n\n（横轴是模型参数，简化为一个参数，纵轴是loss）\n如上图说的，我们在意的是这个初始化参数经过各个task 训练以后的参数在对应任务上的表现，也就是说如果初始参数 ϕ\\phiϕ 在中间位置（如上图），可能这个位置根据当前参数计算的总体loss 不是最好的，但是在各个任务上经过训练以后 θ^\\hatθθ^ 都能得到较低的loss（如 θ^1\\hat{θ}^1θ^1、θ^2\\hat{θ}^2θ^2），那这个初始参数 ϕ\\phiϕ 就是好的，其loss 就是小的。\n反之，在Model pre-training 上：\n\n我们希望直接通过这个 ϕ\\phiϕ 计算出来的各个task 的loss 是最小的，所以它的取值点就可能会是上图的样子。此时在task 2上初始参数可能不能够被更新到global minima，会卡在local minima 的点 θ^2\\hat{θ}^2θ^2。\n综上所述：\n\nMAML 是要找一个 ϕ\\phiϕ 在训练以后具有好的表现，注重参数的潜力，Model Pre-training 是要找一个 ϕ\\phiϕ 在训练任务上得到好的结果，注重现在的表现。\n MAML Only Update One Time\n\n在MAML 中我们只update 参数一次。所以在训练阶段，你只做one-step training ，参数更新公式就变成上图右下角的式子。\n只更新一次是有些理由的：\n\n为了速度，多次计算梯度更新参数是比较耗时的，而且MAML 要train 很多个task\n把只更新一次作为目标\n实际上你可以在training 的时候update 一次，在测试的时候，解testing task 的时候多update 几次结果可能就会更好\n如果是few-shot learning 的task，由于data 很少，update 很多次很容易overfitting\n\n Toy Example\n\n\nSource of images Paper repro: Deep Metalearning using “MAML” and “Reptile”\n\n\n要拟合的目标函数是 y=asin(x+b)y=asin(x+b)y=asin(x+b)\n对每个函数，也就是每个task，sample k个点\n通过这些点做拟合\n\n我们只要sample 不同的a, b就可以得到不同的目标函数。\n来看看对比结果：\n\n可以看到，预训练模型想要让参数在所有的task 上都做好，多个task叠加起来，导致预训练模型最后拟合得到的是一条直线。\nMAML 在测试task 上training step 增加的过程中有明显的拟合效果提升。\n Omniglot &amp; Mini-ImageNet\n在MAML 的原始论文中也把这个技术用于Omniglot &amp; Mini-ImageNet\n\n\nhttps://arxiv.org/abs/1703.03400\n\n我们看上图下侧，MAML, first order approx 和 MAML 的结果很相似，那approx 是怎么做的呢？解释这个东西需要一点点数学：\n Warning of Math\n这一段不想听的同学可以睡一会。😁\n\nMAML 的参数更新方法如上图左上角灰色方框所示，我们来具体看看这个 ∇ϕL(ϕ)\\nabla_\\phi L(\\phi)∇ϕ​L(ϕ) 怎么算，把灰框第二条公式带入，如黄色框所示。其中 ∇ϕln(θ^n)\\nabla_\\phi l^n(\\hat{θ}^n)∇ϕ​ln(θ^n) 就是左下角所示，它就是loss 对初始参数集 ϕ\\phiϕ 的每个分量的偏微分。也就是说 ϕi\\phi_iϕi​ 的变化会通过 θ^\\hat{θ}θ^ 中的每个参数 θ^i\\hat{θ}_iθ^i​ ，影响到最终训练出来的 θ^\\hat{θ}θ^ ，所以根据chain rule 你就可以把左下角的每个偏微分写成上图中间的公式。\n∂l(θ^)∂ϕi=∑j∂l(θ^)∂θ^j∂θ^j∂ϕi\\frac{\\partial{l(\\hat{θ})}}{\\partial{\\phi_i}} = \\sum\\limits_{j}\\frac{\\partial{l(\\hat{θ})}}{\\partial\\hat{θ}_j}\\frac{\\partial\\hat{θ}_j}{\\partial{\\phi}_i}\n∂ϕi​∂l(θ^)​=j∑​∂θ^j​∂l(θ^)​∂ϕi​∂θ^j​​\n上式中前面的项 ∂l(θ^)∂θ^j\\frac{\\partial{l(\\hat{θ})}}{\\partial\\hat{θ}_j}∂θ^j​∂l(θ^)​ 是容易得到的，具体的计算公式取决于你的model 的loss function ，比如cross entropy 或者regression，结果的数值却决于你的训练数据的测试集。\n后面的项 ∂θ^j∂ϕi\\frac{\\partial\\hat{θ}_j}{\\partial{\\phi}_i}∂ϕi​∂θ^j​​ 是需要我们算一下。可以分成两个情况来考虑：\n\n根据灰色框中第三个式子，我们知道 θ^j\\hat{θ}_jθ^j​ 可以用下式代替：\nθ^j=ϕj−ϵ∂l(ϕ)∂ϕj\\hat{θ}_j = \\phi_j - \\epsilon\\frac{\\partial{l(\\phi)}}{\\partial{\\phi_j}}\nθ^j​=ϕj​−ϵ∂ϕj​∂l(ϕ)​\n此时，对于 ∂θ^j∂ϕi\\frac{\\partial\\hat{θ}_j}{\\partial{\\phi}_i}∂ϕi​∂θ^j​​ 这一项，分为i=j 和 i!=j 两种情况考虑，如上图所示。在MAML 的论文中，作者提出一个想法，不计算二次微分这一项。如果不计算二次微分，式子就变得非常简答，我们只需要考虑i=j 的情况，i!=j 时偏微分的答案总是0。\n此时， ∂l(θ^)∂ϕi\\frac{\\partial{l(\\hat{θ})}}{\\partial\\phi_i}∂ϕi​∂l(θ^)​ 就等于 ∂l(θ^)∂θi\\frac{\\partial{l(\\hat{θ})}}{\\partial{θ}_i}∂θi​∂l(θ^)​ 。这样后一项也解决了，那就可以算出上图左下角 ∇ϕl(θ^)\\nabla_\\phi l(\\hat{θ})∇ϕ​l(θ^) ，就可以算出上图黄色框  ∇ϕL(ϕ)\\nabla_\\phi L(\\phi)∇ϕ​L(ϕ) ，就可以根据灰色框第一条公式更新 ϕ\\phiϕ ，结束。😮\n\n在原始paper 中作者把，去掉二次微分这件事，称作using a first-order approximation 。\n当我们把二次微分去掉以后，上图左下角的 ∇ϕl(θ^)\\nabla_\\phi l(\\hat{θ})∇ϕ​l(θ^) 就变成 \\nabla_\\hat{θ} l(\\hat{θ}) ，所以我们就是再用 θ^\\hat{θ}θ^ 直接对 θ^\\hat{θ}θ^ 做偏微分，就变得简单很多。\n MAML-Real Implementation\n来看看MAML 实际上是怎么运行的，据说很简单。\n\n实际上，我们在MAML 中每次训练的时候会拿一个task batch 去做，这里我们就先假定每个batch 就只有一个task好了。如上图，当我们初始化好参数 ϕ0\\phi_0ϕ0​ 我们就开始进行训练，完成task m训练以后，根据一次update 得到 θ^m\\hat{θ}^mθ^m ，我们再计算一下 θ^m\\hat{θ}^mθ^m 对它的loss function 的偏微分，也就是说我们虽然只需要update 一次参数就可以得到最好的参数，但现在我们update 两次参数，第二次更新的参数的用处就是， $\\phi $ 的更新方向就和第二次更新参数的方向相同，可能大小不一样，毕竟它们的learning rate 不一样。\n刚才我们讲了在精神上MAML 和Model Pre-training 的不同，现在我们来看看这两者在实际运作上的不同。如上图，预训练的参数更新完全和每个task 的gradient 的方向相同。\n MAML 实际应用\n这里有一个把MAML 应用到机器翻译的栗子：\n\n\nhttps://arxiv.org/abs/1808.08437\n\n18个不同的task：18种不同语言翻译成英文\n2个验证task：2种不同语言翻译成英文\nRo 是validation tasks 中的任务，Fi 即没有出现在training tasks 也没出现在validation tasks\n横轴是每个task 中的训练资料量。MetaNMT 是MAML 的结果，MultiNMT 是 Model Pre-training 的结果，我们可以看到在所有情况下前者都好过后者，尤其是在训练资料量少的情况下，MAML 更能发挥优势。\n Reptile\n非常简单。\n\n做法就是初始化参数 ϕ0\\phi_0ϕ0​ 以后，通过在task m上训练跟新参数，可以多更新几次，然后根据最后的 θ^m\\hat{θ}^mθ^m 更新 ϕ0\\phi_0ϕ0​ ，同样的继续，训练在task n以后，多更新几次参数，得到 θ^n\\hat{θ}^nθ^n ，据此更新 ϕ1\\phi_1ϕ1​ ，如此往复。\n你可能会说，这不是和预训练很像吗，都是根据参数的更新来更新初始参数，希望最后的参数在所有的任务上都能得到很好的表现。作者自己也说，如上图下侧的摘录。\n Reptile &amp; MAML &amp; Pre-training\n\n通过上图来对比三者在更新参数 ϕ\\phiϕ 的不同，似乎Reptile 在综合两者。但是Reptile 并没有限制你只能走两步，所以如果你多更新几次参数多走几步，或许Reptile 可以考虑到另外两者没有考虑到的东西。\n\n上图中，蓝色的特别惨的线是pre-training ，所以说和预训练比起来meta learning 的效果要好很多。\n More…\n上面所有的讨论都是在初始化参数这件事上，让机器自己学习，那有没有其他部分可以让机器学习呢，当然有很多。\n\n比如说，学习网络结构和激活函数、学习如何更新参数…\n\nhttps://www.youtube.com/watch?v=c10nxBcSH14\n\n Think about it…\n我们使用MAML 或Reptile 来寻找最好的初始化参数，但是这个算法本身也需要初始化参数，那我们是不是也要训练一个模型找到这个模型的初始化参数…\n就好像说神话中说世界在乌龟背上，那这只乌龟应该在另一只乌龟背上…这就是套娃啊🤣\n\n Crazy Idea?\n\n传统的机器学习和深度学习的算法基本上都还是gradient descent ，你能不能做一个更厉害的算法，只要我们给他所有的training data 它就可以返回给我们需要model，它是不是梯度下降train 出来的不重要，它只要能给我一个能完成这个任务的model 就好。\n或者，反之我们最好都要应用到测试集上，那我们干脆就搞一个大黑箱，把training set 和testing set 全部丢给他，它直接返回testing data 的结果，连测试都帮你做好。这些想法能不能做到，留到下一节讲。\n","categories":["李宏毅机器学习笔记"],"tags":["ML","note","Next-Step-of-ML"]},{"title":"Life Long Learning","url":"/Life-Long-Learning/","content":" Life Long Learning (LLL)\n\n开始之前的说明，如果读者是学过transfer learning 的话，学这一节可能会轻松很多，LLL的思想在我看来是和transfer learning是很相似的。\n\n可以直观的翻译成终身学习，我们人类在学习过程中是一直在用同一个大脑在学习，但是我们之前讲的所有机器学习的方法都是为了解决一个专门的问题设计一个模型架构然后去学习的。所以，传统的机器学习的情景和人类的学习是很不一样的，现在我们就要考虑为什么不能用同一个模型学会所有的任务。\n\n也有人把Life Long Learning 称为Continuous Learning，Never Ending Learning，Incremental Learning，在不同的文献中可能有不同的叫法，我们只要知道这些方法都是再指终生学习就可。\n\n现在我们回归初心，我想大多数人在学习机器学习之前的是这样认为的，机器学习就是如上图所示，我们教机器学学会任务1，再教会它任务2，我们就不断地较它各种任务，学到最后它就成了天网🤣。但是实际上我们都知道，现在的机器学习是分开任务来学的，就算是这样很多任务还是得不到很好的结果。所以机器学习现在还是很初级的阶段，在很多任务上都无法胜任。\n我们今天分三个部分来叙述Life-Long Learning：\n\nKnowledge Retention 知识保留\n\nbut NOT Intransigence 但不顽固\n\n\nKnowledge Transfer 知识潜移\nModel Expansion 模型扩展\n\nbut Parameter Efficiency 但参数高效\n\n\n\n上述的几个部分的具体含义会在下面详细解释。\n Knowledge Retention\nbut NOT Intransigence\n知识保留，但不顽固\n\n知识保留但不顽固的精神是：我们希望模型在做完一个任务的学习之后，在学新的知识的时候，能够保留对原来任务能力，但是这种能力的保留又不能太过顽固以至于不能学会新的任务。\n Example - Image\n我们举一个栗子看看机器的脑洞有多大。这里是影像辨识的栗子，来看看在影像辨识任务中是否需要终身学习。\n\n如上图所示，我们有两个任务，都是在做手写数字辨识，但是两个的corpus 是不同的（corpus1 图片上存在一些噪声）。network 的架构是三层，每层都是50个neuron，然后让机器先学任务1，学完第一个任务以后在两个corpus 上进行测试，得到的结果如左边的柱状图（task2的结果更好一点其实是很直觉的，因为corpus2上没有noise，这可以理解为transfer learning）。然后我们在把这个模型用corpus2 进行一波训练，再在两个corpus上进行测试得到的结果如右侧柱状图，发现第一个任务有被遗忘的现象发生。\n这时候你可能会说，这个模型的架构太小了，他只有三层每层只有50个neuron，会发生遗忘的现象搞不好是因为它脑容量有限。但是我们实践过发现并不是模型架构太小。我们把两个corpus 混到一起用同样的模型架构train 一发，得到的结果如下图右下角：\n\n所以说，明明这个模型的架构可以把两个任务都学的很好，为什么先学一个在学另一个的话会忘掉第一个任务学到的东西呢。\n Example - Question Answering\n另一个栗子：问答系统，问答系统（如下图）要做的事情是训练一个Deep Network ，给这个模型看很多的文章和问题，然后你问它一个问题，他就会告诉你答案。具体怎么输入文章和问题，怎么给你答案，怎么设计网络，不是重点就不展开。\n\n\nModel From https://arxiv.org/pdf/1502.05698.pdf\n\n对于QA系统已经被玩烂的corpus 是bAbi 这个数据集，这里面有20种不同的题型，比如问where、what 等。我们一次让模型学习这20种题型，每次学习完成以后我们都用题型五做一次测试，也就是以题型五作为baseline，结果如下：\n\n我们可以看到只有在学完题型五的时候，再问机器题型五的问题，它可以给出很好的答案，但是在学完题型六以后它马上把题型五忘的一干二净了。\n这个现象在以其他的题型作为baseline 的时候同样出现了：\n\nps：有趣的是，在题型十作为baseline 的时候可能是由于题型6、9、17、18和题型10比较相似，所以在做完这些题型的QA任务的时候在题型10上也能得到比较好的结果。\n那你又会问了，是不是因为网络的架构不够大，机器的脑容量太小以至于学不起来。其实不是，当我们同时train这20种题型得到的结果是还不错的：\n\n所以机器的遗忘是和人类很不一样的，他不是因为脑容量不够而忘记的，不知道为什么它在学过一些新的任务以后就会较大程度的遗忘以前学到的东西，这个状况我们叫做Catastrophic Forgetting（灾难性遗忘）。之所以加个形容词是因为这种遗忘是不可接收，就像下面这张图学了就忘了，淦：\n\n Wait a minute…\n你可能会说这个灾难性遗忘的问题你上面不是已经有了一个很好的解决方法了吗，你只要把多个任务的corpus 放在一起train 就好了啊。但是，长远来说这一招是行不通的，因为我们很难一直维护所有使用过的训练数据，而且就算我们很好的保留了所有数据，在计算上也有问题，我们每次学新任务的时候就要重新训练所有的任务，这样的代价是不可接受的。\n另外，多任务同时train 这个方法其实可以作为LLL的上界。\n总之，LLL主要探讨的问题是，让机器在学习新的知识的时候能不要忘记过去学过的东西。\n那这个问题有什么样的解法呢，接下来就来介绍一个经典解法。\n Elastic Weight Consolidation (EWC)\n怎么翻译啊，弹性参数巩固?\n基本精神：网络中的部分参数对先前任务是比较有用的，我们在学新的任务的时候只改变不重要的参数。\n\n如上图所示， θb\\theta^bθb 是模型从先前的任务中学出来的参数。\n每个参数 θib\\theta^{b}_{i}θib​ 都有一个守卫 bib_ibi​ ，这个首位就会告诉我们这个参数有多重要，我们有多么不能更改这个参数。\n我们在做EWC 的时候（train 新的任务的时候）需要再原先的损失函数上加上一个regularization ，如上图所示，我们通过平方差的方式衡量新的参数 θi\\theta_iθi​ 和旧的参数 θib\\theta^{b}_{i}θib​ 的差距，让后用一个守卫值来告诉模型这个参数的重要性，当这个守卫 bib_ibi​ 等于零的时候就是说参数 θi\\theta_iθi​ 是没有约束的，可以根据当前任务随意更改，当守卫 bib_ibi​ 趋近于无穷大的时候，说明这个参数 θi\\theta_iθi​ 对先前的任务是非常重要的，希望模型不要变动这个参数。\n\n所以现在问题是， bib_ibi​ 如何决定。这个问题我们下面来讲，先来通过一个简单的栗子再理解一下EWC的思想：\n\n上图是这样的，假设我们的模型只有两个参数，这两个图是两个task 的error surface ，颜色越深error 越大。假如说我们让机器学task1的时候我们的参数从 θ0θ^0θ0 移动到 θbθ^bθb ，然后我们又让机器学task2，在这学这个任务的时候我们没有加任何约束，它学完之后参数移动到了 θ∗θ^*θ∗ ，这时候模型参数在task1的error surface 上就是一个不太好的点。 这就直观的解释了为什么会出现Catastrophic Forgetting 。\n用了EWC 的话看起来是这样的：\n\n当我们使用EWC 对模型的参数的变化做一个限制，就如上面说的，我们给每个参数加一个守卫 bib_ibi​ ，这个 bib_ibi​ 是这么来的呢？不同文章有不同的做法，这里有一个简单的做法就是算这个参数的二次微分（loss对θ的二次微分体现参数loss变化的剧烈程度，二次微分值越大，原函数图像在该点变化越剧烈），如上图所示。我们可以看出， θ1bθ^b_1θ1b​ 在二次微分曲线的平滑段其变化不会造成原函数图像的剧烈变化，我们要给它一个小的守卫 b1b_1b1​ ， 反之 θ2bθ^b_2θ2b​ 则在谷底其变化会造成二次微分值的增大，导致原函数的变化更剧烈，我们要给它一个大的守卫 b2b_2b2​ 。也就是说，θ1bθ^b_1θ1b​ 能动，θ2bθ^b_2θ2b​ 动不得。\n有了上述的constraint ，我们就能让模型参数尽量不要在 θ2θ_2θ2​ 方向上移动，可以在 θ1θ_1θ1​ 上移动，得到的效果可能就会是这样的：\n\n EWC - Experiment\n我们来看看EWC的原始paper中的实验结果：\n\n三个task其实就是对MNIST 数据集做不同的变换后做辨识任务。每行是模型对该行的task准确率的变化，从第一行可以看出，当我们用EWC的方法做完三个任务学习以后仍然能维持比较好的准确率。值得注意的是，在下面两行中，L2的方法在学习新的任务的时候发生了Intransigence（顽固）的现象，就是模型顽固的记住了以前的任务，而无法学习新的任务。\n EWC Variant\n有很多EWC 的变体，给几个参考：\n\n\n\nElastic Weight Consolidation (EWC)\n\n\nhttp://www.citeulike.org/group/15400/article/14311063\n\n\nSynaptic Intelligence (SI)\n\n\nhttps://arxiv.org/abs/1703.04200\n\n\nMemory Aware Synapses (MAS)\n\n\nSpecial part: Do not need labelled data\n\n\nhttps://arxiv.org/abs/1711.09601\n\n\n\n Generating Data\n上面我们说Mutli-task Learning 虽然好用，但是由于存储和计算的限制我们不能这么做，所以采取了EWC 等其他方法，而Mutli-task Learning 可以考虑为Life-Long Learning 的upper bound。 反过来我们不禁在想，虽然说要存储所有过去的资料很难，但是Multi-task Learning 确实那么好用，那我们能不能Learning 一个model，这个model 可以产生过去的资料，所以我们只要存一个model 而不用存所有训练数据，这样我们就做Multi-task 的learning。（这里暂时忽略算力限制，只讨论数据生成问题）\n\n这个过程是这样的，我们先用training data 1 训练得到解决task 1 的model，同时用这些数据生成train 一个能生成这些数据的generator ，存储这个generator 而不是存储training data ；当来了新的任务，我们就用这个generator 生成task 1的training data 和 task2 的training data 混在一起，用Multi-task Learning 的方法train 出能同时解决task1 和task2 的model，同时我们用混在一起的数据集train 出一个新的generator ，这个generator 能生成这个混合数据集；以此类推。这样我们就可以做Mutli-task Learning ，而不用存储大量数据。但是这个方法在实际中到底能不能做起来，还尚待研究，一个原因是实际上生成数据是没有那么容易的，比如说生成贴合实际的高清的影像对于机器来说就很难，关于generator model 的训练方法这里就不展开了，在生成模型的一节里有讲，GAN是一个解法。以下Generating Data 这种方法的参考：\n\nhttps://arxiv.org/abs/1705.08690\nhttps://arxiv.org/abs/1711.10563\n\n Adding New Classes\n在刚才的讨论种，我们都是假设解不同的任务用的是相同的网络架构，但是如果现在我们的task 是不同，需要我们更改网络架构的话要怎么办呢？比如说，两个分类任务的类别数量不同，我们就要修改network 的output layer 。这里就列一些参考给大家：\n\n\nLearning without forgetting (LwF)\nhttps://arxiv.org/abs/1606.09282\n\n\n\niCaRL: Incremental Classifier and Representation Learning\nhttps://arxiv.org/abs/1611.07725\n\n Knowledge Transfer\n怎么翻译？知识迁移？\n\n我们不仅希望机器可以可以记住以前学的knowledge ，我们还希望机器在学习新的knowledge 的时候能把以前学的知识做transfer。什么意思呢，我来解释一下：我们之前都是每个任务都训练一个单独的模型，这种方式会损失一个很重要的信息，就是解决不同问题之间的通用知识。形象点来说，比如你先学过线性代数和概率论，那你在学机器学习的时候就会应用先前学过的知识，学起来就会很顺利。Life-Long Learning 也是希望机器能够把不同任务之间的知识进行迁移，让以前学过的知识可以应用到解决新的任务上面。如下图，机器从一个憨憨一样的机器人学着学着就高端起来了：\n\n这就是为什么我们期望用同一个模型解决多个任务，而不是为每个任务单独训练一个模型。\n Life-Long v.s. Transfer\n讲了这么多transfer，你可能会说，这不就是在做transfer Learning 吗？\n\nLLL 确实有应用Transfer Learning 的思想，但是它比后者更进一步。怎么说呢，Transfer Learning 的精神是应用先前任务的模型到新的任务上，让模型可以解决或者说更好的解决新的任务，而不在乎此时模型是否还能解决先前的任务；但是LLL 就比Transfer Learning 更进一步，它会考虑到模型在学会新的任务的同时，还不能忘记以前的任务的解法。\n Evaluation\n讲到这里，我们来说一下如何衡量LLL 的好坏。其实，有很多不同的的衡量方法，这里简介一种。\n\n这里每一行是一个模型在每个任务上的测试结果，每一列是用一个任务对一个模型在做完某些任务的训练以后进行测试的结果。\nRi,jR_{i,j}Ri,j​ : 在训练完task i 后，模型在task j 上的performance 。\n如果 i &gt; j : 在学完task i 以后，模型在先前的task j 上的performance。\n如果 i &lt; j : 在学完task i 以后，模型在没学过的task j 上的performance，来说明前面学完的 i 个task 能不能transfer 到 task j 上。\n\nAccuracy 是指说机器在学玩所有T 个task 以后，在所有任务上的平均准确率，所以如上图红框，就把最后一行加起来取平均就是现在这个LLL model 的Accuracy ，形式化公式如上图所示。\nBackward Transfer 是指机器有多会做[Knowledge Retention](#Knowledge Retention)（知识保留），有多不会遗忘过去学过的任务。做法是针对每一个task 的测试集（每列），计算模型学完T 个task 以后的performance 减去模型刚学完对应该测试集的时候的performance ，求和取平均，形式化公式如上图所示。\nBackward Transfer 的思想就是把机器学到最后的表现减去机器刚学完那个任务还记忆犹新的表现，得到的差值通常都是负的，因为机器总是会遗忘的，它学到最后往往就一定程度的忘记以前学的任务，如果你做出来是正的，说明机器在学过新的知识以后对以前的任务有了触类旁通的效果，那就很强😮。\nForward Transfer 是指机器有多会做Knowledge Transfer （知识迁移），有多会把过去学到的知识应用到新的任务上。做法是对每个task 的测试集，计算模型学过task i 以后对task i+1 的performance 减去随机初始的模型在task i+1 的performance ，求和取平均。，形式化公式如下图所示。\n\n Gradient Episodic Memory (GEM)\n上述的Backward Transfer 让这个值是正的就说明，model 不仅没有遗忘过学过的知识，还在学了新的知识以后对以前的任务触类旁通，这件事是有研究的，比如GEM 。\n\n\nGEM: https://arxiv.org/abs/1706.08840\nA-GEM: https://arxiv.org/abs/1812.00420\n\nGEM 想做到的事情是，在新的task 上训练出来的gradient 在更新的参数的时候，要考虑一下过去的gradient ，使得参数更新的方向至少不能是以前梯度的方向（更新参数是要向梯度的反方向更新）。\n需要注意的是，这个方法需要我们保留少量的过去的数据，以便在train 新的task 的时候（每次更新参数的时候）可以计算出以前的梯度。\n形象点来说，以上图为例，左边，如果现在新的任务学出来的梯度是g ，那更新的时候不会对以前的梯度g1 g2 造成反向的影响；右边，如果现在新的情况是这样的，那梯度在更新的时候会影响到g1，g 和g1 的内积是负的，意味着梯度g 会把参数拉向g1 的反方向，因此会损害model 在task 1上的performance。所以我们取一个尽可能接近g 的g’ ，使得g’ 和两个过去任务数据算出来的梯度的内积都大于零。这样的话就不会损害到以前task 的performance ，搞不好还能让过去的task 的loss 变得更小。\n我们来看看GEM 的效果：\n\n很直观，就不解释了，总而言之就是GEM 很强这样子了啦🤣。\n Model Expansion\nbut parameter efficiency\n模型扩张，且参数高效\n\n上面讲的内容，我们都假设模型是足够大的，也就是说模型的参数够多，它是有能力把所有任务都做好，只不过因为某些原因它没有做到罢了。但是如果现在我们的模型已经学了很多任务了，所有参数都被充分利用了，他已经没有能力学新的任务了，那我们就要给模型进行扩张。同时，我们还要保证扩张不是任意的，而是有效率的扩张，如果每次学新的任务，模型都要进行一次扩张，那这样的话你最终就会无法存下你的模型，而且臃肿的模型中大概率很多参数都是没有用的。\n这个问题在2018年老师讲课的时候还没有很多文献可以参考，这里就流水账的记录一下老师讲的流水帐。\n Progressive Neural Networks\n\n\nhttps://arxiv.org/abs/1606.04671\n\n这个方法是这样的，我们在学task 1的时候就正常train，在学task 2的时候就搞一个新的network ，这个网路不仅会吃训练集数据，而且会把训练集数据input 到task 1的network中得到的每层输出吃进去，这时候是fix 住task 1 network，而调整task 2 network 。同理，当学task 3的时候，搞一个新的network ，这个网络不仅吃训练集数据，而且会把训练集数据丢入task 1 network 和 task 2 network ，将其每层输出吃进去，也是fix 住前两个network 只改动第三个network 。\n这是一个早期的想法，2016年就出现了，但是这个方法，终究还是不太能学很多任务。\n Expert Gate\n\n\nhttps://arxiv.org/abs/1611.06194\nAljundi, R., Chakravarty, P., Tuytelaars, T.: Expert gate: Lifelong learning with a network of\nexperts. In: CVPR (2017)\n\n思想是这样的：每一个task 训练一个network 。但是train 了另一个network ，这个network 会判断新的任务和原先的哪个任务最相似，加入现在新的任务和T1 最相似，那他就把network 1最为新任务的初始化network，希望以此做到知识迁移。但是这个方法还是每一个任务都会有一个新的network ，所以还是不太好。\n Net2Net\n如果我们在增加network 参数的时候直接增加神经元进去，可能会破坏这个模型原来做的准确率，那我们怎么增加参数才能保证不会损害模型在原来任务上的准确率呢？Net2Net 是一个解决方法：\n\n\nhttps://arxiv.org/abs/1511.05641\n用到了Net2Net：https://arxiv.org/abs/1811.07017\n\nNet2Net的具体做法是这样的，这里举一个栗子，如上图所示，当我么你要在中间增加一个neuron 时，我们把f 变为f/2 ，这样的话同样的输入在新旧两个模型中得到的输出就还是相同的，同时我们也增加了模型的参数。但是这样做出现一个问题，就是h[2] h[3] 两个神经元将会在后面更新参数的时候完全一样，这样的话就相当于没有扩张模型，所以我们要在这些参数上加上一个小小的noise ，让他们看起来还是有小小的不同，以便更新参数。\n图中第二个引用的文章就用了Net2Net，需要注意，不是来一个任务就扩张一次模型，而是当模型在新的任务的training data 上得不到好的Accuracy 的时候才用Net2Net 扩张模型。\n Curriculum Learning\n\n如上图所示，模型的效果是非常受任务训练顺序影响的。也就是说，会不会发生遗忘，能不能做到知识迁移，和训练任务的先后顺序是有很大关系的。假如说LLL 在未来变得非常热门，那怎么安排机器学习的任务顺序可能会是一个需要讨论的热点问题，这个问题叫做Curriculum Learning 。\n2018年就已经有一篇文献是在将这个问题：\n\n\nhttp://taskonomy.stanford.edu/#abstract CVPR 的best paper\n\n文章目的是找出任务间的先后次序，比如说先做3D-Edges 和 Normals 对 Point Matching 和Reshading 就很有帮助。\n","categories":["李宏毅机器学习笔记"],"tags":["ML","note","Next-Step-of-ML"]},{"title":"Anomaly Detection","url":"/Anomaly-Detection/","content":" Anomaly Detection\n翻译成中文就是，异常探测。异常探测就是要让机器知道&quot;I don’t know&quot;，这件事。\n Problem Formulation\n异常探测问题通常形式化成下述表示形式：\n\n给出一个训练数据集 {x1,x2,...,xN}\\{x^1, x^2,..., x^N\\}{x1,x2,...,xN}\n我们想找出一个函数去探测输入 xxx 是否和训练数据相似\n\n也就是说异常探测的任务就是找一个function，这个function做的事情就是，我们给它一个和训练数据相似的输入，它就返回一个结果告诉我们输入是正常的，反之，当我们输入一个和训练数据差别非常大的样本，它就返回一个结果告诉我们输入是异常的。\n\n通常我们看到anomaly 这个词都是负面的，但是这里说的异常探测并不是负面意思，只是在说某个数据和训练数据集中的数据差别很大而已。Anomaly 有时也会用outlier, novelty, exceptions 替换，使用novelty的时候就是找一个新颖的东西，所以，总的来说我们就是要找出和训练集样本不一样的东西。\n至于怎么定义similarity，这就是Anomaly Detection需要探讨的问题，不同的方法我们要用不同的方式定义相似性。\n What is Anomaly?\n这里我要强调一下什么叫做异常，机器到底要看到什么才应该认定为Anomaly，这其实是取决你提供给机器什么样的训练数据。举个栗子：\n\n如果你提供了很多的雷丘作为训练数据，皮卡丘就是异常的。如果你提供了很多的皮卡丘作为训练数据，雷丘就是异常的。如果你提供很多的宝可梦作为训练数据，数码宝贝就是异常的。\n Applications\n异常探测有很多应用，比如可以应用到欺诈检测（Fraud Detection）\n Fraud Detection\nFraud Detection 应用于银行或保险等行业，Fraud 包括但不限于伪造支票，信用卡欺诈等行为。欺诈探测的目的是预测欺诈行为，防止欺诈交易的发生。\n这种任务的训练资料就是正常的刷卡机行为，收集很多的交易记录，这些交易记录是为正常的交易行为，现在新来一笔交易记录，我们就把它输入到模型中，判断这笔交易是否有异常，甚至可以做到预测和这笔交易有关的对象接下来是否有发生欺诈交易的可能，从而预防欺诈的发生。（直觉上可以简单的想象：如果正常的交易金额比较小，频率比较低，若短时间内有非常多的高额消费，这可能是异常行为）\n这种应用是有一些比赛的，下面是kaggle上的链接：\n\nRef: https://www.kaggle.com/ntnu-testimon/paysim1/home\nRef: https://www.kaggle.com/mlg-ulb/creditcardfraud/home\n实践篇：一个关于Fraud Detection的例子（一）\n\n Network Intrusion Detection\n异常检测还可以应用到网络系统的入侵检测，训练数据是正常的网络连接。现在来了一个新的连接，你希望用Anomaly Detection 让机器自动决定这个新的连接是否为攻击行为。\n\nRef: http://kdd.ics.uci.edu/databases/kddcup99/kddcup99.html\nKDD99老经典了😂\n\n Cancer Detection\n异常检测还可以应用到医疗（癌细胞的侦测），训练数据是正常细胞。现在给出一个新的细胞，让机器判断这个细胞是否为癌细胞。\n\nRef: https://www.kaggle.com/uciml/breast-cancer-wisconsin-data/home\n\n Binary Classification\n接下来就是具体怎么作Anomaly Detection 了，直觉的想法就是：现在我们可以收集正负两种训练数据：\n\n\n正常数据：{x1,x2,⋯,xN}\\{x^1,x^2,⋯,x^N\\}{x1,x2,⋯,xN}\n\n\n异常数据：{x~1,x~2,⋯,x~N}\\{\\widetilde{x}^1, \\widetilde{x}^2, ⋯, \\widetilde{x}^N\\}{x1,x2,⋯,xN}\n\n\n然后train一个二分类器就完事了。\n这个问题其实并没有那么简单，因为不太容易把异常检测看作一个binary classification的问题。为什么这样说呢？\n举个栗子，我们现在将Pokémon 视为正常数据，将其他所有事物视为异常数据，我们要让机器判断输入是不是一个Pokémon ：\n\n那问题来了，首先不是Pokémon 的事物太多了，数码宝贝不是，凉宫春日不是，水壶不是，所以我们很难将除正常数据以外的所以事物都归结为一类，很难收集所有的异常数据，很难将异常数据的特征都让机器记住。\n再者，很多时候你很难收集到异常数据，比如欺诈检测，在所有的交易中，异常交易的比例非常至少。\n Categories\n现在我们把异常检测任务分为两类：\n\n一类，是不只有训练数据 {x1,x2,⋯,xN}\\{x^1,x^2,⋯,x^N\\}{x1,x2,⋯,xN} ，同时这些数据还具有label {y^1,y^2,⋯,y^N}\\{\\hat{y}^1,\\hat{y}^2,⋯,\\hat{y}^N\\}{y^​1,y^​2,⋯,y^​N} 。 用这样的数据集可以train出一个classifier，让机器通过学习这些样本，以预测出新来的样本的label，但是我们希望分类器有能力知道新给样本不属于原本的训练数据的任何类别，它会给新样本贴上“unknown”的标签。训练classifier 可以用generative model、logistic regression、deep learning等方法，你可以从中挑一个自己喜欢的算法train 出一个classifier 。\n上述的这种类型的任务，train出的classifier 具有看到不知道的数据会标上这是未知物的能力，这算是异常检测的其中一种，又叫做Open-set Recognition。我们希望做分类的时候模型是open 的，它可以辨识它没看过的东西，没看过的东西它就贴一个“unknown”的标签。\n另一类，所有训练数据都是没有label 的，这时你只能根据现有资料的特征去判断，新给的样本跟原先的样本集是否相像。这种类型的数据又分成两种情况：\n\nClean：手上的样本是干净的（所有的训练样本都是正常的样本）\nPolluted：手上的样本已经被污染（训练样本已经被混杂了一些异常的样本，更常见）\n\n情况二是更常见的，对于刚才的诈欺检测的例子而言，银行收集了大量的交易记录，它把所有的交易记录都当做是正常的，然后告诉机器这是正常交易记录，然后希望机器可以借此检测出异常的交易。但所谓的正常的交易记录可能混杂了异常的交易，只是银行在收集资料的时候不知道这件事。所以我们更多遇到的是：手上有训练样本，但我没有办法保证所有的训练样本都是正常的，可能有非常少量的训练样本是异常的。\n Case 1: With Classifier\n我们来看看情况一，就是训练样本有label的情况，也就是说我们可以train出一个初始的model。举的栗子是检测输入是不是辛普森家族的人物。\n\n\n我们现在收集的辛普森家庭的人物都具有标注（霸子，丽莎，荷马，美枝），有了这些训练资料以后就可以训练出一个辛普森家庭成员的分类器。如下图所示，我们就可以给分类器看一张照片，它就可以判断这个照片中的人物是辛普森家庭里面的哪个人物。\n\n这个数据集来自kaggle上一个非常喜欢辛普森家庭的人，它collect并且label了数千张数据，以下是参考链接\n\nSource of model: https://www.kaggle.com/alexattia/the-simpsons-characters-dataset/\n\n然后，这个人训练出一个分类器，然后用这个分类器做了测试，结果有百分之九十六的正确率。如下图所示：\n\n现在我们想做的事情是基于这个分类器来进行异常检测，判断输入的人物是否来自辛普森家庭。\n Confidence score\n我们原本是使用分类器来进行分类，现在希望分类器不仅可以来自分类，还会输出一个数值，这个数值代表信心分数（Confidence score ），然后根据这个信心分数做异常检测。\n\n如上图所示，我们可以定义一个阈值 λ\\lambdaλ ，若信心分数大于 λ\\lambdaλ 就说明是来自于辛普森家庭。若信心分数小于 λ\\lambdaλ 就说明不是来自于辛普森家庭.\n那问题就来了，如何计算这个confidence score呢？我们希望这个信心分数应该有这样的能力，当我们将图片输入辛普森家庭的分类器中，若分类器非常的肯定这个图片到底是谁，输出的信心分数就会非常的高，反之则低。\n需要再提一嘴的是，当我们将图片输入分类器时，分类器的输出是一个几率分布（distribution），是这张图片属于各个类别的可能性的得分，如下图所示：\n\n从上图直观的观察来看，当分类器非常确定一个输入的类别的时候，它会给出很集中的得分，反之则是分散的。\n刚才讲的都是定性的分析，现在需要将定性分析的结果化为信心分数。一个非常直觉的方法就是将分类器输出的分布中最高数值作为信心分数，所以上面那张图输出的信心分数为0.97（霸子），下面那张图输出的信心分数为0.26（凉宫春日）。（这个方法就是后面我们做实验用的方法）如下图所示：\n\n根据信心分数来进行异常检测不是唯一的方法，因为分类器输出的是distribution，那么就可以计算交叉熵（cross entropy）。交叉熵越大就代表输出越平均，代表机器没有办法去肯定输出的图片是哪个类别，表示输出的信心分数是比较低。总之我们有不同的方法根据分类器决定它的信心分数.\n那我们实际来做一下，就用分布中最高数值作为信心分数的方法，结果如下图所示：\n\n现在我输入一张训练资料没有的图片（荷马），分类器输出荷马的信心分数是1.00；输入霸子的图片，分类器输出霸子的信心分数为0.81，输出郭董的信心分数为0.12；输入三玖的图片和李老师的图片结果就是相对很分散的。我们可以发现，如果输入的是辛普森家庭的人物，分类器输出比较高信心分数。如果输入不是辛普森家庭的人物，分类器输出的信心分数是比较低。但是输入凉宫春日的图片，分类器输出柯阿三的信心分数为0.99，所以这种方法还是有有一些瑕疵的。\n接下来我们来看看不同的数据集使用这种方法的统计结果：\n\n我们可以看到真正的来自辛普森家庭的人物的信心分数都集中在1.0附近，你仔细看上面的图标中有一些红色的数据（分类错误）比较分散的分布在坐标轴上，所以说分类错误的数据会得到比较低的信心分数，这个方法还是有一点瑕疵的。\n而非辛普森家庭的动漫人物的分布比较分散，90%的数据都是信心分数比较低的，分散在坐标轴上，但是还是有10%的数据信心分数分布在1.0附近。这10%样本不是辛普森家庭的人物，但给了比较高的分数，也说明这个方法是有瑕疵的。\n总的来说，如果你要做异常检测的问题，在你手上有一个分类器或者说能train出一个分类器的情况下，这应该是你第一个要尝试的baseline。虽然很简单，但实际上不见得结果表现会很差。\n Outlook: Network for Confidence Estimation\n\n\nTerrance DeVries, Graham W. Taylor, Learning Confidence for Out-of-Distribution Detection in Neural Networks, arXiv, 2018\n\n你训练一个neuron network时，可以直接让neuron network输出信心分数，这个问题我先不细节，在这里我先引用一个文献（2018年的paper）是有这样的技术的。\n More Detail\n接下来我们要讲一下关于上述在有分类模型的情况下根据信心分数进行异常检测的具体细节，包括threshold（阈值）如何定，如何判断异常判断结果的好坏等问题。\n首先我们先来复习一下model framework：\n\n我们有大量的训练资料，且训练资料具有标注（辛普森家庭的人物），因此我们可以训练一个分类器。不管你用什么算法，目标是可以从分类器中得到对应图片的信心分数。然后就根据信心分数建立异常检测的系统，若信心分数高于某个threshold（阀值）时就认为是正常，若低于某个threshold 时就认为是异常。\nDev Set(development set):\n\n用于训练过程中，指导调整超参数的样本集，使用起来类似于测试集；\n以前机器学习数据量少，超参数少的时候可能是没有这个样本集的，只有训练集（train set）和测试集（test set）。这时测试集作为验证集使用；\n现在数据量多了，可以单独分出一部分样本作为dev set，用于超参数调优，模型经过训练集训练，和验证集调优，然后交给测试集测试性能；\n作者：猛虎神威凯蒂猫\nRef：https://www.zhihu.com/question/53052465/answer/281962767\n来源：知乎\n\n在现在这个异常探测任务中，我们需要的development set 需要包含大量的images，这些images需要被标注上是否是来自辛普森家庭的人物。另外，需要强调的是在训练时所有的样本数据都是来自辛普森家庭的人物，标签是辛普森家庭的哪个人物。但是，我们在做Dev Set 时需要模拟测试数据集，也就是说这里面还需要包含非辛普森家庭的人物的图片。\n有了Dev Set 以后，我们就可以把我们异常探测系统应用在Dev Set ，然后计算异常探测系统在Dev Set上的得分是多少。等下会说明这个得分如何计算。你能够在Dev Set 衡量一个异常探测系统的表现以后，你就可以拿来调整threshold，以找出让最好的threshold 。\n决定hyperparameters（超参）以后，就有了一个异常探测的系统，你就可以让它上线。输入一张图片，系统就会决定是不是辛普森家庭的人物。整个Anomaly Detection System 构建就结束了。但是我们还遗留了一个问题：\n如何计算一个异常侦测系统的性能好坏？\n\n上图是100张辛普森家庭人物的图片（蓝色）和5张不是辛普森家庭人物的图片（红色）输入到Anomaly Detection 中得到的Confidence Score。你会发现上图左侧有一个辛普森家庭的图片的信心分数竟然是非常低的，在这里异常探测系统犯了一个错误，认为它不是辛普森家庭的人物。我们可以看到，五张非辛普森家庭人物的图片的信心得分其实还挺高的，尤其是魔法少女（0.998）。在实践的时候，很多人都发现这些异常图片会得到很高的分数，其实这不是特别大的阻碍，因为真正是辛普森家庭的人物的信心分数会非常集中到1.0，只要异常图片没有正常图片的信心分数那~么~高~，就还是可以得到比较好的异常探测效果的。\n\n我们怎么来评估一个异常检测系统的好坏呢？我们知道异常检测其实是一个二元分类（binary classification）的问题。在二元分类中我们都是用正确率来衡量一个系统的好坏，但是在异常检测中正确率并不是一个好的评估系统的指标。你可能会发现一个系统很可能有很高的正确率，但其实这个系统什么事都没有做。为什么这样呢？因为在异常检测的问题中正常的数据和异常的数据之间的比例是非常悬殊的。在这个例子里面，我们使用了正常的图片有一百张，异常的图片有五张。这会造成只用准确率衡量系统的好坏会得到非常奇怪的结果的。\n举个栗子，如上图所示，我们将threshold λλλ 设为0.3和0.5。 λλλ 以上认为是正常的， λλλ 以下认为是异常的。这时你会发现这个系统的正确率都是95.2%，所以异常侦测问题中不会用正确率来直接当做评估指标。\n首先我们要知道在异常检测中有两种错误：一种错误是异常的数据被判断为正常的数据，另外一种是正常的数据被判为异常的数据。假设我们将 λλλ 设为0.5（0.5以上认为是正常的数据，0.5以下认为是异常的数据），这时就可以计算机器在这两种错误上分别范了多少错误。如下图所示：\n\n我们可以看到机器判断出现了两类错误，miss了4个anomaly data，false alarm了1个normal data 。\n\n若我们将threshold 切在比0.8稍高的部分，如上图所示，这时会发现在五张异常的图片中，其中有两张认为是异常的图片，其余三种被判断为正常的图片；在一百张正确的图片中，其中有六张图片被认为是异常的图片，其余九十四张图片被判断为正常的图片。\n那一个系统比较好呢？其实你是很难回答这个问题。有人可能会很直觉的认为：当阀值为0.5时有五个错误，阀值为0.8时有九个错误，所以认为左边的系统好，右边的系统差。但其实一个系统是好还是坏，取决你觉得false alarm比较严重还是missing比较严重。\n\n所以你在做异常探测时，可能需要有Cost Table 告诉你每一种错误有多大的Cost 。如上图所示我们将两个Cost Table 分别计算一下threshold λλλ 为0.5和0.8的两个系统的cost ，发现取不同的的cost 衡量标准，就会有不同的判定结果。\n为什么要这样呢，举一个形象一点的栗子，如果说你今天要做癌症检测的异常检测，与误判比起来，我们显然更加不能容忍missing，如果一个其实有癌症的人被系统miss 了，耽误了最佳治疗时间，就是生命的代价。\n其实还有很多衡量异常检测系统的指标，这里就不细讲这些，有一个常用的指标为Area under ROC curve。若使用这种衡量的方式，你就不需要决定threshold ，而是看你将测试集的结果做一个排序（高分至低分），根据这个排序来决定这个系统好还是不好。\n Possible Issue\n\n如果我们直接用一个分类器来检测输入的数据是不是异常的，当然这并不是一种很弱的方法，但是有时候无法给你一个perfect 的结果，我们用上图来说明用classifier 做异常侦测时有可能会遇到的问题。假设现在做一个猫和狗的分类器，将属于猫🐱的一类放在一边，属于狗🐕的一类放在一边。若输入一笔资料即没有猫的特征也没有狗的特征（草泥马，马莱貘），机器不知道该放在哪一边，就可能放在这个boundary上，得到的信息分数就比较低，你就知道这些资料是异常的。\n你有可能会遇到这样的状况：有些资料会比猫更像猫（老虎），比狗还像狗（狼）。机器在判断猫和狗时是抓一些猫的特征跟狗的特征，也许老虎在猫的特征上会更强烈，狼在狗的特征上会更强烈。对于机器来说虽然有些资料在训练时没有看过（异常），但是它有非常强的特征会给分类器很大的信心看到某一种类别。\n\n再比如，刚才做辛普森家庭人物的异常检测的时候，黄色的魔法少女和小樱都给了比较高的信心分数，所以我们猜测分类器是看到黄色就给很高的分数，我们就把三玖的脸和头发涂黄，果然分数暴增，在老师的脸上涂黄，效果也是显著的。🤣所以这些都是异常检测的问题。\n To Learn More\n\n当然有些方法可以解这个问题，这里列一些文献给大家进行参考。\n\nKimin Lee, Honglak Lee, Kibok Lee, Jinwoo Shin, Training Confidence-calibrated Classifiers for Detecting Out-of-Distribution Samples, ICLR 2018\n\n其中的一个解决方法是：假设我们可以收集到一些异常的样本，我们可以教机器看到正常样本时不要只学会分类这件事情，要学会一边做分类一边看到正常的资料信心分数就高，看到异常的资料就要给出低的信心分数。\n\nMark Kliger, Shachar Fleishman, Novelty Detection with GAN, arXiv, 2018\n\n但是会遇到的问题是：很多时候不容易收集到异常的数据。有人就想出了一个神奇的解决方法就是：既然收集不到异常的数据，那我们就通过Generative Model 来生成异常的数据。这样你可能遇到的问题是：生成的数据太像正常的资料，那这样就不是我们所需要的。所以还要做一些特别的constraint ，让生成的资料有点像正常的资料，但是又跟正常的资料又没有很像。接下来就可以使用上面的方法来训练你的classifier 。\n Case 2：Without Labels\n在第二种情况下，我们没有classifier ，也就是说我们收集到的数据没有label 。\n Twitch Play Pokémon\n\n这是一个真实的Twitch Plays Pokémon例子，这个的例子是这样的：有人开了一个宝可梦的游戏，全世界的人都可以连接一起玩这个宝可梦的游戏。右边的框是每一个人都在输入指令同时操控这个游戏，这个游戏最多纪录大概有八万人同时玩这个游戏。当大家都在同时操作同一个角色时，玩起来其实是相当崩溃的。（Paperclip2019年做的解密活动中的一部分似乎是在模仿这个游戏模式）\n这个游戏是多年前进行的，不过现在twitch 上居然还有升级版的，直播地址：\n\nhttps://www.twitch.tv/twitchplayspokemon\n\n以下是我录制的一小段gif：\n\n\n人们玩的时候就非常的崩溃，发现根本玩不下去，那么崩溃的原因是什么呢？可能是因为有Troll（网络小白？）。这些人可能根本就不会玩，所以大家都没有办法继续玩下去；或者可能觉得很有趣就乱按按键；或者是不知名的恶意，不想让大家结束这个游戏（脚本小子）。人们相信有一些Troll 潜藏在人们当中，他们想要阻挠游戏的进行。\n假定多数玩家是想要通关的，那我们就可以用异常检测的技术，收集所有玩家的行为（训练数据），我们可以从多数玩家的行为知道正常玩家的行为是咋样的，然后侦测出异常的玩家（Troll）。\n Problem Formulation\n\n我们需要一些训练的样本： {x1,x2,...,xN}\\{x^1,x^2,...,x^N\\}{x1,x2,...,xN} ，每一个 xix^ixi 代表一个玩家，如果我们使用machine learning 的方法来求解这个问题，首先这个玩家要能够表示为feature vector 。举个例子：向量的第一维可以是玩家说垃圾话的频率，第二维是统计玩家无政府状态发言，可以想象有可能是这样的，一个玩家越是喜欢在无政府状态下发言，就越有可能是搞破坏的。\n这个数据集是真实存在的：\n\nhttps://github.com/ahaque/twitch-troll-detection (Albert Haque)\n\n我们现在只有大量的训练数据，但是没有label。我们刚才可以根据分类器的conference来判断是不是异常的资料，我们在没有classifier的情况下可以建立一个模型，这个模型是告诉我们P(x)的几率有多大（根据训练数据，找一个几率模型，这个模型可以告诉我们某一种行为发生的概率有多大）。当我们有了模型P ，我们就可以将玩家的特征向量输入模型，得到一个几率（就是现实情况中发生这名玩家的动作的概率），如果这个几率大于threshold ，就认为是正常玩家，反之则判定为异常玩家。综上所述我们的问题的解决方案就被形式化为下图：\n\n这个模型可以通过生成模型的方式来找，比如说Gaussian Distribution Model ，我们假定玩家数据是在特征空间中符合高斯分布，那我们可以通过现有数据计算出这个分布模型的 μμμ 和 ∑∑∑ ，就得到这个模型了，然后我们就可以按上述进行数据输入，做判定了。\n\n如上图所示，这是一个真实的资料，假设每一个玩家可以用二维的向量来描述（一个是说垃圾话的几率，一个是无政府状态发言的几率）。我们可以观察到，喜欢在无政府状态发言的玩家占多数，喜欢说辣鸡话的玩家占少数，综上，我们直观的理解是正常玩家应该是喜欢在无政府状态发言，而比较少说辣鸡话的。所以我们的模型 P(x)P(x)P(x) 的结果应该符合这样的描述，如上图右上角。\nps：民主状态：统计一段时间内的玩家指令数，选取数量最多的一个执行；无政府状态：一段时间内所有的指令中随机pick 一个执行。\n那为什么会出现这种情况呢，我们来简单分析一下。事实上很多人强烈支持无政府状态，强烈反对民主状态，所以这个游戏多数是在无政府状态下进行。假设一个玩家不考虑自己是要在什么状态下发言，大多数人有八成的几率是在无政府下进行发言，有人觉得多数Troll 是在民主状态下发言，因为进入了民主状态，只要Troll 多发言就能够让他的行为被选中的概率放大，所以小白会特别喜欢在民主状态下发言。\n从这个图上可以很直觉的看出（右上角的三个点）一个玩家落在说垃圾的话几率低，通常在无政府状态下发言，这个玩家有可能是一个正常的玩家， P(x)P(x)P(x) 大。假设有玩家落在有五成左右的几率说垃圾话，二成的几率在无政府状态下发言；或者落在有七成左右的几率说垃圾话，七成的几率在无政府状态下发言，显然这个玩家比较有可能是一个不正常的玩家， P(x)P(x)P(x) 小。\n我们直觉上明白这件事情，但是我们仍然希望用一个数值化的方法告诉我们玩家落在哪里会更加的异常。\n Maximum Likelihood\n这里所指数值化的方法我们需要用到Likelihood 的概念（ps：如果你学过李老师的ML的**这一节课**你应该已经知道这个概念）：\n\n\n\n假设我们的数据是从一个probability density function（概率密度函数） fθ(x)f_{\\theta}(x)fθ​(x)  中sample出来的\n\nθ 是概率密度函数的参数，是未知的，是我们要从数据中学习出来的\nθ 将决定我们概率密度函数的形状\n\n\n\n现在我们要做的事就是找出这个概率密度函数究竟长什么样子\n\n\n在找这个概率密度函数的过程中，我们需要一个衡量方法，来衡量当前找到的这概率密度函数的好坏，我们就是用Likelihood ，这个似然函数的概念和损失函数有点像，Maximum Likelihood 的想法和Minimize Loss 的想法是一样的。\nLikelihood 的意思是：计算概率密度函数 fθf_{\\theta}fθ​ 产生如图所示的所有数据概率有多大。\n若严格说的话， fθ(x)f_{\\theta}(x)fθ​(x) 输出并不是概率，它的输出是probability density ；输出的范围也并不是(0,1)，有可能大于1。如果你概率论学的比较好，那你应该能很好理解这个概率密度，不行的话这里我们就简单的将其理解成概率。 xix^ixi 在以 θ\\thetaθ 为参数的probability density function sample出来的概率是 fθ(xi)f_{\\theta}(x^i)fθ​(xi) ，所以所有的样本从这个 fθ(x)f_{\\theta}(x)fθ​(x) 中sample出来的概率就是各个样本的概率连乘。\n连乘得到的结果就是likelihood 如上图所示 L(θ)L(\\theta)L(θ) 。likelihood 的可能性显然是由 θ\\thetaθ 控制的，选择不同的 θ\\thetaθ 就有不同的probability density function，就可以算法不同的likelihood 。\n而我们现在并不知道这个 θ 是多少，我们要算一个 θ∗θ^*θ∗ 算出来的likelihood是最大的。\n\n那上述的概率密度函数长什么样子呢，我们要确定这个概率密度函数的架构。我们可以假设这个概率密度函数是Gaussian Distribution （高斯分布），高斯分布的函数式就是上图最上面的公式。\n在Gaussian Distribution 的函数式中，参数 θ\\thetaθ 应该是 {mean:μ,convariancematrix:Σ}\\{mean:\\mu, convariance matrix:\\Sigma\\}{mean:μ,convariancematrix:Σ}。假设如上图所示的数据是由左上角的 {μ,Σ}\\{\\mu,\\Sigma\\}{μ,Σ} 来生成的，数据点应该在左上角的蓝色圈圈中sample 出来，它的likelihood 是比较大；如果是右下角 {μ,Σ}\\{\\mu,\\Sigma\\}{μ,Σ} ，它远离高密度区域，这个函数sample 出的数据因该多数在右下角蓝色圈圈中，但这与实际不符，显然这样计算出来的likelihood 是比较低的。\nps：不要问为什么用高斯分布，如果我用其他的分布你也会问同样的问题。再者就是因为它该死的好用，现实中很多数据的分布都比较好的符合高斯分布。你永远可以根据你的数据选择合适的分布模型。另外这里再考虑一下，如果 fθ(x)f_{\\theta}(x)fθ​(x) 是一个很复杂的网络， θ\\thetaθ 是网络中的大量参数，这时候你就会有更多的自由选择一个model 来模拟生成数据点，这样就不会被限制住，在看起来就不像Gaussian 产生的数据却硬要说是Gaussian 产生的数据。因为我们这门课还没有讲到其它进阶的模型，所以现在用Gaussian Distribution 来当做我们资料是由Gaussian Distribution 所产生的。\nGuassian Distribution 中的参数是很好解的，μ∗\\mu^*μ∗ 等于所有training data 做平均， Σ∗\\Sigma^*Σ∗ 等于将x减去 μ∗\\mu^*μ∗ 乘以 xxx 减去 μ∗\\mu^*μ∗ 的转置，然后做平均。得到的结果如下图：\n\n好了，现在我们根据training data 找出了 {μ∗,Σ∗}\\{\\mu^*,\\Sigma^*\\}{μ∗,Σ∗} ，接下来就可以做异常检测了。如上图所示，将 {μ,Σ}\\{\\mu,\\Sigma\\}{μ,Σ} 代入probability density function ，若大于某一个threshold （阈值）就说明是正常的，若小于这个threshold 就说明是异常的。\n\n每一笔资料都可以代入probability density function 算出一个数值，结果如图所示。若落在颜色深的红色区域，就说明算出来的数值越大，越是一般的玩家，颜色浅的蓝色区域，就说明这个玩家的行为越异常。如图所示的两个点：右上角的一个玩家落在很喜欢说垃圾话，多数喜欢在无政府状态下发言的区域，就说明是一个正常的玩家。左下角的一个玩家落在很少说垃圾话，特别喜欢在民主时发言，就说明是一个异常的玩家。\n到这里我们对Twitch Play Pokémon 的异常玩家探测就结束了，我们来稍微总结一下，我们用了两种特征表示玩家，假定了玩家的在这两个特征维度上分布符合Gaussian Distribution ，然后根据数据学除了一个比较好的fit 这些数据的高斯分布模型，然后用阈值分类的方法完成了异常检测。\n上述方法中，我们只是用了两个特征，那如果加上更多的特征或许效果会更好：\n\n再加入一些特征，比如如果有人不停按start 键，那它可能就是一个异常玩家，还有和其他人一不一样，比较一样的可能就会是正常玩家，再有不停唱反调的人可能也是异常玩家。加上这些特征，实践一下得到的结果：\n\n这里取log 的原因是原本的函数算出来的输出太小了。我们可以看到第一个和第三个玩家除了第四个特征都一样，但是第一个玩家和大家的选择完全一样，第三个玩家和大家的选择在大多数情况下是相同的，这是第一个得到的分数反而低，是因为机器会觉得如果你和所有人完全一样这件事就是很异常的。\n Outlook: Auto-encoder\n上述是用Generative Model （生成模型） 来进行异常检测，我们也可以使用Auto-encoder 来做这个任务。\n\n我们把所有的训练资料训练一个Encoder ，Encoder 所做的事情是将输入的图片（比如辛普森）变为code（一个向量），Decoder 所做事情是将code 解回原来的图片。训练时Encoder 和Decoder 是同时训练，训练目标是希望输入和输出越接近越好。\n\n在测试的时候，我们就把一张图片放入这个Auto-Encoder model，得到一个输出图片。因为这个网络训练时使用的是辛普森家庭的图片，所以它对辛普森家庭的图片的还原应该是比较好的，而异常的图片的还原应该会是模糊的，基于此我们可以做Auto-Encoder based Anomaly Detection。\n More …\n\nMachine Learning 中也有其它做异常检测的方法，比如SVM的One-class SVM ，只需要正常的资料就可以训练SVM ，然后就可以区分正常的还是异常的数据。在Random Forset 的Isolated Forest ，它所做的事情跟One-class SVM 所做的事情很像（给出正常的训练进行训练，模型会告诉你异常的资料是什么样子）。\n\nOne-class SVM Ref: https://papers.nips.cc/paper/1723-support-vector-method-for-novelty-detection.pdf\nIsolated Forest Ref: https://cs.nju.edu.cn/zhouzh/zhouzh.files/publication/icdm08b.pdf\n\n","categories":["李宏毅机器学习笔记"],"tags":["ML","note","Next-Step-of-ML"]},{"title":"Explainable ML","url":"/Explainable-ML/","content":" Explainable ML\n 基本概念\n举例来说，在图片识别任务中，我们给机器一张图片，它就会告诉我们图片中有什么事物，但是我们不只希望机器能告诉我们图片中有什么，我们还希望它能告诉我们它判断图片中存在该事物的理由。\n\n如上图所示，可解释的机器学习目前分为两种类型：\n\n\n局部解释\n为什么你认为图中有一只猫🐱？\n\n\n全局解释\n你认为猫🐱应该是什么样子的？\n\n\n 为什么我们需要Explainable ML\n\n举例来说：\n现在很多公司都会想要用机器来协助做履历的判读，我们希望用机器来做履历的判读的时候是公平的，那我怎么让人知道机器是不是公平的呢？也许我们就需要机器它一边判读，一边告诉我们他筛选履历的理由，这样我们就可以知道机器到底是真的在判读一个人的能力，还是仅仅凭借肤色和性别这样的数据进行判断。\n又或者，让机器协助判断一个罪犯是否可以被假释，我们需要知道机器是否真的根据具体的事证来判断这件事，还是仅仅凭借肤色和性别这样的数据进行判断。\n又或者，进行金融相关的决策需要提供判断理由，比如为什么不给某人贷款？\n还有，当我们可以做可解释的机器学习模型时，我们就能做模型诊断，就可以知道机器到底学到了什么，是否和我们的预期相同。（ps: 准确率不足以让我们精确调整模型，只有当我们知道why the answer is wrong, so i can fix it. ）\n\n My Point of View\n我（李宏毅老师）的观点\n可解释机器学习的目标 != 完全了解机器学习模型如何工作 可以说完全了解它是如何工作，这件事未必是必要的。\n\n很多人不信任机器学习模型，因为他们认为它是个黑盒\n但是人类本身也是个黑盒\n\n所以可解释机器学习的目标应该是 让人（你的客户、老板、你自己）舒服 😆\n也许最终在未来，可解释机器学习可以根据不同的人给出不同的解释。比如，给小学生和大学生解释机器为什么会学习这件事，给出不同的解释。\n Interpretable v.s. Powerful\n可解释性 v.s. 强大\n\n简单的模型似乎不会很强\n深度学习模型是复杂的，却是强大的\n\n这里是说，可解释的机器学习不是放弃复杂的模型，只使用简单的模型，而是去尝试解释复杂模型。\n\n那有没有即容易解释又强大的模型呢，比如决策树？\n\n\n但是，事实上只用一棵决策树效果并不好，通常我们都会使用很多决策树结合起来得到比较强的效果，比如随机森林，xgboost 等，这些强大的模型也是很难解释的：\n\nhttps://stats.stackexchange.com/questions/230581/decision-tree-too-large-to-interpret\n Local Explanation Explain the Decision\nQuestion: why do you think this image is a cat?\n我们想知道机器在图片上的那部分看到了猫🐱，或者说它把那部分认成了猫。\n 基本精神\n现在有一个object xxx ，其中有NNN个Component，现在希望知道哪些Component对机器做判定是重要的。\n\n\n\nComponent\n在图片辨识任务中，component可以是一个pixel，或者一个segment（如上图中分割的图片块）\n在NLP任务中，component可以是一个word。\n\n\n关键精神：拿掉某个component或者改动其数值，观察决策变化。如果这个改动使模型决策发生非常大的变化，那这个component就是比较重要的。\n 举栗来说：\n\nReference: Zeiler, M. D., &amp; Fergus, R. (2014). Visualizing and understanding convolutional networks. In Computer Vision–ECCV 2014 (pp. 818-833)\n\n我们使用一个灰色的方块盖住图片的某个位置，观察模型的判断，最终得到如下结果。蓝色的部分是灰色方块覆盖该位置时模型认为该图片大概率不是正确标签的，红色部分表示当灰色方块覆盖该位置时模型认为该图片大概率是正确标签。\n\n上图反映出，该模型确实学习到正确的事物。另外，覆盖图片的方块的颜色、大小都是需要人工调整的参数，这其实是至关重要的。\n\nKaren Simonyan, Andrea Vedaldi, Andrew Zisserman, “Deep Inside Convolutional Networks: Visualising Image Classification Models and Saliency Maps”, ICLR, 2014\n\n\n对于每一个输 xxx 将其某个维度加上一个小小的扰动，然后观察模型判断结果和原判断结果的差值，根据这个扰动造成的结果差值来了解机器对图片中哪些像素比较敏感。（其实这个扰动造成的影响，可以理解为偏微分∂yk∂xn\\frac{\\partial y_k}{\\partial x_n}∂xn​∂yk​​） 根据这个偏微分的值，我们画出了上图的结果，亮度越大表示偏微分值越大。可见，该模型确实学习到正确的事物。\n Case Study: Pokémon v.s. Digimon\n我们想看看Saliency Map能带来什么样的结果，用机器学习辨识输入进来的图片是pokemon还是digimon。\n\n我们看看结果：\n\n好像很厉害哦，但是我们使用saliency map看看机器到底学到了什么。\n\n\n我们看到，机器好像把关注点都放在了本体之外的部分上，原来pokemon和digimon数据集一个是png，一个是jpg，两者的背景一个是透明的一个是纯黑的🤣\n 更多…\n\n\nGrad-CAM (https://arxiv.org/abs/1610.02391)\n\n\nSmoothGrad (https://arxiv.org/abs/1706.03825)\n\n\nLayer-wise Relevance Propagation(https://arxiv.org/abs/1604.00825)\n\n\nGuided Backpropagation(https://arxiv.org/abs/1412.6806)\n\n\n Limitation of Gradient based Approaches\n基于梯度下降的方法的限制\n刚才我们用gradient的方法判断一个pixel或者一个segment是否重要，但是梯度有一个限制就是gradient saturation（梯度饱和）。\n\ngradient saturation\n\n\n举例来说，我们如果通过鼻子的长度来判断一个生物是不是大象，鼻子越长是大象的信心分数越高，但是当鼻子长到一定程度信心分数就不太会变化了，信心分数就封顶了。在上图标记的点，梯度已经很小了，这和我们想要的结果不太一样了，因为我们希望鼻子的长度越长，它是大象的信心分数就越大。回到先前的判断重要性的方法，也就是说当某个特征梯度饱和时，用gradient based的方法判断，这个特征对模型做判别变得不重要了。\n那怎么解决这个问题呢，如图中列出的参考方法。\n Attack Interpretation\n攻击机器学习的解释是可能的吗？（可能的）\n\nInterpretation of Neural Networks is Fragile\nAmirata Ghorbani, Abubakar Abid, James Zou\nhttps://arxiv.org/abs/1710.10547\n\n\n我们可以加一些神奇的扰动到图片上，人眼虽然辨识不出来，也不改变分类结果，但是模型聚焦的判定点却变了。\n Global Explanation: Explain the whole Model\nQuestion: What do you think a “cat” looks like?\n我们想知道为什么机器认为的猫🐱具体长什么样子。（也就是去做一个生成器Generator）\n Activation Maximization(review)\n这个技术之前讲深度学习的时候讲过，参考，是说找一个样本让某一个filter的output或者是某一个neuron的output最大化：\n\n举例来说，在手写数字辨识的任务中，我们想知道机器心里理想的1长什么样子，也就是说要让机器画出一个1。做法是我们照一张图片作为输入x∗x^*x∗让代表1的output的那一个维度的值最大化，即：\nx∗=argmax⁡xyix^* = arg \\max_{x} y_i\nx∗=argxmax​yi​\n画出来的结果如上图所示，很差😐。鉴于之前讲过的Adversarial Attack，这个结果大概也在我们的意料之中了，在Adversarial Attack中我们给图片加上一个微小的扰动，模型就会将其辨识成不同的类别，现在也是一样，我们给机器看一个杂讯，它可能就会认为这个杂讯图片就是1，就是2…所以，我们光是让机器找一张图片，maximize output的某一维度，不足以告诉我们机器心中认为的该output维度代表的数字的样子。\n我们需要加一些额外的限制：\n\n我们不仅要maximize output的某个维度，还要加一个限制函数R(x)R(x)R(x)让xxx尽可能像一个数字图片，这个函数输入是一张图片，输出是这张图片有多像是数字。\n那这个限制函数R(x)R(x)R(x)要怎么定义呢，定义方式有很多种，这里就简单的定义为所有pixel的和的数值，这个值越小说明黑色rgb(0,0,0)越多，这个图片就越像是一个数字，由于这里数字是用白色的笔写的，所以就加一个负号，整体去maximize yiy_iyi​与R(x)R(x)R(x)的和，形式化公式就变成了：\nx∗=argmax⁡x(yi+R(x))x^* = arg \\max_{x}( y_i + R(x) )\nx∗=argxmax​(yi​+R(x))\n得到的结果如上图所示，比加限制函数之前要好很多了，从每个图片中多少能看出一点数字的形状了，尤其是6那一张。\n在更复杂的模型上，比如要在ImgNet这个大语料库训练出的模型上做上述的事情，在不加限制函数的情况下将会得到同样杂乱无章的结果。而且在这种复杂模型中要想得到比较好的Global Explanation结果，往往要加上更多、更复杂、更精妙的限制函数。以下是研究者做出的比较厉害Global Explanation的结果：\n\n参考：https://arxiv.org/abs/1506.06579\n Constraint from Generator\n生成器的约束\n刚才是用人想象的方法（限制函数），告诉机器什么样的东西看起来像是一张正常的图片。现在我们让机器自己生成图片。只要我们收集一大堆图片给机器看，然后训出一个图片生成器。\n\n图片生成器会接收一个低维向量，输出一张图片。这个低维向量zzz可能是从gaussian distribution或者normal distribution中randomly sample出来的点，把这个输入zzz丢入生成器他就会输出一个图片。关于怎么根据这些训练资料训练出这个生成器，你可以使用GAN，VAE等方法，这里不需要展开。\n那怎么把这个图片生成器作为限制函数，限制我们让机器画出来的图片呢？如上图下半部分所示，我们将方法转变为：找一个zzz，将它输入到图片生成器中产生一个图片，再将这个图片输入原来的图片分类器，得到一个类别判断yyy，我们同样要maximize yiy_iyi​. 其实可以考虑为我们把图片生成器和图片分类器连接在一起，fix住两者的参数，通过梯度下降的方法不断改变zzz，最终找到一个合适的z∗z^*z∗可以maximize yiy_iyi​.之后我们只要将这个z∗z^*z∗丢入图片生成器就可以拿到图片x∗x^*x∗了。形式化表述为：\nz∗=argmax⁡xyiz^* = arg \\max_{x} y_i\nz∗=argxmax​yi​\n通过上述的方法（再加上亿点点trick），可以得到的结果如下图：\n\n参考：https://arxiv.org/abs/1612.00005\n Using A Model To Explain Another\n核心思想：Using interpretable model to mimic uninterpretable models.\n用一个可解释的模型，去解释一个复杂的不可解释的模型\n方法：用一个可解释的模型，去模拟一个不可解释的模型的行为。如下图所示：\n\n比如说，训练一个线性模型（蓝色）去模拟神经网络模型（黑色，黑盒）的行为，让两者的输出尽可能相同。但是问题是，由于线性模型太弱了，不足以模拟复杂的神经网络模型，难顶😑。那其实我们可以只模拟神经网络的一部分：\n Local Interpretable Model-Agnostic Explanations (LIME)\n不可知模型的局部解释\n\n如上图所示，黑盒模型是蓝色线条，我们可能无法用线性函数拟合这条曲线，但是我们可以尝试模拟图中标记点的部分，如红线所示。\nLIME的步骤：\n\n给出你想解释的数据点\n在这个点附近sample数据点\n用线性（或其他可解释模型）fit这些点\n解释线性模型\n\n通过上述步骤，我们就能近似的解释我们感兴趣的点的附近区域内，模型的行为。问题又来了，我们怎么定义“附近”这个概念呢，我们对附近的定义不同，我们得到的分析结果就不同，就像上面Local Explanation中覆盖图片的方块的大小和颜色不同会得到不同的结果。对于上面的图，如果我们把附近定的比较宽泛，就会变成这样：\n\n显然，得到的线性模型就很不一样了。所以结论就是，这个附近是需要你自己调整的hyperparameter.\nPS：LIME通常只能用作局部近似分析，因为可解释模型一般都很简单，简单到不能模拟复杂模型的行为。\n LIME - Image 实践\n接下来我们实际把LIME用作解释一个图片分类器：\n\nRef: https://medium.com/@kstseng/lime-local-interpretable-model-agnostic-explanation-技術介紹-a67b6c34c3f8\n\n\n\n\n首先现有一张要解释的图片，我们想知道模型为什么把它认作树蛙\n\n\n在这张图附近sample一些数据\n我们通常会用一些toolkit把图片做一下切割，然后随机的丢掉一些图块，得到的新图片作为原图片附近区域中的数据点。\n\n\n把上述这些图片输入原黑盒模型，得到黑盒的输出\n\n\n用线性模型（或者其他可解释模型）fit上述数据\n在上面的栗子中我们在做图片辨识任务，此时我们可能在将图片丢到线性模型之前先做一个特征抽取，如下图所示：\n\n\n\n​\t\t根据你做的任务不同有不同的trick。\n\n\n解释你的线性模型\n\n如上图所示，当线性模型种某个特征维度对应的weight：\n\n趋近于零，说明这个segment对模型判定树蛙不重要\n正值，说明这个segment对模型判定树蛙有作用\n负值，说明这个segment对模型判定树蛙有反作用\n\n\n\n李宏毅老师实做：\n\n机器认为图中25%的概率有和服，5%的概率有实验袍。用LIME分析一下，机器认为哪部分是和服，哪部分是实验袍。结果如上图所示。🤔\n Decision Tree\n上面使用线性模型解释黑盒模型，我们也可以使用其他的可解释的简单模型来解释，比如决策树。但是如果我们为了让decision tree模仿一个复杂模型的行为，而训练出一个很深的tree，那就是搬起石头砸自己的脚。也就是说，我们不希望训练出一个太深的decision tree，我们需要限制tree的深度。\n\n我们将模拟黑盒模型 θθθ 的决策树模型写作 TθT_θTθ​，我们希望有一个函数 O(Tθ)O(T_θ)O(Tθ​) 表示 TθT_θTθ​ 有多复杂，比如说这个函数可以用 TθT_θTθ​ 的平均深度表示。我们现在希望复杂模型找到一个决策树，同时希望这个决策树的复杂度小，怎么做这件事呢，看看下面这个work：\n\nhttps://arxiv.org/pdf/1711.06178.pdf\n\n\n想法是：我们来训练一个特别的神经网络，我们在训练这个神经网络时就考虑到它将来要被决策树分析，我们在训练它本身的参数时，同时要考虑到最小化后面用来分析它的决策时的复杂度。我们可以在损失函数种加入 O(Tθ)O(T_θ)O(Tθ​) 作为regularization，这个东西也可叫做tree的regularization。\n但是这个 O(Tθ)O(T_θ)O(Tθ​) 是不可微分的，怎么做梯度下降呢？这也是这篇文章神奇的地方，作者说他train了一个神奇的神经网络，我们可以给这个神经网络另一个神经网络的参数，它就可以输出一个数值，来表示输入的网络的复杂度。所以我们只要用这个神奇的网络，替换树正则项，就可以做梯度下降了。其实，文中描述的这个神奇的神经网络只是一个简单的前反馈神经网络而已。这个网络还蛮准确的预测到输入的网络如果用decision tree模拟时，decision tree的深度😮。\n这个神奇的网络怎么来呢？训练这个神奇的网络，可以通过我们自己random的生成一些NN，然后用DT模拟这些NN，再把NN和DT的深度做为样本，train出这个神奇的神经网络。\n细节可以参考上述文献。\n","categories":["李宏毅机器学习笔记"],"tags":["ML","note","Next-Step-of-ML"]},{"title":"Attack and Defense","url":"/Attack-and-Defense/","content":" Attack and Defense\n这里的攻击和防御是指针对人工智能或者说机器学习的模型的攻击与防御。\n Motivation\n做攻击与防御的动机是什么？\n\n我们想把机器学习技术应用到实际生活中\n目前的机器学习模型在多数情况下对噪声的干扰抗性不足\n我希望机器学习的模型不仅能够对抗一般的噪声干扰，同时还能对抗来自人类的恶意攻击的噪声\n特别是对于用来做垃圾邮件分类、恶意软件检测、网络入侵检测等任务的模型，我们更加需要让这样的模型健壮起来。\n\n Attack\n其实，目前多数的模型都是比较容易被攻击，而往往防御攻击是比较困的。\n What do we want to do?\n我们到底要做的是怎么样的攻击呢，这里举一个栗子：\n\n这里有一个图片辨识的模型，你给它一张猫🐱片，他就告诉你这张图片是一只猫，如上图所示。现在我们要做的事是这样，我们在图片 xxx 上加上攻击者特制的噪声 ΔxΔxΔx ，使得机器认不出这张图片，或者机器会把这张图片认成另一种攻击者指定的类别。类似这样的做法就叫做对模型的对抗样本攻击(Adversarial Attack)。\n Loss Function for Attack\n我们通常在train这样的model的时候会minimize一个损失函数，我们希望模型的输出能和真正的标签尽可能相同：\n\n现在我们要对模型进行攻击，攻击分成两种：有目标的攻击和无目标的攻击。\n Non-targeted Attack\n无目标的攻击的概念是这样的：我们要找一个 x′x&#x27;x′ 使其输入到模型后产生的输出能和 ytruey^{true}ytrue 越远越好（可以使用各种衡量方法，比如交叉熵cross entropy）。\n\n这个过程也是需要训练的，不过和原来训练模型的方式不同，我们现在要fixed住模型的参数 θθθ ，去调整 xxx ，使得模型输出 y′y&#x27;y′ 和正确值 ytruey^{true}ytrue 越远越好，所以损失函数可以定义为：\nL(x′)=−C(y′,ytrue)L(x&#x27;) = -C(y&#x27;, y^{true})\nL(x′)=−C(y′,ytrue)\n Targeted-Attack\n有目标攻击的概念是这样的：我们要找一个 x′x&#x27;x′ 使其输入到模型后产生的输出能和攻击者指定的错误输出 yfalsey^{false}yfalse 越近越好（可以使用各种衡量方法，比如交叉熵cross entropy）。\n\n同上，有目标的攻击也是需要训练的，一样的要fixed住模型的参数 θθθ ，去调整 xxx ，使得模型输出 y′y&#x27;y′ 和正确值 ytruey^{true}ytrue 越远越好，同时 y′y&#x27;y′ 能和指定的错误输出 yfalsey^{false}yfalse 越近越好，所以损失函数可以定义为：\nL(x′)=−C(y′,ytrue)+C(y′+yfalse)L(x&#x27;) = -C(y&#x27;, y^{true}) + C(y&#x27; + y^{false})\nL(x′)=−C(y′,ytrue)+C(y′+yfalse)\n但是如果 x’x’x’ 和原来的图片差太多那就没什么意义了，人一眼就可以看出这张图被修改过，甚至都看不出它原来的样子了。所以，我们需要一个限制条件，让 x0x^0x0 和 x′x&#x27;x′ 尽可能相同，让人认为恶意样本和原始样本看起来相同，骗过人类😁。我们的想法是如果这两者之间的差距小到一个阈值，人就不会发现，所以Constraint可以形式化的写成：\nd(x0,x′)≤εd(x^0,x&#x27;) \\le ε\nd(x0,x′)≤ε\n Constraint\n这个约束的distance应该怎么做呢？这里举两个栗子：\n\n L2-norm\n二范数如上图所示，挺好算的，就是取两者之差的平方和。\n L-infinity\nL∞L_\\inftyL∞​ 范数可以理解为取所有差值中最大的一个。\n对于不同的模型攻击时使用的distance应该是不同的，要选择合适的方法去做（根据人类的感知，来判断某个计算distance的方法是否合适）。\n上图中四个颜色的方块，分别使用了两种不同的方法进行了修改，给每一个pixel一点点修改，给一个pixel比较大的修改。彩色的四个方块，所以其实 xxx 有12个分量，调整后的两个图片的二范数distance计算出来的值其实时一样的，但是就人类的感官来看，似乎右下角的图片能看出来绿色更深了。我们再用无穷范数算一下，发现右下角的distance是大的，右上角则较小。所以说，似乎二范数计算distance不能很好的反应人类在图片上的感知。\n How to Attack\n攻击的过程如上面所说的，就和训练神经网络的是相似的，只不过是fix住参数，根据梯度下降改变输入，找到一个 x∗x^*x∗ 满足我们的目的而已。这个过程可以用以下方程描述：\nx∗=argmin⁡d(x0,x′)≤εL(x′)x^* = arg\\min_{d(x^0,x&#x27;) \\le ε} L(x&#x27;)\nx∗=argd(x0,x′)≤εmin​L(x′)\n这里有个限制，似乎会感到困惑，因为一般的模型训练中是没有限制的。没关系这个很好处理的，如下所示：\n\n我们将原始的 x0x_0x0​ 作为初始值，用计算梯度，然后更新输入为 xtx^txt ，然后做个判断，看看 xtx^txt 和 x0x^0x0 之间的距离是否大过阈值，如果超过的话就修正一下输入，让输入符合constraint，否则就继续。那怎么修正呢，这个fix函数可以这样做：\n\nfit()：对于所有符合constraint的 xxx 我们返回一个最接近 xtx^txt 的就可以了。\n如上图所示，对于二范数衡量distance，就是在 x0x^0x0 为圆心，ε 为半径的圆内找一个最接近 xtx^txt 的输入替代 xtx^txt ，如果超出了就找圆心和超出的点所在直线与圆的交点替换就可以了；对于无穷范数衡量distance，就是在任意方向上都不可以超出 ε 远的距离，如果超出了就在各个维度上都拽到边界值上就可以了。图中是二维的，实际上就上述小方块来说是12维的，真实的模型通常是上万维的。\n Example-实践\n使用上述的方法，在ResNet-50这个模型上做Targeted-Attack实验，原本是一张Tiger cat的图片，yfalsey^{false}yfalse 定为Star Fish，update 50次参数的攻击效果如下：\n\n效果是针不戳。我们看看两张图片的差异，因为差异太小肉眼难以看见，所以看看x50倍的效果：\n\n上面我们叠加上去的噪声是我们精心构造的，那我们随机加一些噪声会怎么样呢？\n\n我们可以看到，这些噪声都是比较明显的，但是前两张都被比较正确的识别出来了，至少都还是cat🐱，当我们叠加的噪声十分夸张时机器才没办法辨识这个图片。\n What happend？\n总结一下，其实模型很容易就能被attack。但是这件事为什么会发生呢？我们可以这样来解释一下：\n\n我们可以这样解释，你可以想象 x0x^0x0 是在非常高维空间中的一个点，你把这个点随机的移动，你会发现多数情况下在这个点的附近很大一个范围内都是能正确辨识的空间，当你把这个点推到的很远的时候才会让机器识别成类似的事物，当你把它推的非常远时才会被辨识成不相关的事物。但是，有一些神奇维度，在这些神奇的维度上 x0x^0x0 的正确区域只有非常狭窄的一点点，我们只要将 x0x^0x0 推离原值一点点，就会让模型输出产生很大的变化。\n总结这个现象就是在某一些维度上正确域是很狭窄的，只要在这些维度上推离一点点就会让机器判断出错。上述说法只是在现象上解释，出现了什么样的情况，但是没能真正的弄清楚这个现象是怎么出现的，其背后的原理是什么。\n Attack Approaches\n可供参考的方法：\n\n\nFGSM (https://arxiv.org/abs/1412.6572)\n\n\nBasic iterative method (https://arxiv.org/abs/1607.02533)\n\n\nL-BFGS (https://arxiv.org/abs/1312.6199)\n\n\nDeepfool (https://arxiv.org/abs/1511.04599)\n\n\nJSMA (https://arxiv.org/abs/1511.07528)\n\n\nC&amp;W (https://arxiv.org/abs/1608.04644)\n\n\nElastic net attack (https://arxiv.org/abs/1709.04114)\n\n\nSpatially Transformed (https://arxiv.org/abs/1801.02612)\n\n\nOne Pixel Attack (https://arxiv.org/abs/1710.08864)\n\n\n…… only list a few\n\n\n有很多五花八门的不同的攻击方法，但是这些攻击方法往往不同之处就在于使用不同的distance测量方法作为constraint，还有使用不同的优化Loss的方法：\n\nFGSM（Fast Gradient Sign Method）可能不是一个非常好的方法，但是他确实是一个非常简单的方法。他的做法如上图蓝框框所示，我们只要把 x0x^0x0 减去一个 εΔxεΔxεΔx 就算完了。这个 ΔxΔxΔx 就是Loss对 xxx 的偏微分的正负。举例来说，如果 ∂L/∂x1\\partial L/\\partial x_1∂L/∂x1​ 是正的，那我们就在 x0x^0x0 的第一维减掉一个 ε ，反之则加上 ε 。这个方法就像一拳超人一样，只要攻击一次就达成目标。\n其实，多攻击几次在文献中的结果看来效果是更好的，这种攻击叫做 Iterative FGSM，这里不展开。\n我们来直观的解释一下FGSM：\n\n当我们更新输入的时候，我们只要找到梯度的方向，就把输入朝向梯度的反方向更新一个ε，举例来说，在上图二维平面上，如果梯度指向左下角，不管它指向左下角具体哪个角度，我们都会将参数更新到橙色的点。而如果用原来的方法我们就会更新到梯度方向的反方向上，比如点 x1x^1x1 ，很可能是不会超出constraint的方框的。你可以理解成FGSM是在使用一个巨大的learning rate，大到一次update就会直接跳出constraint，我们再使用fit函数将跑出边界的点拉回到边界的角角上。\n White Box v.s. Black Box\n上述的攻击方法，我们都是fix住神经网络的参数去找 x′x&#x27;x′ . 也就是说要想攻击模型，我们必须知道模型的所有参数 θθθ ，此类攻击我们称为白盒攻击。那如果我们不把模型泄露出去，这样就安全了吗？也不会百分百安全，因为我们还可以采取黑盒攻击。😐\n那黑盒攻击怎么做呢？\n Black Box Attack\n现在我们有一个图片辨识模型，完全不知道这个模型的参数和内部结构，但是，我们能拿到这个模型训练使用的corpus（数据集，语料库）。\n\n我们可以通过这个数据集自己训练一个模型，称为proxy network。然后，我们用上述的对抗样本的方法攻击这个代理模型，得到一个trigger（恶意样本），用这个恶意样本去攻击原模型。神奇的是，这种方法往往能够成功。\n\nhttps://arxiv.org/pdf/1611.02770.pdf\n\n我们来看看这个方法的实验结果，引用自上述论文：\n\n值描述的是模型辨识正确的概率，也就是攻击失败的概率。上述五种神经网络的架构是不一样的，但是我们可以看到即使是不同架构的模型攻击成功的概率也是非常高的，而相同的架构的模型攻击成功率则明显是更高的。\n Universal Adversarial Attack\n核心精神是找一个通用的攻击向量，将其叠加到任意样本上都会让模型辨识出错。\n\nhttps://arxiv.org/abs/1610.08401\n\n\n这件事做成以后，你可以想象，只要在做辨识任务的摄像机前面贴一张噪声照片，就可以让所有结果都出错😮。另外，这个通用攻击甚至也可以做上述的黑盒攻击。\n Adversarial Reprogramming\n对抗重编程攻击？不知道如何翻译合适，这个攻击的核心精神是：通过找一些噪声，让机器的行为发生改变，达到重编程实现其他功能的效果。举个栗子：\n\nGamaleldin F. Elsayed*, Ian Goodfellow,* Jascha Sohl-Dickstein, “Adversarial Reprogramming of Neural Networks”, ICLR, 2019\n\n\n本来模型在做图片辨识，我们通过攻击希望这个模型能做数方块这个任务，当我们把图片中间贴上上图中那种方块图，机器就会帮我们数出途中方块的个数，如果有一个方块会输出tench，有两个方块就输出goldfish… 这件事还挺神奇的，因为我们并没有改变机器的任何参数，我们只是用了和前述相同的方法，找了一个噪声图片，然后把要数方块的图贴在噪声上，输入模型就会让模型帮我们数出方块的个数，真实very amazing啊😃。具体方法细节参考引用文章。\n Attack in the Real World\n我们想知道上述的攻击方法是否能应用在现实生活中，上述的所有方法中加入的噪声其实都非常的小，在数字世界中这些噪声会对模型的判别造成很大影响似乎是很合理的，但是在真实的世界中，机器是通过一个小相机看世界的，这样的小噪声通过相机以后可能就没有了。那我们就来看看其他人做的实验：\n视频连接：https://www.youtube.com/watch?v=zQ_uMenoBCk&amp;feature=youtu.be\n从视频中的结果来看似乎在现实世界中攻击是非常可行的。\n视频中只是在打印出来的图片上加噪声，但是在人脸识别任务中，你总不可能打印出人脸再上加噪声吧，有人就提出在眼睛框上加噪声：\n\nhttps://www.cs.cmu.edu/~sbhagava/papers/face-rec-ccs16.pdf\n\n\n左侧图是在整个人脸上加噪声，但是这种方法只能在实验室做，而右侧是让一个女性带上特制的noise眼镜，就会被是被成一个男性。这个work的作者在做演讲的时候是实际做了现场实验的。总之这种攻击在三次元世界中是切实可行的，但是要做到这件事也是要做非常多的努力的，列出几个难点：\n\n攻击者需要找到超越单个图像的扰动。（人是会动的，所以要保证所有角度都能攻击成功）\n极度不同的相邻像素之间的差异不太可能被相机准确捕捉到。（因相机解析度，人物运动等因素，要保证噪声以较大的色块的方式呈现，且不能出现相邻色块的极端差异，才能被相机正确捕捉）\n保证颜色是设备能够辨认的。（受限于设备工艺，尽量避免超出常见颜色阈值的色彩）\n\n另外一个栗子：\n对路边交通标志进行攻击，希望下列所有表示都能被是被成限速15的标志。他们也考虑了物理世界的种种显示，考虑到不能产生非常奇怪的图案引起怀疑，考虑到不同远近和角度，所以这个攻击确实会在未来自动驾驶道路上造成阻碍。\n\n Attack Beyond Image\n攻击也不知出现在图片辨识上，其他领域也是存在的，具体不展开了，只给出reference。\n\nYou can attack audio\n\nhttps://nicholas.carlini.com/code/audio_adversarial_examples/\nhttps://adversarial-attacks.net\n\n\nYou can attack text\n\nhttps://arxiv.org/pdf/1707.07328.pdf\n\n\n\n Defense\n有人觉得这种攻击能够成功，就是因为它对训练数据过拟合了，但这并不完全正确，因为就算你对model作regularization，作dropout，作model的ensemble，你也不能抗住攻击。可能你会觉得作ensemble尤其可能抵御住攻击，你攻击成功一个模型，我还有其他模型，但是这种想法是不行的，因为上面我们看到了attack是可以跨模型的，只要攻击成功ensemble中的一个模型，对其他模型也往往效果显著。\n现在的防御手段主要分成两大类：\n\nPassive defense：在不修改model的情况下，识别出被污染的输入。类似于在model外面加一层护盾\nProactive defense：在训练模型时考虑对抗攻击的防御，让模型变得robust起来\n\n Passive Defense\n\n可参考：https://evademl.org/\n\n怎么做被动攻击呢？我们来举个几个栗子：\n Filter\n\n我们在模型前加一个Filter，让有噪声的数据输入到Filter中做一个变换（比如，Smoothing），以尽可能消除噪声的影响。一张被加了对抗噪声的猫🐱片被识别为键盘，我们希望通过filter的处理以后能够减轻noise的伤害，而其他的正常图片经过filter处理后对辨识结果的影响尽可能小。\n那什么样的filter可以做到这件事呢，其实不需要太复杂的filter就可以做到这件事，比如作平滑化smoothing。来看看实验结果：\n\n为什么这件事会成功抵御攻击呢，我们可以想象攻击噪声只有在某些特殊的维度上才能造成显著的攻击效果，我们现在一旦加上filter，把那些特殊维度的值改变了，攻击效果就显著下降了，而这样的行为对正常的样本影响并不大。\n基于这样的思想我们再看一种方法，特征压缩。\n Feature Squeeze\n\nhttps://evademl.org/docs/featuresqueezing.pdf\nhttps://github.com/uvasrg/FeatureSqueezing\n\n\n核心思想是对输入样本的特征通过不同方式进行压缩简化，输入到model中得出预测结果和原样本的预测结果进行对别，如果差别较大，就判定为样本是恶意的对抗样本。更多细节可以阅读上述论文。\n Randomization at Inference Phase\n\nhttps://arxiv.org/abs/1711.01991\n\n\n如上图所示，我们将土图片做一些小小的缩放，在旁边加上小小的padding，将padded image输入模型，这样的方法再防御上也是效果显著的。\n上述的三个方法都是在模型前面加一层护盾的形式，类似这种方法的弱点就是，当防御方法被泄露的时候，攻击者就能比较容易的攻破护盾。比如，加Filter的方法，攻击者可以将这个filter想象成模型的一个layer，用传统的攻击方法进行攻击就可以了，从而设计出加了filter护盾的模型的恶意对抗样本；又或者说加上random缩放这个方法，如果你random作padding的方法被泄露出去，攻击有可能是成功的，可能可以做到universal的attack，也就是说针对所有的random padding都通用的attack的noise。\n Proactive Denfense\n核心精神：找出漏洞，补漏洞\n过程如下：\n\n我们训练network的方法就是给出训练数据，然后用这个数据集train出你的model。接下来就是对模型进行修补：\n你要再训练T次迭代，每次迭代过程中，要用对每个样本 xnx^nxn 根据攻击算法找出其可以attack的对抗样本，记作 x~n\\widetilde{x}^nxn ，将这些对抗样本加入数据集重训练model。\n这个增加训练数据的方法有点像Data Augmentation。为什么要作T次迭代呢，因为每次我们重训练以后model的参数就变了，就可能产生新的漏洞所以我们要不断的迭代多次，尽可能修补所有漏洞，实际上是不可能修补所有漏洞的。\n还有一件事，我们要注意在这个过程中找漏洞的algorithm，比如我们使用不同的算法，最后得到的是抗不同算法攻击的model，所以或许我么需要用不同算法，做多次这种防御攻击的重训练。\n To Learn More\n\nReference\n\nhttps://adversarial-ml-tutorial.org/ (Zico Kolter and Aleksander Madry)\n\n\nAdversarial Attack Toolbox:\n\nhttps://github.com/bethgelab/foolbox\nhttps://github.com/IBM/adversarial-robustness-toolbox\nhttps://github.com/tensorflow/cleverhans\n\n\n\n★推荐综述：人工智能系统安全与隐私风险\n\n更多的参考：\nAdversarial Attack Reference:\nIntriguing properties of neural networks\nhttps://arxiv.org/pdf/1312.6199\nExplaining and Harnessing Adversarial Examples\nhttps://arxiv.org/abs/1412.6572\nTowards Evaluating the Robustness of Neural Networks\nhttps://arxiv.org/pdf/1608.04644.pdf\nADVERSARIAL REPROGRAMMING OF NEURAL NETWORKS\nhttps://arxiv.org/pdf/1810.00069.pdf\nAudio Adversarial Examples: Targeted Attacks on Speech-to-Text\nhttps://arxiv.org/pdf/1801.01944.pdf\nTowards Deep Learning Models Resistant to Adversarial Attacks\nhttps://arxiv.org/abs/1706.06083\nObfuscated Gradients Give a False Sense of Security: Circumventing Defenses to Adversarial Examples\nhttps://arxiv.org/pdf/1802.00420.pdf\nOne pixel attack for fooling deep neural networks\n\nSlide :https://media.neurips.cc/Conferences/NIPS2018/Slides/adversarial_ml_slides_parts_1_4.pdf\nTwo hour tutorial: https://www.youtube.com/watch?v=TwP-gKBQyic (good one, with web page)\nGood fellow\nhttps://www.youtube.com/watch?v=CIfsB_EYsVI\nMore videos (I am not sure about the quality)\nhttps://www.youtube.com/watch?v=r8cW1p8VBOU\nhttps://www.youtube.com/watch?v=sh6OS6Lssv4\nTutorial which is too long: http://pralab.diee.unica.it/en/wild-patterns\n科普\nVideo: https://www.youtube.com/watch?v=oZYgaD004Dw\nVideo: https://www.youtube.com/watch?v=SA4YEAWVpbk\n\nhttps://zhuanlan.zhihu.com/p/37922148\nSlide :https://media.neurips.cc/Conferences/NIPS2018/Slides/adversarial_ml_slides_parts_1_4.pdf\nTwo hour tutorial: https://www.youtube.com/watch?v=TwP-gKBQyic (good one, with web page)\nGood fellow\nhttps://www.youtube.com/watch?v=CIfsB_EYsVI\nMore videos (I am not sure about the quality)\nhttps://www.youtube.com/watch?v=r8cW1p8VBOU\nhttps://www.youtube.com/watch?v=sh6OS6Lssv4\nTutorial which is too long: http://pralab.diee.unica.it/en/wild-patterns\n科普\nVideo: https://www.youtube.com/watch?v=oZYgaD004Dw\nVideo: https://www.youtube.com/watch?v=SA4YEAWVpbk\n\n","categories":["李宏毅机器学习笔记"],"tags":["ML","note","Next-Step-of-ML","Adversarial-Attack"]},{"title":"The Next Step for Machine Learning","url":"/The-Next-Step-for-Machine-Learning/","content":" The Next Step for Machine Learning\n现在我们要讲一些新的技术，这些技术目前（2019年）被看作是机器学习的下一步，也就是说要让机器学习的技术真正落地到现实生活还需要哪些技术，还有哪些难题和障碍是需要我们解决的。\n 0x1 机器能不能知道“我不知道” Anomaly Detection\n这个问题听起来有点哲学哦，为什么要研究这个问题呢，举个栗子：你可能训练一个识别动物的model放到网络上供大家使用，但是你以为网友只会给你上传动物的图片吗，搞不好他就给你传个凉宫春日的图片😂：\n\n这种情况下，机器需要有能力说出：这个是我不知道的东西。机器会回答这是我不知道的，这样的技术就是Anomaly Detection。\n 0x2 说出为什么“我知道” Explanation ML\n今天我们看到各式各样机器学习非常强大的力量，感觉机器好像非常的聪明。但这有可能是这样一个故事，过去有一只马叫做汉斯，它非常的聪明，聪明到甚至可以做数学。举例来说：你跟它讲 9\\sqrt{9}9​ 是多少，它就会敲它的马蹄三次，大家欢呼道，这是一只会算数学的马。可以解决根号的问题，大家都觉得非常的惊叹。后面就有人怀疑说：难道汉斯真的这么聪明吗？在没有任何的观众的情况下，让汉斯自己去解决一个数学都是题目，这时候它就会一直踏它的马蹄，一直的不停。这是为什么呢？因为它之前学会了观察旁观人的反应，它知道什么时候该停下来。它可能不知道自己在干什么，它也不知道数学是什么，但是踏对了正确的题目就有萝卜吃，它只是看了旁边人的反应得到了正确的答案。\n今天我们看到种种机器学习的成果，难道机器真的有那么的聪明吗？会不会它会汉斯一样用了奇怪的方法来得到答案的。\n\n这件事其实是有可能发生的，举例来说，有人做了一个马的辨识器，两个model的辨识率都很高。然后分析，机器是根据什么来标识马的。\n\nhttp://iphome.hhi.de/samek/pdf/MonDSP18.pdf\n\n\n\n第一个DNN模型是看到图上黄红部分正常分析出有马，看上去挺正常的；第二个FV模型是看到下面红色部分，辨识出有马，它只是看到左下角的英文，标识出马，并没有学到马真正的样子，其实这些图片都是来自一个网站都有相同的水印。\n我们不知道AI有没有那么聪明，我们需要一些技术，让AI不只是做出决定，还要让它告诉我们说它为什么做出这样的决定。\n 0x3 机器的错觉\n我们知道说，人是有错觉的，比如下面两个圈圈，你觉得哪个圈圈颜色比较深？\n\n你可能会觉得左边比较深，但你大概猜到我在搞你，会说右边更深。\n但是其实：\n\n确实左边的圈圈颜色比较深，这是一个计中计🤣\n机器跟人一样，也很容易被骗，我们可以加一些噪声，让机器本来以为是的后来判断为不是。如本来判断出来是熊喵，加了噪声，就判断错误了。这种就叫做 Adversarial Attack。你甚至可以想象相同的技术应用在自动驾驶领域，有人在交通标识牌上贴一个贴纸，你的自动驾驶汽车就加速撞车，这显然是不可接受的，所以我们要想办法防御这种攻击，具体的后面Attack and Defense一节中会讲到。\n\n 0x4 终身学习 Life-long Learning\n我们希望机器能终生学习。人就是终生学习的，上学期修了线性代数，这学期学机器学习，学好线性代数，机器学习学得更容易。机器能不能跟人一样也做终生学习呢？现在我们一般只让一个模型学习一个任务，比如Alpha Go就只学习下围棋，Alpha star就是玩星际2，它们并不是同一个模型。\n\n今天我们只让一个模型学习一个人任务，显然会存在如下问题\n\n模型的数量无限增长\n之前学到的技能对之后的学习没有帮助\n\n为什么我们今天不让机器去终生学习呢？比如我们先让机器学下围棋，然后再让它学玩星际2，实际上，当机器学完星际2之后它就不会下围棋了。这个现象叫做Catastrophic Forgetting（灾难性忘记？）。如果想让机器做终身学习，还尚在解决的问题。\n 0x5 学习如何学习 Meta-learning (Learn to Learn)\n以前我们设计一个机器学习算法，让机器能够学习；现在，我们能不能写出一个算法实现一个模型，让它能自己设计算法编写模型程序实现具有学习能力的模型。\n\n 0x6 一定要有很多训练数据吗\n在现实生活中有一些任务或者某些情境下只能获取少量的样本，甚至没有样本，比如受限于资金限制等情况，这时候我们要么把机器学习厚重的教科书砸到boss脸上辞职，要么试试下面的方法：\n\nFew-shot learning 让机器看少量的资料\nZero-shot learning 不给机器任何资料，只告诉机器物品的特征描述，然后机器根据描述进行判断\n\n 0x7 增强学习 Reinforcement Learning\nReinforcement Learning 真的有这么强吗？当你用Reinforcement Learning 去玩一些的游戏，Reinforcement Learning也许确实可以跟人做到差不多，但是需要很长时间才能达到，如下图机器需要900多个小时才能达到人类2个小时能达到的效果。机器感觉就是一个天资不佳却勤奋不解的笨小孩，他需要非常久的时间非常多的练习才能和人达到相同的水平，那Reinforcement Learning为什么学得这么慢，有没有办法让它快一点，就是我们要考虑的问题。\n\n\n图片来自：http://web.stanford.edu/class/psych209/Readings/LakeEtAlBBS.pdf\n\n 0x8 神经网络压缩 Network Compression\n如果我们要把机器学习的模型应用到现实生活中，由于设备的运算和存储能力有限，带来的问题就是我们能不能把Network 的架构缩小，但让它维持同样的能力。\n\n\n把一个大的神经网络缩小，减掉多余的神经元\n\n\n\n参数二值化\n都变成“+1”和“-1”。如果是连续数值，就需要大量的运算和内存，如果把所有参数进行二值化，那么运算起来就快，内存也占用少。\n\n\n 0x9 机器学习的谎言\n今天我们在训练的时候，假设训练和测试的数据分布是一样的或者至少非常相似，但是实际上在真实应用里面，这就是一个谎言。\n\n如果我们做手写数字辨识，在实验室情况下，训练资料和测试资料非常相似，可能会很容易达到99.5%。但是实际中，可能图片有背景，正确率变成57.5%，直接烂掉了。\n\n那么怎么解决机器在训练资料和测试资料不同的场景呢？现有可以参考的技术有Unsupervised Domain Adaptation，不在展开。\n","categories":["李宏毅机器学习笔记"],"tags":["ML","note","Next-Step-of-ML"]}]