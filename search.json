[{"title":"Note《Automatic Hot Patch Generation for Android Kernels》","url":"https://ch3nye.top/Note-Automatic-Hot-Patch-Generation-for-Android-Kernels/","content":" Automatic Hot Patch Generation for Android Kernels\n 主要解决什么问题\nAndroid 版本升级覆盖缓慢，往往发布半年的最新Android系统设备覆盖率只有个位数，如此便使得旧版本的Android系统的内核漏洞无法修复，而OEM厂商又无意修补这些漏洞，因为内核的改动需要繁琐的测试保证更改不会影像现有功能和性能。因此，多数设备上搭载的旧版Android系统会保持漏洞存在很长一段时间。\n为了解决上述问题，已经有很多方案被制定出来，在所有可能的解决方案中，热补丁技术提供了一种在不中断程序正常功能的情况下修复漏洞的便捷方法。由于它可以在不重新启动设备的情况下修补内核漏洞，因此极大地改善了用户体验。\n根据正式发布的补丁程序编写热补丁程序是较难的工作。在二进制文件中，很难准确找到要修改的脆弱点。为了在适当的位置打上热补丁，安全专家需要了解官方补丁的语义并编写相应的热补丁。但是，这是一个耗时并且容易出错的过程。受限于快速的开发周期和有限的安全预算，人工开发热补丁往往是商业公司不可接受的。因此，需要开发一种自动解决方案以将正式补丁正确转换为热补丁。\n为此，本文提出了一种自动执行热补丁生成过程的解决方案。为了对漏洞修补程序有完整的了解，我们首先分析了2012年至2016年的大多数Android CVE，并根据其修补行为对其进行了分类。根据这些分析，我们开发了Vulmet，它可以通过程序分析提取官方补丁的语义来自动生成热补丁。Vulmet将在函数中找到合适的位置，构建并应用语义等效的热补丁来修复漏洞。为了测试Vulmet的有效性，我们为实际的Android CVE生成了热补丁。热补丁程序可以以很少的系统开销来修补漏洞。\n概述结论：\n为此，我们首先研究了从2012年到2016年从373个Android内核CVE中自动生成补丁的可行性（结论为可行）。然后，我们开发了一种自动热补丁生成工具，名为Vulmet，该工具通过从官方补丁中学习来生成语义保留热补丁。 Vulmetis的关键思想是使用最弱的前提推理（weakest precondition reasoning）将官方补丁所做的更改转换为hot patch约束。实验表明，Vulmet可以为55个现实世界的Android kernel CVE生成正确的热补丁。热补丁不影响内核的健壮性，并且性能开销较低。\nPS：\n注意1：本文实现的热补丁生成工具Vulmet，需要看官方补丁，从中学习语义，根据Vulmet被定义的约束条件和操作范围，生成可行的热补丁。（在我看来本文想法稍微有点鸡肋，既然发布了官方补丁，OEM厂商只要跟进就好了呀，只能说热补丁具有不需要重启的优势，这种优势在服务器系统补丁上会更明显吧😐）\n注意2：Vulmet并不能百分百根据官方补丁生成热补丁，只能实现满足下述Requirements和Operation Scopes的情况下，官方补丁自动生成热补丁。\n 使用的方法\n本节叙述Vulmet的实现过程：解释热补丁生成，android漏洞类型分析并定义操作范围，自动热补丁生成框架\n Section 2 Automatic Hot Patch Generation\nProblem Definition（定义热补丁生成）：\n给定一个易受攻击的function F及在F的L位置上打上官方的patch P ，我们希望在二进制代码中找到function F的一个合适位置L’来插入自动生成的热补丁P’，其语义与P相同。\nRequirements：\n\nRequirements 1：生成的热补丁应该保留和官方补丁一致的语义，确保其正确性\nRequirements 2：生成的热修补程序不应破坏系统，确保系统运行的健壮性\nRequirements 3：生成的热补丁应该产生较低的开销，确保系统效率\n\nOperation Scopes：\nOperation Rule 1：补丁只能放在函数的开始或结尾处，或函数调用的开始或结尾处\nOperation Rule 2：补丁可以读取内存的有效内容，但禁止修改\nOperation Rule 3：该补丁只能在一个函数范围内进行很小的更改，以尝试修复漏洞\n原文2.4节介绍了一个用本文定义的规则和操作范围修补CVE-2015-8940整数溢出漏洞的过程，大意就是官方补丁检查了几个变量的值，本文做法是在函数开始处检查上述值，由于这些值并不是一开始就知道的，所以通过一个algorithm（in Section4）自动确定这些变量之间的关系，以此生成了和官方语义相同的检查约束添加到函数开始，热补丁结束。详细过程请参考原文。\n Section 3 Patch Type Analysis\n在研究中，我们提供了漏洞补丁分类和分布情况以及从观察中的发现。本节，我们将讨论Vulmet能够支持的漏洞补丁的类型。\n我们的工作聚焦于热补丁生成，所以补丁分类应该反应补丁功能，而不是反应补丁修复漏洞的类型。目前来说针对后者的分类要原多于前者，我们遵从[36]的分类方案思想，形成我们的分类方案，如下图所示：\n\n3.2 Observation\n在漏洞补丁的研究过程中，我们获得了四个有趣的发现。\nObservation 1: 与程序更新相比，漏洞补丁的更改通常很小\nObservation 2: 大型漏洞补丁通常由几个小型独立补丁组成\nObservation 3: 补丁类型和漏洞类型并不是一定相关的，或者说是比较无关的\nObservation 4: 一些补丁同时包括安全补丁和非安全补丁\n3.3 Vulmet Work Scope\n\nSanity Testing 健壮性检查上面的栗子说过了，符合Requirements和Operation Scopes，Vulmet完全支持。\n对于Function Call类型，Vulmet可以进入被调用函数并该函数。如果此补丁的更改不涉及内存写入操作，则Vulmet可以支持该修补程序。\n对于Change of Variables和Change of Data Types都涉及内存写操作，因此不支持。\nVulmet不支持Redesign类型补丁，因为它极大地改变了原始函数的语义并违反了Operation Scopes 3。\n Section 4 Methodology\n\nPatch Filtering\nVulmet将通过对比存在漏洞的代码和已修补的代码之间的不同来提取官方补丁。然后，针对补丁中的每个语句，将其分为正常操作和禁止操作。禁止的操作包括变量或指针值的分配以及对内存修改函数的调用。如果正式补丁不包含禁止的操作，Vulmet会将其选择为生成热补丁的候选对象。否则，补丁将被过滤掉。\nInsertion Location Optimization\n根据第2.3节的规则1，函数头或尾是patch 可以hook的位置。因此，Vulmet只能hook 漏洞函数或者漏洞函数中调用的函数。这些可以hook的位置都是为可能的补丁插入位置。Vulmetis需要寻找最佳的插入位置。具体的寻找算法看原文吧，不赘述了。\nWeakest Precondition Reasoning\n在选择补丁插入点之后，下一步是通过计算和官方补丁的语义等效性的热补丁。在Vulmet中，此过程被重构为最弱的前提推理任务。在编程中，前提条件是在调用函数之前应为true的语句。而后置条件是一个陈述，如果函数完成并且所有先决条件都满足，则该陈述为真。表6展示了语义计算与最弱前提条件推理之间的关系。给定一个正式补丁，其语义可以转换为一个或多个后置条件。漏洞函数中的陈述将定义解决最弱前提的transformer。获得热补丁约束的过程等同于计算最弱的前提条件。产生的最弱前提条件是官方补丁的语义等价的热补丁。（这就是核心算法，看不懂的话就算了，知道是使用这个算法就可）\nBinary Hot Patch Generation\n最后一步是根据precondition constraints生成热补丁。Vulmet使用空函数作为模板，并将函数的输入参数的数量和类型设置为与原始目标函数相同。然后，将所有约束插入其中，并将功能编译为可以热修补到内核的二进制可执行文件。\n关于热补丁生成的细节，包括变量地址，指令集支持，可执行文件生成等细节问题就不多赘述了。\n 结果评估\n我们对Vulmet生成的热补丁的正确性，鲁棒性和效率进行了评估。\n正确性可以量化补丁修复漏洞的能力，健壮性可以量化补丁维护程序稳定性的能力，效率可以量化补丁引入的开销。\n我们设计了实验来测试这三个方面的补丁有效性。在实验中，所有补丁均在具有Android kernel版本7.1.1 r31 bullhead 构建的Android Open SourceProject（AOSP）平台Google Nexus 5X上进行了测试。\n5.1 Correctness Evaluation\n在本节中，我们评估生成的热补丁的正确性。实验包括三个部分。首先，我们使用真实的CVE漏洞测试补丁。其次，对于无法利用的漏洞，我们手动验证补丁的正确性。第三，我们手动编写热补丁，并将生成的热补丁与它们进行比较，以检查生成的补丁是否以与人类专家相同的方式解决了漏洞。\n\n手工收集了四个CVE攻击方法，用来攻击被Vulmet打过补丁的系统。结果表明，所有补丁均已成功阻止了来自攻击的攻击。对于CVE-2018-17182，该热补丁可以成功阻止漏洞利用，但无法阻止系统崩溃。这是因为该修补程序只能部分修复漏洞。\n\n人工验证59个自动生成的热补丁有55个是正常工作的，沃觉得算是相当不错了。\n\n与人类安全专家相比55个自动生成的热补丁只有一个语义不同，具体哪里不同参考原文。\n5.2 Robustness Evaluation\n为了构建测试环境，我们选择要使用Linux内核版本3.10构建的Android bullhead，然后回滚以生成具有许多未修补漏洞的内核。在此特定内核中，Vulmet管理21个漏洞补丁，可以将其转换为热补丁。\n然后我们将这些补丁应用到内核并运行安兔兔Benchmark和CF-bench，监控系统异常，比如崩溃和挂起。表11总结了该实验的结果。出于演示目的，我们选择5个CVE作为示例，并列出所有21个补丁的最终结果\n\n结果表明，所有热补丁都不会使程序崩溃或挂起。为了进一步检查现实情况中补丁的健壮性，我们从Google App Store中选择并安装了前100个Android应用程序。我们使用脚本来打开，加载和关闭已修补系统上的应用程序，并监视异常行为。结果是可以正确执行所有应用程序，这表明这些修补程序在实际情况下保持了良好的鲁棒性。总而言之，生成的补丁不会破坏补丁程序的正常功能\n5.3 Efficiency Evaluation\n我们在Google Nexus 5X设备上使用安兔兔基准测试修补前后的系统性能。我们将实验设置控制为相同，以便一次测试一个热补丁。每个实验重复10次，取平均分以避免噪声引起的变化。表12列出了具有5个单独CVE补丁的内核的性能以及应用所有21个补丁的总体性能。\n\n所有结果都在合理范围内，与原始内核结果相比，该范围略高或略低。因此，补丁使系统的开销较低。\n 缺点和不足（选）\n\nVulmet的前提之一是自动生成的热补丁无法修改内存和原始程序的内容。这就使得诸如变量类型修改，变量数值修改，函数过程重构等补丁类型无法自动生成。\nVulmet依靠官方补丁语义的精确总结来生成正确的热补丁。在实验中，由于Vulmet无法完全提取语义，因此某些生成的热补丁不完整。\n有些补丁过于复杂而无法分析。很难找到大补丁的精确语义。当前Vulmet只能处理，仅对一个函数过程进行修补的补丁。\n\n 对我有什么启发\nHot Patch 技术是我没有接触过的，主要借助此篇文章了解内核补丁技术、热补丁技术，希望能更加深入的了解内核级补丁修补和应用原理。\n 这项研究的未来方向\n可能会和人工智能结合的比较紧密吧，就如缺点2和3所述，补丁的生成依赖语义分析，如果语义分析器能借助人工智能技术的发展，强大到根据漏洞类型自动分析出某段代码的修补方案，这样的话，没有漏洞的世界就完成了。😃\n论文链接：https://www.usenix.org/conference/usenixsecurity20/presentation/xu\n参考文章：应用程序热补丁（一）：几行代码构造免重启修复补丁\n","categories":["论文阅读"],"tags":["Android","论文","笔记","Hot-Patch"]},{"title":"Note《FANS Fuzzing Android Native System Services via Automated Interface Analysis》","url":"https://ch3nye.top/Note-FANS-Fuzzing-Android-Native-System-Services/","content":" FANS: Fuzzing Android Native System Services via Automated Interface Analysis\n 主要解决什么问题\n将Fuzzing技术应用到Android Native system services 面临的问题有：\n\nandroid native system services 通过特殊的程序间通信机制调用，即名为binder的机制，它通过服务特定的接口调用服务。因此Fuzzer 需要辨识所有接口，自动化地生成特定接口的测试用例\n有效的测试用例应该满足所有接口的接口模型\n测试用例也应该满足语义要求，包括变量依赖性和接口依赖性\n\nAndroid 系统中system services 要注册到 Service Manger。用户app 通过查询manger 获得访问目标services 的接口（封装在代理绑定器对象中 Proxy Binder object），然后通过形如IBinder::transact(code,data,reply,flags)的统一远程过程调用(RPC)接口，调用该接口提供的不同事务。code决定要调用的事务，data是事务的输入的序列化形式。因此，我们可以利用这个统一的IPC方法来测试所有的系统服务。为了彻底测试目标服务，我们可以首先找到所有接口和可用的事务，然后用满足该服务特定的格式和语义要求的输入数据来调用它们。具体而言，有三个挑战需要应对：\n\n多级接口识别\n接口模型提取\n生成语义正确的输入\n\npropose a generation-based fuzzing solution FANS to address the aforementioned challenges.\n在https://github.com/iromise/fans.开源了FANS的原型\n总结本文做出的贡献：\n\n我们系统地研究了Android native system services 中接口之间的依赖关系，挖掘了更深的多层接口。\n提出了一种自动提取输入接口模型和语义的解决方案。这种方法也可以应用于其他基于接口的程序。\n我们提出了一个解决方案，通过在序列化和反序列化pair中利用 变量名和类型知识 来推断事务间的依赖关系。\n我们实现了一个FANS原型来系统地模糊Android本地系统服务，并发现了30个独特的本地漏洞和138个独特的Java异常\n\n 使用的方法\n\n上图展示了我们的解决方案FANS的设计概述。首先，接口收集器(第3.3节)收集目标服务中的所有接口，包括顶级接口和多级接口。然后接口模型提取器(第3.4节)为这些接口中的每个候选事务提取输入和输出格式以及变量语义，即变量名和类型。提取器还收集与变量相关的结构、枚举和类型别名的定义。接下来，依赖推断器(第3.5节)推断接口依赖，以及事务内和事务间变量依赖。最后，基于上述信息，模糊引擎(第3.6节)随机生成事务，并调用相应的接口来模糊本地系统服务。fuzzer引擎还有一个管理器，负责同步主机和被测手机之间的数据。\n 3.3Interface Collector\n顶层或多层接口都有调度事务的onTransact方法。因此，我们可以利用这个特性来识别接口。不过，我们并不直接扫描AOSP代码库中的C/C文件来获取onTransact方法。相反，我们检查在AOSP编译命令中作为源出现的每个C/C文件，以便我们可以收集在编译期间由AIDL工具动态生成的接口，否则这些接口将被忽略。\n 3.4Interface Model Extractor\nDesign Choices\nRPC-centric testing：以RPC（remote procedure call）为中心的测试，攻击者必须通过IPC接口与目标service交互，并且只能产生有限数量的事件。为了减少误报，选择通过RPC接口测试目标服务。\nGeneration-based fuzzing：基于生成的fuzzing比mutation-based fuzzing 有更高的代码覆盖率，更好的语义，更少的漏报等优点。\nLearn input model from code：大意就是：基于生成的模糊测试器依赖于输入模型知识来生成有效的测试用例。FANS是通过分析Android 源码自动化构建输入模型。\nInterface Model Extractor\n\n从服务端代码中提取接口\n先将AIDL文件转为C++文件，然后使用AST提取接口模型\n\nTransaction Code Identification\n通过分析AST中的switch-case的case节点，就可以轻松的分析出接口的所有事务，并识别出相关的常量事务代码（constant transaction code）\nInput and Output Variable Extraction\n识别出事务代码以后，需要从向每个事务传递的参数data中提取输入的反序列化。此外，由于我们想推断事务的内部依赖，我们也需要提取事务的输出，输入是被序列化到reply的数据\n事务中使用的变量有三种可能的类别：\n\nSequential Variables 这种类型的变量没有任何前提条件。\nConditional Variables 这种类型的变量取决于一些条件。如果不满足这些条件，变量可能为空，或者不出现在数据中，甚至与满足条件时的类型不同\nLoop Variables 这种类型的变量在循环甚至嵌套循环中被反序列化\n\n这三类变量恰好对应程序中的三类语句，即顺序语句、条件语句和循环语句。因此，我们将主要在AST中处理这类语句。\nType Definition Extraction\n除了提取事务中的输入和输出变量，我们还提取类型定义。它有助于丰富变量语义，以便生成更好的输入。有三种类型需要分析:\n\nStructure-like Definition 这种类型包括联合和结构\nEnumeration Definition 提取所有给定的(常量)枚举值\nType Alias\n\n 3.5Dependency Inferer\n在提取接口模型后，我们推断出两种依赖关系:\n(1)接口依赖。即如何识别和生成多级接口。它还暗示了一个接口如何被其他接口使用。\n(2)变量依赖。事务中的变量之间存在依赖关系。以往的研究很少考虑这些依赖性。\n具体的依赖识别方法就不赘述了\n 3.6Fuzzer Engine\n首先，fuzzer管理器会将fuzzer程序的二进制文件、接口模型和依赖同步到手机上，并在手机上启动fuzzer。然后Fuzzer将生成一个测试用例，即一个事务及其相应的接口来模糊测试远程代码。同时，fuzzer管理器将定期同步手机上的崩溃日志。\n 结果评估\nExperimental Setup As shown in Fig Overview-of-FANS, we implement the first three components on Ubuntu 18.04 with i9-9900K CPU, 32 GB memory, 2.5 T SSD. As for test devices, we use the following Google’s Pixel series products: Pixel * 1, Pixel 2XL * 4, and Pixel 3XL * 1. We flash systems of these smartphones with AOSP build number PQ3A.190801.002, i.e., android-9.0.0_r46, which is a recent version supporting these devices when writing this paper. Although the Android release versions are the same, the source code can be slightly different for different Pixel models. For the following two sections (Section 5.1, Section 5.2), we report the experiment results carried out on Pixel 2XL.\nHow many interfaces have been found? What is the relationship between them? (Section 5.1)\n\n\nWhat does the extracted interface model look like? Is the model complete and precise? (Section 5.2)\n\n虽然不能做到完全精确和完整，但是足够好了\nHow effective is FANS in discovering vulnerabilities of Android native system services? (Section 5.3)\n为了评估FANS的有效性，我们在六部智能手机上断断续续运行了大约30天。我们已经从FANS报告的成千上万的崩溃中发现了30个独特的错误，下表列出了所有30个漏洞。除了在Android原生系统服务中发现的22个漏洞之外，在作为Android原生系统服务中的公共库的libcutils.so、libutils.so和libgui.so库中还有5个漏洞。此外，我们在Linux系统组件中发现了三个漏洞。例如，我们在iptables-restore中发现了一个堆栈溢出。这个程序是firewallconfiguration提供的用户空间程序。这些漏洞证明了FANS生成的输入可以在复杂的约束下驱动控制流进入深层路径。\n\n此外，虽然我们的目标是发现用C++实现的Android原生系统服务中的漏洞，但我们触发了138个Java异常，如FileNotFoundException、DateTimeException、NoSuchElementException和NullPointerException。这可以归因于Java应用程序有时依赖Android原生系统服务的事实。一些本地服务也调用Java方法。由于健壮性和稳定性对于Android原生系统服务很重要，所以这些Java异常应该不会发生。应该实施更严格的检查来解决这个问题。\n我们已经向谷歌报告了所有本地漏洞。其中20个被确认，18个被给予安卓身份证，其中三个与未披露的漏洞报告重复。到目前为止，谷歌已经给安卓ID 143895055和143899228分配了中等严重度。谷歌还将CVE-2019-2088分配给安卓标识143895055，并将在未来将我们放入他们的确认页面。正在提交Java异常。\n 对我有什么启发\n此前没了解过Fuzzing技术，借此文章进行学习。同时，也尝试从不同角度了解android 系统原理。文中提到的Native Services， Services Manager 等技术可以参考文末链接。\n论文链接：https://www.usenix.org/conference/usenixsecurity20/presentation/liu\n参考文章：\nService与Android系统设计（6）— Native Service https://blog.csdn.net/21cnbao/article/details/8087328\nService与Android系统设计（4）-- ServiceManager https://blog.csdn.net/21cnbao/article/details/8087304\nService与Android系统设计（2）-- Parcel https://blog.csdn.net/21cnbao/article/details/8086619\nAndroid系统服务(SystemService)简介 https://blog.csdn.net/geyunfei_/article/details/78851024\n","categories":["论文阅读"],"tags":["Android","论文","笔记","Fuzzing","System-Service"]},{"title":"ELMO, BERT, GPT","url":"https://ch3nye.top/ELMO-BERT-GPT/","content":"\n ELMO, BERT, GPT\n这节课要讲的是关于如何让机器看懂人类的文字，也就是自然语言模型。今天要将的是到今天为止（2019年6月），这个方向上最较新的模型包括BERT 还有和BERT 很相似的也很知名的模型ELMO 和GPT 。\n Representation of Word\n先回顾一下，在计算机中怎么表示一个词。\n\n1-of-N Encoding\n最早的词表示方法。它就是one-hot encoding 没什么好说的，就是说如果词典中有N个词，就用N维向量表示每个词，向量中只有一个位置是1，其余位置都是0.\n但是这样的方式就失去的词意，比如说dog cat bird 都是动物，这些词的表示在数学上要是接近的。\nWord Class\n根据词的类型划分，但是这种方法还是太粗糙了，比如说dog cat 都是哺乳动物，bird是鸟类应该再细分，而dog cat 又不是同一科又可以再分，且cat 和car都是四个腿或许这两个词应该近一些。于是就有了Word Embedding\nWord Embedding\n有点像是soft 的word class，把词嵌入到高维空间中，具有相同属性，或者相似的词之间，距离近一些。Word Embedding的技术过去讲过，参考：https://www.youtube.com/watch?v=X7PH3NuYW0Q\n接下来进入没讲过的知识\n 一词多义\n\n\nhttps://arxiv.org/abs/1902.06006\n\n如上图所示，上面五个bank有三种意思。也就是说同一个词往往在不同的上下文中具有不一样的意思，就是说我们希望一个词可以有多个embedding 。以前要处理这件事，我们是让机器去查词典，看这个词有几种解释就给它设置几个embedding，然后通过语料库train 出这些向量。但是不同的词典对同一个词有不同的解释，比如说上图第五个blood bank（血库），有的词典认为这个bank就是银行的意思，有的词典认为这是区别于银行的第三种意思。事实上，有很多这种微妙的词汇，让人难以判断它应该设置几个embedding 。\n Contextualized Word Embedding\n\n我们希望机器能做到：\n\n\n每个token（不同上下文中的词）都有一个embedding （无论它们在词典解释中是否属于同一类）\n\n\nword token 的embedding 依赖于上下文\n\n\n如上图所示，三个bank是不同的token，它们的上下文是不一样的，未来将有不同的embedding （包括灰色的方块，每个token都会输出一个embedding）。这个技术叫做Contextualized Word Embedding 。\n那怎么做到Contextualized Word Embedding 呢，有一个实现的技术叫做：Embeddings from Language Model (ELMO)\n Embeddings from Language Model (ELMO)\n\n\nhttps://arxiv.org/abs/1802.05365\n\nELMO是一个RNN-based Language Model，训练的方法就是找一大堆的句子，也不需要做标注，然后做上图所示的训练，RNN忘记了的话去看以前的讲解吧。\nRNN-based Language Model 的训练过程就是不断学习预测下一个单词是什么。举例来说，你要训练模型输出“潮水退了就知道谁没穿裤子”，你教model，如果看到一个开始符号&lt;BOS&gt; ，就输出潮水，再给它潮水，就输出退了，再给它退了，就输出就…学完以后你就有Contextualized Word Embedding ，我们可以把RNN 的hidden layer 拿出来作为Embedding 。\n为什么说这个hidden layer 做Embedding 就是Contextualized 呢，因为RNN中每个输出都是结合前面所有的输入做出的。\n你可能会疑惑上图不是只看了单词的前文吗，怎么说是上下文呢？\n事实上，我们是如上图所示做了正反双向的训练，最终的word embedding 是把正向的RNN 得到的token embedding 和反向RNN 得到的token embedding 接起来作为最终的Contextualized Word Embedding 。\n现在出现了新的问题，通常来说RNN都是Deep的，如下图所示：\n\n中间的word embedding通常是多个，怎么解决呢？ELMO的解决方法很简单就是：我全部都要\n\n\n如上图所示，ELMO会将每个词输出多个embedding，这里我们假设LSTM叠两层。ELMO会用做weighted sum，weight是根据你做的下游任务训练出来的，下游任务就是说你用EMLO做SRL（Sematic Role Labeling 语义角色标注）、Coref（Coreference resolution 共指解析）、SNLI（Stanford Natural Language Inference 自然语言推理）、SQuAD（Stanford Question Answering Dataset） 还是SST-5（5分类情感分析数据集）。\n具体来说，你要先train好EMLO，得到每个token 对应的多个embedding，然后决定你要做什么task，然后在下游task 的model 中学习weight 的值。\n原始ELMO的paper 中给出了上图实验结果，Token是说没有做Contextualized Embedding之前的原始向量，LSTM-1、LSTM-2是EMLO的两层得到的embedding，然后根据下游5个task 学出来的weight 的比重情况。我们可以看出Coref 和SQuAD 这两个任务比较看重LSTM-1抽出的embedding，而其他task 都比较平均的看了三个输入。\n Bidirectional Encoder Representations from Transformers (BERT)\nBERT = Encoder of Transformer\n我们在transformer 一节中讲过Transformer 了，BERT 也有提及。\n如果后面讲的东西你没有听的很懂的话，你就记住BERT的用法就是吃一个句子，输出句中每个词的embedding 。\n\n这里老师做了一个提醒，由于中文的词太多，所以用词作为sequence的基本单位，可能导致输入向量过长（one-hot），所以也许使用字作为基本单位更好。\n Training of BERT\npaper上提及的BERT的训练方法有两种，Masked LM 和Next Sentence Prediction 。\nMasked LM\n\n基本思路：盖住输入的句子中15%的词，让BERT把词填回来。具体方法是，挖空的位置用符号[MASK]替换，然后输入到BERT，BERT给出输出向量，将挖空地方的输出向量，输入到一个简单的线性模型中，这个模型会分辨这个向量是那个词。所以说，BERT必须好好做embedding ，只有BERT抽出的向量能很好地表示被挖空的词，这个简单的线性模型才能找到到底是哪个词被Masked。\nNext Sentence Prediction\n\n基本思路：给BERT两个句子，然后判断这两个句子是不是应该接在一起。具体做法是，[SEP]符号告诉BERT交接的地方在哪里，[CLS]这个符号通常放在句子开头，它通过BERT得到的embedding 输入到简单的线性分类器中，分类器判断当前这个两个句子是不是应该接在一起。\n你可能会疑惑[CLS]难道不应该放在句末，让BERT看完整个句子再做判断吗？\n你仔细想想看，BERT里面一般用的是Transformer的Encoder，也就是说它做的是self-attention，我们之前说self-attention layer 不受位置的影响，它会看完整个句子，所以一个token放在句子的开头或者结尾是没有差别的。\n最后提一下上述两个方法中，Linear classifier 是和BERT一起训练的。上述两个方法在文献上是同时使用的，让BERT的输出去解这两个任务的时候会得到最好的训练效果。\n How to use BERT\n你当然可以把BERT当作一个抽embedding 的工具，抽出embedding 以后去做别的task，但是在文献中并不是这样，文献中是把BERT和down stream task 一起做训练😮。举了四个栗子：\n\n如上图，输入一个sentence 输出一个class ，有代表性的任务有情感分析，文章分类等。\n具体做法，以句子情感分析为例，你找一堆带有情感标签的句子，丢给BERT（BERT有trained model），再句子开头设一个判断情感的符号[CLS]，把这个符号通过BERT的输出丢给一个线性分类器做情感分类。线性分类器是随机初始化参数用你的训练资料train 的，这个过程你也可以堆BRET进行fine-tune ，或者就fix 住BRET的参数，都行。\n\n如上图，输入一个句子，输出每个单词的类别。有代表性的栗子，Slot filling。\n和上一个栗子类似，这里你就去train 每个线性分类器就好了，BERT也可以去fine-tune 。\n\n如上图，输入两个句子，输出自然语言推理的结果。举栗来说，给出一个前提一个假设，机器根据这个前提判断这个假设是T/F/unknown.\n做法如上图所示，你在开头设一个提问的符号，在把两个句子用[SEP]接起来，上面接一个线性分类器（3分类），同上的训练方法。\n\n那BERT解QA问题，而且明确是Extraction-based Question Answering ，这种QA问题是说，文中一定能找到问题的答案。\n解这种QA问题的model 如上图所示，模型输入Document D 有N个单词，Query Q有M个单词，模型输出答案在文中的起始位置和结束位置：s和e。举例来说，上图中第一个问题的答案是gravity，是Document 中第17个单词；第三个问题的答案是within a cloud，是Document 中第77到第79个单词。\n怎么用BERT解这个问题呢？\n\n输入如上图所示，Document 中每个词都会有一个向量表示，然后你再去learn 两个向量，上图红色和蓝色条，这两个向量的维度和BERT的输出向量相同，红色向量和Document 中的词汇做点积得到一堆数值，把这些数值做softmax 最大值的位置就是s，同样的蓝色的向量做相同的运算，得到e：\n\n如果e落在s的前面，有可能就是无法回答的问题。\n这个训练方法，你需要label很多data，每个问题的答案都需要给出在文中的位置。两个向量是白手起家learn出来的，BERT可能需要fine-tune 。\n BERT 屠榜\n现在BERT在NLP任务中几乎是屠榜的，几乎每个NLP任务都已经被BERT洗过了。\n\n Enhanced Representation through Knowledge Integration (ERNIE)\n\n\nhttps://arxiv.org/abs/1904.09223\nhttps://zhuanlan.zhihu.com/p/59436589\n\nERNIE是转为中文设计的NLP预训练模型，by 百度。细节可以看一下文章。\n What does BERT learn?\n\n\nhttps://arxiv.org/abs/1905.05950\nhttps://openreview.net/pdf?id=SJzSgnRcKX\n\n思考一下BERT每一层都在做什么，列出上述两个reference 给大家做参考。假如说我们的BERT有24层，down stream task有POS、Consts等等，我们来分析一下上述实验结果。这个实验把BERT的每一层的Contextualized Embedding 抽出来做weighted sum，就和EMLO的做法一样，ELMO就是把每层的Contextualized Embedding 抽出来做weighted sum，然后通过下游任务learn 出weight。\n这里也一样，假设我们是单纯用BERT做embedding ，用得到的词向量做下游任务。那BERT有24层，所以每个词都会抽出24个vector ，把这些vector 做weighted sum，weight 是根据下游任务learn出来的，看最后learn出的weight 的情况，就可以知道这个任务更需要那些层的vector 。\n上图中右侧蓝色的多个柱状图，代表通过不同任务learn 出的BERT各层的weight ，POS是做词性标注任务，会更依赖11-14层，Coref是做分析代词指代，会更依赖BERT高层的向量，而SRL语义角色标注就比较平均地依赖各层抽出的信息。\n Multilingual BERT\n\n\nhttps://arxiv.org/abs/1904.09077\n\nMultilingual BERT 用104种语言去训练。google 到wiki百科上爬了104种语言的百科给BERT学习，虽然BERT没看过这些语言之间的翻译，但是它看过104种语言的百科以后，它似乎自动学会了不同语言之间的对应关系。\n所以，如果你现在要用这个预训练好的BERT去做文章分类，你只要给他英文文章分类的label data set，它学完之后，竟然可以直接去做中文文章的分类 amazing😮。更多细节你可以参考上述reference\n Generative Pre-Training (GPT)\n\n\nhttps://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf\nSource of image: https://huaban.com/pins/1714071707/\n\n从上图你就可以看出，GPT非常巨大，其实它的卖点就是大。\nGPT目前有三个版本，GPT就是最开始版本，它的大小是平平无奇的，只是提出了一种NLP预训练模型架构，GPT-2是GPT的加大版本，GPT-3是上一个版本的加大版本。也就是中杯、大杯和超大杯。尤其是GPT-3在2020年引起了较大轰动，有人说它能为人工智能的热潮续命5年，关于GPT-3的文章已经有很多了，感兴趣可以自行查阅，本节主要探究GPT的架构。\n为什么用独角兽的形象，后面会解释。\n我们上面说BERT是Transformer 的Encoder ，GPT其实是Transformer 的Decoder 。\n\nGPT和一般的Language Model 做的事情一样，就是你给他一些词汇，它预测接下来的词汇。举例来说，如上图所示，把“潮水的”q拿出来做self-attention，然后做softmax 产生 α^\\hat{\\alpha}α^ ，再分别和v做相乘求和得到b，self-attention 可以有很多层（b是vector，上面还可以再接self-attention layer），通过很多层以后要预测“退了”这个词汇。\n\n预测出“退了”以后，把“退了”拿下来，做同样的计算，预测“就”这个词汇，如此往复。\n Result of GPT\n\nGPT-2是一个巨大的预训练模型，它可以在没有更多训练资料的情况下做以下任务：\n\nReading Comprehension\n\nBERT也可以做Reading Comprehension，但是BERT需要新的训练资料train 线性分类器，对BERT本身进行微调。而GPT可以在没有训练资料的情况下做这个任务。\n如上图所示，你就给GPT-2一段文章，给出一个问题，再写一个A:，他就会尝试做出回答。右侧是GPT-2在CoQA上的结果，最大的GPT-2可以和DrQA达到相同的效果，不要忘了GPT-2在这个任务上是zero-shot learning ，从来没有人教过它做QA 。\n\nSummarization\n\n给出一段文章加一个too long don’t read 的缩写&quot;TL;DR:&quot; 就会尝试总结这段文字。\n\nTranslation\n\n以上图所示的形式给出 一段英文=对应的法语，这样的栗子，然后机器就知道要给出第三句英文的法语翻译。\n其实后两个任务效果其实不是很好，Summarization就像是随机生成的句子一样。\n Visualization\n\n\nhttps://arxiv.org/abs/1904.02679\n\n有人分析了一下GTP-2的attention做的事情是什么。\n上图右侧的两列，GPT-2中左列词汇是下一层的结果，右列是前一层需要被attention的对象，我们可以观察到，She 是通过nurse attention 出来的，He是通过doctor attention 出来的，所以机器学到了某些词汇是和性别有关系的（虽然它大概不知道性别是什么）。\n上图左侧，是对不同层的不同head 做一下分析，你会发现一个现象，很多不同的词汇都要attend 到第一个词汇。一个可能的原因是，如果机器不知道应该attend 到哪里，或者说不需要attend 的时候就attend 在第一个词汇。如果真是这样的话，以后我们未来在做这种model 的时候可以设一个特别的词汇，当机器不知道要attend 到哪里的时候就attend 到这个特殊词汇上。这就是Visualization 可以告诉我们的事情。\n GPT-2 write novel\n\nGPT-2是OpenAI 做的，上图是OpenAI 用GPT-2 做了一个续写故事的栗子，他们给机器看第一段，后面都是机器脑部出来的。机器生成的段落中提到了独角兽和安第斯山，所以现在都拿独角兽和安第斯山来隐喻GPT：\n\nOpenAI 担心GPT-2最大的模型过于强大，可能会被用来产生假新闻这种坏事上，所以只发布了GPT-2的小模型。BERT是你能得到模型，GPT-2却是你得不到的，于是上图就很形象的表达了众多平民AI Master 的心情。\n有人用GPT-2的公开模型做了一个在线demo，大家可以去试试：\nhttps://app.inferkit.com/demo\n参考文章：\n\nAttention isn’t all you need！BERT的力量之源远不止注意力：https://zhuanlan.zhihu.com/p/58430637\n\n","categories":["李宏毅机器学习笔记"],"tags":["ML","note","Next-Step-of-ML"]},{"title":"Note《FIRMSCOPE Automatic Uncovering of Privilege-Escala Vulnerabilities in Pre-Installed Apps in Android Firmware》","url":"https://ch3nye.top/Note-FIRMSCOPE-Automatic-Uncovering-of-Privilege-Escala/","content":" FIRMSCOPE: Automatic Uncovering of Privilege-Escalation Vulnerabilities in Pre-Installed Apps in Android Firmware\n 主要解决什么问题\nandroid 系统固件的预装软件中有很多app存在漏洞，而且这些app权限往往比较高，所以容易造成安全威胁。本文意在自动挖掘预装app中存在的权限提升漏洞。\n 使用的方法\n系统架构，workflow：\n\n 1.unpack firmware\n解压Android固件环节存在一个问题，各固件厂商的格式不一样。大部分厂商使用Android Sparse Image (SIMG) format 这种ext4格式的固件压缩方式，使用诸如e2tools这样的工具进行解压和挂载。但是，解压之后处理(unpack)就不是一致的了，有些厂商提供了第三方的unpack工具，比如华为的UApp格式，需要使用Splituapp进行unpack，HTC的RUU架构需要用HTC RUU DecryptTool，Sony 的.sin 架构需要用AnyXperia Dumper。一些厂商使用Sparse Data(SDAT)把image分块，分块文件可以使用Sdat2img重构回SIMG。\n对于没有可用的解包工具的厂商固件，采用启发式的方法搜索一致的SIMG/ext4 头，尝试unpack。通常，镜像文件会被额外的头填充，只要剥离这些额外的头，就能得到标准镜像文件。\n同时，努力寻找镜像文件中的build.prop和default.prop文件，其中可能包含了build fingerprint，os version，build configuration，exact make and model等信息。\n另外，我找到一篇参考文章：Android各厂商Rom包解压方式\n 2.Extracting and Disassembling Apps\n从unpack的image文件中找出所有DEX,ODEX,VDEX,OAT,JAR, 和APK格式文件，关于这些格式的更多信息可以参考here , 因为预装app的格式各不相同，文中将将所有提取出的app转换成APK的规范格式。对于预编译的OAT app，结合Oat2Dex和Baksmali，并参考之前提取出的framework files，把嵌入到ODEX、VDEX、OAT文件中的DEX classes提取出来，这一步输出汇编的DEX classes 或反汇编的smali。\n把所有Dalvik bytecode反汇编成smali 代码，然后转换为中间表示IL（intermediate Language）Jasmin [35] and Jimple[36]，我们还反编译应用程序的二进制的XML manifest文件，并提取有关应用程序及其所有声明的组件的元数据。具体来说，我们提取应用程序包的名称和版本信息，使用和声明的权限，以及所有导出的组件的全名和类型，以及所有指定访问权限的导出组件。\n 3.Static Taint Analysis\n它首先建立了过程间控制流图inter-procedural Control-Flow Graphs（ICFG）；然后，重建类层次结构并解析调用情况（第4.2.1节）；推断Def-Use链，并构建过程间数据流图inter-procedural Data-Flow Graph（IDFG）（第4.2.2节）；最后执行自定义的流敏感、上下文敏感、字段敏感和部分对象敏感的污点分析，以识别易受攻击的执行路径（第4.2.3节）。\nICFG和IDFG的部分跳过，来重点看看污点分析部分：（这部分先硬翻译一下😥）\n根据先前IDFG架构，污点跟踪问题就转变为图遍历问题，从一个污点源到一个污点汇聚点或到多个污点汇聚点，污点源和汇聚点都是多重图中的节点。在遍历过程中，我们应用验证规则集去修剪不会到达敏感点的路径。这些上下文验证规则对于效率和精确度非常有影响，因为先前的方法在实作上在成了不必要的复杂度扩大。例如，由于无法大规模地使用call-site stack提供上下文敏感性，因为为每个call site创建stack在计算上是不可实现的 ，特别是涉及虚拟调用时，每个call site需要维护多个stack。此外，通过 字段 的流必须始终保持流敏感和上下文敏感，但是由于明显过度的开销以及跟踪和将所有这些副本链接复制到其读写整个app所在位置的复杂性，所以我们不能在每次访问该字段时就复制与该字段相应的所有节点。因此，最终，我们的分析 实现了上下文敏感，流敏感和字段敏感以及部分对象敏感：\nContext-Sensitive：\n我们通过将每个call指令与其自身的返回伪寄存器配对，并在污点跟踪期间保持一个调用栈覆盖在每个标记为污点路径的上方，来确保上下文敏感。\nFlow- and Field-Sensitive：\n我们的IDFG构造对流敏感，因为我们考虑了语句顺序并跟踪每个程序点的流。因为我们跟踪每个类字段的流，所以我们的构造也对字段敏感。\nPartially-Insensitive：\n确保类型兼容性使我们的分析对同级和不相关类敏感，对对象敏感，对单定义方法不敏感，对子对象和其祖先之一具有定义的虚拟方法不敏感。\nPath-Insensitive：\nFIRMSCOPE对路径不敏感，因为它虽然根据控制流图流动信息，但信息流动与不相关的条件分支之间可能存在的条件依赖性无关。路径敏感性是一个已知的难题，在实践中没有绝对解决方案[39]。\nDetection Rules\n我们开发了一套规则引擎，用YAML文件写探测规则。具体来说，我们实施了规则和插件来检测以下提权漏洞：\n(i) command injection;\n(ii) arbitrary app installation/removal;\n(iii) code injection;\n(iv) factory reset of the device;\n(v) SMS injection, including accessing, sending, and manipulating text messages;\n(vi) device recording, including audio, video, and screen recording;\n(vii) log leakage to external storage or to other apps;\n(viii) AT Command injection;\n(ix) wireless settings modification;\n(x) system settings modification.\n 结果评估\n我们收集了从v4.0到v9.0的2,017个公开可用的Android固件映像（请参阅附录A以获得详细信息），总共涵盖了100多家Android供应商，其中包括全球排名前20的Android供应商。固件映像包含331,342个应用程序，具有15,144个唯一的软件包名称和39,541个唯一的软件包版本。该数据库的详细信息如表1所示。\n\n我们在三台服务器上部署了FIRMSCOPE，每台服务器在Intel Xeon E5-2630 v4 2.20GHz上运行64位Ubuntu 18.04，具有40个逻辑核心和150 GiB的RAM。我们使用GNU Parallel [41]实现了一个流水线来管理作业并在三个服务器上分配固件映像，并尽可能并行地分析多个应用程序，以保持最大80％的服务器负载而无内存交换。我们全面分析了每个固件映像，无论其某些应用程序是否可能出现在其他分析映像中。\n表2是研究结果摘要。我们发现了850个独特的特权升级漏洞（共3,483个），占分析固件的77％。命令注入漏洞排在最前面，影响了三分之一以上的固件映像。\n\n我们按弱点类别按供应商提供细分：\n\n总结：表4中显示了AOSP与供应商应用程序中已识别的漏洞的总数。大约92％的漏洞是由供应商引入的应用程序（375个唯一的软件包名称）中，而AOSP中只有8％的漏洞（18个唯一的AOSP软件包名称）。这些结果说明，类AOSP的镜像比供应商定制的镜像更安全。供应商的修改通常会带来无法预料的漏洞。\n\n 对我有什么启发\n这文章对我这个初学者来说有几方面的意义：\n\n更深入的接触到android 系统镜像\n对android run time(art), aot(ahead of time), oat(Ahead of time)file format, odex(Optimized Dalvik Executable file)的理解加深\n初次接触到taint analysis（污点分析技术）\nCFG、DFG的构建是以后要学习的内容\n\n 这项研究的未来方向\n其实我感觉它没有用很新的知识，就是把APP漏洞检测转到预装APP上做，多了一个拆包的过程，研究了一下AOSP-like的系统镜像和OEM厂商镜像中提权漏洞的类型和分布，并没有太多的创新点。\n文中构建的FIRMSCOPE系统使用静态检测技术，涉及的主要技术点包括过程间CFG、过程间DFG的构建，污点分析技术。\n未来，对于漏洞检测技术或许可以增加fuzzing、动态符号执行（Concoli）等动态分析技术。\n\nref:https://www.usenix.org/conference/usenixsecurity20/presentation/elsabagh\n\n","categories":["论文阅读"],"tags":["Android","论文","笔记","漏洞挖掘"]},{"title":"Android流量抓包环境配置","url":"https://ch3nye.top/Android流量抓包环境配置/","content":" Android流量抓包环境配置\n 前言\n如果你的手机有magisk 推荐你直接按照步骤-PC端前半部分，安装用户证书，此后参考步骤-最终曲线救国一节就好。\n 环境：\nredmik305G MIUI12 Android 10（with Magisk）\nFiddler v5.0.20204.45441 for .NET 4.6.1Built: 2020年11月3日\n 步骤\n PC端\n安装Fiddler （注意是classic版本不是Fiddler Everywhere）\n配置Fiddler 可以参考Fiddler抓包（Android app） 前四节\n到这里Fiddler的证书是以用户证书的方式安装的，此时你的fiddler已经能收到很多流量了，但是很多APP都默认忽视用户证书，所以还是没办法抓，需要把证书装到system CA列表中。\n然后需要PC端导出Fiddler 证书，导出的证书是cer格式，根据部分APP无法代理抓包的原因及解决方法（flutter 抓包） 这篇文章的叙述，通过以下命令将cer证书转为pem格式：\nopenssl x509 -inform der -in FiddlerRoot.cer -out FiddlerRoot.pem\n再将pem证书重命名为Android system CA证书的格式：\nopenssl x509 -subject_hash_old -in FiddlerRoot.pem -noout\n或者\nopenssl x509 -inform der -subject_hash_old -in FiddlerRoot.cer -noout\n得到subject_hash_old值为269953fb\n重命名FiddlerRoot.pem为269953fb.0 ，后面的.&lt;number&gt;是为了如果有hash重复的证书则加1以区分证书。\n（好像cer格式的证书MIUI12 不认，疯狂卡死，报错，只能重启，装证书的时候可烦死我了😥）\n Android 端\n在修改系统文件之前最好先禁用系统验证：\nadb disable-verity\n将269953fb.0证书文件传到android设备的/system/etc/security/cacerts目录下，如果报只读文件系统的错误就remount一下：\nadb remount\n然后\nadb push ./269953fb.0 /system/etc/security/cacerts/\n这样完成就可以正常用fiddler抓包了。\n 如果你失败了\n但是如果你重启的话可能会发现证书无了，那你就和我遇到同样的问题了。\n如果你此时执行adb remount，就发现这个文件又回来了😥，我猜测是magisk 虚拟system文件系统每次重启都重置了。\n据说要固定文件系统更改要使用sync 命令：\nadb shell sync\n但是实测没有用，他可能是在真实的文件系统（without magisk）的情况下才好用吧😥\n不管是删除文件还是增加文件，似乎执行sync无效，只要一重启就撤销了修改，而remount以后就能看到修改，所以合理猜测，remount以后修改的文件是另外一个分区，每次remount 就挂载了那个分区，我加的证书就是放到那个分区了，而magisk默认启动时挂载的分区中我没有做修改。\n但是一重启证书就没了也不是办法，我们换一种方式，使用magisk模块，开机时自动将用户CA装载到System CA。\n 最终曲线救国\n使用magisk 模块把用户CA装载到system CA列表中，参考：安卓7.0+https抓包新姿势（无需Root） 具体过程就是：\n1.安装magisk（我有）\n2.安装模块：https://github.com/NVISO-BE/MagiskTrustUserCerts\n3.安装fiddler证书（先前以用户方式安装过了）\n4.reboot\n 后记\n我为了验证上面说的，MIUI对pem格式证书和cer证书格式的支持，删除了所有配置重新做了一遍，把自己搞崩了，现在只要一remount就疯狂卡死，报错。disable-verify也没用，我猜是我把remount的那个system分区搞坏了验证出问题了（但是我除了添加删除证书也没做别的呀555），难道以后都不能remount修改分区了吗，我现在感觉耶稣都救不了我😥😥😥。\n先这样吧，靠magisk模块，先用着。\n参考文章都不错：\n\nFiddler抓包（Android app） 第五节开始讲了一些Fiddler的使用方法，值得一看\n部分APP无法代理抓包的原因及解决方法（flutter 抓包） 讲了代理抓HTTPS流量的原理，值得一看\n\n","categories":["备忘"],"tags":["Android","流量抓包","教程","Fiddler"]},{"title":"区块链技术原理笔记","url":"https://ch3nye.top/区块链技术原理笔记/","content":" ox0 前言\n2008年10月31日，一个名为Satoshi Nakamoto （中本聪）的ID 在cypherpunk 论坛发布了一篇名为《Bitcoin: A Peer-to-Peer Electronic Cash System》的文章，现在此文被看作比特币的白皮书，文中提出了&quot;区块链&quot;的概念。随后在2008年11月，中本聪发布了比特币的第一版代码。2009年1月，中本聪挖出了比特币的第一个区块——创世区块，比特币网络正式开始运行。\n现在我们知道这个叫做中本聪的神秘人，凭借这篇文章创造了万亿人民币的比特币市场。\n\n2010年12月12 日，中本聪在比特币论坛中发表了最后一篇文章，随后便不再公开露面，只通过电子邮件与比特币核心开发团队的少数人联系。2011年4月26日，在与比特币核心开发团队领导人加文·安德烈森联系过后，中本聪随即关闭了电子邮件，再也没有与任何人来往过。中本聪十分低调，在网络上可以追溯的行踪很少，他与任何人交流坚持使用PGP加密和Tor网络，而且他在论坛中的发言时间和说话方式也在刻意隐瞒自己的时区，伪装自己的表达习惯。由于他的代码逻辑十分精巧，设计区块链的具体数学细节也十分巧妙，所以有人怀疑中本聪是一个组织公用的ID。但是，时至今日也仍没有确切的证据证明某人或某组织就是中本聪。\n关于神秘人中本聪的故事就不过多叙述了，本文主要记述区块链的技术原理，涉及的要点有：\n\n区块链的技术原理\np2p交易原理\n比特币挖矿\nHD钱包？\n\n本文是我在了解区块链技术过程中的学习笔记，如果你也希望了解其中的技术原理，本文也许会对你有帮助。本文结合比特币讲解区块链技术的原理和应用方法，比特币只是区块链技术的应用之一，文中讨论的具体方法也比较原始，但就简单理解区块链技术来说足以。\n主要摘录整理廖雪峰老师的文章，所有参考文章附在文末。代码以廖雪峰老师给出的javascript 代码为例，每个代码块下方以引用的格式给出运行结果。\n 0x1 区块链原理\n区块链就是一个不断增长的全网总账本，区块链网络中的每个完全节点都拥有完整的区块链，并且，节点总是信任最长的区块链，伪造区块链需要拥有超过51%的全网算力。后面会解释为什么超过51%的全网算力就可以肆意妄为。\n区块链的一个重要特性就是不可篡改。为什么区块链不可篡改？我们先来看区块链的结构。\n区块链是由一个一个区块构成的有序链表，每一个区块都记录了一系列交易（现在一个比特币区块约记录2000~3000笔交易），并且，每个区块都指向前一个区块，从而形成一个链条：\n\n如果我们观察某一个区块，就可以看到，每个区块都有一个唯一的哈希标识，被称为区块哈希（下图中父区块哈希），同时，区块通过记录上一个区块的哈希来指向上一个区块：\n\n每一个区块还有一个Merkle哈希用来确保该区块的所有交易记录无法被篡改。\n区块链中的主要数据就是一系列交易，第一条交易通常是Coinbase交易，也就是矿工的挖矿奖励，后续交易都是用户的交易。\n 不可篡改性\n区块链的不可篡改特性是由哈希算法保证的。\n常用的哈希算法以及它们的输出长度如下：\n\n\n\n哈希算法\n输出长度(bit)\n输出长度(字节)\n\n\n\n\nMD5\n128 bit\n16 bytes\n\n\nRipeMD160\n160 bits\n20 bytes\n\n\nSHA-1\n160 bits\n20 bytes\n\n\nSHA-256\n256 bits\n32 bytes\n\n\nSHA-512\n512 bits\n64 bytes\n\n\n\n比特币使用的哈希算法有两种：SHA-256和RipeMD160\nSHA-256的理论碰撞概率是：尝试 21302^{130}2130 次随机输入，有99.8%的概率碰撞。注意 21302^{130}2130 是一个非常大的数字，大约是1361万亿亿亿亿。以现有的计算机的计算能力，是不可能在短期内破解的。\n比特币使用两种哈希算法，一种是对数据进行两次SHA-256计算，这种算法在比特币协议中通常被称为hash256或者dhash。\n另一种算法是先计算SHA-256，再计算RipeMD160，这种算法在比特币协议中通常被称为hash160。\nconst\n    bitcoin = require('bitcoinjs-lib'),\n    createHash = require('create-hash');\n\nfunction standardHash(name, data) &#123;\n    let h = createHash(name);\n    return h.update(data).digest();\n&#125;\n\nfunction hash160(data) &#123;\n    let h1 = standardHash('sha256', data);\n    let h2 = standardHash('ripemd160', h1);\n    return h2;\n&#125;\n\nfunction hash256(data) &#123;\n    let h1 = standardHash('sha256', data);\n    let h2 = standardHash('sha256', h1);\n    return h2;\n&#125;\n\nlet s = 'bitcoin is awesome';\nconsole.log('ripemd160 = ' + standardHash('ripemd160', s).toString('hex'));\nconsole.log('  hash160 = ' + hash160(s).toString('hex'));\nconsole.log('   sha256 = ' + standardHash('sha256', s).toString('hex'));\nconsole.log('  hash256 = ' + hash256(s).toString('hex'));\n\nripemd160 = 46c047bd035afb64dad2293cba29994a95b8b216\nhash160 = fe56649aa4f8fdb1edf6b88d2d41f3c1f72cf431\nsha256 = 23d4a09295be678b21a5f1dceae1f634a69c1b41775f680ebf8165266471401b\nhash256 = 1c78f53758ac96f43b99ed080f36327d2a823c4df4fa094e59b006d945bbb84d\n\n运行上述代码，观察对一个字符串进行SHA-256、RipeMD160、hash256和hash160的结果\n Merkle Hash\n在区块的头部，有一个Merkle Hash字段，它记录了本区块所有交易的Merkle Hash：\n\n注意到哈希值也可以看做数据，所以可以把a1和a2拼起来，a3和a4拼起来，再计算出两个哈希值b1和b2：\n       ┌───────────────┐               ┌───────────────┐\n       │b1&#x3D;dhash(a1+a2)│               │b2&#x3D;dhash(a3+a4)│\n       └───────────────┘               └───────────────┘\n               ▲                               ▲\n       ┌───────┴───────┐               ┌───────┴───────┐\n       │               │               │               │\n┌─────────────┐ ┌─────────────┐ ┌─────────────┐ ┌─────────────┐\n│a1&#x3D;dhash(tx1)│ │a2&#x3D;dhash(tx2)│ │a3&#x3D;dhash(tx3)│ │a4&#x3D;dhash(tx4)│\n└─────────────┘ └─────────────┘ └─────────────┘ └─────────────┘\n最后，把b1和b2这两个哈希值拼起来，计算出最终的哈希值，这个哈希就是Merkle Hash：\n                     ┌───────────────────┐\n                     │merkle&#x3D;dhash(b1+b2)│\n                     └───────────────────┘\n                               ▲\n               ┌───────────────┴───────────────┐\n               │                               │\n       ┌───────────────┐               ┌───────────────┐\n       │b1&#x3D;dhash(a1+a2)│               │b2&#x3D;dhash(a3+a4)│\n       └───────────────┘               └───────────────┘\n               ▲                               ▲\n       ┌───────┴───────┐               ┌───────┴───────┐\n       │               │               │               │\n┌─────────────┐ ┌─────────────┐ ┌─────────────┐ ┌─────────────┐\n│a1&#x3D;dhash(tx1)│ │a2&#x3D;dhash(tx2)│ │a3&#x3D;dhash(tx3)│ │a4&#x3D;dhash(tx4)│\n└─────────────┘ └─────────────┘ └─────────────┘ └─────────────┘\n如果交易的数量不恰好是4个怎么办？例如，只有3个交易时，第一个和第二个交易的哈希a1和a2可以拼起来算出b1，第三个交易只能算出一个哈希a3，这个时候，就把a3直接复制一份，算出b2，这样，我们也能最终计算出Merkle Hash：\n                     ┌───────────────────┐\n                     │merkle&#x3D;dhash(b1+b2)│\n                     └───────────────────┘\n                               ▲\n               ┌───────────────┴───────────────┐\n               │                               │\n       ┌───────────────┐               ┌───────────────┐\n       │b1&#x3D;dhash(a1+a2)│               │b2&#x3D;dhash(a3+a3)│\n       └───────────────┘               └───────────────┘\n               ▲                               ▲\n       ┌───────┴───────┐               ┌───────┴───────┐\n       │               │               │               │\n┌─────────────┐ ┌─────────────┐ ┌─────────────┐ ┌ ─ ─ ─ ─ ─ ─ ┐\n│a1&#x3D;dhash(tx1)│ │a2&#x3D;dhash(tx2)│ │a3&#x3D;dhash(tx3)│\n└─────────────┘ └─────────────┘ └─────────────┘ └ ─ ─ ─ ─ ─ ─ ┘\n如果有5个交易，我们可以看到，a5被复制了一份，以便计算出b3，随后b3也被复制了一份，以便计算出c2。总之，在每一层计算中，如果有单数，就把最后一份数据复制，最后一定能计算出Merkle Hash：\n                  ┌─────────┐\n                  │ merkle  │\n                  └─────────┘\n                       ▲\n           ┌───────────┴───────────┐\n           │                       │\n         ┌───┐                   ┌───┐\n         │c1 │                   │c2 │\n         └───┘                   └───┘\n           ▲                       ▲\n     ┌─────┴─────┐           ┌─────┴─────┐\n     │           │           │           │\n   ┌───┐       ┌───┐       ┌───┐       ┌ ─ ┐\n   │b1 │       │b2 │       │b3 │        b3\n   └───┘       └───┘       └───┘       └ ─ ┘\n     ▲           ▲           ▲\n  ┌──┴──┐     ┌──┴──┐     ┌──┴──┐\n  │     │     │     │     │     │\n┌───┐ ┌───┐ ┌───┐ ┌───┐ ┌───┐ ┌ ─ ┐\n│a1 │ │a2 │ │a3 │ │a4 │ │a5 │  a5\n└───┘ └───┘ └───┘ └───┘ └───┘ └ ─ ┘\n从Merkle Hash的计算方法可以得出结论：修改任意一个交易哪怕一个字节，或者交换两个交易的顺序，都会导致Merkle Hash验证失败，也就会导致这个区块本身是无效的，所以，Merkle Hash记录在区块头部，它的作用就是保证交易记录永远无法修改。\n Block Hash\n区块本身用Block Hash——也就是区块哈希来标识。但是，一个区块自己的区块哈希并没有记录在区块头部，而是通过计算区块头部的哈希得到的：\n\n区块头部的Prev Hash记录了上一个区块的Block Hash，这样，可以通过Prev Hash追踪到上一个区块。\n由于下一个区块的Prev Hash又会指向当前区块，这样，每个区块的Prev Hash都指向自己的上一个区块，这些区块串起来就形成了区块链。\n区块链的第一个区块（又称创世区块）并没有上一个区块，因此，它的Prev Hash被设置为00000000...000。\n如果一个恶意的攻击者修改了一个区块中的某个交易，那么Merkle Hash验证就不会通过。所以，他只能重新计算Merkle Hash，然后把区块头的Merkle Hash也修改了。这时，我们就会发现，这个区块本身的Block Hash就变了，所以，下一个区块指向它的链接就断掉了。\n\n由于比特币区块的哈希必须满足一个难度值，因此，攻击者必须先重新计算这个区块的Block Hash，然后，再把后续所有区块全部重新计算并且伪造出来，才能够修改整个区块链。\n在后面的挖矿中，我们会看到，修改一个区块的成本就已经非常非常高了，要修改后续所有区块，这个攻击者必须掌握全网51%以上的算力才行，所以，修改区块链的难度是非常非常大的，并且，由于正常的区块链在不断增长，同样一个区块，修改它的难度会随着时间的推移而不断增加。\n\n0x1 区块链原理总结：\n区块链依靠安全的哈希算法保证所有区块数据不可更改；\n交易数据依靠Merkle Hash确保无法修改，整个区块依靠Block Hash确保区块无法修改；\n保证修改区块链的难度非常巨大的机制：工作量证明机制（PoW-Proof of Work 即挖矿）在后面章节介绍\n\n 0x2 P2P交易原理\n比特币的交易是一种无需信任中介参与的P2P（Peer-to-peer）交易。\n传统的电子交易，交易双方必须通过银行这样的信任机构作为中介，这样可以保证交易的安全性，因为银行记录了交易双方的账户资金，能保证在一笔交易中，要么保证成功，要么交易无效，不存在一方到账而另一方没有付款的情况。但是在比特币这种去中心化的P2P网络中，并没有一个类似银行这样的信任机构存在，要想在两个节点之间达成交易，就必须实现一种在零信任的情况下安全交易的机制。\n一种创建交易的方法是：小明声称他给了小红一万块钱，只要能验证这个声明确实是小明作出的，并且小明真的有1万块钱，那么这笔交易就被认为是有效的：\n\n 数字签名\n如何验证这个声明确实是小明作出的呢？数字签名就可以验证这个声明是否是小明做的，并且，一旦验证通过，小明是无法抵赖的。在比特币交易中，付款方就是通过数字签名来证明自己拥有某一笔比特币，并且，要把这笔比特币转移给指定的收款方。使用签名是为了验证某个声明确实是由某个人做出的。例如，在付款合同中签名，可以通过验证笔迹的方式核对身份：\n\n而在计算机中，用密码学理论设计的数字签名算法比验证笔迹更加可信。使用数字签名时，每个人都可以自己生成一个秘钥对，这个秘钥对包含一个私钥和一个公钥：私钥被称为Secret Key或者Private Key，私钥必须严格保密，不能泄漏给其他人；公钥被称为Public Key，可以公开给任何人。这种使用一对公私密钥进行加密解密的方法就是非对称加密，早已经普遍应用于现实世界中，如果具备非对称加密的基础知识理解后面的内容会相当容易，推荐非对称加密算法RSA原理一、RSA原理二。\n当私钥持有人，例如，小明希望对某个消息签名的时候，他可以用自己的私钥对消息进行签名，然后，把消息、签名和自己的公钥发送出去：\n\n其他任何人都可以通过小明的公钥对这个签名进行验证，如果验证通过，可以肯定，该消息是小明发出的。这里的签名和验证就是指非对称加密中的加密和解密过程。\n\n这里要解释一下非对称加密应用的时候公钥和私钥到底谁负责加密谁负责解密？\n第一种用法：公钥加密，私钥解密。——用于加解密\n第二种用法：私钥签名，公钥验签。——用于签名\n有点混乱，不要去硬记，总结一下:\n你只要想：\n既然是加密，那肯定是不希望别人知道我的消息，所以只有我才能解密，所以可得出公钥负责加密，私钥负责解密；\n既然是签名，那肯定是不希望有人冒充我发消息，只有我才能发布这个签名，所以可得出私钥负责签名，公钥负责验证。\nref:https://blog.csdn.net/qq_23167527/article/details/80614454\n\n数字签名算法在电子商务、在线支付这些领域有非常重要的作用：\n首先，签名不可伪造，因为私钥只有签名人自己知道，所以其他人无法伪造签名。\n其次，消息不可篡改，如果原始消息被人篡改了，那么对签名进行验证将失败。\n最后，签名不可抵赖。如果对签名进行验证通过了，那么，该消息肯定是由签名人自己发出的，他不能抵赖自己曾经发过这一条消息。\n\n简单地说来，数字签名的三个作用：防伪造，防篡改，防抵赖。\n\n 签名和验证过程\n上面我们说了在p2p网络中数字签名的大致定义、作用和效果，我们来稍微具体讲一下区块链数字签名的签名过程和验证过程：\n签名\n对消息进行签名，实际上是对消息的哈希进行签名，这样可以使任意长度的消息在签名前先转换为固定长度的哈希数据。对哈希进行签名相当于保证了原始消息的不可伪造性。\n我们来看看使用ECDSA如何通过私钥对消息进行签名。关键代码是通过sign()方法签名，并获取一个ECSignature对象表示签名：\nconst bitcoin = require('bitcoinjs-lib');\nlet\n    message = 'a secret message!', // 原始消息\n    hash = bitcoin.crypto.sha256(message), // 消息哈希\n    wif = 'KwdMAjGmerYanjeui5SHS7JkmpZvVipYvB2LJGU1ZxJwYvP98617',\n    keyPair = bitcoin.ECPair.fromWIF(wif);\n// 用私钥签名:\nlet signature = keyPair.sign(hash).toDER(); // ECSignature对象\n// 打印签名:\nconsole.log('signature = ' + signature.toString('hex'));\n// 打印公钥以便验证签名:\nconsole.log('public key = ' + keyPair.getPublicKeyBuffer().toString('hex'));\n\nsignature = 304402205d0b6e817e01e22ba6ab19c0ab9cdbb2dbcd0612c5b8f990431dd0634f5a96530220188b989017ee7e830de581d4e0d46aa36bbe79537774d56cbe41993b3fd66686\npublic key = 02d0de0aaeaefad02b8bdc8a01a1b8b11c696bd3d66a2c5f10780d95b7df42645c\n\nECSignature对象可序列化为十六进制表示的字符串。\n验证\n在获得签名、原始消息和公钥的基础上，可以对签名进行验证。验证签名需要先构造一个不含私钥的ECPair，然后调用verify()方法验证签名：\nconst bitcoin = require('bitcoinjs-lib');\nlet signAsStr = '304402205d0b6e817e01e22ba6ab19c0'\n              + 'ab9cdbb2dbcd0612c5b8f990431dd063'\n              + '4f5a96530220188b989017ee7e830de5'\n              + '81d4e0d46aa36bbe79537774d56cbe41'\n              + '993b3fd66686'\n\nlet\n    signAsBuffer = Buffer.from(signAsStr, 'hex'),\n    signature = bitcoin.ECSignature.fromDER(signAsBuffer), // ECSignature对象\n    message = 'a secret message!', // 原始消息\n    hash = bitcoin.crypto.sha256(message), // 消息哈希\n    pubKeyAsStr = '02d0de0aaeaefad02b8bdc8a01a1b8b11c696bd3d66a2c5f10780d95b7df42645c',\n    pubKeyAsBuffer = Buffer.from(pubKeyAsStr, 'hex'),\n    pubKeyOnly = bitcoin.ECPair.fromPublicKeyBuffer(pubKeyAsBuffer); // 从public key构造ECPair\n\n// 验证签名:\nlet result = pubKeyOnly.verify(hash, signature);\nconsole.log('Verify result: ' + result);\n\nVerify result: true\n\n注意上述代码只引入了公钥，并没有引入私钥。\n修改signAsStr、message和pubKeyAsStr的任意一个变量的任意一个字节，再尝试验证签名，则不能通过。\n数字签名的验证实际上就是非对称加密中的解密过程，如果交易创造者使用私钥加密交易数据的hash得到的签名值（密文）能顺利被网络中其他人手中对应该私钥的公钥解密，即解密值==hash(交易数据)，就算是签名验证成功。数字签名算法，私钥和公钥的生成方式以及公钥和地址的对应关系我们在下一节数字签名算法中叙述。\n比特币对交易数据进行签名和对消息进行签名的原理是一样的，只是格式更加复杂。通过私钥可以对消息进行签名，签名可以保证消息防伪造，防篡改，防抵赖。对交易签名确保了只有持有私钥的人才能够花费对应地址的资金。\n 数字签名算法\n这一小节讲介绍比特币采用的：\n\n签名算法\n私钥和公钥的生成方式\n公钥和地址的对应关系。\n\n 签名算法\n常用的数字签名算法有：RSA算法，DSA算法和ECDSA算法。比特币采用的签名算法是椭圆曲线签名算法：ECDSA，使用的椭圆曲线是一个已经定义好的标准曲线secp256k1：\ny2=x3+7y^2 = x^3 + 7\ny2=x3+7\n这条曲线的图像长这样：\n\n比特币采用的ECDSA签名算法需要一个私钥和公钥组成的秘钥对：私钥本质上就是一个1～22561～2^{256}1～2256的随机数，公钥是由私钥根据ECDSA算法推算出来的，通过私钥可以很容易推算出公钥，所以不必保存公钥，但是，通过公钥无法反推私钥，只能暴力破解。\n 私钥\n比特币的私钥是一个随机的非常大的22562^{256}2256位整数。它的上限，确切地说，比22562^{256}2256要稍微小一点：\n\n0xFFFF FFFF FFFF FFFF FFFF FFFF FFFF FFFE BAAE DCE6 AF48 A03B BFD2 5E8C D036 4140\n\n而比特币的公钥是根据私钥推算出的两个256位整数。\n如果用银行卡作比较的话，比特币的公钥相当于银行卡卡号，它是两个256位整数：\n\n比特币的私钥相当于银行卡密码，它是一个256位整数：\n\n18E14A7B6A307F426A94F8114701E7C8E774E7F9A47E2C2035DB29A206321725\n\n银行卡的卡号由银行指定，银行卡的密码可以由用户随时修改。而比特币“卡”和银行卡的不同点在于：密码（实际上是私钥）由用户先确定下来，然后计算出“卡号”（实际上是公钥），即卡号是由密码通过ECDSA算法推导出来的，不能更换密码。\n由于比特币账本是全网公开的，所以，任何人都可以根据公钥查询余额，但是，不知道持卡人是谁。这就是比特币的匿名特性。\n\n如果丢失了私钥，就永远无法花费对应公钥的比特币！\n\n在比特币的P2P网络中不存在中央节点，私钥只有持有人自己知道，因此，丢失了私钥，对应的比特币就永远无法花费。如果私钥被盗，黑客就可以花费对应公钥的比特币，并且这是无法追回的。\n我们以JavaScript为例，演示如何创建比特币私钥。在JavaScript中，内置的Number类型使用56位表示整数和浮点数，最大可表示的整数最大只有9007199254740991。其他语言如Java一般也仅提供64位的整数类型。要表示一个256位的整数，只能使用数组来模拟。bitcoinjs使用bigi这个库来表示任意大小的整数。\n下面的代码演示了使用一个随机生成的数字，通过ECPair创建一个新的私钥后，表示私钥的整数就是字段d，我们把它打印出来：\nconst bitcoin = require('bitcoinjs-lib');\nlet keyPair = bitcoin.ECPair.makeRandom();\n// 打印私钥:\nconsole.log('private key = ' + keyPair.d);\n// 以十六进制打印:\nconsole.log('hex = ' + keyPair.d.toHex());\n// 补齐32位:\nconsole.log('hex = ' + keyPair.d.toHex(32));\n\nprivate key = 81014991225650123107594446292560463009575145732738301659501234189040494639354\nhex = b31cdc2d854be1d2047b0290d7b3039977bd29c036a62b63b2550c87d28cf8fa\nhex = b31cdc2d854be1d2047b0290d7b3039977bd29c036a62b63b2550c87d28cf8fa\n\n想要记住一个256位的整数是非常困难的，并且，如果记错了其中某些位，这个记错的整数仍然是一个有效的私钥，因此，比特币有一种对私钥进行编码的方式，这种编码方式就是带校验的Base58编码。\n对私钥进行Base58编码有两种方式，一种是非压缩的私钥格式，一种是压缩的私钥格式，它们分别对应非压缩的公钥格式和压缩的公钥格式。\n具体地来说，非压缩的私钥格式是指在32字节的私钥前添加一个0x80字节前缀，得到33字节的数据，对其计算4字节的校验码，附加到最后，一共得到37字节的数据：\n0x80           256bit             check\n┌─┬──────────────────────────────┬─────┐\n│1│              32              │  4  │\n└─┴──────────────────────────────┴─────┘\n计算校验码非常简单，对其进行两次SHA256，取开头4字节作为校验码。\n对这37字节的数据进行Base58编码，得到总是以5开头的字符串编码，这个字符串就是我们需要非常小心地保存的私钥地址，又称为钱包导入格式：WIF（Wallet Import Format）。可以使用wif这个库实现WIF编码：\nconst wif = require('wif');\n// 十六进制表示的私钥:\nlet privateKey = '0c28fca386c7a227600b2fe50b7cae11ec86d3bf1fbe471be89827e19d72aa1d';\n// 对私钥编码:\nlet encoded = wif.encode(\n        0x80, // 0x80前缀\n        Buffer.from(privateKey, 'hex'), // 转换为字节\n        false // 非压缩格式\n);\nconsole.log(encoded);\n\n5HueCGU8rMjxEXxiPuD5BDku4MkFqeZyd4dZ1jvhTVqvbTLvyTJ\n\n（如果你去验证了这个算法发现5HueC…解不出来0c28…，或者800c28…aa1d507a5b8d编码结果不是5HueC…，是因为你用的base58工具把你拼接的38字节数据看作字符串解析的，而实际上应该是看作16进制数值解析。私钥地址总是以5开头就是因为十六进制私钥以0x80开头base58结果总是5）\n另一种压缩格式的私钥编码方式，与非压缩格式不同的是，压缩的私钥格式会在32字节的私钥前后各添加一个0x80字节前缀和0x01字节后缀，共34字节的数据，对其计算4字节的校验码，附加到最后，一共得到38字节的数据：\n0x80           256bit           0x01 check\n┌─┬──────────────────────────────┬─┬─────┐\n│1│              32              │1│  4  │\n└─┴──────────────────────────────┴─┴─────┘\n对这38字节的数据进行Base58编码，得到总是以K或L开头的字符串编码。通过代码实现压缩格式的WIF编码如下：\nconst wif = require('wif');\n// 十六进制表示的私钥:\nlet privateKey = '0c28fca386c7a227600b2fe50b7cae11ec86d3bf1fbe471be89827e19d72aa1d';\n// 对私钥编码:\nlet encoded = wif.encode(\n        0x80, // 0x80前缀\n        Buffer.from(privateKey, 'hex'), // 转换为字节\n        true // 压缩格式\n);\nconsole.log(encoded);\n\nKwdMAjGmerYanjeui5SHS7JkmpZvVipYvB2LJGU1ZxJwYvP98617\n\n目前，非压缩的格式几乎已经不使用了。bitcoinjs提供的ECPair总是使用压缩格式的私钥表示：\nconst\n    bitcoin = require('bitcoinjs-lib'),\n    BigInteger = require('bigi');\nlet\n    priv = '0c28fca386c7a227600b2fe50b7cae11ec86d3bf1fbe471be89827e19d72aa1d',\n    d = BigInteger.fromBuffer(Buffer.from(priv, 'hex')),\n    keyPair = new bitcoin.ECPair(d);\n// 打印WIF格式的私钥:\nconsole.log(keyPair.toWIF());\n\nKwdMAjGmerYanjeui5SHS7JkmpZvVipYvB2LJGU1ZxJwYvP98617\n\n\n私钥小结：\n比特币的私钥本质上就是一个256位整数，对私钥进行WIF格式编码可以得到一个带校验的字符串。\n使用非压缩格式的WIF是以5开头的字符串，使用压缩格式的WIF是以K或L开头的字符串。\n\n 公钥\n比特币的公钥是根据私钥计算出来的。\n私钥本质上是一个256位整数，记作k。根据比特币采用的ECDSA算法，可以推导出两个256位整数，记作(x, y)，这两个256位整数即为非压缩格式的公钥。\n由于ECC曲线的特点，根据非压缩格式的公钥(x, y)的x实际上也可推算出y，但需要知道y的奇偶性，因此，可以根据(x, y)推算出x'，作为压缩格式的公钥。\n压缩格式的公钥实际上是，根据y的奇偶性在x前面添加02或03前缀，y为偶数时添加02，否则添加03，这样，得到一个1+32=33字节的压缩格式的公钥数据，记作x'，这个284位的整数作为压缩格式的公钥。\n注意压缩格式的公钥和非压缩格式的公钥是可以互相转换的，但均不可反向推导出私钥。\n非压缩格式的公钥目前已很少使用，原因是非压缩格式的公钥签名脚本数据会更长。\n我们来看看如何根据私钥推算出公钥：\nconst bitcoin = require('bitcoinjs-lib');\nlet\n    wif = 'KwdMAjGmerYanjeui5SHS7JkmpZvVipYvB2LJGU1ZxJwYvP98617',\n    ecPair = bitcoin.ECPair.fromWIF(wif); // 导入私钥\n// 计算公钥:\nlet pubKey = ecPair.getPublicKeyBuffer(); // 返回Buffer对象\nconsole.log(pubKey.toString('hex')); // 02或03开头的压缩公钥\n\n02d0de0aaeaefad02b8bdc8a01a1b8b11c696bd3d66a2c5f10780d95b7df42645c\n\n构造出ECPair对象后，即可通过getPublicKeyBuffer()以Buffer对象返回公钥数据。\n 地址\n要特别注意，比特币的地址并不是公钥，而是公钥的哈希，即从公钥能推导出地址，但从地址不能反推公钥，因为哈希函数是单向函数。\n以压缩格式的公钥为例，从公钥计算地址的方法是，首先对1+32=33字节的公钥数据进行Hash160（即先计算SHA256，再计算RipeMD160），得到20字节的哈希。然后，添加0x00前缀，得到1+20=21字节数据，再计算4字节校验码，拼在一起，总计得到1+20+4=25字节数据：\n0x00      hash160         check\n┌─┬──────────────────────┬─────┐\n│1│          20          │  4  │\n└─┴──────────────────────┴─────┘\n对上述25字节数据进行Base58编码，得到总是以1开头的字符串，该字符串即为比特币地址。使用JavaScript实现公钥到地址的编码如下：\nconst bitcoin = require('bitcoinjs-lib');\nlet\n    publicKey = '02d0de0aaeaefad02b8bdc8a01a1b8b11c696bd3d66a2c5f10780d95b7df42645c',\n    ecPair = bitcoin.ECPair.fromPublicKeyBuffer(Buffer.from(publicKey, 'hex')); // 导入公钥\n// 计算地址:\nlet address = ecPair.getAddress();\nconsole.log(address); // 1开头的地址\n\n1LoVGDgRs9hTfTNJNuXKSpywcbdvwRXpmK\n\n计算地址的时候，不必知道私钥，可以直接从公钥计算地址，即通过ECPair.fromPublicKeyBuffer构造一个不带私钥的ECPair即可计算出地址。\n要注意，对非压缩格式的公钥和压缩格式的公钥进行哈希编码得到的地址，都是以1开头的，因此，从地址本身并无法区分出使用的是压缩格式还是非压缩格式的公钥。\n以1开头的字符串地址即为比特币收款地址，可以安全地公开给任何人。\n仅提供地址并不能让其他人得知公钥。通常来说，公开公钥并没有安全风险。实际上，如果某个地址上有对应的资金，要花费该资金，就需要提供公钥。如果某个地址的资金被花费过至少一次，该地址的公钥实际上就公开了。\n私钥、公钥以及地址的推导关系如下：\n┌───────────┐      ┌───────────┐\n│Private Key│─────▶│Public Key │\n└───────────┘      └───────────┘\n      ▲                  │\n      │                  │\n      ▼                  ▼\n┌───────────┐      ┌───────────┐\n│    WIF    │      │  Address  │\n└───────────┘      └───────────┘\n\n对数字签名这一节做一个总结：\n1.数字签名应用于p2p交易过程，目的是对交易发起者（付款人）进行验证。数字签名的三个作用：防伪造，防篡改，防抵赖。\n2.比特币使用的数字签名算法是ECDSA\n3.比特币私钥本质上是一个256位随机整数，对私钥进行WIF格式编码可以得到一个带校验的字符串。私钥由持有者自行保管，一旦丢失不可找回，钱包再也无法找回\n4.比特币公钥根据私钥计算的来，算法是ECDSA（即非对称加密私钥计算公钥），本质上是两个256位整数，公钥有压缩和非压缩两种表示方法，可互相转换\n5.比特币的地址是公钥哈希的编码，并不是公钥本身，通过公钥可推导出地址；通过地址不可推导出公钥，通过公钥不可推导出私钥。\n\n 比特币钱包\n比特币钱包实际上就是帮助用户管理私钥的软件。因为比特币的钱包是给普通用户使用的，它有几种分类：\n\n本地钱包：是把私钥保存在本地计算机硬盘上的钱包软件，如Electrum；\n手机钱包：和本地钱包类似，但可以直接在手机上运行，如Bitpay；\n在线钱包：是把私钥委托给第三方在线服务商保存；\n纸钱包：是指把私钥打印出来保存在纸上；\n脑钱包：是指把私钥记在自己脑袋里。\n\n对大多数普通用户来说，想要记住私钥非常困难，所以强烈不建议使用脑钱包。\n作为用户，可以生成任意数量的私钥-公钥对，公钥是接收别人转账的地址，而私钥是花费比特币的唯一手段，钱包程序可以帮助用户管理私钥-公钥对。\n 交易\n我们再来看记录在区块链上的交易。每个区块都记录了至少一笔交易，一笔交易就是把一定金额的比特币从一个输入转移到一个输出：\n\n例如，小明把两个比特币转移给小红，这笔交易的输入是小明，输出就是小红。实际记录的是双方的公钥地址。\n如果小明有50个比特币，他要转给小红两个比特币，那么剩下的48个比特币应该记录在哪？比特币协议规定一个输出必须一次性花完，所以，小明给小红的两个比特币的交易必须表示成：\n\n小明给小红2个比特币，同时小明又给自己48个比特币，这48个比特币就是找零。所以，一个交易中，一个输入可以对应多个输出。\n当小红有两笔收入时，一笔2.0，一笔1.5，她想给小白转3.5比特币时，就不能单用一笔输出，她必须把两笔钱合起来再花掉，这种情况就是一个交易对应多个输入和1个输出：\n\n果存在找零，这笔交易就既包含多个输入也包含多个输出：\n\n在实际的交易中，输入比输出要稍微大一点点，这个差额就是隐含的交易费用，交易费用会算入当前区块的矿工收入中作为矿工奖励的一部分：\n\n计算出的交易费用：\n交易费用 &#x3D; 输入 - 输出 &#x3D; (2.0 + 1.5) - (2.99 + 0.49) &#x3D; 3.5 - 3.48 &#x3D; 0.02\n比特币实际的交易记录是由一系列交易构成，每一个交易都包含一个或多个输入，以及一个或多个输出。未花费的输出被称为UTXO（Unspent Transaction Ouptut）。\n当我们要简单验证某个交易的时候，例如，对于交易f36abd，它记录的输入是3f96ab，索引号是1（索引号从0开始，0表示第一个输出，1表示第二个输出，以此类推），我们就根据3f96ab找到前面已发生的交易，再根据索引号找到对应的输出是0.5个比特币，所以，交易f36abd的输入总计是0.5个比特币，输出分别是0.4个比特币和0.09个比特币，隐含的交易费用是0.01个比特币：\n\n再来解释一下上图，防止大家看不懂。上图中每一行都是一次交易，一次交易中input 必须来自过去发生的交易的output （直到追溯到Coinbase交易，即挖矿者创建的铸币交易），UTXO 的值索引过去的某一笔交易中的某个输出地址和钱数，比如说，f36abd此次交易的input 是交易3f96ab中输出给1mPvuPA的0.5个比特币，此次交易输出给16Gr9nB和1vg47TL各0.4，0.09个比特币，总输出比输入少了0.01个比特币，这些钱就是交易费用，给矿工了。\n\n交易小结\n比特币使用数字签名保证零信任的可靠P2P交易：\n\n私钥是花费比特币的唯一手段；\n钱包软件是用来帮助用户管理私钥；\n所有交易被记录在区块链中，可以通过公钥查询所有交易信息。\n\n\n 0x3 挖矿原理\n在比特币的P2P网络中，有一类节点，它们时刻不停地进行计算，试图把新的交易打包成新的区块并附加到区块链上，这类节点就是矿工。因为每打包一个新的区块，打包该区块的矿工就可以获得一笔比特币作为奖励。所以，打包新区块就被称为挖矿。\n比特币的挖矿原理就是一种工作量证明机制。工作量证明POW是英文Proof of Work的缩写。\n在讨论POW之前，我们先思考一个问题：在一个新区块中，凭什么是小明得到50个币的奖励，而不是小红或者小军？\n\n当小明成功地打包了一个区块后，除了用户的交易，小明会在第一笔交易记录里写上一笔“挖矿”奖励的交易，从而给自己的地址添加50个比特币。为什么比特币的P2P网络会承认小明打包的区块，并且认可小明得到的区块奖励呢？\n因为比特币的挖矿使用了工作量证明机制，小明的区块被认可，是因为他在打包区块的时候，做了一定的工作，而P2P网络的其他节点可以验证小明的工作量。\n 工作量证明\n比特币的工作量证明需要归结为计算机计算，也就是数学问题。工作量证明问题转为如何构造一个数学问题来实现工作量证明。\n在比特币网络中，矿工的挖矿也是一种工作量证明，要让计算机实现工作量证明，必须找到一种工作量算法，让计算机无法在短时间内算出来。这种算法就是哈希算法。\n通过改变区块头部的一个nonce字段的值，计算机可以计算出不同的区块哈希值：\n\n直到计算出某个特定的哈希值的时候，计算结束。这个哈希和其他的哈希相比，它的特点是前面有好几个0：\nhash256(block data, nonce&#x3D;0) &#x3D; 291656f37cdcf493c4bb7b926e46fee5c14f9b76aff28f9d00f5cca0e54f376f\nhash256(block data, nonce&#x3D;1) &#x3D; f7b2c15c4de7f482edee9e8db7287a6c5def1c99354108ef33947f34d891ea8d\nhash256(block data, nonce&#x3D;2) &#x3D; b6eebc5faa4c44d9f5232631f39ddf4211443d819208da110229b644d2a99e12\nhash256(block data, nonce&#x3D;3) &#x3D; 00aeaaf01166a93a2217fe01021395b066dd3a81daffcd16626c308c644c5246\nhash256(block data, nonce&#x3D;4) &#x3D; 26d33671119c9180594a91a2f1f0eb08bdd0b595e3724050acb68703dc99f9b5\nhash256(block data, nonce&#x3D;5) &#x3D; 4e8a3dcab619a7ce5c68e8f4abdc49f98de1a71e58f0ce9a0d95e024cce7c81a\nhash256(block data, nonce&#x3D;6) &#x3D; 185f634d50b17eba93b260a911ba6dbe9427b72f74f8248774930c0d8588c193\nhash256(block data, nonce&#x3D;7) &#x3D; 09b19f3d32e3e5771bddc5f0e1ee3c1bac1ba4a85e7b2cc30833a120e41272ed\n...\nhash256(block data, nonce&#x3D;124709132) &#x3D; 00000000fba7277ef31c8ecd1f3fef071cf993485fe5eab08e4f7647f47be95c\n比特币挖矿的工作量证明原理就是，不断尝试计算区块的哈希，直到计算出一个特定的哈希值，它比难度值要小。这里的小是说哈希值看作数值比目标小，举例来说如果目标的是前5位是零，那你找到的哈希值需要小于0x00000fffffff…这就是比特币挖矿的工作量证明方法。\n比特币使用的SHA-256算法可以看作对随机输入产生随机输出，这个随机很重要，就是说hash值的每一位都是随机的。例如，我们对字符串Hello再加上一个数字计算两次SHA-256，根据数字的不同，得到的哈希是完全无规律的256位随机数：\nhash256(&quot;Hello?&quot;) &#x3D; ????????????????????????????????????????????????????????????????\n大约计算16次，我们可以在得到的哈希中找到首位是0的哈希值，因为首位是0出现的概率是1/16：\nhash256(&quot;Hello1&quot;) &#x3D; ffb7a43d629d363026b3309586233ab7ffc1054c4f56f43a92f0054870e7ddc9\nhash256(&quot;Hello2&quot;) &#x3D; e085bf19353eb3bd1021661a17cee97181b0b369d8e16c10ffb7b01287a77173\n...\nhash256(&quot;Hello15&quot;) &#x3D; 0442e1c38b810f5d3c022fc2820b1d7999149460b83dc680abdebc9c7bd65cae\n如果我们要找出前两位是0的哈希值，理论上需要计算256次，因为00出现的概率是16^2=256，实际计算44次：\nhash256(&quot;Hello44&quot;) &#x3D; 00e477f95283a544ffac7a8efc7decb887f5c073e0f3b43b3797b5dafabb49b5\n如果我们要找出前6位是0的哈希值，理论上需要计算16^6=1677万次，实际计算1558万次：\nhash256(&quot;Hello15583041&quot;) &#x3D; 0000009becc5cf8c9e6ba81b1968575a1d15a93112d3bd67f4546f6172ef7e76\n每增加一个0，理论计算量将增加16倍。\n对于比特币挖矿来说，就是先给定一个难度值，然后不断变换nonce，计算Block Hash，直到找到一个比给定难度值低的Block Hash，就算成功挖矿。\n我们用简化的方法来说明难度，例如，必须计算出连续17个0开头的哈希值，矿工先确定Prev Hash，Merkle Hash，Timestamp，bits，然后，不断变化nonce来计算哈希，直到找出连续17个0开头的哈希值。我们可以大致推算一下，17个十六进制的0相当于计算了1617次，大约需要计算2.9万亿亿次。\n17个0 &#x3D; 16^17 &#x3D; 295147905179352825856 &#x3D; 2.9万亿亿次\n实际的难度是根据bits由一个公式计算出来，比特币协议要求计算出的区块的哈希值比难度值要小，这个区块才算有效。例如：\nDifficulty &#x3D; 402937298\n           &#x3D; 0x180455d2\n           &#x3D; 0x0455d2 * 2^(8 * (0x18 - 3))\n           &#x3D; 106299667504289830835845558415962632664710558339861315584\n           &#x3D; 0x00000000000000000455d2000000000000000000000000000000000000000000\n上述计算过程，0x180455d2是压缩的难度标记，末3字节作为底，前面0x18作为幂，做上述运算得到难度值。\n注意，难度值越小，说明哈希值前面的0越多，计算难度越大。\n 难度调整\n比特币网络的难度值是不断变化的，它的难度值保证大约每10分钟产生一个区块，而难度值在每2015个区块调整一次：如果区块平均生成时间小于10分钟，说明全网算力增加，难度值也会增加，如果区块平均生成时间大于10分钟，说明全网算力减少，难度值也会减少。因此，难度值随着全网算力的增减会动态调整。\n关于比特币挖矿难度与收益计算可以参考：https://zhuanlan.zhihu.com/p/28805231\n\n比特币设计时本来打算每2016个区块调整一次难度，也就是两周一次，但是由于第一版代码的一个bug，实际调整周期是2015个区块。\n\n根据比特币每个区块的难度值和产出时间，就可以推算出整个比特币网络的全网算力。\n比特币网络的全网算力一直在迅速增加。目前，全网算力已经超过了100EH/每秒，也就是大约每秒钟计算1万亿亿次哈希：\n\n所以比特币的工作量证明被通俗地称之为挖矿。在同一时间，所有矿工都在努力计算下一个区块的哈希。而挖矿难度取决于全网总算力的百分比。举个例子，假设小明拥有全网总算力的百分之一，那么他挖到下一个区块的可能性就是1%，或者说，每挖出100个区块，大约有1个就是小明挖的。\n由于目前全网算力超过了100EH/s，而单机CPU算力不过几M，GPU算力也不过1G，所以，单机挖矿的成功率几乎等于0。比特币挖矿已经从早期的CPU、GPU发展到专用的ASIC芯片构建的矿池挖矿。\n\n当某个矿工成功找到特定哈希的新区块后，他会立刻向全网广播该区块。其他矿工在收到新区块后，会对新区块进行验证，如果有效，就把它添加到区块链的尾部。同时说明，在本轮工作量证明的竞争中，这个矿工胜出，而其他矿工都失败了。失败的矿工会抛弃自己当前正在计算还没有算完的区块，转而开始计算下一个区块，进行下一轮工作量证明的竞争。\n为什么区块可以安全广播？因为Merkle Hash锁定了该区块的所有交易，而该区块的第一个Coinbase交易输出地址是该矿工地址。每个矿工在挖矿时产生的区块数据都是不同的，所以无法窃取别人的工作量。\n比特币总量被限制为约2100万个比特币，初始挖矿奖励为每个区块50个比特币，以后每4年减半。\n 共识算法\n如果两个矿工在同一时间各自找到了有效区块，注意，这两个区块是不同的，因为coinbase交易不同，所以Merkle Hash不同，区块哈希也不同。但它们只要符合难度值，就都是有效的。这个时候，网络上的其他矿工应该接收哪个区块并添加到区块链的末尾呢？答案是，都有可能。\n通常，矿工接收先收到的有效区块，由于P2P网络广播的顺序是不确定的，不同的矿工先收到的区块是有可能的不同的。这个时候，我们说区块发生了分叉：\n\n在分叉的情况下，有的矿工在绿色的分叉上继续挖矿，有的矿工在蓝色的分叉上继续挖矿：\n\n但是最终，总有一个分叉首先挖到后续区块，这个时候，由于比特币网络采用最长分叉的共识算法，绿色分叉胜出，蓝色分叉被废弃，整个网络上的所有矿工又会继续在最长的链上继续挖矿。\n由于区块链虽然最终会保持数据一致，但是，一个交易可能被打包到一个后续被孤立的区块中。所以，要确认一个交易被永久记录到区块链中，需要对交易进行确认。如果后续的区块被追加到区块链上，实际上就会对原有的交易进行确认，因为链越长，修改的难度越大。一般来说，经过6个区块确认的交易几乎是不可能被修改的。\n\n\n0x3 挖矿原理小结\n比特币挖矿是一种带经济激励的工作量证明机制\n工作量证明保证了修改区块链需要极高的成本，从而使得区块链的不可篡改特性得到保护\n比特币的网络安全实际上就是依靠强大的算力保障的\n比特币网络的难度值是不断变化的，它的难度值保证大约每10分钟产生一个区块\n最长分叉共识算法保证了区块链网络中只有一条合法链存在\n\n 0x4 更多细节\n通过上面的叙述，相信你已经理解了区块链的基本原理和比特币的交易方式。本节讲笼统的叙述一些细节上的概念和原理，不再展开感兴趣的话可以去廖雪峰老师的网站上了解细节。\n 可编程支付原理\n比特币的所有交易的信息都被记录在比特币的区块链中，任何用户都可以通过公钥查询到某个交易的输入和输出金额。当某个用户希望花费一个输出时，例如，小明想要把某个公钥地址的输出支付给小红，他就需要使用自己的私钥对这笔交易进行签名，而矿工验证这笔交易的签名是有效的之后，就会把这笔交易打包到区块中，从而使得这笔交易被确认。\n但比特币的支付实际上并不是直接支付到对方的地址，而是一个脚本，这个脚本的意思是：谁能够提供另外一个脚本，让这两个脚本能顺利执行通过，谁就能花掉这笔钱。\n比特币的脚本通过不同的指令还可以实现更灵活的功能。例如，多重签名可以让一笔交易只有在多数人同意的情况下才能够进行。最常见的多重签名脚本可以提供3个签名，只要任意两个签名被验证成功，这笔交易就可以成功。\n支付的本质\n从比特币支付的脚本可以看出，比特币支付的本质是由程序触发的数字资产转移。这种支付方式无需信任中介的参与，可以在零信任的基础上完成数字资产的交易，这也是为什么数字货币又被称为可编程的货币。\n由此催生出了智能合约：当一个预先编好的条件被触发时，智能合约可以自动执行相应的程序，自动完成数字资产的转移。保险、贷款等金融活动在将来都可以以智能合约的形式执行。智能合约以程序来替代传统的纸质文件条款，并由计算机强制执行，将具有更高的更低的信任成本和运营成本。\n 多重签名\n由比特币的签名机制可知，如果丢失了私钥，没有任何办法可以花费对应地址的资金。\n这样就使得因为丢失私钥导致资金丢失的风险会很高。为了避免一个私钥的丢失导致地址的资金丢失，比特币引入了多重签名机制，可以实现分散风险的功能。\n具体来说，就是假设N个人分别持有N个私钥，只要其中M个人同意签名就可以动用某个“联合地址”的资金。\n\n以3开头的地址就是比特币的多重签名地址，但从地址本身无法得知签名所需的M/N。\n\n UTXO模型\n比特币的区块链由一个个区块串联构成，而每个区块又包含一个或多个交易。\n如果我们观察任何一个交易，它总是由若干个输入（Input）和若干个输出（Output）构成，一个Input指向的是前面区块的某个Output，只有Coinbase交易（矿工奖励的铸币交易）没有输入，只有凭空输出。所以，任何交易，总是可以由Input溯源到Coinbase交易。\n还没有被下一个交易花费的Output被称为UTXO：Unspent TX Output，即未花费交易输出。给定任何一个区块，计算当前所有的UXTO金额之和，等同于自创世区块到给定区块的挖矿奖励之和。\n钱包的当前余额总是钱包地址关联的所有UTXO金额之和。\n如果刚装了一个新钱包，导入了一组私钥，在钱包扫描完整个比特币区块之前，是无法得知当前管理的地址余额的。\n那么，给定一个地址，要查询该地址的余额，难道要从头扫描几百GB的区块链数据？\n当然不是。\n要做到瞬时查询，我们知道，使用关系数据库的主键进行查询，由于用了索引，速度极快。\n因此，对区块链进行查询之前，首先要扫描整个区块链，重建一个类似关系数据库的地址-余额映射表。这个表的结构如下：\n\n\n\naddress\nbalance\nlastUpdatedAtBlock\n\n\n\n\naddress-1\n50.0\n0\n\n\n\n一开始，这是一个空表。每当扫描一个区块的所有交易后，某些地址的余额增加，另一些地址的余额减少，两者之差恰好为区块奖励：\n\n\n\naddress\nbalance\nlastUpdatedAtBlock\n\n\n\n\naddress-1\n50.0\n0\n\n\naddress-2\n40.0\n3\n\n\naddress-3\n50.0\n3\n\n\naddress-4\n10.0\n3\n\n\n\n这样，扫描完所有区块后，我们就得到了整个区块链所有地址的完整余额记录，查询的时候，并不是从区块链查询，而是从本地数据库查询。大多数钱包程序使用LevelDB来存储这些信息，手机钱包程序则是请求服务器，由服务器查询数据库后返回结果。\n总而言之，重建整个地址-余额数据库需要扫描整个区块链，并按每个交易依次更新记录，即可得到当前状态，每当新的块加入区块链，数据库也同时更新。此后，查询余额只需要从数据库查询即可。\n 0x5 Q&amp;A\n\n\nQ：区块链的安全性由哪些因素决定？\n我认为区块链的安全性主要由以下几部分决定：\n数字签名安全性：采取的ECDSA或其他非对称加密算法的安全性\n工作量证明机制安全性：比如比特币网络中，hash算法的防止碰撞的强度\n区块链网络安全性：存在足够多的“矿工”节点，保证区块不断更新\n\n\nQ：区块链中每个区块都包含那些元素，这些元素是怎么得来的，作用分别是什么？\n以比特币网络为例主要包括：\n父区块hash：=hash(父区块)\nMerkle根：按序hash区块体中的记录\n难度目标：工作量证明机制的目标，计算方法参考0x3-工作量证明\n时间戳：做工作量证明之前估计一个值就可以，要比上一个区块大\nnonce：需要不断变化，计算区块hash，以证明工作量\n区块大小：如名\n区块体：交易记录\n\n\nQ：p2p交易如比特币交易，每个块能存储多少笔交易？\n目前一个比特币区块中约有2000-3000笔交易。我的理解是：具体多少是由比特币网络规定的保证约10分钟打包一个区块，这10分钟产生的交易数量决定的。\n\n\nQ：挖矿的工作量整明，实际上是怎么做的？\n实际上就是通过变换区块头中的nonce的值，计算整个区块的hash值，要求hash值前n位是0，如果计算出来了，你就获得了这个区块的打包权力，就可以向区块链网络发出广播，你挖了一个新区块，这个区块中有一笔交易是给你的钱包添加一笔挖矿奖励，来源就是是挖矿。\nn是难度的象征，比特币网络每挖出2015个区块进行一次难度调整，维持10分钟一个区块的难度。\n\n\n参考文章：\n\n廖雪峰区块链教程：https://www.liaoxuefeng.com/wiki/1207298049439968\nSatoshi Nakamoto：Bitcoin: A Peer-to-Peer Electronic Cash System https://bitcoin.org/bitcoin.pdf\nhttps://www.bitmain.com/\n阮一峰数字签名是什么：http://www.ruanyifeng.com/blog/2011/08/what_is_a_digital_signature.html\n比特币挖矿难度与收益计算：https://zhuanlan.zhihu.com/p/28805231\nhttps://www.528btc.com/college/58842.html\n\n","categories":["笔记"],"tags":["区块链","比特币"]},{"title":"Transformer","url":"https://ch3nye.top/Transformer/","content":"\n Transformer\n\nTransformer 现在有一个非常有名的应用就是BERT，这一节还不会讲到BERT，我们先讲Transformer。BERT就是无监督train 的Transformer。Transformer是一个seq2seq model，之前讲的课程中教过seq2seq model 所以这里就假设大家都懂这个东西。Transformer它特别的地方就是在seq2seq model中大量使用了&quot;self attention&quot;这种layer。接下来我们要讲的就是&quot;self attention&quot;这种layer 它具体是在做什么。\n Sequence\n一般遇到处理sequence 的问题最常想到的model 就是RNN，无论是单向还是双向总之RNN就是比较适合处理输入是sequence 的问题。\n\nRNN的架构就如上图左侧所示（BiDirectional），它的输入和输出都是一个vector 。在单向RNN中在输出b4的时候a1-a4它都看过了，在输出b3的时候a1-a3它都看过了，在双向RNN中，在输出任意一个b的时候，所有输入它都要看过，所以它的问题是不好做平行运算的，因为他计算后一个输入的时候要依赖前一个输入。\n为了解决这个问题，有人提出了用CNN替代RNN的想法，如上图右侧所示。每个三角形代表一个filter，它就吃输入的一小段，输出一个数值，filter扫过输入产生一排输出，即上图一排红色的点，还会有其他的filter 扫过输入产生一排输出，即上图黄色的点…这些filter 可以平行计算。那你就会说了，CNN它没有考虑所有输入，RNN是看过所有输入才计算出输出，而CNN只看了一部分呀。只看一层的话确实是这样，但是我们可以叠很多层CNN（如蓝色三角示例），这样上层的CNN就会看到更多的输入。\n总而言之，CNN确实可以平行计算了，但是它需要叠很多层才能看到全部输入的信息，那有没有更好的办法呢，接下来就介绍Self Attention 。\n Self Attention\nSelf Attention 就是想要取代RNN 做的事情，并且能克服RNN不能平行计算的缺点。下面的内容如果你看不下去，你就只要记得Self Attention 能做到和RNN 一摸一样的事情，它也是吃一个sequence 吐出一个sequence ，输出sequence 的每个元素都是看过所有输入计算出来的，而且是平行计算出来的。\n\n关键就是你可以在你要做的事情上用Self Attention 取代RNN 。\n\n\n最早提出self-attention 替换RNN 的paper（by google）：Attention Is All You Need\nhttps://arxiv.org/abs/1706.03762\n\n输入是 x1x_1x1​ 到 x4x_4x4​ 的sequence ，输入x通过一个matrix W 做embedding 得到sequence a 。（注意 xix_ixi​ 和 aia^iai 应该都是vector，列向量） 然后每个 aia^iai 都乘上三个不同的matrix 得到三个输出向量q k v （列向量）。\nq代表query 它是要去match 别人的\nk代表key 它是要被match的\nv代表information 就是要被抽取出来的信息\n接下来我们就拿每个q对每个k做attention ：\n\nattention 怎么做呢？（根据google论文中所述这里讲解的attention function是类似于dot-proudct attention）self-attention layer 它要做的就是拿 q1q^1q1 和 k1k^1k1 做attention，得到 α1,1\\alpha_{1,1}α1,1​ ，如上图所示同样的做法得到 α1,2\\alpha_{1,2}α1,2​ ，α1,3\\alpha_{1,3}α1,3​ ，α1,4\\alpha_{1,4}α1,4​ 。attention 的具体公式就是上图所示的，对于 α1,i\\alpha_{1,i}α1,i​ attention就是指把 q1q^1q1 点积 kik^iki 然后除以根号d，d是q和k的维度。（α\\alphaα 是数值）\n为什么除以根号d，而不是除以其他东西呢？在google 的Self-attention 的paper 中3.2.1节attention公式下有解释，你可以自己去看看，大意好像就是为了避免dot-product attention将softmax函数推入梯度极小的区域，我也解释不太好😥。老师也没有实践这个除以根号d对结果有多大影响。\n另外，你还需要知道attention function显然不止一种，paper中另外有提到addictive attention，不过没有实践替换attention function 会对结果有多大影响。\n\n然后，你要把上述得到的 α\\alphaα 通过一个softmax layer 得到 α^\\hat\\alphaα^ ，softmax layer做的事情就是上图公式所示，相当于做一个normalization 。\n\n得到 α^\\hat\\alphaα^ 以后要把每个 α^\\hat\\alphaα^ 和对应的v 相乘，再求和就得到 b1b^1b1 。我们上面说self-attention 也是输入一个sequence 输出一个sequence，这个 b1b^1b1 就是输出的第一个element 。之后我们只要用同样的办法求出 b2b^2b2 ， b3b^3b3 ， b4b^4b4  就结束了。这个过程中每个output element 的计算都是独立的，都不依赖其他的output element 所以可以并行计算。\n我们再看一下上图中self-attention layer 的结构，对于 b1b^1b1 来说它看过了整个input sequence ，而且我们也可以让它看部分的input sequence，只要把不希望它看的部分产生的 α\\alphaα 的值设为0就可以了。所以使用self-attention layer 对于output sequence 中的element 来说它可以看任意多个任意位置的input sequence 中的element 。\n\nb2b^2b2 的计算也是一样的，如上图所示，q2q^2q2 和 kik^iki 做attention 得到对应的 α2,i\\alpha_{2,i}α2,i​ 然后做softmax 得到 α^2,i\\hat\\alpha_{2,i}α^2,i​ 再乘 viv^ivi 做summation 就得到 b2b^2b2 。以此类推平行计算所有b：\n\n如果你觉得上面的讲解比较乱，看不懂，你可以只记住self-attention layer 的输入输出就好。\n接下来我们要具体讲解一下attention 中的一连串矩阵运算，为什么是容易被平行计算，为什么是容易被加速的。\n Why Could Speed Up\n\n首先来看列向量q、k、v的计算方法，根据上面说过的，如上图右上角所示，我们可以把列向量 aia^iai 排在一起，形成一个矩阵 III 。将四个q、k、v的计算转换成矩阵运算。\n\n再来看数值 α\\alphaα 的计算，以 α1,i\\alpha_{1,i}α1,i​ 为例，我们可以把k和q的内积转换为 kTk^TkT 与 qqq 的积，进一步可以把四个 α1,i\\alpha_{1,i}α1,i​ 的运算转换为上图右下角的矩阵乘向量。更进一步，对于 α2,i\\alpha_{2,i}α2,i​ 也是一样的，所以得到了下述公式：\n\nmatrix A 中每个元素都是我们要求的值，A可以转换为matrix 乘积 KTQK^T QKTQ 。再把A中每一列元素做一下softmax 就能得到 A^\\hat AA^ 。\n\n接着，把列向量 v1,v2,v3,v4v^1,v^2,v^3,v^4v1,v2,v3,v4 分别乘以 α1,1^,α1,2^,α1,3^,α1,4^\\hat{\\alpha_{1,1}},\\hat{\\alpha_{1,2}},\\hat{\\alpha_{1,3}},\\hat{\\alpha_{1,4}}α1,1​^​,α1,2​^​,α1,3​^​,α1,4​^​ 再加起来就得到 b1b^1b1 ，对于其他的b也是一样的，就这样结束。\n我们再来总的看一下从input III 到output OOO 经历了哪些步骤：\n\n Multi-head Self-attention\n接下来讲一个Self-attention 的变形 (2 heads as example)\n\n在这种方法中q、k、v都会有多个，生乘的输出也会有多组，这里以2 head 举例所以画了两个，具体的做法可以是你把 qiq^iqi 乘一个matrix Wq,1W^{q,1}Wq,1 得到 qi,1q^{i,1}qi,1 ，乘一个matrix Wq,2W^{q,2}Wq,2 得到 qi,2q^{i,2}qi,2 。然后你在做attention 的时候就是每个head 生乘的q、k、v只在本组内做运算，举例来说如上图所示，如果你要计算head 1组的 bi,1b^{i,1}bi,1 你就用  aia^iai 生成的 qi,1,ki,1,vi,1q^{i,1},k^{i,1},v^{i,1}qi,1,ki,1,vi,1 和 aja^jaj 生成的同head组的 qj,1,kj,1,vj,1q^{j,1},k^{j,1},v^{j,1}qj,1,kj,1,vj,1 做计算。对于head 2组的 bi,2b^{i,2}bi,2：\n\n输出的两组b会concatenate 起来，如果你不希望b的维度增加，你可以用下图所示的方法做一下降维：\n\nmulti-head的好处是，不同的head 可以考虑不同的事情。比如说有的head 只需要近期的信息，所以只看local 的信息，有的head 需要长时间以前的信息，看global 的信息。\n Positional Encoding\n\nself-attention layer 它没有考虑input sequence 的顺序，上面我们说了，对于self-attention 的输出可以看任意数量任意位置的input，所以对它来说没有顺序的概念，也就是说你今天输入&quot;我吃饭了&quot;和&quot;饭吃我了&quot;对它来说可能是完全一样的，我们显然不希望是这样。\nPositional Encoding就是要解决这个问题，做法就是在列向量 aia^iai 上加上一个标识位置的列向量 eie^iei 这个 eie^iei 在原始paper 中是人设定的，位置1就是 e1e^1e1 位置2就是 e2e^2e2… 你可能会问为什么是加，为什么不concatenate起来，你把一个向量加上去那原来的信息不就变了吗，concatenate不是会更好吗。\n为了让你理解这个做法，这里老师用另一种方式来讲解，假如说我们在x都后面concatenate 一个表示位置的one-hot向量p，然后再做本来做的那个乘矩阵W转换为a的步骤，如果我们把W拆分成 WI,WPW^I,W^PWI,WP 来看待，根据线性代数的知识，这个公式就可以拆分成上图所示的两两相乘再相加样子， WIxiW^I x^iWIxi 就是 aia^iai ， WPpiW^P p^iWPpi 就是 eie^iei 。这样是不是就好理解了。\n让人难以理解的是你当然可以learn WPW^PWP ，在self-attention 的paper 中有提到以前用CNN做seq2seq model 的时候就有人试过learn WPW^PWP 了，结果没有更好，他们的 WPW^PWP 是用一个奇怪的公式产生出来的，它就长这个样子：\n\n Seq2seq with Attention\n上面讲的是在Seq2seq model 中我们可以用self-attention 取代RNN ，接下来我们就来讲在Seq2seq model 中self-attention 是怎么应用的。\n\n\nReview: https://www.youtube.com/watch?v=ZjfjPzXw6og&amp;feature=youtu.be\n\n\n\nsource:https://ai.googleblog.com/2017/08/transformer-novel-neural-network.html\n\n这是google 用self-attention 做的翻译模型的gif演示。首先每个step 会把input sequence 两两之间做attention，然后弹出一排一排的小点是告诉你这些step 是平行运算的，每一层都做self-attention，接着做Decoding ，会对input 的encoder做attention，在decode 第二个word 的时候它就不只对input 做attention 还会对之前产生的东西做attention 。\n Transformer\n\n上图是一个seq2seq model ，做的任务是中英互译，左半部是encoder 右半部是decoder，我们以中翻英为例，输入&quot;机器学习&quot;，encoder 进行encoding 然后输入给deocder ，decoder 吃进去产生输出，machine 然后把machine 当作输入再吃进去，在产生learning ，如此运行知道输出句末标志结束。\n下面来介绍一下self-attention 在这个模型中的具体运作方式：\n\n\nLayer Norm: https://arxiv.org/abs/1607.06450\nBatch Norm: https://www.youtube.com/watch?v=BZh1ltr5Rkg\n\n先看encoder，input 经过embedding 和 positional encoding 以后进入灰色的block，这部分会重复N次。在这个灰色的block中，先是一个multi-head attention layer；下一个layer是Add&amp;Norm，意思是说把multi-head attention layer的input和output加起来，再把得到的b′b&#x27;b′做Layer Normalization；（如果要进一步了解Layer Norm可以参考上述文献）\n我们之前讲过Batch Norm，Batch Norm是说我们希望一个Batch的data中每个维度的mean=0，variance=1；Layer Norm是不需要考虑Batch 的，举例来说给一笔Data 我们希望它所有维度的mean=0，variance=1。Layer Norm一般会搭配RNN使用，transformer 很像RNN，这可能就是Layer Norm用在这里的原因。\n回到decoder，再往后，就如图说是，进入一个前馈神经网络，然后在跟一个Add&amp;Norm。\n再看Deocder，Deocder的输入是前一个step 产生的output，前面的处理都是一样的，灰色的block会重复N次，其中上来就是一个Masked Multi-head Attention，这个Masked意思是在做self-attention的时候decoder 只会attend已经产生的sequence，这也是很合理嘛，毕竟还没有产生出来的东西你怎么做self-attention。接着是一个Multi-head Attention 这个layer attend之前encoder 的输出，接着还有一个Add&amp;Norm，再后面的东西也是和encoder 一样的不再赘述。\n Attention Visualization\n\n\nhttps://arxiv.org/abs/1706.03762\n\n这是google的paper中最终版本得到的结果，所有单词两两之间都会有attention，颜色越深表示attention的weight越大。\n\n\nThe encoder self-attention distribution for the word “it” from the 5th to the 6th layer of a Transformer trained on English to French translation (one of eight attention heads).\nhttps://ai.googleblog.com/2017/08/transformer-novel-neural-network.html\n\n上图是这个意思，左边输入的句子是这只动物没走过街道，因为它太累了，机器学到了it指代的是animal，这两个词之间的attention很深，而当我们把之后的tired换成wide，句子变成这只动物没走过街道，因为它太宽了，机器就学到it是指代街道。amazing\n Multi-head Attention\n\n上如图所示是两组q、k、v，也就是两个head 做出的结果，显然上面就偏向于attend全局信息，下面就偏向于attend局部信息。\n Example Appplication\n\nTransformer 可以用在哪里呢？基本上原来用seq2seq model 做的任务都可以换成transformer，现在这些任务基本上已经被洗过一轮了，都被做干了。\n\nhttps://arxiv.org/abs/1801.10198\n\n比较惊人的是做Summarizer，这篇文章是google 做的，他们的input是一堆文章，然后写出这堆文章的总结和摘要，要求有wiki的风格，所以输出也是一篇文章。比如说你搜索台湾大学，把google出来的文章都作为input，机器就会写一个台湾大学的wiki。这个任务的训练资料是很多的，出现transformer之前，大概是做不起来的。\n Universal Transformer\n\n\nhttps://ai.googleblog.com/2018/08/moving-beyond-translation-with.html\n\n出现Transformer后提出的，时间上换成transformer，纵向深度上仍然用RNN，具体细节可以参考上述链接。\n Self-Attention GAN\n\n\nhttps://arxiv.org/abs/1805.08318\n\ntransformer 最早提出来是用在文字上，现在它也可以被用在影像上，举例来说，有一个Self-attention GAN ，你在处理影像的时候可以让每个pixel 都去attention 其他的pixel ，所以你在处理影像的时候可以考虑到比较global 的信息。\n","categories":["李宏毅机器学习笔记"],"tags":["ML","note","Next-Step-of-ML"]},{"title":"符号执行技术笔记","url":"https://ch3nye.top/符号执行技术笔记/","content":" 符号执行技术笔记\n此前没有接触过符号执行，所以本文是在我学习过程中对一些好的文章的摘录，算是方便我日后回顾学习，特别感谢r1ce和K0rz3n的文章，所有参考连接将放在文末。如果我后续研究到符号执行技术相关的内容大概会补充进来。文末有一些我读文章时候的疑问，读完本文再看，可能有助于你的理解。\n 0x1 通俗地解释符号执行\nWiki中的定义是：在计算机科学中，符号执行技术指的是通过程序分析的方法，确定哪些输入向量会对应导致程序的执行结果为某个向量的方法(绕)。通俗的说，如果把一个程序比作DOTA英雄，英雄的最终属性值为程序的输出（包括攻击力、防御力、血槽、蓝槽），英雄的武器出装为程序的输入（出A杖还是BKB）。那么符号执行技术的任务就是，给定了一个英雄的最终属性值，分析出该英雄可以通过哪些出装方式达到这种最终属性值效果。\n可以发现，符号执行技术是一种白盒的静态分析技术。即分析程序可能的输入需要能够获取到目标源代码的支持。同时，它是静态的，因为并没有实际的执行程序本身，而是分析程序的执行路径。如果把上述英雄的最终属性值替换成程序形成的bug状态，比如，存在数组越界复制的状态，那么，我们就能够利用此技术挖掘漏洞的输入向量了。\n可以发现，符号执行技术是一种白盒的静态分析技术。即，分析程序可能的输入需要能够获取到目标源代码的支持。同时，它是静态的，因为并没有实际的执行程序本身，而是分析程序的执行路径。如果把上述英雄的最终属性值替换成程序形成的bug状态，比如，存在数组越界复制的状态，那么，我们就能够利用此技术挖掘漏洞的输入向量了。\n这里再举一个简单的例子，让大家有深刻的理解。\n以下面的源代码为例子：\nint m&#x3D;M, n&#x3D;N, q&#x3D;Q; \nint x1&#x3D;0,x2&#x3D;0,x3&#x3D;0;\nif(m!&#x3D;0)\n&#123;\n    x1&#x3D;-2;\n&#125;\nif(n&lt;12)\n&#123;\n    if(!m &amp;&amp; q)\n    &#123;\n        x2&#x3D;1;\n    &#125;\n    x3&#x3D;2;\n&#125;\nassert(x1+x2+x3!&#x3D;3) &#x2F;&#x2F; if &#x3D;3 then program crash\n上述代码是一个简单的c语言分支结构代码，它的输入是M,N,Q三个变量；输出是x1,x2,x3的三个变量的和。我们这里设置的条件是想看看什么样的输入向量&lt;M,N,Q&gt;的情况下，得到的三个输出变量的和等于3.\n那么我们通过下面的树形结构来看看所有的情况：\n\n上面的分析图把所有可能的情况都列举出来了，其中，叶子节点显示的数值表示当前输入情况下，可以得到的数值。（比如，如果英雄出装是M^(N&lt;12)，那么最终的属性值R=0）。其中M^(N&lt;12)表达的是，M是非零值且N要小于12，Q为任意值的情况下，得到R=0。可以发现，当条件为~M^(N&lt;5)^Q时，得到了最终结果等于3.即，我们通过这种方式逆向发现了输入向量。如果把结果条件更改为漏洞条件，理论上也是能够进行漏洞挖掘了。\n对于如何根据最终得到的结果求解输入向量，已经有很多现成的数学工具可以使用。上述问题其实可以规约成约束规划的求解问题（更详细的介绍看这里：Constraint_programming ）。比较著名的工具比如SMT（Satisfiability Modulo Theory，可满足性模理论）和SAT。\n但是在实际的漏洞分析过程中，目标程序可能更加复杂，没有我们上面的例子这么简单。实际的程序中，可能包含了与外设交互的系统函数，而这些系统函数的输入输出并不会直接赋值到符号中，从而阻断了此类问题的求解。\n比如下面的这个包含了文件读写的例子：\nint main(int argc, char* argv[])\n&#123;\n    FILE *fop &#x3D; fopen(&quot;test.txt&quot;);\n    ...\n    if(argc &gt; 3)\n    &#123;\n        fputs(&quot;Too many parameters, exit.&quot;, fop);\n    &#125;\n    else\n    &#123;\n        fputs(&quot;Ok, we will run normally.&quot;, fop);\n    &#125;\n    ...\n    output &#x3D; fgets(..., fop);\n    assert(!strcmp(output, &quot;Ok, we will run normally.&quot;)); &#x2F;&#x2F; if str is &quot;OK,...&quot; then program crash\n    return 0;\n&#125;\n上述示例代码中，想要发现什么情况下会得到输出”Ok, we will run normally.”这个字符串。通过一系列的执行到if语句，此时，根据程序输入的参数个数将会产生两个分支。分支语句中将执行系统的文件写操作。**在传统的符号执行过程中，此类函数如果继续沿着系统函数的调用传递下去的话，符号数值的传递将会丢失。**而在之后的output = fgets(…, fop);这行代码中，符号从外部获得的数值也将无法正常的赋值到output中。因此，符号执行无法求解上述问题，因为在调用系统函数与外设交互的时候，符号数值的赋值过程被截断了。\n\n为了解决这个问题，最经典的项目就是基于LLVM的KLEE（klee)它把一系列的与外设有关的系统函数给重新写了一下，使得符号数值的传递能够继续下去。从比较简化的角度来说，就是把上面的fputs函数修改成，字符串赋值到某个变量中，比如可以是上面的fop里面。再把fgets函数修改成从某个变量获取内容，比如可以是把fop的地址给output。这样，就能够把符号数值的传递给续上。当然，这里举的例子是比较简单的例子，实际在重写函数的时候，会要处理更复杂的情况。在KLEE中，它重新对40个系统调用进行了建模，比如open, read, write, stat, lseek, ftruncate, ioctl。感兴趣的读者可以进一步阅读他们发表在OSDI2008年的论文（KLEE-OSDI08)他们的文章深入浅出，非常适合学习。\n 0x2 从公式原理上理解符号执行\n符号执行的关键思想就是，把输入变为符号值，那么程序计算的输出值就是一个符号输入值的函数。这个符号化的过程在上一篇AEG文章中已有简要阐述，简而言之，就是一个程序执行的路径通常是true和false条件的序列，这些条件是在分支语句处产生的。在序列的第i位置如果值是true，那么意味着第i个条件语句走的是then这个分支；反之如果是false就意味着程序执行走的是else分支。\n那么，如何形式化地表示符号执行的过程呢？程序的所有执行路径可以表示为树，叫做执行树。接下来我们就以一个例子来阐述通过符号执行遍历程序执行树的过程。\nint twice(int v)&#123;\n\treturn 2*v;\n&#125;\n\nvoid testme(int x, int y)&#123;\n\tz &#x3D; twice(y); \n\tif(z &#x3D;&#x3D; x)&#123;\n\t\tif(x &gt; y+10)\n\t\t\tERROR;\n\t\t&#125;\n\t&#125;\n&#125;\n\n&#x2F;* simple driver exercising testme() with sym inputs *&#x2F;\nint main()&#123;\n    x &#x3D; sym_input();\n    y &#x3D; sym_input();\n    testme(x, y);\n    return 0;\n&#125;\n\n代码中，testme()函数有3条执行路径，组成了上图中的执行树。直观上来看，我们只要给出三组输入就可以遍历这三个路径，即图中圆圈中的x和y取值。符号执行的目标就是能够生成这样的输入集合，在给定的时间内探索所有的路径。\n为了形式化地完成这个任务，符号执行会在全局维护两个变量。其一是符号状态 σ\\sigmaσ ，它表示的是一个从变量到符号表达式的映射。其二是符号化路径约束PC（或者叫路径条件），这是一个==无量词的一阶公式==，用来表示路径条件。在符号执行的开始，符号状态 σ\\sigmaσ 会先初始化为一个空的映射，而符号化路径约束PC初始化为true。 σ\\sigmaσ 和PC在符号执行的过程中会不断更新。在符号执行结束时，PC就会用约束求解器进行求解，以生成实际的输入值。这个实际的输入值如果用程序执行，就会走符号执行过程中探索的那条路径，即此时PC的公式所表示的路径。\n我们以上图的例子来阐述这个过程。当符号执行开始时，符号状态 σ\\sigmaσ 为空，符号路径约束PC为true。当我们遇到一个读语句，形式为var=sym_input()，即接收程序输入，符号执行就会在符号状态 σ\\sigmaσ 中加入一个映射 var→svar \\rightarrow svar→s ，这里s就是一个新的未约束的符号值。上述代码中，main()函数的前两行会得到结果 σ={x→x0,y→y0}\\sigma=\\lbrace x \\rightarrow x_0, y \\rightarrow y_0\\rbraceσ={x→x0​,y→y0​} ，其中 x0x_0x0​ 和 y0y_0y0​ 是两个初始的未约束的符号化值。\n当我们遇到一个赋值语句，形式为v=e，符号执行就会将符号状态 σ\\sigmaσ 更新，加入一个v到 σ(e)\\sigma(e)σ(e) 的映射，其中 σ(e)\\sigma(e)σ(e) 就是在当前符号化状态计算e得到的表达式。例如，在代码执行完第6行时， σ={x→x0,y→y0,z→2y0}\\sigma=\\lbrace x \\rightarrow x_0, y \\rightarrow y_0, z \\rightarrow 2y_0\\rbraceσ={x→x0​,y→y0​,z→2y0​} 。\n当我们遇到条件语句if(e) S1 else S2，PC会有两个不同更新。首先是PC更新为 PC∧σ(e)PC \\wedge \\sigma(e)PC∧σ(e) ，这就表示then 分支；然后是建立一个新的路径约束PC’，初始化为 PC∧¬σ(e)PC \\wedge \\neg \\sigma(e)PC∧¬σ(e) ，这就表示else分支。如果PC是可满足的，即可以给符号分配一些实际值，那么程序执行就会走then 分支，此时的状态为：符号状态 σ\\sigmaσ 和符号路径约束PC。同样，如果PC’是可满足的，那么会建立另一个符号执行实例，其符号状态为 σ\\sigmaσ ，符号路径约束为PC’，走else分支（请注意，与具体执行不同，符号执行技术可以同时使用两个分支，从而产生两个执行路径）。如果PC和PC’都不能满足，那么执行就会在对应路径终止。例如，第7行建立了两个不同的符号执行实例，路径约束分别是 x0=2y0x_0=2y_0x0​=2y0​ 和 x0≠2y0x_0 \\not= 2y_0x0​​=2y0​ 。在第8行，又建立了两个符号执行实例，路径约束分别是 (x0=2y0)∧(x0&gt;y0+10)(x_0=2y_0) \\wedge (x_0 &gt; y_0 + 10)(x0​=2y0​)∧(x0​&gt;y0​+10) 和 (x0=2y0)∧(x0≤y0+10)(x_0=2y_0) \\wedge (x_0 \\leq y_0 + 10)(x0​=2y0​)∧(x0​≤y0​+10) .\n如果符号执行遇到了exit语句或者错误（指的是程序崩溃、违反断言等），符号执行的当前实例会终止，利用约束求解器对当前符号路径约束赋一个可满足的值，而可满足的赋值就构成了测试输入：如果程序执行这些实际输入值，就会在同样的路径结束。例如，在上图例子中，经过符号执行的计算会得到三个测试输入: {x=0,y=1}\\lbrace x=0,y=1\\rbrace{x=0,y=1}, {x=2,y=1}\\lbrace x=2,y=1\\rbrace{x=2,y=1}, {x=30,y=15}\\lbrace x=30,y=15\\rbrace{x=30,y=15} .\nvoid testme_inf() &#123;          \n    int sum &#x3D; 0;\n    int N &#x3D; sym_input();      \n    while (N &gt; 0) &#123;\n        sum &#x3D; sum + N\n        N &#x3D; sym_input();\n    &#125;\n&#125;\n当我们遇到了循环和递归应该怎么办呢？如果循环或递归的终止条件是符号化的，包含循环和递归的符号执行会导致无限数量的路径。比如上面代码中这个例子，这段代码就有无数条执行路径，每条路径的可能性有两种：要么是任意数量的true加上一个false结尾，要么是无穷多数量的true。我们形式化地表示包含n个true条件和1个false条件的路径，其符号化约束如下：\n(∧i∈[1,n]Ni&gt;0)∧(Nn+1≤10)(\\wedge_{i \\in [1,n]}N_i &gt; 0) \\wedge (N_{n+1} \\leq 10)\n(∧i∈[1,n]​Ni​&gt;0)∧(Nn+1​≤10)\n其中每个 NiN_iNi​ 都是一个新的符号化值，执行结尾的符号状态是：\n{N→Nn+1,sum→∑i∈[1,n]Ni}\\Big\\lbrace N \\rightarrow N_{n+1}, sum \\rightarrow \\sum_{i \\in [1,n]} N_i \\Big\\rbrace\n{N→Nn+1​,sum→i∈[1,n]∑​Ni​}\n其实这就是符号执行面临的问题之一，即如何处理循环中的无限多路径。在实际中，有一些方法可以应对，比如对搜索加入限制，要么是限制搜索时间的长短，要么是限制路径数量、循环迭代次数、探索深度等等。\n还需要考虑到的一个问题就是，如果符号路径约束包含不能由求解器高效求解的公式怎么办？比如说，如果原本的代码发生变化，把twice函数替换为下述语句：\nint twice(int v) &#123;\n    return (v*v) % 50;\n&#125;\n那么符号执行就会产生路径约束 x0≠(y0y0)mod50x_0 \\neq (y_0y_0)mod50x0​​=(y0​y0​)mod50 以及  x0=(y0y0)mod50x_0 = (y_0y_0)mod50x0​=(y0​y0​)mod50 。我们做另外一个假设，如果twice是一个我们得不到源码的函数，也就是我们不知道这个函数有什么功能，那么符号执行会产生路径约束 x0≠twice(y0)x_0 \\neq twice(y_0)x0​​=twice(y0​) 以及  x0=twice(y0)x_0 = twice(y_0)x0​=twice(y0​) ，其中twice是一个未解释的函数。这两种情况下，约束求解器都是不能求解这样的约束的，所以符号执行不能产生输入。\n其实我们上述介绍的内容，应该属于纯粹的静态符号执行的范畴。我们提出的两个问题，是导致静态符号执行不能够实用的原因之一。符号执行的概念早在1975年[4]就提出了，但是符号执行技术真正得到实用，却是在一种方式提出之后，即混合实际执行和符号执行，称为concolic execution，是真正意义上的动态符号执行。我们会在后面的小节中介绍。\n 0x3 动态符号执行 Concolic Execution\n最早将实际执行(concrete execution)和符号执行(symbolic execution)结合起来的是2005年发表的DART[5]，全称为“Directed Automated Random Testing”(或称concolic testing)，以及2005年发表的CUTE[6]，即“A concolic unit testing engine for C”。\n**Concolic执行维护一个实际状态(concrete state)和一个符号化状态(symbolic state)：实际状态将所有变量映射到实际值，符号状态只映射那些有非实际值的变量。Concolic执行首先用一些给定的或者随机的输入来执行程序，收集执行过程中条件语句对输入的符号化约束，然后使用约束求解器去推理输入的变化，从而将下一次程序的执行导向另一条执行路径。**简单地说来，就是在已有实际输入得到的路径上，对分支路径条件进行取反，就可以让执行走向另外一条路径。这个过程会不断地重复，加上系统化或启发式的路径选择算法，直到所有的路径都被探索，或者用户定义的覆盖目标达到，或者时间开销超过预计。\n我们依旧以上面那个程序的例子来说明。Concolic执行会先产生一些随机输入，例如{x=22, y=7}，然后同时实际地和符号化地执行程序。这个实际执行会走到第7行的else分支，符号化执行会在实际执行的路径上生成一个路径约束：x0≠2y0x_0 \\neq 2y_0x0​​=2y0​ 。然后Concolic执行会将路径约束的连接词取反，得到新的路径约束 x0=2y0x_0 = 2y_0x0​=2y0​ ，约束求解得到一个新的测试输入{x=2, y=1}，这个新输入就会让执行走向一条不同的路径。之后，Concolic执行会在这个新的测试输入上再同时进行实际的和符号化的执行，执行会取与此前路径不同的分支，即第7行的then分支和第8行的else分支，这时产生的约束就是 (x0=2y0)∧(x0≤y0+10)(x_0=2y_0) \\wedge (x_0 \\leq y_0 + 10)(x0​=2y0​)∧(x0​≤y0​+10) ，生成新的测试输入让程序执行没有被执行过的路径。再探索新的路径，就需要将上述新加入的约束条件取反，也就是 (x0=2y0)∧(x0&gt;y0+10)(x_0=2y_0) \\wedge (x_0 &gt; y_0 + 10)(x0​=2y0​)∧(x0​&gt;y0​+10) ，通过求解约束得到测试输入{x=30, y=15}，程序会在这个输入上遇到ERROR语句。如此一来，我们就完成了所有3条路径的探索。\nPS：注意在这个搜索过程中，其实Concolic执行使用了深度优先的搜索策略。也就是说每次产生新的输入Concolic执行都将执行完整个程序，直到程序结束，然后才返回上一个没有测试的路径约束条件，取反约束条件，得到新的输入，进入新的路径。\n这上述过程中，我们从一个实际输入{x=22, y=7}出发，得到第一个约束条件 x0≠2y0x_0 \\neq 2y_0x0​​=2y0​ ，第一次取反得到 x0=2y0x_0 = 2y_0x0​=2y0​ ，从而得到测试输入{x=2, y=1}和新约束 (x0=2y0)∧(x0≤y0+10)(x_0=2y_0) \\wedge (x_0 \\leq y_0 + 10)(x0​=2y0​)∧(x0​≤y0​+10) ；第二次取反得到 (x0=2y0)∧(x0&gt;y0+10)(x_0=2y_0) \\wedge (x_0 &gt; y_0 + 10)(x0​=2y0​)∧(x0​&gt;y0​+10) ，从而求解出测试输入{x=30, y=15}。\nCristian Cadar在2006年发表EXE，以及2008年发表EXE的改进版本KLEE，对上述Concolic执行的方法做了进一步优化。其创新点主要是在实际状态和符号状态之间进行区分，称之为执行生成的测试（Execution-Generated Testing），简称EGT。这个方法在每次运算前动态检查值是不是都是实际的，如果都是实际的值，那么运算就原样执行，否则，如果至少有一个值是符号化的，运算就会通过更新当前路径的条件符号化地进行。例如，对于我们的例子程序，第17行把y=sym_input()改变成y=10，那么第6行就会用实际参数10去调用函数twice，并实际执行。然后第7行变成if(20==x)，符号执行若走then路径，则加入约束x=20；对条件进行取反就可以走else路径，约束是x≠20。在then路径上，第8行变成if(x&gt;20)，那么该if的then路径就不能走了，因为此时有约束x=20。简言之，EGT本质上还是将实际执行与符号执行相结合，通过路径取反探索所有可能路径。\n正是因为concolic执行的出现，让传统静态符号执行遇到的很多问题能够得到解决——那些符号执行不好处理的部分、求解器无法求解的部分，用实际值替换就好了。使用实际值，可以让因外部代码交互和约束求解超时造成的不精确大大降低，但付出的代价就是，会有丢失路径的缺陷，牺牲了路径探索的完全性。我们举一个例子来说明这一点。假设我们原始例子程序做了改动，即把twice函数的定义改为返回(v*v)%50。假设执行从随机输入{x=22, y=7}开始，if(x==z)条件取反生成路径约束 x0≠(y0y0)mod50x_0 \\neq (y_0y_0)mod50x0​​=(y0​y0​)mod50 。因为约束求解器无法求解非线性约束，所以concolic执行的应对方法是，把符号值用实际值替换，此处会把 y0y_0y0​ 的值替换为7，这就将程序约束简化为 x0≠49x_0 \\neq 49x0​​=49 。通过求解这个约束，可以得到输入{x=49, y=7}，走到一个此前没有走到的路径。传统静态符号执行是无法做到这一步的。但是，在这个例子中，我们无法生成路径true-false的输入，即约束 (x0≠(y0y0)mod50)∧(x0≤y0+10)(x_0 \\neq (y_0y_0)mod50) \\wedge (x_0 \\leq y_0 + 10)(x0​​=(y0​y0​)mod50)∧(x0​≤y0​+10)  ，因为 y0y_0y0​ 的值已经实际化了，这就造成了丢失路径的问题，造成不完全性。\n然而总的来说，Concolic执行的方法是非常实用的，有效解决了遇到不支持的运算以及应用与外界交互的问题。比如调用库函数和系统调用的情况下，因为库和系统调用无法插桩，所以这些函数相关的返回值会被实际化，以此获得简化的程序约束。\n 0x3 面临挑战&amp;解决方案\n符号执行曾经遇到过很多问题，使其难以应用在真实的程序分析中。经过研究者的不懈努力，这些问题多多少少得到了解决，由此也产生了一大批优秀的学术论文。这一部分将简单介绍其中的一些关键挑战以及对应的解决方案。\n 1.路径爆炸(Path Explosion)\n由于在每一个条件分支都会产生两个不同约束，符号执行要探索的执行路径依分支数指数增长。在时间和资源有限的情况下，应该对最相关的路径进行探索，这就涉及到了路径选择的问题。通过路径选择的方法缓解指数爆炸问题，主要有两种方法：\n1）使用启发式函数对路径进行搜索，目的是先探索最值得探索的路径；\n2）使用一些可靠的程序分析技术减少路径探索的复杂性。\n启发式搜索是一种路径搜索策略，比深度优先或者宽度优先要更先进一些。大多数启发式的主要目标在于获得较高的语句和分支的覆盖率，不过也有可能用于其他优化目的。最简单的启发式大概是随机探索的启发式，即在两边都可行的符号化分支随机选择走哪一边。还有一个方法是，使用静态控制流图（CFG）来指导路径选择，尽量选择与未覆盖指令最接近的路径。另一个方法是符号执行与进化搜索相结合，其fitness function用来指导输入空间的搜索，其关键就在于fitness function的定义，例如利用从动态或静态分析中得到的实际状态(concrete state)信息或者符号信息来提升fitness function。\n用程序分析和软件验证的思路去减少路径探索的复杂性，也是一种缓解路径爆炸问题的方式。一个简单的方法是，通过静态融合减少需要探索的路径，具体说来就是使用select表达式直接传递给约束求解器，但实际上是将路径选择的复杂性传递给了求解器，对求解器提出了更高的要求。还有一种思路是重用，即通过缓存等方式存储函数摘要，可以将底层函数的计算结果重用到高级函数中，不需要重复计算，减小分析的复杂性。还有一种方法是剪枝冗余路径，RWset技术的关键思路就是，如果程序路径与此前探索过的路径在同样符号约束的情况下到达相同的程序点，那么这条路径就会从该点继续执行，所以可以被丢弃。\n 2.约束求解(Constraint Solving.)\n符号执行在2005年之后的突然重新流行，一大部分原因是因为求解器能力的提升，能够求解复杂的路径约束。但是约束求解在某种程度上依然是符号执行的关键瓶颈之一，也就是说符号执行所需求的约束求解能力超出了当前约束求解器的能力。所以，实现约束求解优化就变得十分重要。这里主要介绍两种优化方法：不相关约束消除，增量求解。\n a.不相关约束消除(Irrelevant constraint elimination)\n在符号执行的约束生成过程中，尤其是在concolic执行过程中，通常会通过条件取反的方式增加约束，一个已知路径约束的分支谓词会取反，然后由此产生的约束集会检查可满足性以识别另一条路径是否可行。一个很重要的现象是，一个程序分支通常只依赖一小部分程序变量，所以我们可以尝试从当前路径条件中移除与决定当前分支结果不相关的约束。例如，当前的路径条件是 (x+y&gt;10)∧(z&gt;0)∧(y&lt;12)∧(z−x=0)(x+y&gt;10) \\wedge (z&gt;0) \\wedge (y&lt;12) \\wedge (z−x=0)(x+y&gt;10)∧(z&gt;0)∧(y&lt;12)∧(z−x=0) 假如我们想对y&lt;12这个分支条件探索新路径，可以通过约束 (x+y&gt;10)∧(z&gt;0)∧¬(y&lt;12)(x+y&gt;10) \\wedge (z&gt;0) \\wedge \\neg (y&lt;12)(x+y&gt;10)∧(z&gt;0)∧¬(y&lt;12) 求解新的输入， ¬(y&lt;12)\\neg (y&lt;12)¬(y&lt;12) 是取反的条件分支，我们可以去掉对z的约束，是因为z对 ¬(y&lt;12)\\neg (y&lt;12)¬(y&lt;12) 的分支是不会有影响的。减小的约束集会给出x和y的新值，我们用此前执行的z值就可以生成新输入了。如果更形式化地说，算法会计算在取反条件所依赖的所有约束的传递闭包。\n（其实沃觉得是有影响的，x=z，z&gt;0，那不就是x&gt;0吗，如果去掉x-z=0，相当于去掉x&gt;0，x会通过第一项约束条件影响y啊，不是吗？所以这个栗子可能不恰当，这里理解不相关约束消除的概念就行了，如有不同意见欢迎指正）\n b.增量求解(Incremental solving)\n本质上也是利用重用的思想。符号执行中生成的约束集有一个重要特性，就是表示为程序源代码中的静态分支的固定集合。所以，很多路径有相似的约束集，可以有相似的解决方案。通过重用以前相似请求的结果，可以利用这种特性来提升约束求解的速度，这种方法在CUTE和KLEE中都有实现。举个例子来说明，在KLEE中，所有的请求结果都保存在缓存中，该缓存将约束集映射到实际变量赋值。例如，缓存中的一个映射可能是：\n(x+y&lt;10)∧(x&gt;5)⇒{x=6,y=3}(x+y&lt;10) \\wedge (x&gt;5) \\Rightarrow \\lbrace x=6,y=3 \\rbrace\n(x+y&lt;10)∧(x&gt;5)⇒{x=6,y=3}\n使用这些映射，KLEE可以迅速解答一些相似的请求类型，包括已经缓存的约束集的子集和超集。比如对于请求(x+y&lt;10)∧(x&gt;5)∧(y≥0)(x+y&lt;10) \\wedge (x&gt;5) \\wedge (y \\geq 0)(x+y&lt;10)∧(x&gt;5)∧(y≥0) ，KLEE可以迅速检查{x=6,y=3}是一个可行的答案。这样就可以让求解过程加快很多。\n 3.内存建模(Memory Modeling)\n程序语句转换为符号约束的精度对符号执行能实现的覆盖率以及约束求解的可伸缩性有很大影响。例如，使用数学整数(actual mathematical integers)去近似去代替固定宽度的整数变量，这样的内存模型可能会更有效率，但另一方面，根据诸如算术溢出之类的极端情况，可能导致代码分析不精确-这可能会导致符号执行遗漏路径或探索不可行的路径。\n另外一个问题是指针。在访问内存的时候，内存地址（指针）用来引用一个内存单元，当这个地址的引用来自于用户输入时，内存地址就成为了一个表达式。当符号化执行时，我们必须决定什么时候将这个内存的引用进行实际化。一个可靠的策略是，考虑为从任何可能满足赋值的加载，但这个可能值的空间很大，如果实际化不够精确，会造成代码分析的不精确。\n还有一个是别名问题，即地址别名导致两个内存运算引用同一个地址，比较好的方法是进行别名分析，事先推理两个引用是否指向相同的地址，但这个步骤要静态分析完成。KLEE使用了别名分析和让SMT考虑别名问题的混合方法。而DART和CUTE压根没解决这个问题，只处理线性约束的公式，不能处理一般的符号化引用。\n符号化跳转也是一个问题，主要是switch这样的语句，常用跳转表实现，跳转的目标是一个表达式而不是实际值。以往的工作用三种处理方法。\n1）使用concolic执行中的实际化策略，一旦跳转目标在实际执行中被执行，就可以将符号执行转向这个实际路径，但缺陷是实际化导致很难探索完全的状态空间，只能探索已知的跳转目标。\n2）使用SMT求解器。当我们到达符号跳转时，假设路径谓词为 Π\\PiΠ ，跳转到e，我们可以让SMT求解器找到符合 Π∧e\\Pi \\wedge eΠ∧e 的答案。但是这种方案相比其他方案效率会低很多。\n3）使用静态分析，推理整个程序，定位可能的跳转目标。\n实际中，源代码的间接跳转分析主要是指针分析。二进制的跳转静态分析推理在跳转目标表达式中哪些值可能被引用。例如，函数指针表通常实现为可能的跳转目标表。\n 4.处理并发(Handling Concurrency)\n大型程序通常是并发的。因为这种程序的内在特性，动态符号执行系统可以被用来高效地测试并发程序，包括复杂数据输入的应用、分布式系统以及GPGPU程序。\n 0x4 发展脉络(至2017)\n本节仍然摘录自r1ce的文章，主要是以时间的顺序梳理符号执行技术的发展脉络，同时对值得关注的项目和论文做一个小小总结和推荐。\n符号执行最初提出是在70年代中期，主要描述的是静态符合执行的原理，到了2005年左右突然开始重新流行，是因为引入了一些新的技术让符号执行更加实用。Concolic执行的提出让符号执行真正成为可实用的程序分析技术，并且大量用于软件测试、逆向工程等领域。在2005年作用涌现出很多工作，如DART[5]、CUTE[6]、EGT/EXE[7]、CREST[8]等等，但真正值得关注和细读的，应该是2008年Cristian Cadar开发的KLEE[3]。KLEE可以说是源代码符号执行的经典作品，又是开源的，后来的许多优秀的符号执行工具都是建立在KLEE的基础上，因此我认为研究符号执行，KLEE的文章是必读的。\n基于二进制的符号执行工具则是2009年EPFL的George Candea团队开发的S2E[9]最为著名，其开创了选择符号执行这种方式。2012年CMU的David Brumley团队提出的Mayhem[10]则采用了混合offline和online的执行方式。2008年UC Berkeldy的Dawn Song团队提出的BitBlaze[11]二进制分析平台中的Rudder模块使用了online的执行方式，也值得一看。总之，基于二进制的符号执行工作了解这三个就足够了。其中，S2E有开源的一个版本，非常值得仔细研究。最近比较火的angr[12]，是一个基于Python实现的二进制分析平台，完全开源且还在不断更新，其中也实现了多种不同的符号执行策略。\n在优化技术上，近几年的两个工作比较值得一看其一是2014年David Brumley团队提出的路径融合方法，叫做Veritesting[13]，是比较重要的工作之一，angr中也实现了这种符号执行方式。另一个是2015年Stanford的Dawson Engler（这可是Cristian Cadar的老师）团队提出的Under-Constrained Symbolic Execution[14]。\n另外，近年流行的符号执行与fuzzing技术相结合以提升挖掘漏洞效率，其实早在DART和2012年微软的SAGE[15]工作中就已经有用到这种思想，但这两年真正火起来是2016年UCSB的Shellphish团队发表的Driller[16]论文，称作符号辅助的fuzzing（symbolic-assisted fuzzing），也非常值得一看。\n 0x5 总结\n符号执行已是一种有效的程序测试技术，它提供了一种自动生成&quot;触发软件错误的输入&quot;的方法，这些错误从底层程序崩溃到高层语义特性不等。符号执行技术可以用于测试例的自动生成，也可以用于源代码安全性的检测。这两项工作的成效都十分依赖于约束求解器的性能，同时还受硬件设备处理能力的影响。\n虽然需要更多的研究来提高符号执行的代码覆盖率，测试精度，分析效率等等指标，但是事实证明，现有工具可以有效地测试和发现各种软件中的错误，从低级网络和操作系统代码到高级应用程序代码，不一而足。\n从实用性的角度来说：\n（2018年）现有的符号执行工具，在开源方面，主要还是基于KLEE项目的。可见对于KLEE项目的深入理解，将有助于我们打造更加高效的工具。有了高效的工具，就能够使得我们一边学习理论，一遍验证，从而走上高速公路。Inception工具是就ARM架构，而对于路由器中常使用的MIPS架构，现在应该还尚未有类似的符号执行工具发布（如果已经由类似工具，欢迎读者留言）。其中，基于IDA的脚本工具bugscam，经过揭秘路由器0DAY漏洞的作者修改之后，也能够支持分析MIPS架构的漏洞了。然而，其误报率非常之高，该工具报告的漏洞基本都不可用。因此，如何基于上述符号执行的思想，结合IDA工具中强大的反汇编能力，开发也具有符号执行功能的MIPS架构漏洞分析工具，相信也是非常有价值的。\n 0x6 To Learn More\n摘自r1ce的文章：\n对于符号执行入门，有两篇文章可以参考。其一是2010年David Brumley团队在S&amp;P会议上发表的《All You Ever Wanted to Know about Dynamic Taint Analysis and Forward Symbolic Execution (but Might Have Been Afraid to Ask)》[1]。这篇文章同时介绍了动态污点分析和前向符号执行的基本概念，作者构造了一种简化的中间语言，用来形式化地描述这两种程序分析技术的根本原理。其二是2011年Cristian Cadar发表在ACM通讯上的一篇短文《Symbolic execution for software testing: three decades later》[2]，以较为通俗的语言和简单的例子阐述了符号执行的基本原理，并介绍了符号执行技术的发展历程和面临挑战。\n其实这两篇文章的作者都是二进制分析领域大名鼎鼎的人物，卡内基梅隆的David Brumley是AEG的提出者，其带领团队是DARPA CGC比赛的第一名；英国帝国理工的Cristian Cadar则是符号执行引擎KLEE[3]的作者——KLEE在符号执行领域的地位不言而喻。这两篇文章各有千秋：前者更加学术化一些，用中间语言进行的形式化描述有些晦涩难懂，但对于进一步研究符号执行引擎的源码很有帮助；后者则更通俗一些，有助于初学者的理解，且对于符号执行的发展脉络有更多的介绍。\nQ&amp;A:\nA:如何通俗易懂的总结符号执行技术？\nQ:符号执行是一种白盒静态代码分析技术，用符号表示程序的输入，根据分析程序语义得到的约束条件，去求解能得到你所希望的程序输出所需要的输入，的一种方法。更容易理解的说法可以参开0x1第一段。\nQ:路径约束PC到底长什么样子？\nA:一长串约束条件，比如0x2中的示例： (∧i∈[1,n]Ni&gt;0)∧(Nn+1≤10)(\\wedge_{i \\in [1,n]}N_i &gt; 0) \\wedge (N_{n+1} \\leq 10)(∧i∈[1,n]​Ni​&gt;0)∧(Nn+1​≤10)\nQ:约束求解器是怎么运作的？\nA:\nQ:Concolic执行的出现，让传统静态符号执行遇到的很多问题能够得到解决（解决了哪些问题？怎么解决的？）\nA:Concolic执行的方法有效解决了遇到不支持约束求解的运算以及应用与外界交互的问题；具体方法就是，那些传统静态符号执行不好处理的部分、求解器无法求解的部分，用实际值替换就好了\nQ:Concolic执行和传统静态符号执行哪里不同？\nA:我的理解是，1.传统静态符号执行使用符号表示变量，比如符号状态 σ\\sigmaσ 中存在变量x的映射x→x0x \\rightarrow x_0x→x0​，2.使用约束求解器判定哪条分支可行，并根据预先设计的路径调度策略实现对该过程所有路径的遍历分析，最后输出每条可执行路径的分析结果，即每条路径对应的输入和输出；\n而Concolic执行来说，1.它使用具体数值作为输入来模拟执行程序代码，2.从当前路径的分支语句的谓词中搜集所有符号约束。然后修改该符号约束内容构造出一条新的可行的路径约束，并用约束求解器求解出一个可行的新的具体输入，接着符号执行引擎对新输入值进行一轮新的分析。通过使用这种输入迭代产生变种输入的方法分析程序。\n参考文章：\n\nK0rz3n：https://www.k0rz3n.com/2019/02/28/简单理解符号执行技术/\nr1ce：https://zhuanlan.zhihu.com/p/26927127\n符号执行——从入门到上高速：https://www.anquanke.com/post/id/157928\nhttp://pwn4.fun/2017/03/20/符号执行基础/\n[2]* Symbolic execution for software testing: three decades later：https://people.eecs.berkeley.edu/~ksen/papers/cacm13.pdf\n[1] Schwartz E J, Avgerinos T, Brumley D. All You Ever Wanted to Know about Dynamic Taint Analysis and Forward Symbolic Execution (but Might Have Been Afraid to Ask) [C]// Security &amp; Privacy. DBLP, 2010:317-331.\n[2] Cadar C, Sen K. Symbolic execution for software testing: three decades later[M]. ACM, 2013.\n[3] C. Cadar, D. Dunbar, and D. Engler. KLEE: Unassisted and Automatic Generation of High-Coverage Tests for Complex Systems Programs. In Proceedings of the 8th USENIX Symposium on Operating Systems Design and Implementation (OSDI’08), volume 8, pages 209–224, 2008.\n[4] R. S. Boyer, B. Elspas, and K. N. Levitt. SELECT – a formal system for testing and debugging programs by symbolic execution. SIGPLAN Not., 10:234–245, 1975.\n[5] P. Godefroid, N. Klarlund, and K. Sen. DART: Directed Automated Random Testing. In PLDI’05, June 2005.\n[6] K. Sen, D. Marinov, and G. Agha. CUTE: A concolic unit testing engine for C. In ESEC/FSE’05, Sep 2005.\n[7] C. Cadar, V. Ganesh, P. M. Pawlowski, D. L. Dill, and D. R. Engler. EXE: Automatically Generating Inputs of Death. In Proceedings of the 13th ACM Conference on Computer and Communications Security, pages 322–335, 2006.\n[8] J. Burnim and K.Sen,“Heuristics for scalable dynamic test generation,” in Proc. 23rd IEEE/ACM Int. Conf. Autom. Software Engin., 2008, pp. 443–446.\n[9] V. Chipounov, V. Georgescu, C. Zamfir, and G. Candea. Selective Symbolic Execution. In Proceedings of the 5th Workshop on Hot Topics in System Dependability, 2009.\n[10] S. K. Cha, T. Avgerinos, A. Rebert, and D. Brumley. Unleashing Mayhem on Binary Code. In Proceedings of the IEEE Symposium on Security and Privacy, pages 380–394, 2012.\n[11] Song D, Brumley D, Yin H, et al. BitBlaze: A New Approach to Computer Security via Binary Analysis[C]// Information Systems Security, International Conference, Iciss 2008, Hyderabad, India, December 16-20, 2008. Proceedings. DBLP, 2008:1-25.\n[12] Yan S, Wang R, Salls C, et al. SOK: (State of) The Art of War: Offensive Techniques in Binary Analysis[C]// Security and Privacy. IEEE, 2016:138-157.\n[13] T. Avgerinos, A. Rebert, S. K. Cha, and D. Brumley. Enhancing Symbolic Execution with Veritesting. pages 1083–1094, 2014.\n[14] D. a. Ramos and D. Engler. Under-Constrained Symbolic Execution: Correctness Checking for Real Code. In Proceedings of the 24th USENIX Security Symposium, pages 49–64, 2015.\n[15] P. Godefroid, M. Y. Levin, and D. Molnar. SAGE: Whitebox Fuzzing for Security Testing. ACM Queue, 10(1):20, 2012.\n[16] N. Stephens, J. Grosen, C. Salls, A. Dutcher, R. Wang, J. Corbetta, Y. Shoshitaishvili, C. Kruegel, and G. Vigna. Driller: Augmenting Fuzzing Through Selective Symbolic Execution. In Proceedings of the Network and Distributed System Security Symposium, 2016.\n\n","categories":["笔记"],"tags":["符号执行","总结","学习"]},{"title":"Flow-based Generative Model","url":"https://ch3nye.top/Flow-based-Generative-Model/","content":" Flow-based Generative Model\n本节要讲一个新的生成模型，一个新的Generative 技术，它和GAN的目的相同，但是不如GAN有名，而且实际上也没有胜过GAN，但是这个方法有比较新颖的思想所以还是讲一讲。这个方法叫做Flow-based Generative Model，这里的Flow就是流的意思，后面会解释为什么说它是流。\n\n\nLink: https://youtu.be/YNUek8ioAJk\nLink: https://youtu.be/8zomhgKrsmQ\n\n上图是从以前的课程中截取出来的，如果你有看过以前的课程（链接如上）你就能知道这里的三个方法的具体细节。我们之前说Generative Model 有三种，第一种Component-by-component ，上课的时候我们以生成宝可梦图片为例子，以像素为component 一个像素一个像素的生成，这种方法更常见的名字是Autoregressive Model。我们也讲过Variational Autoencoder（VAE），和Generative Adversarial Network（GAN），上述三种是我们之前见过的生成模型，今天我们要介绍第四种Flow-based Generative Model 。\n 过去的Generative Models 的缺点\n\n我们之前介绍的生成模型都有各自的问题，如上图所示。\nAuto-regressive Model 是一个一个component 地生成的，所以component 的最佳生成顺序是难以求解的。比如说生成图像的例子中，我们就是一个一个pixel 地，从左上角开始一行一行地生成。当然，还有一些任务的生成目标就是有顺序的，比如说生成语音，后面的component 依赖前面的component ，这样的目标比较适合用Auto-regressive Model ，但是现在这个模型太慢了，如果你要生成一秒像是人类说的语音你需要做90分钟合成，这显然是不实用的。\nVariational Auto-encoder 介绍这个模型的时候我们证明了，VAE optimize的对象是likelihood 的lower bound，它不是去maximize 我们要它maximize 的likelihood 而是去maximize likelihood 的lower bound，我们不知道这个lower bound 和真正想要maximize 的对象的差距到底有多大。Flow-base generative model 会解决这个问题，它并不会用approximation，并不是optimize lower bound 而是optimize probability 的本体，后面我们会介绍flow 是怎么做到这件事的。\nGenerative Adversarial Network 是现在做生成得到的目标质量最好的模型。但是我们都知道GAN是很难train的，因为你的Generative 和Discriminator 目标是不一致的，很容易train崩掉。\n Generator 的内涵\n\n我们抛开具体模型，先来看看Generator 的定义。Generator 是一个网络，该网络定义了一个可能性分布pGp_GpG​ 。怎么理解这句话呢，看上图，有一个Generator GGG 输入一个 zzz 输出一个 xxx ，如果现在是在做人脸生成，  xxx 是一张人脸图片，可以看成一个高维向量其中每一个element 就是图片的一个pixel， zzz 我们通常假设它是从一个简单的空间中sample 出来的，比如说Normal Distribution，虽然 zzz 是从一个简单的分布中sample 出来的，但是通过 GGG 以后 xxx 可能会形成一个非常复杂的分布，这个Distribution 我们用 PGP_GPG​ 来表示。对我们而言我们会希望通过 GGG 得到的Distribution 能和real data 也就是真实人脸数据集的Distribution PdataP_dataPd​ata 尽可能相同。\n那怎么能让 PGP_GPG​ 和 PdataP_dataPd​ata 尽可能相同呢，常见的做法是我们训练G的时候训练目标定为maximize likelihood ，讲的具体一点就是从人脸图片数据集中sample 出m笔data $\\lbrace x1,x2,…,x^m\\rbrace $ ，然后你就是希望这m张图片是从 PGP_GPG​ 这个分布中sample 出来的概率越大越好。就是maximize 上图中左下角的公式，这就是我们熟知的maximize likelihood。如果你难以理解为什么要让产生这些data 的概率越大越好的话，老师提供了一个更直观的解释：maximize likelihood 等同于minimize  PGP_GPG​ 和 PdataP_dataPd​ata 的K-L变换。Ref: https://youtu.be/DMA4MrNieWo\n总而言之就是让 PGP_GPG​ 和 PdataP_dataPd​ata 这两个分布尽可能相同。\nFlow 这个模型有什么厉害的地方呢？Generator 是个一个网络，所以 PGP_GPG​ 显然非常复杂，一般我们不知道怎么optimize objective function，而Flow 可以直接optimize objective function，可以直接maximize likelihood。\n Flow 的数学基础\nmath warning\n\n这可能也是Flow-based Generative Model 不太出名的一个原因，其他生成模型都比较容易理解，而此模型用到了比较多的数学知识。你要知道上图中的三个概念：Jacobian、Determinant、Change of Variable Theorem 。\n Jacobian Matrix\n\n把 fff 看作生成模型，zzz 就是输入，xxx 就是生成的输出。这里我们的栗子中输入输出的维度是相同的，实际做的时候往往是不同的。Jacobian Matrix 记为 JfJ_fJf​ ，就定义为输出和输入向量中element 两两组合做偏微分组成的矩阵，如上图左下角所示，输出作为列，输入作为行， JfJ_fJf​ 每个element 都是输出对输入做偏微分。 Jf−1J_{f^{-1}}Jf−1​ 的定义同上只是变成了输入对输出的偏微分。\n举一个栗子，如上图右侧，自己看一下就好，就不赘述了。需要强调一下的是 JfJf−1J_f J_{f^{-1}}Jf​Jf−1​ 得到单位矩阵，function 之间有inverse的关系，得到的Jacobian Matrix 也有inverse的关系。\n Determinant\n\nDeterminant ：方阵的行列式是一个标量，它提供有关方阵的信息。你只要记得上图左下角行列式的性质，矩阵的det和矩阵逆的det互为倒数。\n\n矩阵的行列式的绝对值可以表示行列式每一行表示的向量围成的空间的&quot;面积&quot;（3维空间中是体积，更高维空间中也是类似的概念）。或者你可以想象它是矩阵所表示的线性变换前后的“面积”放缩比例。\n💖关于行列式的含义你可以参考3Blue1Brown的线性代数的本质05行列式 。\n Change of Variable Theorem\nDeterminant 就是为了解释Change of Variable Theorem\n\n假设我们有一个分布 π(z)\\pi(z)π(z) ，zzz 带入 fff 会得到 xxx ， xxx 形成了分布 p(x)p(x)p(x) ，我们现在想知道 π(z)\\pi(z)π(z) 和  p(x)p(x)p(x) 之间的关系。如果我们可以写出两者之间的关系，就可以分析一个Generator 。\n两者之间的关系能写出来吗？是可以的。我们现在要问的问题是：假设说分布 π(z)\\pi(z)π(z) 上有一个 z′z&#x27;z′ 对应 π(z′)\\pi(z&#x27;)π(z′) ， z′z&#x27;z′ 通过 fff 得到 x′x&#x27;x′ 对应 p(x′)p(x&#x27;)p(x′) ，现在想知道 π(z′)\\pi(z&#x27;)π(z′) 和 p(x′)p(x&#x27;)p(x′) 之间的关系是什么？\n举一个简单的栗子来说明上述问题：\n\n假设说两个分布如上图所示，我们知道probability density function 的积分等于1，所以两者的高度分别为1和12\\frac{1}{2}21​，假设 xxx 和 zzz 的关系是 x=2z+1x=2z+1x=2z+1 ，底就变成原来的两倍，高度就变成原来的二分之一。现在从 π(z′)\\pi(z&#x27;)π(z′) 到 p(x′)p(x&#x27;)p(x′) 中间的关系很直觉就是二分之一的关系。\n那我们再来看一个更general 的case：\n\n两个分布如上图浅蓝色和浅绿色线所示，现在我们不知道这两者的Distribution，假设我们知道 xxx 和 zzz 的关系即知道 fff ，那我们就可以求出 π(z′)\\pi(z&#x27;)π(z′) 和 p(x′)p(x&#x27;)p(x′) 之间的关系，这就是Change of Variable Theorem 的概念。\n我们把 z′z&#x27;z′ 加一个 ΔzΔzΔz 映射到 x′+Δxx&#x27;+Δxx′+Δx ，此时上图中蓝色方块和绿色方块需要有同样的面积，所以有上图右侧所示公式。\np(x′)=π(z′)∣dzdx∣p(x&#x27;) = \\pi(z&#x27;)\\left|\\frac{dz}{dx}\\right|\np(x′)=π(z′)∣∣∣∣​dxdz​∣∣∣∣​\n这个绝对值是因为有时会出现上图左下角的情况，通过 fff 变换后，形成了交叉。\n我们在举一个 xxx 和 zzz 都是二维的栗子：\n\n我们在 zzz 的分布中取一个小小的矩形区域高是 Δz2Δz_2Δz2​ 宽是 Δz1Δz_1Δz1​ ，它通过一个function fff 投影到 xxx 的分布中变成一个菱形。我们用向量 [Δx11,Δx21][Δx_{11}, Δx_{21}][Δx11​,Δx21​] 表示菱形的右下边，用向量[Δx22,Δx12][Δx_{22}, Δx_{12}][Δx22​,Δx12​] 表示菱形的左上边。根据上面Determinant 一节中提到的知识点，我们可以用这两个向量组成的矩阵的determinant 表示该矩阵的面积。π(z′)\\pi(z&#x27;)π(z′) 作为正方形为底的三维形体的高， p(x′)p(x&#x27;)p(x′) 作为以绿色菱形为底的三维形体的高，这两个形体的积分值相等，所以得到了如上图所示的公式。\n另外再讲一下四个Δ值的意思：\nΔx11Δx_{11}Δx11​是z1z_1z1​（维度上）改变的时候x1x_1x1​（维度上）的变化量\nΔx21Δx_{21}Δx21​是z1z_1z1​改变的时候x2x_2x2​的变化量\nΔx12Δx_{12}Δx12​是z2z_2z2​改变的时候x1x_1x1​的变化量\nΔx22Δx_{22}Δx22​是z2z_2z2​改变的时候x2x_2x2​的变化量\n 整理公式得出结论\n\n接下来就是做一波整理，相信大家都看得懂（看不懂就重学一下线代😜），就不赘述了。\n最终的结论就是上图左下角的公式：\n总而言之， π(z′)\\pi(z&#x27;)π(z′) 乘上 ∣det(Jf−1)∣\\left| det(J_{f^{-1}}) \\right|∣∣​det(Jf−1​)∣∣​ 就得到 p(x′)p(x&#x27;)p(x′) ，这就是 π(z′)\\pi(z&#x27;)π(z′) 和 p(x′)p(x&#x27;)p(x′) 的关系。\n如果你没看太懂数学推导，你就记住这个结论就好， π(z′)\\pi(z&#x27;)π(z′) 和 p(x′)p(x&#x27;)p(x′) 之间就是差一个 ∣det(Jf−1)∣\\left| det(J_{f^{-1}}) \\right|∣∣​det(Jf−1​)∣∣​ 。\nend of math warning\n Flow-based Model\n接下来我们就正式进入Flow的部分，我们先回到Generation 上。\n\n一个Generator 是怎么训练出来的呢，Generator 训练的目标就是要maximize PG(xi)P_G(x^i)PG​(xi) ，xix^ixi 是从real data 中sample 出来的一笔data。那 PG(xi)P_G(x^i)PG​(xi) 长什么样子呢，有了前面的Change of Variable Theorem 的公式我们就可以把 PG(xi)P_G(x^i)PG​(xi) 写出来了：\nPG(xi)=π(zi)∣det(JG−1)∣P_G(x^i) = \\pi(z^i)\\left|det(J_{G^{-1}})\\right|\nPG​(xi)=π(zi)∣det(JG−1​)∣\n而 ziz^izi 就是把xix^ixi 带入GGG inverse 以后的结果。\n我们在给上式加一个log，把公式右侧的乘转为加。现在我们知道 log(PG(xi))log(P_G(x^i))log(PG​(xi)) 长什么样子了，我们只要能maximize 这个式子我们就可以train Generator 就结束了，但是我们要想maximize 这个式子需要有一些前提：\n\n要知道如何计算 det(JG)det(J_G)det(JG​)\n要知道 G−1G^{-1}G−1\n\n要做Generator 的Jacobian 其实还比较好做，你只要知道如何计算∂x/∂z\\partial{x}/\\partial{z}∂x/∂z ，但是问题是这个Jacobian Matrix 可能会很大，比如说 xxx 和 zzz 都有1000维，那你的Jacobian Matrix 就是一个1000*1000的矩阵，你要算它的det，计算量是非常大的，所以我们要好好设计Generator ，让它的Jacobian Matrix 的det 好算一些。\n另一个前提是：我们要把 ziz^izi 变成 xix^ixi ，就要知道G−1G^{-1}G−1 。我们同时也要让det(JG−1)det(J_{G^{-1}})det(JG−1​) 变得容易算。为了确保G是invertible，我们设计的Generator 的时候 zzz 和 xxx 的维度是相同的。 zzz 和 xxx 的维度相同不能保证G一定是invertible，但是如果不相同G一定不是invertible。\n所以，到这里我们就能看出来，Flow-based Model 和GAN 不一样，GAN是低维生成高维，但是Flow这个技术中输入输出的维度必须相同，这一点是有点奇怪的。另外，为了保证G是可逆的，我们要限制G的架构，这样的做法必然会限制整个生成模型的能力。\n\n既然一个G不够强，你就多加一些。最终maximize的目标就是上图最下面的公式。\nG∗=argmaxG∑imlog(pK(xi))G^* = arg\\mathop{max}\\limits_{G}\\sum_{i}^{m}log(p_K(x^i))\nG∗=argGmax​i∑m​log(pK​(xi))\n 实作上怎么做Flow-based Model\n\n我们先考虑一个G，在公式中其实只有出现 G−1G^{-1}G−1 ，所以实际上我们在训练的时候我们训练的是 G−1G^{-1}G−1 ，在生成的时候把 G−1G^{-1}G−1 倒过来用 GGG 生成目标。在训练的时候，从real data 中sample 一些数据出来，输入 G−1G^{-1}G−1 中得到 ziz^izi 。\n看一下上图中的公式，我们的目标是maximize 它，这个公式有两项，先看第一项。因为 π\\piπ 是一个normal distribution，要这一项的值最大只要让 G−1(xi)G^{-1}(x^i)G−1(xi) 是零向量就好了，但是显然这么做会出大问题，如果 ziz^izi 变0，JG−1J_{G^{-1}}JG−1​ 将是一个零矩阵，则 det(JG−1)=0det(J_{G^{-1}}) = 0det(JG−1​)=0 。此时第二项，就是负无穷。\n为了maximize log(PG(xi))log(P_G(x^i))log(PG​(xi)) ，如上图所示，一方面前一项要让 G−1(xi)G^{-1}(x^i)G−1(xi) 向原点靠拢，一方面第二项又会对 G−1G^{-1}G−1 做一些限制，让它不会把所有的 zzz mapping 到零向量。\n这就是我们在实作Flow-based Model 时所做的事情。\n Coupling Layer\ncoupling layer 是一个实用的G，NICE 和Real NVP 这两个Flow-based Model 都用到了coupling layer ，接下来将介绍这个coupling layer 的具体做法。\n\n\nNICE\nhttps://arxiv.org/abs/1410.8516\nReal NVP\nhttps://arxiv.org/abs/1605.08803\n\n如上图所示，输入和输出都是D维向量，然后把输入输出的向量拆成两部分，前d维为一组，后D-d维为一组。对于上面的第一部分直接通过copy z得到x。对于第二部分，我们将第一部分通过两个变换F和H（这两个function 可以任意设计，多复杂都可以）得到向量β和γ，再将第二部分和β做内积再加上γ，就得到第二部分的x。\n接下来要讲怎么取coupling layer 的inverse。就是说在不知道z，只有后面的x，这样的情况下怎么把z找出来呢？\n\n对于第一部分直接把x copy过去就可以了，对于第二部分，我们就把刚才得到的z的第一部分通过F和H得到β和γ，然后将x的第二部分通过 xi−γiβi\\frac{x_i-γ_i}{β_i}βi​xi​−γi​​ 就可以算出z ，结束。\n\n不只要能算G的inverse，还要算 JGJ_GJG​ ，计算方法就如上图所示。\n左上角显然是单位矩阵，因为x和z是copy的。\n右上角是浅蓝色对应深绿色的部分，上图右上角可以明显看出两者没有关系，偏微分结果为0.\n左下角我们并不关心了，因为现在矩阵的左上角是单位矩阵，右上角是零矩阵，整个矩阵的det就是右下角矩阵的det。\n右下角矩阵是Diagonal ，因为你想想看 zd+1z_{d+1}zd+1​ 只和 xd+1x_{d+1}xd+1​ 有关系，改行其他值都是0，同样的 zDz_{D}zD​ 只和 xDx_{D}xD​ 有关系，所以整个右下角矩阵就是一个对角矩阵。\n所以这这个矩阵的det：\ndet(JG)=∂xd+1∂zd+1∂xd+2∂zd+2...∂xD∂zD=βd+1βd+2...βDdet(J_G) =\\frac{\\partial{x_{d+1}}}{\\partial{z_{d+1}}}\\frac{\\partial{x_{d+2}}}{\\partial{z_{d+2}}}...\\frac{\\partial{x_D}}{\\partial{z_D}} \\\\\\\\\n=β_{d+1}β_{d+2}...β_{D}\ndet(JG​)=∂zd+1​∂xd+1​​∂zd+2​∂xd+2​​...∂zD​∂xD​​=βd+1​βd+2​...βD​\n stacking\n\n如果你是简单的把coupling layer 叠在一起，那你会发现最后生成的东西有一部分是noise，因为上面是这部分向量是直接一路copy到输出的。\n所以我们做一些手脚，把其中一些coupling layer 做一下反转，如上图下侧所示。\n\n我们再讲的更具体一点，如果说我们在做图像生成，有两种拆分向量的做法：一种是把横轴纵轴切分成多条，横轴纵轴的index之和是奇数就copy偶数就不copy做transform 。或者是，一张image 通常由rgb三个channel，你就其中某几个channel 做copy 某几个channel 做transform，每次copy 和transform 的channel 可能是不一样，你有很多层coupling layer 嘛。当然，你也可以把这两种做法混在一起用，有时用第一种拆分方法，有时候用第二种拆分方法。\n 1x1 Convolution\n\n\nGLOW\nhttps://arxiv.org/abs/1807.03039\n\n讲道理，Flow-based Generative Model 其实很早就被提出来了，上面说的coupling layer 做Generator 的方法中提到的NICE 和Real NVP 这两个Flow-based Model 都是14、15年左右被提出来了，算是很古老的东西了，为什么重提旧事呢，就是因为GLOW这个技术是去年（指2018年）被提出来了，这个技术的结果还是蛮惊人的。虽然，这个方法的结果还是没有打败GAN，但是你会惊叹&quot;我靠，不用GAN也可以做到这种程度啊！&quot;\nGLOW除了Coupling Layer，还有个神奇的layer 叫做1x1 Convolution，举例来说我们现在要产生图片，我们就用一个3x3的W矩阵，将z一个像素一个像素地转换为x 。W是learn 出来的，它很可能能够做到的事情是shuffle channel，如上图所示的栗子，就是把z的channel 做了交换。\n如果W是invertible ，那就很容易算出 W−1W^{-1}W−1 ，但是W是learn 出来的，这样的话它一定是invertible 的吗，文献中作者在initial 的时候就用了一个invertible matrix，他也许是期待initial invertible matrix，最后的结果也能是invertible matrix。但是实际上invertible matrix 是比较容易出现的，你随机设置一个matrix 大概率就是invertible ，你想想看只有det是0才是非invertible，所以实作上也没太大问题，老师讲如果你看到有文献解释或者考虑这件事情了记得call他，也记得call我，方便我在这里做一些补充。\n\n我们来举一个栗子，如上图所示，我们把一个像素 [x1,x2,x3][x_1,x_2,x_3][x1​,x2​,x3​] 乘上W，做一个 转换，求这个W的Jacobian Matrix 的公式如上图所示，你自己细算一下就会发现，JfJ_fJf​ 就是W。\n\n所以现在全部的z和x，做变换使用的Generator （也就是很dxd个W）的Jacobian Matrix 就是上图所示的对角矩阵，因为只有对角线上的元素对应的蓝色pixel 和绿色pixel 才是有关系的，其它地方的偏微分都是0。\n这个矩阵的det是什么呢，如果你线代够好的话，就知道这个值就是 (det(W))d×d(det(W))^{d \\times d}(det(W))d×d ，而W是3x3的，det很好算，所以大矩阵的det也就很好算出来了，结束。\n Demo of OpenAI (GLOW)\nFlow-based Model 有一个很知名的结果就是GLOW这个Model，OpenAI有做一个GLOW Model demo 的网站：\n\nhttps://openai.com/blog/glow/\n\n你可以做人脸的结合：\n\n可以把脸做种种变形，比如说让人笑起来：\n\n你需要先收集一堆笑的人脸、一堆不笑的人脸，把这两个集合的图片的z求出来分别取平均，然后相减的到 zsmilez_{smile}zsmile​ ，zsmilez_{smile}zsmile​ 就是从不笑到笑移动多少距离。然后，你只要往不笑的脸上加上一个 zsmilez_{smile}zsmile​ 通过G，就可以得到笑的脸了。\n To Learn More …\n\nGLOW 现在做的最多的就是语音合成。在语音合成任务上，不知道为什么用GAN做的结果都不是很好，Auto-regressive Model 就是一个一个sample 产生出来，结果是很好，但是计算太慢不太实用，所以现在都在用GLOW ，就留给大家自学了。\n","categories":["李宏毅机器学习笔记"],"tags":["ML","note","Next-Step-of-ML"]},{"title":"GAN(Quick Review)","url":"https://ch3nye.top/GAN(Quick-Review)/","content":" GAN(Quick Review)\n本节的目的就是帮大家复习一下GAN，如果你对GAN不是特别感兴趣，你就可以通过这节课了解本课程需要用到的GAN的知识。如果听完以后对GAN产生了浓厚的兴趣，你可以去看2018年的GAN的课程视频：https://youtube.com/playlist?list=PLJV_el3uVTsMq6JEFPW35BCiOQTsoqwNw\n Three Categories of GAN\n我们把GAN分为以下三种：\n\n 1.Typical GAN\n第一种经典的GAN就是训练一个Generator，这个生成器吃一个向量，生成一个目标对象。比如说，你要做二次元人脸图片的生成，你就collect 一大堆的动漫人脸图片，然后喂给Generator 去训练，Generator 就会学会生成二次元人脸的图像。\n那怎么训练这个Generator 呢，模型的架构是什么样子的呢？\n\n在经典的GAN方法中，我们会有一个Generator 和一个Discriminator ，这两者是交替的训练出来的，怎么交替训练后面叙述。我们继续以生成二次元人脸图像为例，Generator 就是的input 是一个低维向量，output 是一个高维向量。Discriminator 输入就是Generator 生成的图片，输出是一个分数，这个分数就是说输入有多像二次元人脸，如果很想的话就给出高分，否则就给出低分。这就是经典GAN的基本架构。\n Algorithm\n现在来讲一下Generator 和Discriminator 是怎么被训练出来的。这里先明确G和D都是神经网络，这两者的具体架构根据具体的任务而定，比如说你要做图像生成那你可能就会用CNN，如果你要让Generator 通过一些词生成文章，你可能就会用RNN…我们今天不做过多讨论这方面的内容，而是关注于G和D是怎么交替训练的。\n\n首先，G和D是迭代的训练的，在每个training iteration 中有两个步骤：\nstep1：固定住G，只更新D的参数。也就是说通过当前iteration分配的一个mini batch data 进行Discriminator 的训练。你可以把这个训练看作是分类问题，也可以看作回归问题，相当于如果是动漫人物头像就输出1，否则输出0。这就达到了我们的目的，训练一个Discriminator 它看到好的图片就给一个高分，看到差的就给低分。另外提一下，Database 中都是我们收集的正类样本，而此时Generator 产生样本就可以作为负类。\n\nstep2：固定住D，只更新G。这个步骤你可以把G和D两个网络连起来看成一个大的网络，这个网络输入就是低维vector，输出就是一个分数，中间某个维度拿出来就是一张图片，这整个网络仍然用backpropagation更新参数，只不过是固定住后面D的参数。这一步训练目的是为了让Generator 努力骗过Discriminator ，这样G生成的图片就会更像是我们收集的数据集中的样本。\n你也可以这样考虑，我们原本train 一个网络是去optimize 人定的objective function，或者是minimize 人定的loss function，但是现在我们训练的Discriminator 就相当于是让机器学了一个objective function或者说loss function，让G朝着这个方向努力。\n\n实际在训练的时候就是如上图所示，train D、train G、train D、train G…交替进行。在网络上比较好的栗子：\n\nYour browser does not support the video tag.\n\nhttps://crypko.ai/#/\n GAN is hard to train…\n众所周知，GAN这个技术是比较难train起来的，实际上有很多人提出了更好的训练GAN的方法，WGAN，improve WGAN…如果你有兴趣就去看文章开头链接的课程吧。\n 2.Conditional GAN\n上面我们说的是用GAN的技术随机生成目标，更多的情况下我们是希望控制机器产生的东西。比如说我们输入一端文字，让机器根据这段文字生成一张图像，比如下图所示，如果我们输入“Girl with red hair”机器就会生成一张夏娜的图片。\n\n这种根据某个特定输入产生对应的输出的Generator 叫做Conditional GAN 。\n我们以文字产生影像为例子，假如说你能收集到文字和图像对应的数据：\n\n接下来你就可以套用传统的监督学习方法，直接训练一个网络输入一段文字，输出一张图片。但是这样训练方法往往不理想，举一个直观的栗子，如上图所示，如果文字是火车，对应火车的图片有火车的各个方向，还有各种不同的火车，这样机器在考虑火车的特征的时候就会把所有的已知特征取平均，这样的话结果就会垮掉。\n\n所以我们就需要新的技术，比如说Conditional GAN，我们会用这个技术做给出文字生成对应影像的任务。Conditional GAN 也是需要label data 的也就是说同样是监督学习，但是它和传统的监督学习的学习目标是不太一样的（事实上Conditional GAN 也可以在无监督的情况下做，这个我们会在后面讲到）。怎么个不一样呢，如上图所示，Generator 的输入是一段描述，输出对应图片，它不去看输入输出是否对应而是看Discriminator 给出的得分。关于Discriminator 我们不仅要输入Generator 生成的图片，还要输入。原因是这样，如果D只吃一张图片的话，那G只要生成出像真实世界的图片就可以骗过D了，这样的话G就会变得很懒惰，就学不起来。如上图所示，Generator 忽视了input condition。\n\n所以我们要给Discriminator 多输入一段描述，Discriminator 输出的分数包含两个含义，一个是x到底有多像真实世界的影像，一个是c和x有多match。训练D的时候要用到如上图所示的输入，text-image pair 需要包含文字匹配真实图像，文字匹配G生成影像，文字不匹配真实图像这些情况，这是第一次实作Conditional GAN容易忽略的点。\n Conditional GAN - Sound-to-image\n其实上述使用Conditional GAN 根据文字生成影像的应用已经满坑满谷了，其实只要你有label data 你都可以尝试使用Conditional GAN 来试试，这里老师实作了一个根据声音生成图像的栗子。\n\n从video中提取音轨和图像，就得到了一段声音和图像的对应关系，就可以作为data set 使用上述的Conditional GAN 方法进行训练。\n\n\nThe images are generated by Chia-Hung Wan and Shun-Po Chuang.\nhttps://wjohn1483.github.io/audio_to_scene/index.html\n\n上图老师实作的结果，第一行是一段类似电视雪花的声音，是识别结果是小溪，然后调大声音，识别结果渐渐变成了瀑布。第二行是类似螺旋桨轰鸣声，识别结果是海上的快艇，随着声音增大，快艇周围的水花逐渐增大。这两个结果其实是有carry kick 的，第三行是一些音乐，识别结果就烂掉了，但总体来说我觉得还不错，有兴趣的话可以去上面那个链接看看。👀\n Conditional GAN - Image-to-label\n我们反向思考，可以做根据图像生成对应标签的模型，把Conditional GAN 用在multi-label image classification 任务上：\n\n所谓multi-label 就是说目标的类别不止一个，比如上图所示，示例图片中有person、sports ball、baseball bat、baseball glove，这张图片属于上述所有类别，或者说这张图片拥有这些属性。我们可以把一张图片拥有多个类别想象成一个生成问题，就是说给出一张图片生成它可能拥有的类别或者说属性。\n你就把这个问题当作一般的Conditional GAN 做下去就可以了，图片作为condition 输入，类别作为Generator 的输出。\n\n上图是实验的结果，用F1F_1F1​分数来评价，分数越高表示分类准确率越好，使用了两个corpus：MS-COCO和NUS-WIDE，几个不同架构的模型的表现都是加上GAN就会变得更好。\n Talking Head\n这里还有一个效果更好的，根据一张人脸照片，一个人脸line mark，去产生另外一张人脸。能做你拿一张蒙娜丽莎的图片，再拿一张人脸line mark，就可以让蒙娜丽莎摆出人脸line mark的表情。\n\n\nhttps://arxiv.org/abs/1905.08233\n\n直观一点，效果就是：\n\n\n 3.Unsupervised Conditional GAN\n最后，我们要讲的是无监督的GAN，上面讲的Conditional GAN 是需要输入输出的对应关系的，实际上我们是有机会能在不知道输入输出的对应关系的情况下，教会机器把输入转成输出。这种技术最常见的应用场景是image style transformation ：\n\n Cycle GAN\n\n\nhttps://arxiv.org/abs/1703.10593\nhttps://junyanz.github.io/CycleGAN/\nBerkeley AI Research (BAIR) laboratory, UC Berkeley\n\n这里我们以Cycle GAN 为例，如上图所示，这个GAN的架构和Conditional GAN 好像没什么不同。G就是将输入转换成一个同样大小的输出但是风格产生变化，D就是将G的输出和梵高的画作做对比，相似性较高的就给出高分，较低就给出低分。我们希望用这样的方法使得G产生的output是带有梵高风格的画作。\n\n但是这样做G有可能就学会只输出梵高的某一幅画或者某几幅画来骗过D，所以这样是不行的。我们要做改进：\n\n通过加一个G将前一个G生成的图片还原回原图片的方式，限制G通过产生固定输出骗过D的情况出现。这种方法叫做cycle consistency 所以这种GAN叫做cycle GAN。\n\ncycle GAN 可以是双向的：刚才讲的是先X→YX \\rightarrow YX→Y 再Y→XY \\rightarrow XY→X，你现在可以先Y→XY \\rightarrow XY→X再X→YX \\rightarrow YX→Y ，你同样要一个X domain 的discriminator 看看Y→XY \\rightarrow XY→X的结果想不想是真实的X domain 的图片，用另外一个X→YX \\rightarrow YX→Y的Generator 把输出转回输入，让两者越接近越好。\n同样的技术也可以用在其他任务上，举例来说，假设现在X domain Y domain 是两个人的声音，那你就可以用Cycle GAN 做音色转换。再比如说，X domain Y domain 是不同风格的文字，那你就可以用Cycle GAN 做文字风格转换。再比如说，X domain Y domain 是正面和负面的句子，我们就可以用同样的训练的想法将负面的句子转成正面的句子。\n\n Discrete Issue\n需要提醒一下，如果直接把影像的技术套用到文字上是会有一些问题的。\n\n上图就是Cycle GAN 的架构，大概率会使用seq2seq的模型来处理文字，而中间生成的positive 的句子作为一个hidden layer的话，就不可以微分了，因为这个句子他是一个token，是离散的值，所以没办法用backpropagation更新参数。之前做的图像的任务G生成的图像是连续值，是可微分的。那怎么办呢，其实在文献上是有各种而样的办法的，这里就不展开了：\n\n\n\nGumbel-softmax\n\n[Matt J. Kusner, et al, arXiv, 2016]\n\n\n\nContinuous Input for Discriminator\n\n[Sai Rajeswar, et al., arXiv, 2017][Ofir Press, et al., ICML workshop, 2017][Zhen Xu, et al., EMNLP, 2017][Alex Lamb, et al., NIPS, 2016][Yizhe Zhang, et al., ICML, 2017]\n\n\n\n“Reinforcement Learning”\n\n[Yu, et al., AAAI, 2017][Li, et al., EMNLP, 2017][Tong Che, et al, arXiv, 2017][Jiaxian Guo, et al., AAAI, 2018][Kevin Lin, et al, NIPS, 2017][William Fedus, et al., ICLR, 2018]\n\n\n\n\n\n\n这是一个实作的结果。\n Speech Recognition\n\n上述的Unsupervised GAN的技术其实不只能用于风格迁移、正负面转换这些场景，还可以有很多其他的应用，比如说一个很有意义的就是语音识别。\n在语音识别任务中，监督学习需要大量的语音注释。然而，大多数语言资源不足。我们这么考虑，一个domain是语音，一个domain是文字，而无标签的语音和文字的数据在网络上是由很多很多的，所以有没有可能用Unsupervised GAN的技术在无标签的情况下就让机器学会语音辨识，细节其实比较复杂，这里就跳过，感兴趣就去看文章开头的课程。\n Experimental Result\n\n这里讲一下结果，上图李宏毅老师的实验室做的结果。TIMIT Benchmark Corpus 上进行测试，在Nonmatched PER上达到了33.1% 是一个很低的错误率了。Nonmatched是说训练数据中音频和文字是不对应的，并不是说文字是根据音频人工写出来的，而是不对应的。PER（phoneme error rate）你可以理解为音表错误率。\n\n可以看到在准确率上，这个无监督的GAN训练出来的模型可以和三十年内的有监督学习的匹敌。\n","categories":["李宏毅机器学习笔记"],"tags":["ML","note","Next-Step-of-ML"]},{"title":"污点分析技术笔记","url":"https://ch3nye.top/污点分析技术笔记/","content":" 污点分析技术\n同符号执行一样,污点分析也是我们分析代码漏洞,检测攻击方式的重要手段,在漏洞自动化扫描或者检测工具中有着十分广泛的应用,本文主要参考(copy)王蕾等人2016年的二进制程序动态污点分析技术研究综述[1]和K0rz3n大佬的简单理解污点分析技术这篇文章做一些个人学习总结,文末附上参考的论文和文章链接.\n 0x1 污点分析基本原理\n 1.污点分析定义\n污点分析可以抽象成一个三元组&lt;sources,sinks,sanitizers&gt;的形式,其中,source 即污点源,代表直接引入不受信任的数据或者机密数据到系统中;sink 即污点汇聚点,代表直接产生安全敏感操作(违反数据完整性)或者泄露隐私数据到外界(违反数据保密性);sanitizer 即无害处理,代表通过数据加密或者移除危害操作等手段使数据传播不再对软件系统的信息安全产生危害.污点分析就是分析程序中由污点源引入的数据是否能够不经无害处理,而直接传播到污点汇聚点.如果不能,说明系统是信息流安全的;否则,说明系统产生了隐私数据泄露或危险数据操作等安全问题.\n在漏洞分析中,使用污点分析技术将所感兴趣的数据(通常来自程序的外部输入,假定所有输入都是危险的)标记为污点数据,然后通过跟踪和污点数据相关的信息的流向,可以知道它们是否会影响某些关键的程序操作,进而挖掘程序漏洞.即将程序是否存在某种漏洞的问题转化为污点信息是否会被 Sink 点上的操作所使用的问题.\n污点分析的处理过程可以分成 3 个阶段(如图 2 所示):\n(1) 识别污点源和汇聚点;(根据所分析的系统的不同使用定制的识别策略)\n(2) 污点传播分析;(利用特定的规则跟踪分析污点信息在程序中的传播过程)\n(3) 漏洞检测、无害处理\n\n 2.识别污点源和汇聚点\n识别污点源和污点汇聚点是污点分析的前提.目前,在不同的应用程序中识别污点源和汇聚点的方法各不相同.缺乏通用方法的原因一方面来自系统模型、编程语言之间的差异.另一方面,污点分析关注的安全漏洞类型不同,也会导致对污点源和污点汇聚点的收集方法迥异.表 1 所示为在 Web 应用程序漏洞检测中的污点源示例[29],它们是 Web 框架中关键对象的属性.\n\n现有的识别污点源和汇聚点的方法可以大致分成 3 类:\n(1)使用启发式的策略进行标记,例如把来自程序外部输入的数据统称为“污点”数据,保守地认为这些数据有可能包含恶意的攻击数据(如 PHP Aspis);\n(2)根据具体应用程序调用的 API 或者重要的数据类型,手工标记源和汇聚点(如 DroidSafe[12]);\n(3)使用统计或机器学习技术自动地识别和标记污点源及汇聚点.[11]\n 3.污点传播分析\n污点传播分析就是分析污点标记数据在程序中的传播途径.按照分析过程中关注的程序依赖关系的不同, 可以将污点传播分析分为显式流分析和隐式流分析.\n (1)显示流分析\n污点传播分析中的显式流分析就是分析污点标记如何随程序中变量之间的数据依赖关系传播.\n\n以图 3 所 示的程序为例,变量 a 和 b 被预定义的污点源函数 source 标记为污点源.假设 a 和 b 被赋予的污点标记分别为taint_a 和 taint_b.由于第 5 行的变量 x 直接数据依赖于变量 a,第 6 行的变量 y 直接数据依赖于变量 b,显式流分析会分别将污点标记 taint_a 和 taint_b 传播给第 5 行的变量 x 和第 6 行的变量 y.又由于 x 和 y 分别可以到达第 7 行和第 8 行的污点汇聚点(用预定义的污点汇聚点函数 sink 标识),图 3 所示的代码存在信息泄漏的问题.我们将在后面具体介绍目前污点传播分析中显式流分析面临的主要挑战和解决方法.\n (2)隐式流分析\n污点传播分析中的隐式流分析是分析污点标记如何随程序中变量之间的控制依赖关系传播,也就是分析污点标记如何从条件指令传播到其所控制的语句.\n\n在图 4 所示的程序中,变量 X 是被污点标记的字符串类型变量,变量 Y 和变量 X 之间并没有直接或间接的数据依赖关系(显式流关系),但 X 上的污点标记可以经过控制依赖隐式地传播到 Y.\n具体来说,由第 4 行的循环条件控制的外层循环顺序地取出 X 中的每一个字符,转化成整型后赋给变量 x,再由第 7 行的循环条件控制的内层循环以累加的方式将 x 的值赋给 y,最后由外层循环将 y 逐一传给 Y.最终,第 12 行的 Y 值和 X 值相同,程序存在信息泄漏问题.但是,如果不进行隐式流污点传播分析,第 12 行 的变量 Y 将不会被赋予污点标记,程序的信息泄漏问题被掩盖.\n隐式流污点传播一直以来都是一个重要的问题,和显式流一样,如果不被正确处理,会使污点分析的结果不精确.由于对隐式流污点传播处理不当导致本应被标记的变量没有被标记的问题称为欠污染(under-taint)问题.相反地,由于污点标记的数量过多而导致污点变量大量扩散的问题称为过污染(over-taint)问题.目前,针对隐式流问题的研究重点是尽量减少欠污染和过污染的情况.我们将在后面具体介绍现有技术是如何解决上述问题的.\n 4.无害处理\n污点数据在传播的过程中可能会经过无害处理模块,无害处理模块是指污点数据经过该模块的处理后,数据本身不再携带敏感信息或者针对该数据的操作不会再对系统产生危害.换言之,带污点标记的数据在经过无害处理模块后,污点标记可以被移除.正确地使用无害处理可以降低系统中污点标记的数量,提高污点分析的效率,并且避免由于污点扩散导致的分析结果不精确的问题.\n在应用过程中,为了防止敏感数据被泄露(保护保密性),通常会对敏感数据进行加密处理.此时,加密库函数应该被识别成无害处理模块.这一方面是由于库函数中使用了大量的加密算法,导致攻击者很难有效地计算出密码的可能范围;另一方面是加密后的数据不再具有威胁性,继续传播污点标记没有意义.\n此外,为了防止外界数据因为携带危险操作而对系统关键区域产生危害(保护完整性),通常会对输入的数据进行验证.此时,输入验证(input validation)模块应当被识别成无害处理模块.例如,为了防止代码注入漏洞,PHP 提供的 htmlentities 函数可以将特殊含义的 HTML 字符串转化成HTML实体(例如,将’&lt;’转化成’&amp;lt;’).输入字符串经过上述转化后不会再携带可能产生危害的代码,可以安全地 发送给用户使用.除了语言自带的输入验证函数外,一些系统还提供了额外的输入验证工具,比如ScriptGard[50],CSAS[51],XSS Auditor[]52,Bek[53].这些工具也应被识别成无害处理模块.\n综上,目前对污点源、污点汇聚点以及无害处理模块的识别通常根据系统或漏洞类型使用定制的方法.由于这些方法都比较直接,本文将不再进行更深入的探讨.下一节将重点介绍污点传播中的关键技术.\n总结一下,使用污点分析检测程序漏洞的工作原理如下图所示：\n\n\n基于数据流的污点分析(显示流分析).在不考虑隐式信息流的情况下,可以将污点分析看做针对污点数据的数据流分析.根据污点传播规则跟踪污点信息或者标记路径上的变量污染情况,进而检查污点信息是否影响敏感操作.\n基于依赖关系的污点分析(隐式流分析).考虑隐式信息流,在分析过程中,根据程序中的语句或者指令之间的依赖关系,检查 Sink 点处敏感操作是否依赖于 Source 点处接收污点信息的操作.\n\n 0x2 污点传播分析的关键技术\n污点传播分析是当前污点分析领域的研究重点.与程序分析技术相结合,可以获得更加高效、精确的污点分析结果.根据分析过程中是否需要运行程序,可以将污点传播分析分为静态污点分析和动态污点分析.本节主要介绍如何使用动/静态程序分析技术来解决污点传播中的显式流分析和隐式流分析问题.\n笔者认为显式流分析和隐式流分析是从两种不同的角度(数据流和控制流)来观察污点传播,所以在实作的时候是应该两种分析方式同时应用的,这样才能全面的分析到污点的传播情况,静态和动态方法则可以取其一,也可以结合使用.\n 1.污点传播中的显式流分析\n (1)静态分析技术\n静态污点传播分析(简称静态污点分析)是指在不运行且不修改代码的前提下,通过分析程序变量间的数据依赖关系来检测数据能否从污点源传播到污点汇聚点.\n静态污点分析的对象一般是程序的源码或中间表示.可以将对污点传播中显式流的静态分析问题转化为对程序中静态数据依赖的分析:\n(1)首先,根据程序中的函数调用关系构建调用图(call graph,简称CG);\n(2)然后,在函数内或者函数间根据不同的程序特性进行具体的数据流传播分析.常见的显式流污点传播方式包括直接赋值传播、通过函数(过程)调用传播以及通过别名(指针)传播.\n以图 5 所示的 Java 程序为例:\n\n第 3 行的变量 b 为初始的污点标记变量,程序第 4 行将一个包含变量 b 的算术表达式的计算结果直接赋给变量 c.由于变量 c 和变量 b 之间具有直接的赋值关系,污点标记可直接从赋值语句右部的变量传播到左部,也就是上述 3种显式流污点传播方式中的直接赋值传播.\n接下来,变量 c 被作为实参传递给程序第 5 行的函数 foo,c 上的污点标记也通过函数调用传播到 foo 的形参 z,z 的污点标记又通过直接赋值传播到程序第 8 行的 x.f.由于 foo 的另外两个参数对象 x 和 y 都是对对象 a 的引用,二者之间存在别名,因此,x.f的污点标记可以通过别名传播到第 9 行的污点汇聚点,程序存在泄漏问题.\n目前,利用数据流分析解决显式污点传播分析中的直接赋值传播和函数调用传播已经相当成熟,研究的重点是如何为别名传播的分析提供更精确、高效的解决方案.由于精确度越高(上下文敏感、流敏感、域敏感、对象敏感等)的程序静态分析技术往往伴随着越大的时空开销,追求全敏感且高效的别名分析难度较大.又由于静态污点传播分析关注的是从污点源到污点汇聚点之间的数据流关系,分析对象并非完整的程序,而是确定的入口和出口之间的程序片段.这就意味着可以尝试采用按需(on-demand)定制的别名分析方法来解决显式流态污点分析中的别名传播问题.文献[10]使用按需的上下文敏感的别名分析的污点分析方法来检测Java应用程序漏洞.TAJ[25]工具使用了混合切片结合对象敏感的别名分析来进行 Java Web 应用上的污点分析.Andromeda[61]工具使用了按需的对象敏感别名分析技术解决对象的访问路径(access path)问题,FlowDroid[13]工具提出一种按需的别名分析,从而提供上下文敏感、流敏感、域敏感、对象敏感的污点分析,用以解决 Android的隐私泄露问题.\n (2)动态分析技术\n动态污点传播分析(简称动态污点分析)是指在程序运行过程中,通过实时监控程序的污点数据在系统程序中的传播来检测数据能否从污点源传播到污点汇聚点.动态污点传播分析首先需要为污点数据扩展一个污点标记(tainted tag)的标签并将其存储在存储单元(内存、寄存器、缓存等)中,然后根据指令类型和指令操作数设计相应的传播逻辑传播污点标记.\n动态污点传播分析按照实现层次被分为基于硬件、基于软件以及混合型的污点传播分析这3类.\n 1.硬件\n基于硬件的污点传播分析需要定制的硬件支持,一般需要在原有体系结构上为寄存器或者内存扩展一个标记位,用来存储污点标记,代表的系统有 Minos[62],Raksha[63]等.\n 2.基于软件\n基于软件的污点传播分析通过修改程序的二进制代码来进行污点标记位的存储与传播,代表的系统有 TaintEraser[64],TaintDroid[19]等.\n基于软件的污点传播的优点在于不必更改处理器等底层的硬件,并且可以支持更高的语义逻辑的安全策略(利用其更贴近源程序层次的特点),但缺点是使用插桩(instrumentation 在保证被测程序原有逻辑完整性的基础上在程序中插入一些探针)或代码重写(code rewriting)修改程序往往会给分析系统带来巨大的开销.相反地,基于硬件的污点传播分析虽然可以利用定制硬件降低开销,但通常不能支持更高的语义逻辑的安全策略,并且需要对处理器结构进行重新设计.\n 3.混合型\n混合型的污点分析是对上述两类方法的折中,即,通过尽可能少的硬件结构改动以保证更高的语义逻辑的安全策略,代表的系统有 Flexitaint[65],PIFT[19]等.\n目前,针对动态污点传播分析的研究工作关注的首要问题是如何设计有效的污点传播逻辑,以确保精确的污点传播分析.\nTaintCheck[67]利用插桩工具Valgrind[68]对其中间表示Ucode插桩并提供移动指令、算术指令以及除移动和算术外其他指令的3类传播逻辑实现对x86程序的动态污点分析.Privacy Scope[69],Dytan[70]和Libdft[71]以插桩工具PinTool为基础,实现针对x86程序的动态污点分析,并解决了一系列x86指令污点传播逻辑的问题.TaintDroid[19]提供了一套基于Android Dalvik虚拟机的DEX格式[73]的污点传播分析方法.由于DEX的指令包含多数常用的指令和具有面向对象特性的指令,普适性高,这里以TaintDroid中污点传播方法为示例,介绍动态污点传播的逻辑.\nDEX支持的变量类型有5种:本地变量、方法参数、类静态域、类实例域和数组.TaintDroid用υX\\upsilon_XυX​代表本地变量和方法参数,fXf_XfX​代表类的静态域,υY(fX)\\upsilon_Y(f_X)υY​(fX​)代表实例域,其中,υY\\upsilon_YυY​是具体实例的变量引用.υX[⋅]\\upsilon_X[⋅]υX​[⋅]代表数组,其中,υX\\upsilon_XυX​表示数组的对象引用.同时,TaintDroid使用虚拟污点映射函数τ(⋅)\\tau(⋅)τ(⋅)来辅助污点传播,对于变量υ\\upsilonυ, τ(υ)\\tau(\\upsilon)τ(υ)返回变量的污点标记ttt. τ(υ)\\tau(\\upsilon)τ(υ)可以被赋值给其他的变量.符号←代表将位于符号右部的变量的污点标记传播给左部的变量.具体的污点传播逻辑规则见表2.\n\n如果你上面一段没看懂,没关系,你只要知道TaintDroid使用的动态污点传播分析的传播逻辑就是上表所示.\n举例来说,对于指令move-op υA\\upsilon_AυA​ υB\\upsilon_BυB​ 污点传播规则就是将B的标记传播给A,也就是说如果B被标记为污点则A也应该被标记为污点.\n 2.污点传播中的隐式流分析\n污点传播分析中的隐式流分析就是分析污点数据如何通过控制依赖进行传播,如果忽略了对隐式流污点传播的分析,则会导致欠污染的情况;如果对隐式流分析不当,那么除了欠污染之外,还可能出现过污染的情况.与显式流分析类似,隐式流分析技术同样也可以分为静态分析和动态分析两类.\n (1)静态分析技术\n静态隐式流分析面临的核心问题是精度与效率不可兼得的问题.精确的隐式流污点传播分析需要分析每一个分支控制条件是否需要传播污点标记.路径敏感的数据流分析往往会产生路径爆炸问题,导致开销难以接受.为了降低开销,一种简单的静态传播(标记)分支语句的污点标记方法是将控制依赖于它的语句全部进行污点标记,但该方法会导致一些并不携带隐私数据的变量被标记,导致过污染情况的发生.过污染会引起污点的大量扩散,最终导致用户得到的报告中信息过多,难以使用.\n (2)动态分析技术\n动态隐式流分析关注的首要问题是如何确定污点控制条件下需要标记的语句的范围.由于动态执行轨迹并不能反映出被执行的指令之间的控制依赖关系,目前的研究多采用离线的静态分析辅助判断动态污点传播中的隐式流标记范围.Clause等人[70]提出,利用离线静态分析得到的控制流图节点间的后支配(post-dominate)关系来解决动态污点传播中的隐式流标记问题.\n例如,如图 6(a)所示,程序第 3 行的分支语句被标记为污点源,当document.cookie 的值为 abc 时,会发生污点数据泄露.根据基于后支配关系的标记算法,会对该示例第 4 行语句的指令目的地,即 x 的值进行污点标记.(ps:因为根据该分支控制下的语句的执行结果可以判定污染源document.cookie的值,造成污点数据泄露)\n\n动态分析面临的第 2 个问题是由于部分泄漏(partially leaked)导致的漏报.部分泄漏是指污点信息通过动态未执行部分进行传播并泄漏.Vogt等人[29]发现,只动态地标记分支条件下的语句会发生这种情况.\n仍以图 6(a)中的程序为例:当第 3 行的控制条件被执行时,对应的 x 会被标记.此时,x 的值为 true,而 y 值没有变化,仍然为 false.在后续执行过程中,由于第 9行的污点汇聚点不可达,而第 12 行的汇聚点可达,动态分析没有检测到污点数据泄漏.但攻击者由第 11 行 y 等于 false 的条件能够反推出程序执行了第 3 行的分支条件,程序实际上存在信息泄漏的问题.这个信息泄露是由第 6 行未被执行到的 y 的赋值语句所触发的.因此,y 应该被动态污点传播分析所标记.为了解决部分泄漏问题,Vogt等人在传统的动态污点分析基础上增加了离线的静态分析,以跟踪动态执行过程中的控制依赖关系,对污点分支控制范围内的所有赋值语句中的变量都进行标记.具体到图 6(a)所示的例子,就是第 4 行和第 6 行中的变量均会被污点标记.但是,Vogt 等人的方法仍然会产生过污染的情况.\n动态分析需要解决的第 3 个问题是如何选择合适的污点标记分支进行污点传播.鉴于单纯地将所有包含污点标记的分支进行传播会导致过污染的情况,可以根据信息泄漏范围的不同,定量地设计污点标记分支的选择策略.\n以图 6(b)所示的程序为例,第 2 行的变量 a 为初始的污点标记变量.第 5 行、第 7 行、第 9 行均为以 a作为源操作数的污点标记的分支.如果传播策略为只要分支指令中包含污点标记就对其进行传播,那么第 5 行、第 7 行、第 9 行将分别被传播给第 6 行、第 8 行、第 10 行,并最终传播到第 12 行的污点汇聚点.如果对这段程序进行深入分析会发现,3个分支条件所提供的信息值(所能泄露的信息范围)并不相同,分别是 a 等于 10、a大于 10 且小于或等于 13(将 w 值代入计算)以及 a 小于 10.对于 a 等于 10 的情况,攻击者可以根据第 12 行泄漏的 x 的值直接还原出污点源处 a 的值(这类分支也被称为能够保存完整信息的分支);对于 a 大于 10 且小于或等于 13 的情况,攻击者也只需要尝试 3 次就可以还原信息;而对于 a 小于 10 的情况,攻击者所获得的不确定性较大,成功还原信息的几率显著低于前两种,对该分支进行污点传播的实际意义不大.\nBao等人[76]只将严格控制依赖(strict control dependence)识别成需要污点传播的分支,其中,严格控制依赖即分支条件表达式的两端具有常数差异的分支.但是,Bao 的方法只适用于能够在编译阶段计算出常数差异的分支.\nKang 等人[77]提出的 DTA++ 工具使用基于离线执行踪迹(trace)的符号执行的方法来寻找进行污点传播的分支,但该方法只关注信息被完整保存的分支,即图 6(b)中第 5 行的 a==10 会被选择污点传播,但是信息仍然能够通过另一个范围(第 7 行的分支)而泄露.\nCox 等人[78]提出的 SpanDex 的主要思想是:动态地获得控制分支中污点数据的范围,根据数据的更改以及数据间的依赖关系构建一个基于操作的有向无环图(OP-DAG),再结合一个在线的约束求解器(CSP solver)确定隐式流中传播的隐私数据值的范围,通过预先设定的阈值,选择是否对数据进行污点传播.该方法对图 6(b)所示例子中的第 5 行、第 7行的分支进行污点传播.但是目前,该方法只能求解密码字符的范围,暂不支持对复杂操作(位、除法、数组等操作)的求解.\n 0x3 污点分析方法实现\n为了更进一步理解污点分析技术的实现,加入这一节,主要摘录自CTF-All-In-One[2]\n 1.静态污点分析技术\n静态污点分析系统首先对程序代码进行解析,获得程序代码的中间表示,然后在中间表示的基础上对程序代码进行控制流分析等辅助分析,以获得需要的控制流图、调用图等.在辅助分析的过程中,系统可以利用污点分析规则在中间表示上识别程序中的 Source 点和 Sink 点.最后检测系统根据污点分析规则,利用静态污点分析检查程序是否存在污点类型的漏洞.\n (1)基于数据流的污点分析\n在基于数据流的污点分析中,常常需要一些辅助分析技术,例如别名分析、取值分析等,来提高分析精度.辅助分析和污点分析交替进行,通常沿着程序路径的方向分析污点信息的流向,检查 Source 点处程序接收的污点信息是否会影响到 Sink 点处的敏感操作.\n过程内的分析中,按照一定的顺序分析过程内的每一条语句或者指令,进而分析污点信息的流向.\n\n记录污点信息.在静态分析层面,程序变量的污染情况为主要关注对象.为记录污染信息,通常为变量添加一个污染标签.最简单的就是一个布尔型变量,表示变量是否被污染.更复杂的标签还可以记录变量的污染信息来自哪些 Source 点,甚至精确到 Source 点接收数据的哪一部分.当然也可以不使用污染标签,这时我们通过对变量进行跟踪的方式达到分析污点信息流向的目的.例如使用栈或者队列来记录被污染的变量.\n程序语句的分析.在确定如何记录污染信息后,将对程序语句进行静态分析.通常我们主要关注赋值语句、控制转移语句以及过程调用语句三类.\n\n赋值语句.\n\n对于简单的赋值语句,形如 a = b 这样的,记录语句左端的变量和右端的变量具有相同的污染状态.程序中的常量通常认为是未污染的,如果一个变量被赋值为常量,在不考虑隐式信息流的情况下,认为变量的状态在赋值后是未污染的.\n对于形如 a = b + c 这样带有二元操作的赋值语句,通常规定如果右端的操作数只要有一个是被污染的,则左端的变量是污染的(除非右端计算结果为常量).\n对于和数组元素相关的赋值,如果可以通过静态分析确定数组下标的取值或者取值范围,那么就可以精确地判断数组中哪个或哪些元素是污染的.但通常静态分析不能确定一个变量是污染的,那么就简单地认为整个数组都是污染的.\n对于包含字段或者包含指针操作的赋值语句,常常需要用到指向分析的分析结果.\n\n\n控制转移语句.\n\n在分析条件控制转移语句时,首先考虑语句中的路径条件可能是包含对污点数据的限制,在实际分析中常常需要识别这种限制污点数据的条件,以判断这些限制条件是否足够包含程序不会受到攻击.如果得出路径条件的限制是足够的,那么可以将相应的变量标记为未污染的.\n对于循环语句,通常规定循环变量的取值范围不能受到输入的影响.例如在语句 for (i = 1; i &lt; k; i++)&#123;&#125; 中,可以规定循环的上界 k 不能是污染的.\n\n\n过程调用语句.\n\n可以使用过程间的分析或者直接应用过程摘要进行分析.污点分析所使用的过程摘要主要描述怎样改变与该过程相关的变量的污染状态,以及对哪些变量的污染状态进行检测.这些变量可以是过程使用的参数、参数的字段或者过程的返回值等.例如在语句 flag = obj.method(str); 中,str 是污染的,那么通过过程间的分析,将变量 obj 的字段 str 标记为污染的,而记录方法的返回值的变量 flag 标记为未污染的.\n在实际的过程间分析中,可以对已经分析过的过程构建过程摘要.例如前面的语句,其过程摘要描述为：方法 method 的参数污染状态决定其接收对象的实例域 str 的污染状态,并且它的返回值是未受污染的.那么下一次分析需要时,就可以直接应用摘要进行分析.\n\n\n\n\n代码的遍历.一般情况下,常常使用流敏感的方式或者路径敏感的方式进行遍历,并分析过程中的代码.如果使用流敏感的方式,可以通过对不同路径上的分析结果进行汇集,以发现程序中的数据净化规则.如果使用路径敏感的分析方式,则需要关注路径条件,如果路径条件中涉及对污染变量取值的限制,可认为路径条件对污染数据进行了净化,还可以将分析路径条件对污染数据的限制进行记录,如果在一条程序路径上,这些限制足够保证数据不会被攻击者利用,就可以将相应的变量标记为未污染的.\n\n过程间的分析与数据流过程间分析类似,使用自底向上的分析方法,分析调用图中的每一个过程,进而对程序进行整体的分析.\n (2)基于依赖关系的污点分析\n在基于依赖关系的污点分析中,首先利用程序的中间表示、控制流图和过程调用图构造程序完整的或者局部的程序的依赖关系.在分析程序依赖关系后,根据污点分析规则,检测 Sink 点处敏感操作是否依赖于 Source 点.\n分析程序依赖关系的过程可以看做是构建程序依赖图的过程.程序依赖图是一个有向图.它的节点是程序语句,它的有向边表示程序语句之间的依赖关系.程序依赖图的有向边常常包括数据依赖边和控制依赖边.在构建有一定规模的程序的依赖图时,需要按需地构建程序依赖关系,并且优先考虑和污点信息相关的程序代码.\n 静态污点分析实例分析\n在使用污点分析方法检测程序漏洞时,污点数据相关的程序漏洞是主要关注对象,如 SQL 注入漏洞、命令注入漏洞和跨站脚本漏洞等.\n下面是一个存在 SQL 注入漏洞 ASP 程序的例子：\n&lt;%\n    Set pwd &#x3D; &quot;bar&quot;\n    Set sql1 &#x3D; &quot;SELECT companyname FROM &quot; &amp; Request.Cookies(&quot;hello&quot;)\n    Set sql2 &#x3D; Request.QueryString(&quot;foo&quot;)\n    MySqlStuff pwd, sql1, sql2\n    Sub MySqlStuff(password, cmd1, cmd2)\n    Set conn &#x3D; Server.CreateObject(&quot;ADODB.Connection&quot;)\n    conn.Provider &#x3D; &quot;Microsoft.Jet.OLEDB.4.0&quot;\n    conn.Open &quot;c:&#x2F;webdata&#x2F;foo.mdb&quot;, &quot;foo&quot;, password\n    Set rs &#x3D; conn.Execute(cmd2)\n    Set rs &#x3D; Server.CreateObject(&quot;ADODB.recordset&quot;)\n    rs.Open cmd1, conn\n    End Sub\n%&gt;\n首先对这段代码表示为一种三地址码的形式,例如第 3 行可以表示为：\na &#x3D; &quot;SELECT companyname FROM &quot;\nb &#x3D; &quot;hello&quot;\nparam0 Request\nparam1 b\ncallCookies\nreturn c\nsql1 &#x3D; a &amp; c\n解析完毕后,需要对程序代码进行控制流分析,这里只包含了一个调用关系(第 5 行).\n具体的分析过程如下:\n\n调用 Request.Cookies(“hello”) 的返回结果是污染的,所以变量 sql1 也是污染的.\n调用 Request.QueryString(“foo”) 的返回结果 sql2 是污染的.\n函数 MySqlStuff 被调用,它的参数 sql1,sql2 都是污染的.分了分析函数的处理过程,根据第 6 行函数的声明,标记其参数 cmd1,cmd2 是污染的.\n第 10 行是程序的 Sink 点,函数 conn.Execute 执行 SQL 操作,其参数 cmd2 是污染的,进而发现污染数据从 Source 点传播到 Sink 点.因此,认为程序存在 SQL 注入漏洞\n\n 2.动态污点分析\n动态污点分析是在程序运行的基础上,对数据流或控制流进行监控,从而实现对数据在内存中的显式传播、数据误用等进行跟踪和检测.动态污点分析与静态污点分析的唯一区别在于静态污点分析技术在检测时并不真正运行程序,而是通过模拟程序的执行过程来传播污点标记,而动态污点分析技术需要运行程序,同时实时传播并检测污点标记.\n动态污点分析技术可分为三个部分：\n\n污点数据标记：程序攻击面是程序接受输入数据的接口集,一般由程序入口点和外部函数调用组成.在污点分析中,来自外部的输入数据会被标记为污点数据.根据输入数据来源的不同,可分为三类：网络输入、文件输入和输入设备输入.\n污点动态跟踪：在污点数据标记的基础上,对进程进行指令粒度的动态跟踪分析,分析每一条指令的效果,直至覆盖整个程序的运行过程,跟踪数据流的传播.\n\n动态污点跟踪通常基于以下三种机制\n\n动态代码插桩：可以跟踪单个进程的污点数据流动,通过在被分析程序中插入分析代码,跟踪污点信息流在进程中的流动方向.\n全系统模拟：利用全系统模拟技术,分析模拟系统中每条指令的污点信息扩散路径,可以跟踪污点数据在操作系统内的流动.\n虚拟机监视器：通过在虚拟机监视器中增加分析污点信息流的功能,跟踪污点数据在整个客户机中各个虚拟机之间的流动.\n\n\n污点动态跟踪通常需要影子内存(shadow memory)来映射实际内存的污染情况,从而记录内存区域和寄存器是否是被污染的.对每条语句进行分析的过程中,污点跟踪攻击根据影子内存判断是否存在污点信息的传播,从而对污点信息进行传播并将传播结果保存于影子内存中,进而追踪污点数据的流向.\n一般情况下,数据移动类和算数类指令都将造成显示的信息流传播.为了跟踪污点数据的显示传播,需要在每个数据移动指令和算数指令执行前做监控,当指令的结果被其中一个操作数污染后,把结果数据对应的影子内存设置为一个指针,指向源污染点操作数指向的数据结构.\n\n\n污点误用检查：在正确标记污点数据并对污点数据的传播进行实时跟踪后,就需要对攻击做出正确的检测即检测污点数据是否有非法使用的情况.\n\n动态污点分析的优缺点：\n\n优点：误报率较低,检测结果的可信度较高.\n缺点：\n\n漏报率较高：由于程序动态运行时的代码覆盖率决定的.\n平台相关性较高：特定的动态污点分析工具只能够解决在特定平台上运行的程序.\n资源消耗大：包括空间上和时间上.\n\n\n\n 动态污点分析的方法实现\n (1)污点数据标记\n污点数据通常主要是指软件系统所接受的外部输入数据,在计算机中,这些数据可能以内存临时数据的形式存储,也可能以文件的形式存储.当程序需要使用这些数据时,一般通过函数或系统调用来进行数据访问和处理,因此只需要对这些关键函数进行监控,即可得到程序读取或输出了什么污点信息.另外对于网络输入,也需要对网络操作函数进行监控.\n识别出污点数据后,需要对污点进行标记.污点生命周期是指在该生命周期的时间范围内,污点被定义为有效.污点生命周期开始于污点创建时刻,生成污点标记,结束于污点删除时刻,清除污点标记.\n\n污点创建\n\n将来自于非可靠来源的数据分配给某寄存器或内存操作数时\n将已经标记为污点的数据通过运算分配给某寄存器或内存操作数时\n\n\n污点删除\n\n将非污点数据指派给存放污点的寄存器或内存操作数时\n将污点数据指派给存放污点的寄存器或内存地址时,此时会删除原污点,并创建新污点\n一些会清除污点痕迹的算数运算或逻辑运算操作时\n\n\n\n (2)污点动态跟踪\n当污点数据从一个位置传递到另一个位置时,则认为产生了污点传播.污点传播规则：\n\n\n\n指令类型\n传播规则\n举例说明\n\n\n\n\n拷贝或移动指令\nT(a)&lt;-T(b)\nmov a, b\n\n\n算数运算指令\nT(a)&lt;-T(b)\nadd a, b\n\n\n堆栈操作指令\nT(esp)&lt;-T(a)\npush a\n\n\n拷贝或移动类函数调用指令\nT(dst)&lt;-T(src)\ncall memcpy\n\n\n清零指令\nT(a)&lt;-false\nxor a, a\n\n\n\n注：T(x) 的取值分为 true 和 false 两种,取值为 true 时表示 x 为污点,否则 x 不是污点.\n对于污点信息流,通过污点跟踪和函数监控,已经能够进行污点信息流流动方向的分析.但由于缺少对象级的信息,仅靠指令级的信息流动并不能完全给出要分析的软件的确切行为.因此,需要在函数监控的基础上进行视图重建,如获取文件对象和套接字对象的详细信息,以方便进一步的分析工作.\n污点动态跟踪的实现通常使用：\n\n影子内存：真实内存中污点数据的镜像,用于存放程序执行的当前时刻所有的有效污点.\n污点传播树：用于表示污点的传播关系.\n污点处理指令链：用于按时间顺序存储与污点数据处理相关的所有指令.\n\n当遇到会引起污点传播的指令时,首先对指令中的每个操作数都通过污点快速映射查找影子内存中是否存在与之对应的影子污点从而确定其是否为污点数据,然后根据污点传播规则得到该指令引起的污点传播结果,并将传播产生的新污点添加到影子内存和污点传播树中,同时将失效污点对应的影子污点删除.同时由于一条指令是否涉及污点数据的处理,需要在污点分析过程中动态确定,因此需要在污点处理指令链中记录污点数据的指令信息.\n (3)污点误用检查\n污点敏感点,即 Sink 点,是污点数据有可能被误用的指令或系统调用点,主要分为：\n\n跳转地址：检查污点数据是否用于跳转对象,如返回地址、函数指针、函数指针偏移等.具体操作是在每个跳转类指令(如call、ret、jmp等)执行前进行监控分析,保证跳转对象不是污点数据所在的内存地址.\n格式化字符串：检查污点数据是否用作printf系列函数的格式化字符串参数.\n系统调用参数：检查特殊系统调用的特殊参数是否为污点数据.\n标志位：跟踪标志位是否被感染,及被感染的标志位是否用于改变程序控制流.\n地址：检查数据移动类指令的地址是否被感染.\n\n根据sink 出现的误用情况,确定污点的漏洞类型.即在进行污点误用检查时,通常需要根据一些漏洞模式来进行检查,首先需要明确常见漏洞在二进制代码上的表现形式,然后将其提炼成漏洞模式,以更有效地指导自动化的安全分析.\n 动态污点分析的实例分析\n下面我们来看一个使用动态污点分析的方法检测缓冲区溢出漏洞的例子.\nvoid fun(char *str)\n&#123;\n    char temp[15];\n    printf(\"in strncpy, source: %s\\n\", str);\n    strncpy(temp, str, strlen(str));        // Sink 点\n&#125;\nint main(int argc, char *argv[])\n&#123;\n    char source[30];\n    gets(source);                           // Source 点\n    if (strlen(source) &lt; 30)\n        fun(source);\n    else\n        printf(\"too long string, %s\\n\", source);\n    return 0;\n&#125;\n漏洞很明显, 调用 strncpy 函数存在缓冲区溢出.\n程序接受外部输入字符串的二进制代码如下：\n0x08048609 &lt;+51&gt;:    lea    eax,[ebp-0x2a]\n0x0804860c &lt;+54&gt;:    push   eax\n0x0804860d &lt;+55&gt;:    call   0x8048400 &lt;gets@plt&gt;\n...\n0x0804862c &lt;+86&gt;:    lea    eax,[ebp-0x2a]\n0x0804862f &lt;+89&gt;:    push   eax\n0x08048630 &lt;+90&gt;:    call   0x8048566 &lt;fun&gt;\n程序调用 strncpy 函数的二进制代码如下：\n0x080485a1 &lt;+59&gt;:    push   DWORD PTR [ebp-0x2c]\n0x080485a4 &lt;+62&gt;:    call   0x8048420 &lt;strlen@plt&gt;\n0x080485a9 &lt;+67&gt;:    add    esp,0x10\n0x080485ac &lt;+70&gt;:    sub    esp,0x4\n0x080485af &lt;+73&gt;:    push   eax\n0x080485b0 &lt;+74&gt;:    push   DWORD PTR [ebp-0x2c]\n0x080485b3 &lt;+77&gt;:    lea    eax,[ebp-0x1b]\n0x080485b6 &lt;+80&gt;:    push   eax\n0x080485b7 &lt;+81&gt;:    call   0x8048440 &lt;strncpy@plt&gt;\n首先,在扫描该程序的二进制代码时,能够扫描到 call &lt;gets@plt&gt;,该函数会读入外部输入,即程序的攻击面.确定了攻击面后,我们将分析污染源数据并进行标记,即将 [ebp-0x2a] 数组(即源程序中的source)标记为污点数据.程序继续执行,该污染标记会随着该值的传播而一直传递.在进入 fun() 函数时,该污染标记通过形参实参的映射传递到参数 str 上.然后运行到 Sink 点函数 strncpy().该函数的第二个参数即 str 和 第三个参数 strlen(str) 都是污点数据.最后在执行 strncpy() 函数时,若设定了相应的漏洞规则(目标数组小于源数组),则漏洞规则将被触发,检测出缓冲区溢出漏洞.\n 0x4 污点分析在实际应用中的关键技术\n污点分析被广泛地应用在系统隐私数据泄露、安全漏洞等问题的检测中.在实际应用过程中,由于系统框架、语言特性等方面的差异,通用的污点分析技术往往难以适用.比如:系统框架的高度模块化以及各模块之间复杂的调用关系导致污点源到汇聚点的传播路径变得复杂、庞大,采用通用的污点分析技术可能面临开销难以接受的问题;通用的污点分析技术对新的语言特性支持有限等.为此,需要针对不同的应用场景,对通用的污点分析技术进行扩展或定制.\n本节以两个代表性的应用场景——智能手机的隐私泄漏检测和Web应用安全漏洞检测为切入点,总结近10年来污点分析技术在上述领域的应用实践过程中所面临的问题和关键解决技术.\n 1.检测智能手机隐私泄露\n针对Android的污点传播分析也围绕组件展开,按照传播可能通过的模块的不同,分为组件内污点传播、组件间污点传播、组件与库函数之间的污点传播这3类(如图7所示).接下来将分别介绍针对这3类传播问题的静态和动态污点传播分析技术.\n\n 1.静态污点分析\n (1)组件内污点传播分析\n组件内部污点分析面临的主要问题是如何构建完整的分析模型.不同于传统的C/C++程序(有唯一的Main函数入口),Android应用程序存在有多个入口函数的情况.这个情况源于Android应用程序复杂的运行生命周期(例如onCreate,onStart,onResume,onPause等)以及程序中大量存在的回调函数和异步函数调用.由于任何的程序入口都有可能是隐私数据的来源,在静态的污点分析开始之前必须构建完整的应用程序模型,以确保程序中每一种可能的执行路径都会被静态污点传播分析覆盖到.\nLeakMiner[14]和CHEX[15]尝试使用增量的方法构建系统调用图.\nArzt等人设计的FlowDroid[13]提出了一种更系统的构建Android程序完整分析模型的方法:首先,通过XML配置文件提取与Android生命周期相关的入口函数,将这些方法作为节点,并根据Android生命周期构建调用图(如图8所示);其次,对于生命周期内的回调函数,在该调用图的基础上增加不透明谓词节点(即图8中菱形的P节点);然后,增量式地将回调函数加入这个函数调用图;最后,将调用图上所有的执行入口连接到一个虚假的Main函数上.FlowDroid中的一次合法的执行,就是对调用图进行的一次遍历.\n\nGordon等人[12]提出的DroidSafe使用Android设备实现(Android device implementation)来构建Android的完整分析模型.Android设备实现是对Android运行环境的一个简单模拟,它使用Java语言,结合Android Open Source Porject(AOSP),实现了与原Android接口语义等价的模型,并使用精确分析存根(accurate analysis stub)将AOSP代码之外的函数加入到模型中.\n (2)组件间污点传播分析\n即使正确分析了组件内的数据流关系,污点数据仍然可能通过组件间的数据流来传递,从而造成信息泄露.如上图7左侧所示,即使保证了对组件A内部污点传播的精确分析,组件A仍然可能通过调用方法startActivityforResult()将信息传递给组件B,再通过组件B产生泄露.因此,针对Android应用的污点分析还需要分析出组件间所有可能的数据流信息.组件间通信是通过组件发送Intent消息对象完成的.Intent按照参数字段是否包含目标组件名称分为显式Intent和隐式Intent.如图9所示:显式Intent对象使用一个包含目标组件名称的参数显式地指定通信的下一个组件;隐式Intent使用action,category等域隐式地让Android系统通过Intent Filter自动选择一个组件调用.目前,解决该问题的主要思想是利用Intent参数信息分析组件间的数据流.\n\n解决组件间数据流的前提是解析Intent的目的地,解析Intent目的地包括解析显式Intent的目的地和隐式Intent的目的地.由于显式Intent的目的地可以直接通过初始化Intent的地址字符串参数获得,目前,解析显式Intent目的地的常用方法是使用字符串分析工具(例如JSA[84])提取Intent中字符串参数的信息.\n解析隐式Intent目的地的主要方法是分析配置文件信息与Intent Filter注册器之间的映射关系,建立发送Intent组件和接受Intent组件之间的配对关系.在解析出Intent目的地之后,问题的重点转移到如何提高组件间数据流分析的精度上.\nKlieber等人[17]尝试在已经建立好的组件内污点分析的基础上,结合推导规则来分析组件间数据流.在分析之前,需要收集组件内部的污点源和汇聚点以及组件内Intent的发送目的地标签等信息.表4和表5给出了推导规则的前提定义和具体的推导规则,其中,一次完整的分析是指根据已知组件内部的信息src→sinksrc→sinksrc→sink以及推导规则识别所有src′→sink′src′→sink′src′→sink′的流集合.\nOcteau等人[18]尝试使用现有的程序分析方法提高组件间数据流分析的精度,他们将组件间数据流分析问题转化成IDE(interprocedural  distributive environment)问题[58]进行求解.DroidSafe设计了一种对象敏感的别名分析技术,在此基础上提供的精度优化方法包括:提取Intent的目的地的字符串参数、将Intent目的地的初始化函数嵌入到目的组件当中以提高别名分析的精度,同时,增加处理Android Service的支持.\n\n\n (3)组件与库函数之间的污点传播分析\n组件与库函数之间的污点传播分析面临的主要问题包括对Android库函数自身庞大的代码量的分析以及组件和某些库函数使用的实现语言不同(Android组件通常用Java实现,而本地库则采用C/C++代码编写)这两方面.\n目前的一类方法是使用手动定制来解决上述问题.比如,FlowDroid[13]尝试手动地分析库函数的语义,根据参数与返回值之间的关系为其提供显式传播规则.对于没有制定规则的库函数,FlowDroid使用保守的策略,即:只要库函数的参数中包含污点数据参数,就对函数的返回值进行污点标记.DroidSafe[12]使用精确分析存根实现了3176个本地代码库,这些存根是使用Java代码手动实现的,且与原本地代码的语义等价.对本地代码的调用被传递到对应的存根代码上进行分析.此类工作需要在人工理解程序语义的基础上加以实现.\n与之相对的是自动推导的方法.StubDroid[85]首次提出了用自动推导解决库函数传播的问题,该方法将产生供分析的摘要文件(summary file),将推导的污点数据流存储到摘要文件中,并提供了相应的部署策略.StubDroid的分析单元是一个API方法,将该方法内部的所有访问路径(包括方法参数、方法this引用和所有静态可见域)标记成源,将方法的返回值作为汇聚点.基于此,问题被转化成方法内部的污点分析问题. StubDroid利用现有的静态污点分析工具FlowDroid自动地处理库内部的污点数据流,最终得到的摘要文件包含API方法中的访问路径源是否能够传播到返回值的信息.另外,StubDroid还能够解决由于别名和访问路径带导致库函数传播中的精确度下降的问题,最后的摘要文件进一步被应用到FlowDroid框架,完成对整个app的分析.与FlowDroid类似,StudDroid同样无法对本地库函数进行自动分析.\n 2.动态污点分析\nAndroid系统中的动态污点同样需要分析组件内污点传播、组件间污点传播以及组件代码与本地库之间的污点传播.动态污点分析面临的主要挑战是系统信息除了在系统内部通过DEX指令传播以外,还会经过其他的通道,如本地库、网络、文件等.\nTaintDroid[19]首先提出了面向Android平台上的动态污点分析工具,之后的工具大多是基于它的优化或者应用扩展(Appsplaygroud[20],VetDroid[21],BayesDroid[22],AppFence[23]).本节重点介绍TaintDoid上的3种污点传播处理：\n1.组件内的污点传播主要是在Dalvik虚拟机DEX指令的变量级别传播,详见第2.1.2节动态分析技术中的关于TaintDroid的介绍.\n2.组件间的污点传播利用了Binder机制.Android底层使用Binder机制完成IPC调用,数据被存储在包(parcels)对象结构中.TaintDroid的污点传播方法是对包对象的结构进行重新设计,使之附带与污点相关的信息.TaintDroid提供了两种包结构扩展方法:\n(a) 使用一个标签变量将污点信息存储到包中,然后通过网络进行传输.当接受者接收到这个结构时,将其中包含的标签变量提取出来并继续传输;\n(b) 在方法(a)的基础上进行改进,提出了基于污点标签向量的传播技术.\n3.组件与本地库函数间的污点传播包括污点数据通过本地库代码或文件进行传播.TaintDroid通过设计后置条件对本地代码的函数进行污点传播.后置条件为:\n(a) 所有本地代码访问的外部变量都会被标记上污点标签;\n(b) 根据预定义规则将被赋值的函数返回值也标记上污点标签.\nTaintDroid使用了人工插桩与启发式相结合的方法来提供这些后置条件.TaintDroid解决污点数据通过文件进行传播的方法是以文件为单位添加污点标签,当文件被写入或者被读出到缓冲区时进行污点传播.如果一个被标记的数据被写入一个文件,就需要这个文件进行标记.该文件在后续的操作中将作为污点源,如果有对该文件的读取操作,那么读出的数据也需要被标记.\n 2.Java Web 框架上的污点分析技术\n不记录了,有需要的话可以读[1]原文4.2节\n 0x5 总结\n污点分析作为信息流分析的一种实践技术,被广泛应用于互联网及移动终端平台上应用程序的信息安全保障中.本文介绍了污点分析的基本原理和通用技术,并针对近年来污点分析在解决实际应用程序安全问题时遇到的问题和关键解决技术进行了分析综述.不同于基于安全类型系统的信息流分析技术,污点分析可以不改变程序现有的编程模型或语言特性,并提供精确信息流传播跟踪.在实际应用过程中,污点分析还需要借助传统的程序分析技术的支持,例如静态分析中的数据流分析、动态分析中的代码重写等技术.另外,结合测试用例生成技术、符号执行技术以及虚拟机技术,也会给污点分析带来更多行之有效的解决方案.\n如果文中有笔误，欢迎指正。😁\n 0x-1 参考\n\n[1] 王蕾,李丰,李炼,冯晓兵.污点分析技术的原理和实践应用.软件学报,2017,28(4):860−882.http://www.jos.org.cn/1000-9825/5190.htm\n[2] https://github.com/firmianay/CTF-All-In-One/blob/master/doc/5.5_taint_analysis.md\n[3] 宋铮，王永剑，金波，等.二进制程序动态污点分析技术研究综述[J].信息网络安全，2016（3）：77-83.\n[10] Livshits  VB,  Lam  MS.  Finding  security  vulnerabilities  in  Java  applications  with  static  analysis.  In:  Proc.  of  the  Conf.  on  Usenix Security Symp. USENIX Association, 2005. 262−266. https://www.usenix.org/legacy/event/sec05/tech/full_papers/livshits/livshits_html/\n[11] Rasthofer S, Arzt S, Bodden E. A machine-learning approach for classifying and categorizing android sources and sinks. In: Proc. of the Network and Distributed System Security Symp. (NDSS). 2014. [doi: 10.14722/ndss.2014.23039]\n[12] Gordon MI, Kim D, Perkins JH, Gilham L, Nguyen N, Rinard MC. Information flow analysis of Android applications in DroidSafe. In: Proc. of the NDSS 2015. 2015. [doi: 10.14722/ndss.2015.23089]\n[13] Arzt S, Rasthofer S, Fritz C, Bodden E, Bartel A, Klein J, Le Traon Y, Octeau D, McDaniel P. Flowdroid: Precise context, flow, field,  object-sensitive  and  lifecycle-aware  taint  analysis  for  Android  apps.  ACM  SIGPLAN  Notices,  2014,49(6):259−269.  [doi:  10.1145/2594291.2594299]\n[14] Yang  Z,  Yang  M.  Leakminer:  Detect  information  leakage  on  Android  with  static  taint  analysis.  In:  Proc.  of  the  Software  Engineering. IEEE, 2012. 101−104. [doi: 10.1109/WCSE.2012.26]\n[15] Lu L, Li Z, Wu Z, Lee W, Jiang G. Chex: Statically vetting Android apps for component hijacking vulnerabilities. In: Proc. of the 2012 ACM Conf. on Computer and Communications Security. ACM Press, 2012. 229−240. [doi: 10.1145/2382196.2382223]\n[19] Enck  W,  Gilbert  P,  Han  S,  Tendulkar  V,  Chun  BG,  Cox  LP,  Jung  J,  McDaniel  P,  Sheth  AN.  TaintDroid:  An  information-flow  tracking  system  for  realtime  privacy  monitoring  on  smartphones.  ACM  Trans.  on  Computer  Systems,  2014,32(2):393−407.  [doi:  10.1145/2619091]\n[25] Tripp O, Pistoia M, Fink SJ, Sridharan M, Weisman O. TAJ: Effective taint analysis of Web applications. ACM SIGPLAN Notices, 2009,44(6):87−97. [doi: 10.1145/1542476.1542486]\n[26] Papagiannis I, Migliavacca M, Pietzuch P. PHP ASPIS: Using partial taint tracking to protect against injection attacks. In: Proc. of the Usenix Conf. on Web Application Development. USENIX Association, 2011. 2. https://www.usenix.org/conference/webapps11/php-aspis-using-partial-taint-tracking-protect-against-injection-attacks\n[29] Vogt  P,  Nentwich  F,  Jovanovic  N,  Kirda  E,  Kruegel  C,  Vigna  G.  Cross  site  scripting  prevention  with  dynamic  data  tainting  and  static analysis. In: Proc. of the NDSS 2007. 2007. 12. http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.72.4505\n[50] Saxena P, Molnar D, Livshits B. SCRIPTGARD: Automatic context-sensitive sanitization for large-scale legacy Web applications. In: Proc. of the 18th ACM Conf. on Computer and Communications Security. ACM Press, 2011. 601−614. [doi: 10.1145/2046707. 2046776]\n[51] Samuel M, Saxena P, Song D. Context-Sensitive auto-sanitization in Web templating languages using type qualifiers. In: Proc. ofthe 18th ACM Conf. on Computer and Communications Security. ACM Press, 2011. 587−600. [doi: 10.1145/2046707.2046775]\n[52] Bates D, Barth A, Jackson C. Regular expressions considered harmful in client-side XSS filters. In: Proc. of the 19th Int’l Conf. on World Wide Web. ACM Press, 2010. 91−100. [doi: 10.1145/1772690.1772701]\n[53] Hooimeijer  P,  Livshits  B,  Molnar  D,  Saxena  P,  Veanes  M.  Fast  and  precise  sanitizer  analysis  with  BEK.  In:  Proc.  of  the  20th  USENIX Conf. on Security. USENIX Association, 2011. 1−1. http://dl.acm.org/citation.cfm?id=2028068\n[61] Tripp O, Pistoia M, Cousot P, Cousot R, Guarnieri S. Andromeda: Accurate and scalable security analysis of Web applications. In: Proc. of the Int’l Conf. on Fundamental Approaches to Software Engineering. Berlin, Heidelberg: Springer-Verlag, 2013. 210−225. [doi: 10.1007/978-3-642-37057-1_15]\n[62] Crandall JR, Chong FT. Minos: Control data attack prevention orthogonal to memory model. In: Proc. of the 37th Int’l Symp. on Microarchitecture (MICRO-37). IEEE, 2004. 221−232. [doi: 10.1109/MICRO.2004.26]\n[63] Dalton  M,  Kannan  H,  Kozyrakis  C.  Raksha:  A  flexible  information  flow  architecture  for  software  security.  ACM  SIGARCH  Computer Architecture News, 2007,35(2):482−493. [doi: 10.1145/1273440.1250722]\n[64] Zhu DY, Jung J, Song D, Kohno T, Wetherall D. Tainteraser: Protecting sensitive data leaks using applicationlevel taint tracking. ACM SIGOPS Operating Systems Review, 2011,45(1):142−154. [doi: 10.1145/1945023.1945039]\n[65] Venkataramani G, Doudalis I, Solihin Y, Prvulovic M. Flexitaint: A programmable accelerator for dynamic taint propagation. In: Proc. of the 2008 IEEE 14th Int’l Symp. on High Performance Computer Architecture. IEEE, 2008. 173−184. [doi: 10.1109/HPCA. 2008.4658637]\n[67] Newsome J, Song D. Dynamic taint analysis for automatic detection, analysis, and signature generation of exploits on commodity software. In: Proc. of the Network and Distributed System Security Symp. 2005. 720−724.\n[68] Nethercote  N,  Seward  J.  Valgrind:  A  program  supervision  framework.  Electronic  Notes  in  Theoretical  Computer  Science,  2003,  89(2):44−66. [doi: 10.1016/S1571-0661(04)81042-9]\n[69] Zhu Y, Jung J, Song D, Kohno T, Wetherall D. Privacy scope: A precise information flow tracking system for finding application leaks. Technical Report, EECS-2009-145, Berkeley: University of California, 2009.\n[70] Clause  J,  Li  W,  Orso  A.  DYTAN:  A  generic  dynamic  taint  analysis  framework.  In:  Proc.  of  the  2007  Int’l  Symp.  on  Software  Testing and Analysis. ACM Press, 2007. 196−206. [doi: 10.1145/1273463.1273490]\n[71] Kemerlis  VP,  Portokalidis  G,  Jee  K,  Keromytis  AD.  libdft:  Practical  dynamic  data  flow  tracking  for  commodity  systems.  ACM  SIGPLAN Notices, 2012,47(7):121−132. [doi: 10.1145/2365864.2151042]\n[76] Bao T, Zheng Y, Lin Z, Zhang X, Xu D. Strict control dependence and its effect on dynamic information flow analyses. In: Proc. of the 19th Int’l Symp. on Software Testing and Analysis. ACM Press, 2010. 13−24. [doi: 10.1145/1831708.1831711]\n[77] Kang MG, McCamant S, Poosankam P, Song D. DTA++: Dynamic taint analysis with targeted control-flow propagation. In: Proc. of the Network and Distributed System Security Symp. (NDSS 2011). San Diego, 2011.\n[78] Cox LP, Gilbert P, Lawler G, Pistol V, Razeen A, Wu B, Cheemalapati S. Spandex: Secure password tracking for Android. In: Proc.of the 23rd USENIX Security Symp. (USENIX Security 2014). 2014. 481−494. https://www.usenix.org/node/184402\n[84] Christensen  AS,  Møller  A,  Schwartzbach  MI.  Precise  analysis  of  string  expressions.  In:  Proc.  of  the  Int’l  Static  Analysis  Symp.Berlin, Heidelberg: Springer-Verlag, 2003. 1−18. [doi: 10.1007/3-540-44898-5_1]\n[85] Arzt S, Bodden E. StubDroid: Automatic inference of precise data-flow summaries for the Android framework. In: Proc. of the 38th Int’l Conf. on Software Engineering. ACM Press, 2016. 725−735. [doi: 10.1145/2884781.2884816]\n\n","categories":["笔记"],"tags":["总结","学习","污点分析"]},{"title":"Network Compression","url":"https://ch3nye.top/Network-Compression/","content":" Network Compression\n做网络压缩的原因是这样：我们未来希望把network 放到很多的设备上使用，这些设备上存储空间有限，计算能力有限，所以我们希望能把网络做压缩，而尽可能小的损失其准确度，以适应这些设备。\n\n Outline\n先列一下本节的大纲：\n\n\nNetwork Pruning\n\n\nKnowledge Distillation\n\n\nParameter Quantization\n\n\nArchitecture Design\n\n\nDynamic Computation\n\n\n另外，我们不会讨论硬件加速和优化相关的内容。\n Network Pruning\nNetwork Pruning（网络修剪）就是把一个大的network 把一些neuron 去掉，以达到network compression 的目的。我们之所以能做到这件事，是因为我们相信我们通常训练出来的神经网络是over-parameterized，也就是说网络中的很多参数是没有用的。就是我们不需要那么多参数就能解出当前的问题，但是我们给了网络过多的参数。如果你去分析训出来的network 中的参数，你会发现很多的neuron 的output 总是0，有些weight 是非常接近0的，这些参数是没有作用的。我们就把这些没用的东西剪掉。\n这个概念是非常古老的，在90s就已经有了这样的想法：\n\n\nOptimal Brain Damage 意思是最优脑损伤，你可以在直觉上的这样考虑，机器做Network Pruning 就好像人类的大脑发育中的一个现象：如上图所示人类在出生时候脑中神经连接是比较少的在发育的过程中经历了增加又减少的过程。\n weight/neuron pruning\n\n做weight pruning 的过程如上图所示，我们先要有一个训练好的network ，然后评估每个参数（neuron或weight）的重要性。那怎么评估参数重要性呢，已经有很多的评估方法被提出来了。举例来说，对于weight 的重要性我们可以之间看他的数值大小，如果他的值很接近零就可能是不重要的weight，如果它的绝对值很大就可能是重要的参数，所以你可以通过计算weight的L1，L2的数值来看它是不是重要的；对于neuron 的重要性，你给network 一批数据，如果这个neuron 的输出几乎都是零的话，这个neuron 就可能是不重要的。\n接下来，你就根据重要新排序weight 或者neuron 然后去除不重要的，你就做好的裁剪的动作。然后，你要做一次fine-tune，去修补你做裁剪的时候损失的准确度，也就是说要修补你做裁剪对模型造成的损伤。接着，你就看看当前这个模型的大小和准确度你是不是满意，如果满意的话网络压缩就结束，否则你就回去步骤二重新评估重要性，以此类推。\n这里有个一点需要注意，通常来说你在每次prune 的时候都是裁剪掉一点点，这样迭代多次，而不要一次prune 太多以至于造成没办法修补的损伤。\n关于network pruning 的做法就介绍到这里，接下来我考虑一个问题：\n Why Pruning?\n我们为什么要做network pruning ，我是说我们为什么不直接train 一个更小的网络？\n\n\n小的神经网络比较难训\n\n\n大的神经网络更容易优化？ https://www.youtube.com/watch?v=_VuWvQUMQVk\n\n\nLottery Ticket Hypothesis（大乐透假设）\n\nhttps://arxiv.org/abs/1803.03635\n\n\n\n第一点没什么好说的，众所周知小的神经网络比较难训。\n大的神经网络更容易优化，原因就是大的神经网络比较不容易卡在local minima、saddle point 这些点上。这也许就是为什么小的network 比较难train 的原因。所以说，我们通常选择train 好一个大的network 再做pruning 。\n关于大乐透假设，我们来解释一下：\n Lottery Ticket Hypothesis\n\n如上图所示，我们将train 好的一个大network 进行pruning 得到一个小的network 如右上角所示，我们随机初始化这个小网络的参数，重新训练这个小的网络，发现train 不起来。我们再把上述紫色的pruned 的network 设为最初始的参数，重新训练小网络，发现train 起来了。\n根据这个现象，作者提出了大乐透假说，就是说train network 就像买乐透一样，不同的random initialized parameters 得到不同的初始神经网络，有的train 得起来有的则不行。一个巨大的network 是由很多小的network 组成，这些小的network 就有的能train 起来有的不行，而大的network 中只要有一个小的network 能train 起来，整个大的network 就train 起来。所以你可以直觉上这样想，大的network 就相当于你一次买了很多乐透，增加中奖几率。然后你再把大的network 做pruning 找出那个能train 起来的小的network ，这个小的network 最开始初始的参数是能使它被train 起来的，所以我们看到了上图所述的现象。\n下面是一个与上述对立的看法。\n Rethinking the Value of Network Pruning\n\nhttps://arxiv.org/abs/1810.05270\n\n这篇文章主要就是讲小的network 也是train 的起来的。\n\n这个文章的实验中的随机初始化参数就是真的随机初始化参数，而不是从原始模型的初始化参数copy 过来。\n上图是实验结果，我们可以对比Unpruned 和Fine-tuned 两列，似乎就是pruned network 也是train 的起来的。\n所以说，上述这两篇paper 的结论就是矛盾对立的，两篇文章都是发表在ICLR ，且是open review 的，网络上有reviewer 问到两篇paper 的对立观点，他们的作者也都对此做了一些解释，有兴趣可以自行搜索。\n Network Pruning-Practical Issue\nNetwork Pruning 有一些实作上的问题是我们需要注意的，我们上面说你可以衡量weight 或者neuron 的重要性，然后prune 掉不重要的，那weight 和neuron 两者prune哪个比较好呢。\n如果我们prune weight：\n\n你prune 掉不重要的weight 后，你得到的网络架构是不规则的，所谓不规则是说，在同一层中有的neuron 吃两个input 有的吃四个input，这样的网络的算法程序你比较难实现，就算你真的实现了这种算法，你也不好用GPU 加速矩阵运算。\n所以实作上你做weight pruning 的话你就会把weight 设零，而不是拿掉weight，但是这么做你并没有实际上丢掉weight，模型的大小是没有变的，所以这不能达到我们的network compression 的目的。\n\n\nhttps://arxiv.org/pdf/1608.03665.pdf\n\n紫色线是我们prune 的量，几乎都在95%以上。我们可以看到prune 以后，集中模型的速度大部分都是有所下降的。所以得不偿失，你以为prune 以后会更快，但实际上变得更慢了。\n所以说prune neuron 是比较好实作也比较好加速的：\n\n Knowledge Distillation\n\n\nKnowledge Distillation\nhttps://arxiv.org/pdf/1503.02531.pdf\nDo Deep Nets Really Need to be Deep?\nhttps://arxiv.org/pdf/1312.6184.pdf\n\n如上图所示，Knowledge Distillation 就是用一个小的network 去学习大的network 的行为。我们不是较Student Net 正确的结果是什么，而是告诉他当前输入可能是什么，举例来说，当输入是图片1的时候，Student Net 去学习Teacher Net 的输出，它会学到当前图片有0.7的可能性是1，有0.2的可能性是7，有0.1的可能性是9。在这个过程中，Student Net 不仅会学习到当前输入的图片可能是什么，它还会学习到1和7和9是相似的，所以这样的学习方式是可以学习到更丰富的信息的。所以，有可能即使Student Net 没见过图片7，它只见过图片1和9，但是这么学完以后它是可以触类旁通的认出7的。\n\nKnowledge Distillation 的一个用处是用一个小的Student Net 模拟一个巨大的ensemble 的Teacher Net 。通常来说ensemble 的方法可以让你的模型的准确度更上一层楼，但是这么做是牺牲了算力和空间的，此时我们就可以用Knowledge Distillation 的方法，用一个小的network 模拟大的ensemble network 达到相近的准确度。\n Temperature\n在Knowledge Distillation 的实作上有一个技巧叫做Temperature：\n\nTemperature 就是如上图所示，我们在做classification 的network 的最后会有一个softmax layer ，softmax layer 就是会把network output的值取Exponential 然后做一个normalize。我们通常会在做softmax 之前对network 的output 做除上temperature 的动作，temperature 通常是一个大于1的值。\n为什么这样做呢，我们举个例子。首先，我们知道Knowledge Distillation 之所以会有用是因为大的network 的输出是可能性，而不是one-hot 的向量，如果是后者那就失去了不同class 之间的相似性信息，所以为了让不同的label 之间的分数拉近一点，我们就除上temperature。本来的x通过softmax layer 得到的y的后两个维度都接近0，而x除上temperature 后通过softmax layer 得到的y的各个维度之间的分数就被拉近了。\n但是，在实际上Knowledge Distillation 没有特别有用。🤣\n Parameter Quantization\n参数量化，这一节将在参数上做一点文章。\n\nUsing less bits to represent a value\n\n这没什么好说的，就是去掉一些参数的精度，来换取存储空间的下降。\n\nWeight clustering\n\n\n如上图所示，Weight clustering 就是将相近的参数聚簇，然后用一个映射表来存储，以降低参数占用的内存空间。一个cluster 中的参数可以取均值作为这个cluster 的值。这样做你也是会损失一些进度，但是换来了很好的模型压缩率。\n\nRepresent frequent clusters by less bits, represent rare clusters by more bits\n\n更进一步，你就可以把常见的clusters 用比较短的coding 表示，比较罕见的clusters 用比较长的coding 表示，以进一步提高模型压缩率。比如使用哈夫曼编码。\n Binary Weights\nParameter Quantization 这种思想和方法的极致就是你可不可以只用±1来表示一个weight。其实文献上有一些尝试是可以直接train binary  weight 的network，最早的一篇paper 就是下面这个Binary Connect 。\n\n\nBinary Connect: https://arxiv.org/abs/1511.00363\nBinary Network:\nhttps://arxiv.org/abs/1602.02830\nXNOR-net:\nhttps://arxiv.org/abs/1603.05279\n\nBinary Weights 的精神就是你的参数是二元化的用±1来表示。上图灰色的点代表参数空间，每一个点都可以看作是一个binary weight 的network，这个network 中所有参数都是二元化的都是+1或者-1。\n然后你就初始一组参数，这组参数可以是real value 的，你就现根据当前network 的参数找一个最接近的binary weight 的network 去计算gradient ，根据这个gradient 更新当前network 的参数，然后再去根据更新后的network 找一个最接近的binary weight 的network 去计算gradient ，根据这个gradient 更新当前network 的参数，以此类推。\n这个Binary Connect 根据文献上的结果看起来还不错：\n\n\nhttps://arxiv.org/abs/1511.00363\n\n从上面来看，Binary Connect 居然还比原来的network 的结果还要好，为什么呢？你可以这样想，Binary Connect 可以看作在做regularization ，它限制参数的值只能是±1。只是这个方法还是没有做Dropout 更好就是了。\n Architecture Design\n调整network 的架构设计让它变得只需要较少的参数，以实现network compression 。这也许是现在实作上最有效的做法。\n先来看看fully connected network ：\n Fully Connected Network\n Low rank approximation\n\n如上图左上角，我们有一个network ，其中M、N两层中间有参数W，这两层分别各有M、N个neuron，然后我们在这两层中间加一个neuron比较少的linear hidden layer K，你仔细想想看，这样做参数其实是变少了。\n原先有M×NM\\times NM×N那么多参数，然后变成M×K+K×NM\\times K+K\\times NM×K+K×N 个参数。如果我们调控好K的值，就可以做到减少参数的效果。\n但是这个trick 会对network 有一定的限制，这也是不可避免的。\n CNN-network compression\n Review: Standard CNN\n\n如果我们的input 有两个channel ，我们的filter 就要对应有两个，如上图中间所示，我们通常有多个filter 比如4个的话，这样我们得到的output 就是四个，如图右边所示。所以这个CNN 的filter 的参数个数是72个，后面我们要做network compression 看看能从72减少到多少。\n Depthwise Separable Convolution\n\nDepthwise Separable Convolution 是把convolution拆成两个步骤：\n\nstep1 Depthwise Convolution\n\n\nfilter 数量=输出的channel\n每个filter 都只处理一个channel\nfilter 是k*k 的矩阵\n不同的channel 之间互相没有影响的\n\n这样每个filter 就不再考虑其他channel ，卷积得到输出是两层\n\n\nstep2 Pointwise Convolution\n\n\n第二步骤，是说每个filter 都只用一个value ，用这样的filter 去处理第一步得到的两层输出，这一步的输出就和经典的CNN 的卷积输出相同了。\n综合这两步，filter 的参数总量是24。\n下面来解释一下，这个拆解的步骤和原来的CNN 有什么样的关系。\n\n上图上侧是一般的CNN 的卷积过程。\n下侧是Depthwise Separable Convolution ，观察卷积过程，第一步将得到中间产物两层的输出，然后经过第二步的每个filter 得到最终输出的每一层。这个过程你可以这样考虑，第一步使用的filter 处理了9个input ，然后把输出结果丢给第二步的filter 处理这两个output，产生一个output。这个过程和经典CNN 中的卷积做的事情类似，经典CNN 中使用一个节点处理18个input 产生一个输出，而我们现在通过叠加两层处理过程，用更少的参数做到了经典CNN 一层处理过程做到的事。\n接下来算一下这个方法理论上的模型压缩程度：\n\n计算过程如上图所示，最后结果就是从(k×k×I)×O(k\\times k\\times I)\\times O(k×k×I)×O ==&gt;k×k×I+I×Ok\\times k\\times I + I\\times Ok×k×I+I×O\n To learn more ……\n\n\n\nSqueezeNet\n\nhttps://arxiv.org/abs/1602.07360\n\n\n\nMobileNet\n\nhttps://arxiv.org/abs/1704.04861\n\n\n\nShuffleNet\n\nhttps://arxiv.org/abs/1707.01083\n\n\n\nXception\n\nhttps://arxiv.org/abs/1610.02357\n\n\n\n\n Dynamic Computation\n\n我们希望模型能调整它所的计算能力，再资源充足的时候努力做到最好，在资源紧张的时候降低计算精度以提高持续服务的时间。比如在手机快没电的时候语音助手的运行功耗的调整。\n这边来介绍一些可能的解决方法：\n\n\n训练多个模型，根据设备的情况选择适当的网络\n\n但是这样的方法比较吃存储空间，所以不太好。\n\n训练中间层也可以做分类器的网络\n\n这个做法是说，正常情况下通过整个模型的计算输出结果，但是在资源紧张的情况下把模型的中间某些层的结果直接拿出来通过一个简单的计算就可以得到结果，以此自由调整network的运算量。\n如果你直接这么做结果往往是不太好的，因为你在train 整个network 的时候前面的layer 往往是学习识别很简单的信息，后面的layer 才能综合这些信息做判断。\n上图左下角是中有一个实验结果，纵轴就是准确率，横轴从左到右就是从模型的前到后抽出中间结果。显而易见，越深的地方抽出的中间结果才越能准确的做好任务。\n还有一个问题是，如果你在中间加一些classifier ，这些分类器是和整个network 一起train 的，这些classifier 的训练会伤害到整个network 的功能布局。原来network 的前几层要抽出一些基础信息，但是你现在强加的classifier 要求前几层同时能够综合这些信息，就导致前几层不能把所有的注意力都用在抽基础信息上。实验结果如上图右下角，你在比较浅的地方加classifier 整个network 的表现就会暴跌，在靠后的位置加classifier 对network 的影响就会小一些。\n\nhttps://arxiv.org/abs/1703.09844\n\n有没有方法能解决这些问题呢？有的：\n\n\nhttps://arxiv.org/abs/1703.09844\n\n你可以自行查阅这篇paper 。\n","categories":["李宏毅机器学习笔记"],"tags":["ML","note","Next-Step-of-ML"]},{"title":"More about Auto-Encoder","url":"https://ch3nye.top/More-about-Auto-Encoder/","content":" More about Auto-Encoder\n这节课要讲的是在以前的课程上没有讲的， 近年来的热门的Auto-Encoder 技术。\n Review\n\nAuto-Encoder 就是有一个Encoder 的network ，有个Decoder 的network ，中间会产出一个vector ，你可以叫它Embedding，Latent Representation，Latent Code等。Decoder 会根据vector 产出一个输出，我们希望这个输出和原始的输入越接近越好，以此作为loss 更新模型参数。\n今天要讲的内容分两个部分：\n\n为什么一定要Minimize reconstruction error ，有没有其他的做法呢\n怎么让Encoder 输出的vector 更容易被解读\n\n接下来我们要先讲第一个问题，这里是通过引入Discriminator ，来替代minimize Reconstruction Error 。详情且看下面的描述。\n What is good embedding?\n我们回想一下为什么要做embedding 呢，我们是希望通过这个embedding 来代表原来的输入。举例来说，我们看到耳机就能想到三玖，而不会想到一花。🤣\n\n Beyond Reconstruction\nEncoder 吃进一个输入，就产生一个对应的代表该输入的embedding：\n\n那我们怎么知道这个产生的embedding 的代表性好不好呢，我们训一个Discriminator ，input 是一个张图片和embedding ，output 就是这一对输入是不是对应的，你可以想象这是一个二分类器。如上图左上方所示。\n这个Discriminator 的训练资料就是很多（图片+embedding）的二元组，这些二元组有的是对应的有的是故意用不对应的组合，并且打上标签。如上图右下角所示。\n然后就要定一个loss function LDL_DLD​，对于这个Discriminator 最简单的就是用Binary cross entropy ，没什么好说的。\n接下来就是训练这个Discriminator 去minimize loss function ，去找一组最好的Discriminator 的参数 ϕ\\phiϕ 。找到的最好的 ϕ\\phiϕ 达到的最低的loss 我们把此时的损失函数值叫做 LD∗{L_D}^*LD​∗ 。\n如果说这个 LD∗{L_D}^*LD​∗ 很小，那意味着现在这个Encoder 是很好的，Discriminator 很容易就能知道embedding 之间是不同的，可以轻易的分辨出各个embedding 应该对应的原始输入；如果说这个 LD∗{L_D}^*LD​∗ 很大，那就意味着现在这个Encoder 就是比较差的，这个Encoder 得到的embedding 都比较相似，Discriminator 难以分辨这些embedding 之间的区别。\n如果形象点用颜色来说，好的Encoder 得到的embedding 应该会是如上图颜色各异的，而差的Encoder 得到的embedding 可能就是下面这样颜色相近的：\n\n接下来我们要做的事，就是根据 LD∗{L_D}^*LD​∗ 调整Encoder 的参数 θ\\thetaθ ，使得Encoder 得到的embedding 在通过Discriminator 时候能被很好的分辨，得到较低的 LD∗{L_D}^*LD​∗ 。\n\n上述的这个方法，被用在Deep InfoMax(DIM) 这篇paper 中。这个Discriminator 可以做不同的设计，比如用不同的loss 计算方法等，结果会不太一样。\n在训练的时候，我们一起train Encoder 和Discriminator ，这件事就好像我们在train 一般的Auto-Encoder 时，一起train Encoder 和Decoder 去minimize Reconstruction Error 一样。\n这就是引入Discriminator ，来替代minimize Reconstruction Error 的做法。\n Typical auto-encoder is a special case\n经典的auto-encoder 可以看作上述的方法的一个特例。怎么说呢\n\n你可以把Discriminator 看作它根据vector 做了一个Decoder 做的事情，然后把输出和原始输入算一个Reconstruction Error 分数。\n Sequential Data\n如果你的数据是序列性的，比如说文章，这时你就可以做更多的变化。\n\n Skip thought\n\nhttps://papers.nips.cc/paper/5950-skip-thought-vectors.pdf\n\n如上图所示，Skip thought 就是根据当前输入的的句子预测上下文。这件事和训 WordEmbedding 是很像的，WordEmbedding 是说把文章中出现在同样或者相似上下文的单词语义应该是一样的，所以用相同或者相似的Embedding 来表示。Skip thought 的思想也是这样，只是扩展到句子，如果两个句子的上下文是相同或者相似的，那这两个句子应该是语义相近的。比如说，A提问这个东西有多贵，B回答10块，A提问这个东西要多少钱，B回答10块。Skip thought 就会知道有多贵和要多少钱的语义是相似的。\nSkip thought 不仅要训练Encoder（做embedding） 还要训练Decoder （做预测上下文），让机器产生预测结果，这是比较耗时的。\n Quick thought\n\nhttps://arxiv.org/pdf/1803.02893.pdf\n\n这是Skip thought 的升级版，速度比较快。我就只learn encoder 不去learn decode 了。\n我们现在把每个句子都同过encoder 得到各自的embedding ，每个句子要跟它的下个句子的embedding 越近越好。如果是其他的句子，那它们的embedding 就要和当前句子的embedding 越远越好。\n真正实作的时候就是，有个classifier 它吃当前句子embedding 、下个句子embedding 和一些随机sample 的句子的embedding ，然后给出哪个句子是当前句子的下一句。如上图所示。\nclassifer 和encoder 是共同训练的。文章中的classifier 是很简单的，它就是算当前句子embedding 和其他句子embedding 之间的内积，内积越大就越相信这个句子是当前句子的下一个。\n Contrastive Predictive Coding (CPC)\n\n\nhttps://arxiv.org/pdf/1807.03748.pdf\n\n看图就大概能明白它在做什么，输入是一段声音，这段声音分成小段，通过encoder 得到对应的embedding ，希望得到的embedding 能去预测接下来的同一个encoder 会output 的embedding 。就和上面Quick thought 的概念有点像。\n More interpretable embedding\n接下来我们进入下一个主题，怎么让embedding 更具解释性。\n\n Feature Disentangle\nDisentangle 这个词的中文意思是&quot;解开&quot;，大概就是下图中一团东西缠在一起想要解开的那个解开的意思。\n\n我们encoding 的对象包含各式各样的信息，比如说一段声音信号，包含语者的语义、语者的语调、环境噪音等等的信息，文字也是一样，有语义的信息、文法的信息等，图片也是，内容的信息、图片风格的信息等。\n以语音信息为例，encoder 得到的embedding 可能包括语者的语义信息、语调信息、环境噪声等很多信息，但是我们不知道这个向量中那些维度对应那些信息。现在，我们希望decoder 可以告诉我们那些维度是语义信息、哪些维度是语者的信息等等。\n\n这个想法，可以通过两个思路来做。第一种，如上图上半部分所示，我们希望encoder 输出的embedding 有一部分代表语音信息，一部分代表语者信息。第二种，做个变形如上图下半部分所示，我们搞两个encoder 一个提取出语义信息的embedding 一个提取出语者信息的embedding ，把两个embedding 拼在一起放入Decoder 才能还原输入。\n这里是做了简化，假设只有两个信息，实际上有更多的。\n那这样的话能做到的什么呢？\n Feature Disentangle-Voice Conversion\n一看就知道李老师也是老变声器了。🤣\n\n上图很好理解了，我们如上图所示做训练，我们用语音信息训练这个auto-encoder 模型，让encoder output 的embedding 可以把语者和语义信息分开来。然后：\n\n我们做如上图所示的embedding 拼接，丢给decoder 它就能输出男生说&quot;How are you?&quot;的音频。😮\n你可能会说这有什么用？这当然有用，比如用新垣结衣的声音劝你读博士，你可能就会满口答应下来。🤣\n\n那要怎么做才能让encoder 把不同的信息分开到不同的维度上呢？下面介绍几种做法。\n Feature Disentangle-Adversarial Training\n用GAN的思想\n\n如上图所示，做法是这样的，我们训练一个语者辨识的classifier ，Encoder 想办法去骗过speaker classifier 。比如说把embedding 的前100维给speaker classifier 做辨识，当Encoder 尝试骗过speaker classifier 的时候，Encoder 就可能会把有关语者的信息藏在embedding 后面的维度中。以此，来做到把语者和语义信息分开到不同的维度上。\n所以，从GAN的角度来说，speaker classifier 就是Discriminator ，Encoder 就是Generator，speaker classifier 和Encoder 是迭代train的，就是你先train speaker classifier 再train Encoder ，如此交替往复。\n Feature Disentangle-Designed Network Architecture\n\n我们还可以直接修改Encoder 的架构，如上图所示，直接让不同的Encoder 输出对应的信息，滤掉不需要的信息。举例来说，有一种神经网络的layer 叫instance normalization ，这种layer 我们就不展开讲了，它能做到的就是移除global information ，global information 就是所有样本都具有的信息。那当我们把同一个人说的话都输入网络，网络种instance normalization layer 就可能会滤掉语者信息。\n但是这样是不够的，这样就算我们保证了Encoder1 只包含了语义信息，我们也不能保证Encoder2 只包含语者信息。所以我们要在Decoder 上加一个adaptive instance normalization layer ，如下图所示：\n\nEncoder1 的embedding 直接input 给Deocder ，Encoder2 的embedding input 到AdaIN 这个layer ，AdaIN 会调整输出的global 的information ，也就是说如果Encoder2 在其输出的embedding 中放了语义信息，这个embedding 就会很大程度上改变Deocder 的输出。用这种方法，使Encoder2 输出的embedding 尽量不包含语义信息。\n用这种改变Encoder 架构的方法来实现，将不同信息分离到不同维度上。\n这里老师演示了他的学生Ju-chieh Chou 用Adversarial Training 得到的结果，可以参考：\n\nhttps://jjery2243542.github.io/voice_conversion_demo/\n\n Discrete Representation\n接下来我们要讲的是，过去我们在训Auto-Encoder 的时候得到的向量都是连续值，这个向量具体表示什么，你可能自己也不是很清楚。现在我们考虑Encoder 能不能输出离散的embedding ，这样我们就更容易解读这个embedding 的含义，也更容易做分群，比如说Encoder 输出是1的图片是一类，输出是2的图片是一类。所以，Encoder 如果能输出Discrete 的embedding ，那解读起来会更容易。\n\n\nhttps://arxiv.org/pdf/1611.01144.pdf\n\n举例来说，我们就让embedding 是One-hot embedding，如上图所示，我们就在Encoder 输出embedding 后面加点东西，它做到的事情就是把整个embedding 中最大的一维设为1，其他都是设0就好了。\n如果你不想要one-hot vector ，那也可以让embedding 转为binary vector ，如上图下半部分所示，某个维度的值大于0.5就设1，否则就设0。\n你可能会说那这个东西没法微分啊，但是实际上还是有一些技巧可以做的，这里就不展开了，可以自行查阅论文。\n Vector Quantized Variational Auto-encoder (VQVAE)\n上述的做法在文献上有个非常知名的做法叫做VQVAE\n\n\nhttps://arxiv.org/abs/1711.00937\nhttps://arxiv.org/pdf/1901.08810.pdf\n\nVQVAE 的做法是这样的，有一个Codebook 其中包含多个vector，这里假设只有5个好了，这些vector 是从数据中学出来的。输入一张图片给Encoder 输出一个数值上是连续的embedding ，那这个embedding 和Codebook 中的vector 算相似度，取相似度最高的作为Encoder 的输入，然后取minimize reconstruction error，结束。\n你会说，你算完相似度然后取最相似的vector 这个步骤相当于是在做Discrete，但是这不是没法微分吗，实际上是可以做的，有一些技巧，不展开了，自行读文献吧。\n还有一个重要的事情是，如果你用VQVAE 或者其他的Discrete embedding的方法，那你就能做到让Deocder 得到的Discrete embedding 只包含语义信息而不包含语者信息和环境噪声等，也就是说只有有关文字的信息才会被存下来。原因是这样的，你想想看这些Discrete embedding 就是容易存Discrete 的信息，而声音信息、环境噪声都是连续的，但文字信息是一个一个的token，是离散的，所以说文字信息被保留下来，其他信息被滤掉了。\n Sequence as Embedding\n我们上面说让embedding 变成离散的会更容解读，那我们甚至可以让中间表示不再是一个向量，让它用word sequence 表示：\n\n\nhttps://arxiv.org/abs/1810.02851\n\n假如说我们的Encoder 的输入对象是document，我们可以learn 一个seq2seq2seq 的model ，Encoder 做文章压缩，得到中间的word sequence ，Decoder 根据中间词序列做文章还原。这样的话我们直觉上会觉得，中间的word sequence 就是对文章的summary 。但是事实上如果我们直接这么train 下去中间的结果是不可读的。因为，Encoder 和Decoder 都是机器，他们会自己的暗语，说一些只有他们才能懂的word sequence 。比如说，台湾大学，中结果可能不是&quot;台大&quot;，而是是&quot;湾学&quot;，反正只要Deocder 能正确解回台湾大学就可以了。\n那怎么让Encoder 输出的sequence 让人能看懂呢，我们就用GAN 的技术。\n\n我们再那一个Discriminator 来，它来判断一个句子是不是人写的，让Encoder 努力学习骗过Discriminator 。这样就能让中间的word sequence 变得人类可读。\n你可能又说，这里又不能微分啊，中间的latent representation 是一个word sequence ，也是Discrete 的，Encoder Decoder 整个network 合起来不能微分啊。确实不能微分，所以实作的时候是用reinforcement learning 硬train Encoder 和Deocder 的。\n下面是一些实验结果：\n\n\n Tree as Embedding\n\n\nhttps://arxiv.org/abs/1806.07832\nhttps://arxiv.org/abs/1904.03746\n\n另外还有一些比较新的研究成果，供大家参考。\n Conclusion\n\n总结一下就是讲了：\n\n除了reconstruction error 以外有没有别的做法\n\nUsing Discriminator\nSequential Data\n\n\n有没有比较好的解释embedding 的方法\n\nFeature Disentangle\nDiscrete and Structured\n\n\n\n","categories":["李宏毅机器学习笔记"],"tags":["ML","note","Next-Step-of-ML"]},{"title":"Meta Learning-Metric-based","url":"https://ch3nye.top/Meta-Learning-Metric-based/","content":" Meta Learning-Metric-based Approach\n加下来我们就要实践我们之前提到的疯狂的想法：直接学一个function，输入训练数据和对应的标签，以及测试数据，直接输出测试数据的预测结果。也就是说这个模型把训练和预测一起做了。\n\n虽然这个想法听起很不错，好像挺难的，但是实际上现实生活中有在使用这样的技术，举例来说：手机的人脸验证\n\n我们在使用手机人脸解锁的时候需要录制人脸信息，这个过程中我们转动头部，就是手机在收集资料，收集到的资料就是作为few-shot learning 的训练资料。另外，语音解锁Speaker Verification 也是一样的技术，只要换一下输入资料和network 的架构。\n这里需要注意Face Verification 和Face Recognition 是不一样的，前者是说给你一张人脸，判定是否是指定的人脸，比如人脸验证来解锁设备；后者是辨别一个人脸是人脸集合里面谁，比如公司人脸签到打卡。\n下面我们就以Face Verification 为例，讲一下Metric-based Meta Learning\n Training Tasks &amp; Testing Tasks\n\n训练任务集中的任务都是人脸辨识数据，每个任务的测试集就是某个人的面部数据，测试集就是按标准（如手机录制人脸）收集的人脸数据，如果这个人和训练集相同就打一个Yes 标签，否则就打一个No 标签。测试任务和训练任务类似。总的来说，network 就是吃训练的人脸和测试的人脸，它会告诉你Yes or No 。\n Siamese Network\n实际上是怎么做的呢，使用的技术是Siamese Network（孪生网络）。\n\nSiamese Network 的结构如上图所示，两个网络往往是共享参数的，根据需要有时候也可以不共享，假如说你现在觉得Training data 和Testing data 在形态上有比较大的区别，那你就可以不共享两个网路的参数。\n从两个CNN 中抽出两个embedding ，然后计算这两个embedding 的相似度，比如说计算conference similarity 或者Euclidean Distance ，你得到一个数值score ，这个数值大就代表Network 的输出是Yes ，如果数值小就代表输出是No 。\n Siamese Network - Intuitive Explanation\n接下来从直觉上来解释一下孪生网络。\n\n如上图所示，你可以把Siamese Network 看成一个二分类器，他就是吃进去两张人脸比较一下相似度，然后告诉我们Yes or No 。这样解释会比从Meta Learning 的角度来解释更容易理解。\n\n如上图所示，Siamese Network 做的事情就是把人脸投影到一个空间上，在这个空间上只要是同一个人的脸，不管他是往哪边看，不管机器看到的是他的哪一侧脸，都能被投影到这个空间的同一个位置上。\n那你就会想了，这种图片降维的方法，这和Auto-Encoder 有什么区别呢，他比Auto-Encoder 好在哪？\n你想你在做Auto-Encoder 的时候network 不知道你要解的任务是什么，它会尽可能记住图片中所有的信息，但是它不知道什么样的信息是重要的什么样的信息是不重要的。上图右侧，如果用Auto-Encoder 它可能会认为一花（左下）和三玖（右上）是比较接近的，因为他们的背景相似，这好吗，这不好。在Siamese Network 中，因为你要求network 把一花（左下）和三玖（右上）拉远，把三玖（右上）和三玖（右下）拉近，它可能会学会更加注意头发颜色的信息要忽略背景的信息。\n To learn more…\n\nWhat kind of distance should we use?\n\nSphereFace: Deep Hypersphere Embedding for Face Recognition\nAdditive Margin Softmax for Face Verification\nArcFace: Additive Angular Margin Loss for Deep Face Recognition\n\n\nTriplet loss（三元是指：Data 可以包括训练人脸，正确的测试人脸，错误的测试人脸）\n\nDeep Metric Learning using Triplet Network\nFaceNet: A Unified Embedding for Face Recognition and Clustering\n\n\n\n N-way Few/One-shot Learning\n刚才的栗子中，训练资料都只有一张，机器只要回答Yes or No 。那现在如果是一个分类的问题呢？现在我们打算把同样的概念用在5-way 1-shot 的任务上该怎么办呢？\n\n5-way 1-shot 就是说5个类别，每个类别中只有1个样本。就比如说上图，《五等分花嫁》中的五姐妹，要训一个模型分辨一个人脸是其中的谁，而训练资料是每个人只有一个样本。我们期待做到的事情是，Network 就把这五张带标签的训练图片外加一张测试图片都吃进去，然后模型就会告诉我们测试图片的分辨结果。\n那模型的架构要怎么设计呢，比如说一个经典的设计：\n Prototypical Network\n这是一个经典的做法：\n\n\nhttps://arxiv.org/abs/1703.05175\n\n这个方法和Siamese Network 非常相似，只不过从input 一张training data 扩展到input 多张training data 。\n来解释一下这个方法，如上图所示，把每张图片丢到同一个CNN 中算出一个embedding 用橙色条表示，然后把测试图片的embedding 和所有训练图片的embedding 分别算出一个相似度 sis_isi​ 。黄色的方块表示计算相似度。接下来，取一个softmax ，这样就可以和正确的标签做cross entropy ，去minimize cross entropy，这就和一般的分类问题的loss function 相同的，就可以根据这个loss 做一次gradient descent ，因为是1-shot 所以只能做一次参数更新。\n？？？？这里有问题呀，如果testing data 是已知标签的数据，用来做了1-shot，那真正的要预测的数据呢？？？？？\n如果你看明白了，请务必教教我🙏🙏🙏发邮件@sud0su@163.com或者加我Q@2948335218\n那如果是Few-shot 呢，怎么用Prototypical Network 解决呢。如右上角，我们把每个类别的几个图片用CNN 抽出的embedding 做average 来代表这个类别就好了。进来一个Testing Data 我们就看它和哪个class 的average 值更接近，就算作哪一个class 。\n Matching Network\nMatching Network 和Prototypical Network 最不同的地方是，Matching Network 认为也许Training data 中的图片互相之间也是有关系的，所以用Bidirectional LSTM 处理Training data，把Training data 通过一个Bidirectional LSTM 也会得到对应的embedding ，然后的做法就和Prototypical Network 是一样的。\n\n\nhttps://arxiv.org/abs/1606.04080\n\n事实上是Matching Network 先被提出来的，然后人们觉得这个方法有点问题，问题出在Bidirectional LSTM 上，就是说如果输入Training data 的顺序发生变化，那得到的embedding 就变了，整个network 的辨识结果就可能发生变化，这是不合理的。\n Relation Network\n\n\nhttps://arxiv.org/abs/1711.06025\n\n这个方法和上面讲过的很相似，只是说我们之前通过人定的相似度计算方法计算每一类图片和测试图片的相似度，而Relation Network 是希望用另外的模型 gϕg_\\phigϕ​ 来计算相似度。\n具体做法就是先通过一个 fϕf_\\phifϕ​ 计算每个类别的以及测试数据的embedding ，然后把测试数据的embedding 接在所有类别embedding 后面丢入 gϕg_\\phigϕ​ 计算相似度分数。\n Few-shot learning for Imaginary Data\n我们在做Few-Shot Learning 的时候的难点就是训练数据量太少了，那能不能让机器自己生成一些数据提供给训练使用呢。这就是Few-shot learning for Imaginary Data 的思想。\n\n\nhttps://arxiv.org/abs/1801.05401\n\nLearn 一个Generator GGG ，怎么Learn 出这个Generator 我们先不管，你给Generator 一个图片，他就会生成更多图片，比如说你给他三玖面无表情的样子，他就会YY出三玖卖萌的样子、害羞的样子、生气的样子等等。然后把生成的图片丢到Network 中做训练，结束。\n实际上，真正做训练的时候Generator 和Network 是一起训的，这就是Few-shot learning for Imaginary Data 的意思。具体的做法，这里不展开了。\n Meta Learning-Train+Test as RNN\n我们在讲Siamese Network 的时候说，你可以把Siamese Network 或其他Metric-based 的方法想成是Meta Learning ，但其实你是可以从其他更容易理解的角度来考虑这些方法。总的来说，我们就是要找一个function，这个function 可以做的到就是吃训练数据和测试数据，然后就可以吐出测试数据的预测结果。我们实际上用的Siamese Network 或者Prototypical Network 、Matching Network 等等的方法多可以看作我们为了实现这个目的做模型架构的变形。\n现在我们想问问，有没有可能直接用常规的network 做出这件事？有的。\n\n用LSTM 把训练数据和测试数据吃进去，在最后输出测试数据的判别结果。训练图片通过一个CNN 得到一个embedding ，这个embedding 和这个图片的label（one-hot vector）做concatenate（拼接）丢入LSTM 中，Testing data 我们不知道label 怎么办，我们就用0 vector 来表示，然后同样丢入LSTM ，得到output 结束。这个方法用常规的LSTM 是train 不起来的，我们需要修改LSTM 的架构，有两个方法：\n\n具体方法我们就不展开讲了，放出参考链接：\n\n\nOne-shot Learning with Memory-Augmented Neural Networks\nhttps://arxiv.org/abs/1605.06065\nA Simple Neural Attentive Meta-Learner\nhttps://arxiv.org/abs/1707.03141\n\nSNAIL 你看看他的架构图就能了解他在做什么，如上图右侧所示，和我们上面刚说过想法的是一样的，输入一堆训练数据给RNN 然后给他一个测试数据它输出预测结果，唯一不同的东西就是，它不是一个单纯的RNN ，它里面有在做回顾这件事，它在input 第二笔数据的时候会回去看第一笔数据，在input 第三笔数据的时候会回去看第一第二笔数据…在input 测试数据额时候会回去看所有输入的训练数据。\n所以你会发现这件事是不是和prototypical network 和matching network 很相似呢，matching network 就是计算input 的图片和过去看过的图片的相似度，看谁最像，就拿那张最像的图片的label 当作network 的输出。SNAIL 的回顾过去看过的数据的做法就和matching network 的计算相似度的做法很像。\n所以说，你虽然想用更通用的方法做到一个模型直接给出测试数据预测结果这件事，然后你发现你要改network 的架构，改完起了个名字叫SNAIL 但是他的思想变得和原本专门为这做到这件事设计的特殊的方法如matching network 几乎一样了，有点殊途同归的意思。\n Experiment\n\n总之SNAIL 和其他方法相比都是最好的，没什么好说的了。\n","categories":["李宏毅机器学习笔记"],"tags":["ML","note","Next-Step-of-ML"]},{"title":"Meta Learning-Gradient Descent as LSTM","url":"https://ch3nye.top/Meta-Learning-Gradient-Descent-as-LSTM/","content":" Meta Learning - Gradient Descent as LSTM\n上节课讲了MAML 和Reptile ，我们说Meta Learning 就是要让机器自己learn 出一个learning 的algorithm。今天我们要讲怎么把我们熟悉的learning algorithm ：Gradient Descent ，当作一个LSTM 来看待，你直接把这个LSTM train下去，你就train 出了Gradient Descent 这样的Algorithm 。（也就是说我现在要把学习算法，即参数的更新算法当作未知数，用Meta Learning 训练出来）\n\n上周我们讲的MAML 和Reptile 都是在Initial Parameters 上做文章，用Meta Learning 训练出一组好的初始化参数，现在我们希望能更进一步，通过Meta Learning 训练出一个好的参数update 算法，上图黄色方块。\n我们可以把整个Meta Learning 的算法看作RNN，它和RNN 有点像的，同样都是每次吃一个batch 的data ，RNN 中的memory 可以类比到Meta Learning 中的参数 θ\\thetaθ 。\n把这个Meta Learning 的算法看作RNN 的思想主要出自两篇paper ：\n\nOptimization as a Model for Few-Shot Learning | OpenReview\nSachin Ravi, Hugo Larochelle\n[1606.04474] Learning to learn by gradient descent by gradient descent (arxiv.org)\nMarcin Andrychowicz, Misha Denil, Sergio Gomez, Matthew W. Hoffman, David Pfau, Tom Schaul, Brendan Shillingford, Nando de Freitas\n\n第二篇文章的题目非常有趣，也说明了此篇文章的中心：让机器学习用梯度下降学习这件事，使用的方法就是梯度下降。\n Review: RNN\n从与之前略微不同的角度快速回顾一下RNN。\n\nRNN就是一个function f，这个函数吃h,x 吐出 h’,y  ，每个step 会有一个x（训练样本数据）作为input，还有一个初始的memory 的值 h0h_0h0​ 作为input，这个初始参数有时候是人手动设置的，有时候是可以让模型learn 出来的，然后输出一个y和一个 h1h^1h1 。到下一个step，它吃上一个step 得到的 h1h^1h1 和新的x，也是同样的输出。需要注意的是，h的维度都是一致的，这样同一个f 才能吃前一个step 得到h 。这个过程不断重复，就是RNN。\n所以，无论多长的input/output sequence 我们只需要一个函数f 就可以运算，无论你的输入再怎么多，模型的参数量不会变化，这就是RNN 厉害的地方，所以它特别擅长处理input 是一个sequence 的状态。（比如说自然语言处理中input 是一个长句子，用word vector 组成的很长的sequence）\n我们如今用的一般都是RNN 的变形LSTM，而且我们现在说使用RNN 基本上就是在指使用LSTM 的技术。那LSTM 相比于RNN 有什么特别的地方呢。\n\n如上图，LSTM（右）相比于RNN ，把input 的h 拆解成两部分，一部分仍然叫做 hhh ，一部分我们叫做 ccc 。为什么要这样分呢，你可以想象是因为 ccc 和 hhh 扮演了不同的角色。\n\nccc 变化较慢，通常就是把某个向量加到上一个 ct−1c^{t-1}ct−1 上就得到了新的 ctc^tct ，这个 ctc^tct 就是LSTM 中memory cell 存储的值，由于这个值变化很慢，所以LSTM 可以记住时间比较久的数据\nhhh 变化较快， ht−1h^{t-1}ht−1 和 hth^tht 的变化是很大的\n\n Review: LSTM\n我们接下来看看LSTM 的做法和结构：\n\nct−1c^{t-1}ct−1 是memory 记忆单元，把x和h拼在一起乘上一个权重矩阵W，再通过一个tanh 函数得到input z，z是一个向量。同样的x和h拼接后乘上对应的权重矩阵得到对应向量input gate ziz^izi ，forget gate zfz^fzf ，output  gate zoz^ozo ，接下来：\n\nzf⋅ct−1z^f \\cdot c^{t-1}zf⋅ct−1 决定是否保留上个memory， zi⋅zz^i \\cdot zzi⋅z 决定是否把现在的input 存到memory；\n通过 zo⋅tanh(ct)z^o \\cdot tanh(c^t)zo⋅tanh(ct) 得到新的 hth^tht ；\nW′W&#x27;W′ 乘上新的 hth^tht ，再通过一个sigmoid function 得到当前step 的output yty^tyt ；\n重复上述步骤，就是LSTM 的运作方式：\n\n好，讲了这么多，它和Gradient Descent 到底有什么样的关系呢？\n LSTM similar to gradient descent based algorithm\n我们把梯度下降参数θ更新公式和LSTM 的memory c更新公式都列出来，如下图所示：\n\n我们知道在gradient descent 中我们在每个step 中，把旧的参数减去，learning rate 乘梯度，作为更新后的新参数，如上图所示，此式，和LSTM 中memory 单元 ccc 有些相似，我们就把 ccc 替换成 θ\\thetaθ 看看：\n\n接下来我们再做一些变换。输入ht−1h^{t-1}ht−1 来自上一个step，xtx^txt 来自外界输入，我们就把ht−1h^{t-1}ht−1 xtx^txt 换成$-\\nabla_\\theta l $ 。然后我们假设从input 到z 的公式中乘的matrix 是单位矩阵，所以z 就等于$-\\nabla_\\theta l $ 。再然后，我们把zfz^fzf 定位全1的列向量，ziz^izi 定位全为learning rate 的列向量，此时LSTM 的memory ccc 的更新公式变得和Gradient Descent 一摸一样：\n\n所以你可以说Gradient Descent 就是LSTM 的简化版，LSTM中input gate 和forget gate是通过机器学出来的，而在梯度下降中input gate 和forget gate 都是人设的，input gate 永远都是学习率，forget gate 永远都是不可以忘记。😮\n现在，我们考虑能不能让机器自己学习gradient descent 中的input gate 和forget gate 呢？\n另外，input的部分刚才假设只有gradient 的值，实作上可以拿更多其他的数据作为input，比如常见的做法，可以把 ct−1c^{t-1}ct−1 在现在这个step算出来的loss 作为输入来control 这个LSTM的input gate 和forget gate 的值。\n\n如果们可以让机器自动的学input gate 和forget gate 的值意味着什么，意味着我们可以拥有动态的learning rate，每一个step 中learning rate 都是不一样的而不是一个不变的值。而 zfz^fzf 就像一个正则项，它做的事情是把前一个step 算出来的参数缩小。我们以前做的L2 regularization 又叫做Weight Decade ，为什么叫Weight Decade，因为如果你把它微分的式子拿出来看，每个step 都会把原来的参数稍微变小，现在这个zfz^fzf 就扮演了像是Weight Decade 的角色。但是我们现在不是直接告诉机器要做多少Weight Decade 而是要让机器学出来，它应该做多少Weight Decade 。\n LSTM for Gradient Descent\n我们来看看一般的LSTM和for Gradient Descent 的LSTM：\n\nTypical LSTM 就是input x ，output c 和 h，每个step 会output 一个y ，希望y 和label 越接近越好。\nGradient Descent 的LSTM是这样：我们先sample 一个初始参数θ ，然后sample 一个batch 的data ，根据这一组data 算出一个gradient ∇θl\\nabla_\\theta l∇θ​l ，把负的gradient input 到LSTM 中进行训练，这个LSTM 的参数过去是人设死的，我们现在让参数在Meta Learning 的架构下被硬learn 出来。上述的这个update 参数的公式就是：\nθt=zf⋅θt−1+zi⋅−∇θl\\theta^t = z^f \\cdot \\theta^{t-1} + z^i \\cdot -\\nabla_\\theta l\nθt=zf⋅θt−1+zi⋅−∇θ​l\nzfz^fzf ziz^izi 以前是人设死的，现在LSTM 可以自动把它学出来。\n现在就可以output 新的参数θ1\\theta^1θ1 ，接着就是做一样的事情：再sample 一组数据，算出梯度作为新的input，放到LSTM 中就得到output θ2\\theta^2θ2 ，以此类推，不断重复这个步骤。最后得到一组参数θ3θ^3θ3（这里假设只update 3次，实际上要update 更多次），拿这组参数去应用到Testing data 上算一下loss ： l(θ3)l(θ^3)l(θ3) ，这个loss 就是我们要minimize 的目标，然后你就要用gradient descent 调LSTM 的参数，去minimize 最后的loss 。\n\n到这里可能比较懵了，我在这里写一下我的理解不一定对，欢迎指正。看完下面的[Experimental Results](#Experimental Results) 一节可以回来再看一遍这个解释：\n一般来说我们使用network 作为模型，其中会有很多参数θ，这些参数每一个都会拿到这个LSTM 中做如上述训练，一方面在train LSTM 中的参数，一方面在train 每一个参数θ。当network中所有θ都经过一轮上述的LSTM 的训练以后，得到的一组参数放回network 中，用testing data 计算loss of θ，据此用梯度下降回调LSTM 参数。如此往复，去minimize loss，最后就得到了一组比较好的参数，使得network 能在testing data 上取得比较好的成绩，这个过程中LSTM 担任了以前使用的梯度下降来update 参数的角色，而且LSTM 中的zfz^fzf ziz^izi 还是动态变化的，可能要比经典梯度下降效果好。\n\n这里有一些需要注意的地方。在一般的LSTM 中c 和x 是独立的，LSTM 的memory 存储的值不会影响到下一次的输入，但是Gradient Descent LSTM 中参数θ会影响到下一个step 中算出的gradient 的值，如上图虚线所示。所以说在Gradient Descent LSTM 中现在的参数会影响到未来看到的梯度。所以当你做back propagation 的时候，理论上你的error signal 除了走实线的一条路，它还可以走θ到−∇θl-\\nabla_\\theta l−∇θ​l 虚线这一条路，可以通过gradient 这条路更新参数。但是这样做会很麻烦，和一般的LSTM 不太一样了，一般的LSTM c 和x 是没有关系的，现在这里确实有关系，为了让它和一般的LSTM 更像，为了少改一些code ，我们就假设没有虚线那条路，结束。现在的文献上其实也是这么做的。\n另外，在LSTM input 的地方memory 中的初始值可以通过训练直接被learn 出来，所以在LSTM中也可以做到和MAML相同的事，可以把初始的参数跟着LSTM一起学出来。\n Real Implementation\nLSTM 的memory 就是要训练的network 的参数，这些参数动辄就是十万百万级别的，难道要开十万百万个cell 吗？平常我们开上千个cell 就会train 很久，所以这样是train不起来的。在实际的实现上，我们做了一个非常大的简化：我们所learn 的LSTM 只有一个cell 而已，它只处理一个参数，所有的参数都公用一个LSTM。所以就算你有百万个参数，都是使用这同一个LSTM 来处理。\n\n也就是说如上图所示，现在你learn 好一个LSTM以后，它是直接被用在所有的参数上，虽然这个LSTM 一次只处理一个参数，但是同样的LSTM 被用在所有的参数上。θ1θ^1θ1 使用的LSTM 和θ2θ^2θ2 使用的LSTM 是同一个处理方式也相同。那你可能会说，θ1θ^1θ1 和 θ2θ^2θ2 用的处理方式一样，会不会算出同样的值呢？会不，因为他们的初始参数是不同的，而且他们的gradient 也是不一样的。在初始参数和算出来的gradient 不同的情况下，就算你用的LSTM的参数是一样的，就是说你update 参数的规则是一样的， 最终算出来的也是不一样的 θ3θ^3θ3 。\n这就是实作上真正implement LSTM Gradient Descent 的方法。\n这么做有什么好处：\n\n在模型规模上问题上比较容易实现\n在经典的gradient descent 中，所有的参数也都是使用相同的规则，所以这里使用相同的LSTM ，就是使用相同的更新规则是合理的\n训练和测试的模型架构可以是不一样的，而之前讲的MAML 需要保证训练任务和测试任务使用的model architecture 相同\n\n Experimental Results\n\n\nhttps://openreview.net/forum?id=rJY0-Kcll&amp;noteId=ryq49XyLg\n\n我们来看一个文献上的实验结果，这是做在few-shot learning 的task上。横轴是update 的次数，每次train 会update 10次，左侧是forget gate zfz^fzf 的变化，不同的红色线就是不同的task 中forget gate 的变化，可以看出zfz^fzf 的值多数时候都保持在1附近，也就是说LSTM 有learn 到θt−1θ^{t-1}θt−1 是很重要的东西，没事就不要给他忘掉，只做一个小小的weight decade，这和我们做regularization 时候的思想相同，只做一个小小的weight decade 防止overfitting 。\n右侧是input gate ziz^izi 的变化，红线是不同的task，可以看出它的变化有点复杂，但是至少我们知道，它不是一成不变的固定值，它是有学到一些东西的，是动态变化的，放到经典梯度下降中来说就是learning rate 是动态变化的。\n LSTM for Gradient Descent (v2)\n只有刚才的架构还不够，我们还可以更进一步。想想看，过去我们在用经典梯度下降更新参数的时候我们不仅会考虑当前step 的梯度，我们还会考虑过去的梯度，比如RMSProp、Momentum 等。\n\n\n在刚才的架构中，我们没有让机器去记住过去的gradient ，所以我们可以做更进一步的延伸。我们在过去的架构上再加一层LSTM，如下图所示：\n\n蓝色的一层LSTM 是原先的算learning rate、做weight decade 的LSTM，我们再加入一层LSTM ，让算出来的gradient −∇θl-\\nabla_\\theta l−∇θ​l 先通过这个LSTM ，把这个LSTM 吐出来的东西input 到原先的LSTM 中，我们希望绿色的这一层能做到记住以前算过的gradient 这件事。这样，可能就可以做到Momentum 可以做的的事情。\n上述的这个方法，是老师自己想象的，在learning to learn by gradient descent by gradient descent 这篇paper 中上图中蓝色的LSTM 使用的是一般的梯度下降算法，而在另一篇paper 中只有上面没有下面，而老师觉得这样结合起来才是实现，能考虑过去的gradient 的gradient descent 算法的完全体。\n Experimental Result 2\nlearning to learn by gradient descent by gradient descent 这篇paper 的实验结果。\n\n\nhttps://arxiv.org/abs/1606.04474\n\n第一个实验图，是做在toy example 上，它可以制造一大堆训练任务，然后测试在测试任务上，然后发现，LSTM 来当作gradient descent 的方法要好过人设计的梯度下降方法。\n第二张图把这个技术应用带MNIST 上，这个实验是训练任务测试任务都是MNIST。\n第三张图是说虽然训练和测试任务都是相同的dataset也是相同的，但是train 和test 的时候network 的架构是不一样的。 在train 的时候network 是只有一层，该层只有20个neuron。这张图是training 的结果。\n第四张图是上述改变network 架构后在testing 的结果，testing 的时候network 只有一层该层40个neuron。从图上看还是做的起来，而且比一般的gradient descent 方法要好很多。\n第五张图是上述改变network 架构后在testing 的结果，testing 的时候network 有两层。从图上看还是做的起来，而且比一般的gradient descent 方法要好很多。\n第六张图是上述改变network 激活函数后在testing 的结果，training 的时候激活函数是sigmoid 而testing 的时候改成ReLU。从图上看做不起来，崩掉了，training 和testing 的network 的激活函数不一样的时候，LSTM 没办法跨model 应用。\n","categories":["李宏毅机器学习笔记"],"tags":["ML","note","Next-Step-of-ML"]},{"title":"Meta Learning-MAML","url":"https://ch3nye.top/Meta-Learning-MAML/","content":" Meta Learning-MAML\nMeta learning 总体来说就是让机器学习如何学习。\n\n如上图，我们希望机器在学过一些任务以后，它学会如何去学习更有效率，也就是说它会成为一个更优秀的学习者，因为它学会了学习的技巧。举例来说，我们教机器学会了语音辨识、图像识别等模型以后，它就可以在文本分类任务上做的更快更好，虽然说语音辨识、图像识别和文本分类没什么直接的关系，但是我们希望机器从先前的任务学习中学会了学习的技巧。\n讲到这里，你可能会觉得Meta Learning 和 Life-Long Learning 有点像，确实很像，但是LLL 的着眼点是用同一个模型apply 到不同的任务上，让一个模型可以不断地学会新的任务，而Meta Learning 中不同的任务有不同的模型，我们的着眼点是机器可以从过去的学习中学到学习的方法，让它在以后的学习中更快更好。\n我们先来看一下传统的ML 的做法：\n\n我们过去学过的ML ，通常来说就是定义一个学习算法，然后用训练数据train ，吐出一组参数（或者说一个参数已定的函数式），也就是得到了模型，这个模型可以告诉我们测试数据应该对应的结果。比如我们做猫狗分类，train 完以后，给模型一个猫的照片，它就会告诉我们这是一只猫。\n我们把学习算法记为 FFF ，这个学习算法吃training data 然后吐出目标模型 f∗f^*f∗ ，形式化记作：\nf∗=F(Dtrain)f^* = F(D_{train})\nf∗=F(Dtrain​)\nMeta Learning 就是要让机器自动的找一个可以吃training data 吐出函数 f∗f^*f∗ 的函数 FFF 。\n总结一下：\n\nMachine Learning 和Meta Learning 都是让机器找一个function ，只不过要找的function 是不一样的。\n我们知道Machine Learning 一共分三步（如下图），Meta Learning 也是一样的，你只要把Function fff 换成**学习的算法 FFF **这就是Meta Learning 的步骤：\n\n\n我们先定义一组Learning 的Algorithm 我们不知道哪一个算法是比较好的，\n\n\n然后定义一个Learning Algorithm 的Loss ，它会告诉你某个算法的好坏，\n\n\n最后，去train 一发找出哪个Learning Algorithm比较好。\n\n\n所以接下来我们将分三部分来讲Meta Learning 的具体过程。\n\n Three Step of Meta-Learning\n Define a set of learning algorithm\n什么是一组learning algorithm 呢？\n\n如上图所示，灰色框中的，包括网络（模型）架构，初始化参数的方法，更新参数的方法，学习率等要素构成的整个process ，可以被称为一个learning algorithm 。在训练的过程中有很多要素都是人设计的，当我们选择不同的设计的时候就相当于得到了不同的learning algorithm 。现在，我们考虑能不能让机器自己学出某一环节，或者全部process 的设计。比如说，我们用不同的初始化方法得到不同的初始化参数以后，保持训练方法其他部分的相同，且用相同的数据来训练模型，最后都会得到不同的模型，那我们就考虑能不能让机器自己学会初始化参数，直接得到最好的一组初始化参数，用于训练。\n我们就希望通过Meta Learning 学习到初始化参数这件事，好，现在我们有了一组learning algorithm ，其中各个算法只有初始化参数的方法未知，是希望机器通过学习得出来的。\n那现在我们怎么衡量一个learning algorithm 的好坏呢？\n Define the goodness of a function F\n\n我们需要很多个task，每个task都有training set 和testing set，然后就把learning algorithm 应用到每个task上，用training set 训练，用testing set 测试，得到每一个task 的loss lil^ili，对于一个learning algorithm FFF 的整体loss 就可以用每个task 的loss 进行求和。\nL(F)=∑n=1NlnL(F) = \\sum\\limits_{n=1}^{N}l^n\nL(F)=n=1∑N​ln\n从这里我们就能看出，meta learning 和传统的machine learning 在训练资料上是有些不同的：\n\n做meta learning 的话你可能需要准备成百上千个task，每个task 都有自己的training set 和testing set 。这里为了区分，我们把meta learning的训练集叫做Training Tasks，测试集叫做Testing Tasks，其中中每个task 的训练集叫做Support set ，测试集叫做 Query set 。\nPS：这里有学生提问，根据老师的回答，大意就是，training tasks 和testing tasks可以是不同类型的任务，比如全部用影像辨识任务训练然后用语音辨识任务测试也是可以做得起来的。这取决于你的meta learning 的algorithm 是什么样的，有的训练和测试任务的类型可以是不一样的有些是需要一样的。\n讲到这里你可能觉得比较抽象，后面会讲到实际的栗子，你可能就理解了meta learning 的实际运作方法。Meta learning 有很多方法，加下来会讲几个比较常见的算法，本节课会讲到一个最有名的叫做MAML ，以及MAML 的变形叫做Reptile 。\n Find the best function F∗F^*F∗\n定好了loss function 以后我们就要找一个最好的F* ，这个F∗F^*F∗可以使所有的training tasks 的loss 之和最小，形式化的写作下图下面的公式（具体计算方法后面再讲）：\n\n现在我们就有了meta learning 的algorithm ，我们可以用testing tasks 测试这个F∗F^*F∗，把测试任务的训练集放丢入F∗F^*F∗，就会得到一个f∗f^*f∗ ，再用测试任务的测试集去计算这个f∗f^*f∗ 的loss ，这个loss 就是整个meta learning algorithm 的loss ，来衡量这个方法的好坏。\n Omniglot Corpus\n Introduction\n\nhttps://github.com/brendenlake/omniglot\n\n\n这是一个corpus，这里面有一大堆奇怪的符号，总共有1623个不同的符号，每个符号有20个不同的范例。上图下侧就是那些符号，右上角是一个符号的20个范例。\n Few-shot Classification\n\nN-ways K-shot classification 的意思是，分N个类别，每个类别有K个样例。\n所以，20 ways 1shot 就是说分20类，每类只有一个样例。这个任务的数据集就例如上图中间的20张support set 和1张query set 。\n\n把符号集分为训练符号集和测试符号集\n\n从训练符号集中随机抽N个符号，从这N个符号的范例中各随机抽K个样本，这就组成了一个训练任务training task 。\n从测试符号集中随机抽N个符号，从这N个符号的范例中各随机抽K个样本，这就组成了一个测试任务testing task 。\n\n\n\n Techniques Today\n\n这两个大概是最近（2019年）比较火的吧，Reptile 可以参考一下openai的这篇文章。\n\nReptile: A Scalable Meta-Learning Algorithm (openai.com)\n\n接下来先将MAML（详细） ，再讲Reptile（简略）。\n MAML\n\nMAML要做的就是学一个初始化的参数。过去你在做初始化参数的时候你可能要从一个distribution 中sample 出来，现在我们希望通过学习，机器可以自己给出一组最好的初始化参数。\n做法就如上图所示，我们先拿一组初始化参数 ϕ\\phiϕ 去各个training task 做训练，在第n个task 上得到的最终参数记作 θ^n\\hat{θ}^nθ^n ，而ln(θ^n)l^n(\\hat{θ}^n)ln(θ^n) 代表第n个task 在其testing set 上的loss，此时整个MAML 算法的Loss 记作：\nL(ϕ)=∑n=1Nln(θ^n)L(\\phi) = \\sum\\limits_{n=1}^{N}l^n(\\hat{θ}^n)\nL(ϕ)=n=1∑N​ln(θ^n)\n这里提示一下，MAML就是属于需要所有任务的网络架构相同的meta learning algorithm，因为其中所有的function 要共用相同的初始化参数 ϕ\\phiϕ 。\n那怎么minimize L(ϕ)L(\\phi)L(ϕ) 呢？\n答案就是Gradient Descent，你只要能得到 L(ϕ)L(\\phi)L(ϕ) 对 ϕ\\phiϕ 的梯度，那就可以更新 ϕ\\phiϕ 了，结束。😮\nϕ=ϕ−η∇ϕL(ϕ)\\phi = \\phi - \\eta \\nabla_{\\phi}L(\\phi)\nϕ=ϕ−η∇ϕ​L(ϕ)\n这里我们先假装已经会算这个梯度了，先把这个梯度更新参数的思路理解就好，具体的更新算法在[Warning of Math](#Warning of Math)中会将到，我们先来康一下MAML 和 Model Pre-training 在Loss Function 上的区别。\n MAML v.s. Model Pre-training\n\n通过上图我们仔细对比两个损失函数，可以看出，MAML是根据训练完的参数 θ^n\\hat{θ}^nθ^n 计算损失，计算的是训练好的参数在各个task 的训练集上的损失；而预训练模型的损失函数则是根据当前初始化的参数计算损失，计算的是当前参数在要应用pre-training 的task 上的损失。\n再举一个形象一点的栗子：\n\n（横轴是模型参数，简化为一个参数，纵轴是loss）\n如上图说的，我们在意的是这个初始化参数经过各个task 训练以后的参数在对应任务上的表现，也就是说如果初始参数 ϕ\\phiϕ 在中间位置（如上图），可能这个位置根据当前参数计算的总体loss 不是最好的，但是在各个任务上经过训练以后 θ^\\hatθθ^ 都能得到较低的loss（如 θ^1\\hat{θ}^1θ^1、θ^2\\hat{θ}^2θ^2），那这个初始参数 ϕ\\phiϕ 就是好的，其loss 就是小的。\n反之，在Model pre-training 上：\n\n我们希望直接通过这个 ϕ\\phiϕ 计算出来的各个task 的loss 是最小的，所以它的取值点就可能会是上图的样子。此时在task 2上初始参数可能不能够被更新到global minima，会卡在local minima 的点 θ^2\\hat{θ}^2θ^2。\n综上所述：\n\nMAML 是要找一个 ϕ\\phiϕ 在训练以后具有好的表现，注重参数的潜力，Model Pre-training 是要找一个 ϕ\\phiϕ 在训练任务上得到好的结果，注重现在的表现。\n MAML Only Update One Time\n\n在MAML 中我们只update 参数一次。所以在训练阶段，你只做one-step training ，参数更新公式就变成上图右下角的式子。\n只更新一次是有些理由的：\n\n为了速度，多次计算梯度更新参数是比较耗时的，而且MAML 要train 很多个task\n把只更新一次作为目标\n实际上你可以在training 的时候update 一次，在测试的时候，解testing task 的时候多update 几次结果可能就会更好\n如果是few-shot learning 的task，由于data 很少，update 很多次很容易overfitting\n\n Toy Example\n\n\nSource of images Paper repro: Deep Metalearning using “MAML” and “Reptile”\n\n\n要拟合的目标函数是 y=asin(x+b)y=asin(x+b)y=asin(x+b)\n对每个函数，也就是每个task，sample k个点\n通过这些点做拟合\n\n我们只要sample 不同的a, b就可以得到不同的目标函数。\n来看看对比结果：\n\n可以看到，预训练模型想要让参数在所有的task 上都做好，多个task叠加起来，导致预训练模型最后拟合得到的是一条直线。\nMAML 在测试task 上training step 增加的过程中有明显的拟合效果提升。\n Omniglot &amp; Mini-ImageNet\n在MAML 的原始论文中也把这个技术用于Omniglot &amp; Mini-ImageNet\n\n\nhttps://arxiv.org/abs/1703.03400\n\n我们看上图下侧，MAML, first order approx 和 MAML 的结果很相似，那approx 是怎么做的呢？解释这个东西需要一点点数学：\n Warning of Math\n这一段不想听的同学可以睡一会。😁\n\nMAML 的参数更新方法如上图左上角灰色方框所示，我们来具体看看这个 ∇ϕL(ϕ)\\nabla_\\phi L(\\phi)∇ϕ​L(ϕ) 怎么算，把灰框第二条公式带入，如黄色框所示。其中 ∇ϕln(θ^n)\\nabla_\\phi l^n(\\hat{θ}^n)∇ϕ​ln(θ^n) 就是左下角所示，它就是loss 对初始参数集 ϕ\\phiϕ 的每个分量的偏微分。也就是说 ϕi\\phi_iϕi​ 的变化会通过 θ^\\hat{θ}θ^ 中的每个参数 θ^i\\hat{θ}_iθ^i​ ，影响到最终训练出来的 θ^\\hat{θ}θ^ ，所以根据chain rule 你就可以把左下角的每个偏微分写成上图中间的公式。\n∂l(θ^)∂ϕi=∑j∂l(θ^)∂θ^j∂θ^j∂ϕi\\frac{\\partial{l(\\hat{θ})}}{\\partial{\\phi_i}} = \\sum\\limits_{j}\\frac{\\partial{l(\\hat{θ})}}{\\partial\\hat{θ}_j}\\frac{\\partial\\hat{θ}_j}{\\partial{\\phi}_i}\n∂ϕi​∂l(θ^)​=j∑​∂θ^j​∂l(θ^)​∂ϕi​∂θ^j​​\n上式中前面的项 ∂l(θ^)∂θ^j\\frac{\\partial{l(\\hat{θ})}}{\\partial\\hat{θ}_j}∂θ^j​∂l(θ^)​ 是容易得到的，具体的计算公式取决于你的model 的loss function ，比如cross entropy 或者regression，结果的数值却决于你的训练数据的测试集。\n后面的项 ∂θ^j∂ϕi\\frac{\\partial\\hat{θ}_j}{\\partial{\\phi}_i}∂ϕi​∂θ^j​​ 是需要我们算一下。可以分成两个情况来考虑：\n\n根据灰色框中第三个式子，我们知道 θ^j\\hat{θ}_jθ^j​ 可以用下式代替：\nθ^j=ϕj−ϵ∂l(ϕ)∂ϕj\\hat{θ}_j = \\phi_j - \\epsilon\\frac{\\partial{l(\\phi)}}{\\partial{\\phi_j}}\nθ^j​=ϕj​−ϵ∂ϕj​∂l(ϕ)​\n此时，对于 ∂θ^j∂ϕi\\frac{\\partial\\hat{θ}_j}{\\partial{\\phi}_i}∂ϕi​∂θ^j​​ 这一项，分为i=j 和 i!=j 两种情况考虑，如上图所示。在MAML 的论文中，作者提出一个想法，不计算二次微分这一项。如果不计算二次微分，式子就变得非常简答，我们只需要考虑i=j 的情况，i!=j 时偏微分的答案总是0。\n此时， ∂l(θ^)∂ϕi\\frac{\\partial{l(\\hat{θ})}}{\\partial\\phi_i}∂ϕi​∂l(θ^)​ 就等于 ∂l(θ^)∂θi\\frac{\\partial{l(\\hat{θ})}}{\\partial{θ}_i}∂θi​∂l(θ^)​ 。这样后一项也解决了，那就可以算出上图左下角 ∇ϕl(θ^)\\nabla_\\phi l(\\hat{θ})∇ϕ​l(θ^) ，就可以算出上图黄色框  ∇ϕL(ϕ)\\nabla_\\phi L(\\phi)∇ϕ​L(ϕ) ，就可以根据灰色框第一条公式更新 ϕ\\phiϕ ，结束。😮\n\n在原始paper 中作者把，去掉二次微分这件事，称作using a first-order approximation 。\n当我们把二次微分去掉以后，上图左下角的 ∇ϕl(θ^)\\nabla_\\phi l(\\hat{θ})∇ϕ​l(θ^) 就变成 \\nabla_\\hat{θ} l(\\hat{θ}) ，所以我们就是再用 θ^\\hat{θ}θ^ 直接对 θ^\\hat{θ}θ^ 做偏微分，就变得简单很多。\n MAML-Real Implementation\n来看看MAML 实际上是怎么运行的，据说很简单。\n\n实际上，我们在MAML 中每次训练的时候会拿一个task batch 去做，这里我们就先假定每个batch 就只有一个task好了。如上图，当我们初始化好参数 ϕ0\\phi_0ϕ0​ 我们就开始进行训练，完成task m训练以后，根据一次update 得到 θ^m\\hat{θ}^mθ^m ，我们再计算一下 θ^m\\hat{θ}^mθ^m 对它的loss function 的偏微分，也就是说我们虽然只需要update 一次参数就可以得到最好的参数，但现在我们update 两次参数，第二次更新的参数的用处就是， $\\phi $ 的更新方向就和第二次更新参数的方向相同，可能大小不一样，毕竟它们的learning rate 不一样。\n刚才我们讲了在精神上MAML 和Model Pre-training 的不同，现在我们来看看这两者在实际运作上的不同。如上图，预训练的参数更新完全和每个task 的gradient 的方向相同。\n MAML 实际应用\n这里有一个把MAML 应用到机器翻译的栗子：\n\n\nhttps://arxiv.org/abs/1808.08437\n\n18个不同的task：18种不同语言翻译成英文\n2个验证task：2种不同语言翻译成英文\nRo 是validation tasks 中的任务，Fi 即没有出现在training tasks 也没出现在validation tasks\n横轴是每个task 中的训练资料量。MetaNMT 是MAML 的结果，MultiNMT 是 Model Pre-training 的结果，我们可以看到在所有情况下前者都好过后者，尤其是在训练资料量少的情况下，MAML 更能发挥优势。\n Reptile\n非常简单。\n\n做法就是初始化参数 ϕ0\\phi_0ϕ0​ 以后，通过在task m上训练跟新参数，可以多更新几次，然后根据最后的 θ^m\\hat{θ}^mθ^m 更新 ϕ0\\phi_0ϕ0​ ，同样的继续，训练在task n以后，多更新几次参数，得到 θ^n\\hat{θ}^nθ^n ，据此更新 ϕ1\\phi_1ϕ1​ ，如此往复。\n你可能会说，这不是和预训练很像吗，都是根据参数的更新来更新初始参数，希望最后的参数在所有的任务上都能得到很好的表现。作者自己也说，如上图下侧的摘录。\n Reptile &amp; MAML &amp; Pre-training\n\n通过上图来对比三者在更新参数 ϕ\\phiϕ 的不同，似乎Reptile 在综合两者。但是Reptile 并没有限制你只能走两步，所以如果你多更新几次参数多走几步，或许Reptile 可以考虑到另外两者没有考虑到的东西。\n\n上图中，蓝色的特别惨的线是pre-training ，所以说和预训练比起来meta learning 的效果要好很多。\n More…\n上面所有的讨论都是在初始化参数这件事上，让机器自己学习，那有没有其他部分可以让机器学习呢，当然有很多。\n\n比如说，学习网络结构和激活函数、学习如何更新参数…\n\nhttps://www.youtube.com/watch?v=c10nxBcSH14\n\n Think about it…\n我们使用MAML 或Reptile 来寻找最好的初始化参数，但是这个算法本身也需要初始化参数，那我们是不是也要训练一个模型找到这个模型的初始化参数…\n就好像说神话中说世界在乌龟背上，那这只乌龟应该在另一只乌龟背上…这就是套娃啊🤣\n\n Crazy Idea?\n\n传统的机器学习和深度学习的算法基本上都还是gradient descent ，你能不能做一个更厉害的算法，只要我们给他所有的training data 它就可以返回给我们需要model，它是不是梯度下降train 出来的不重要，它只要能给我一个能完成这个任务的model 就好。\n或者，反之我们最好都要应用到测试集上，那我们干脆就搞一个大黑箱，把training set 和testing set 全部丢给他，它直接返回testing data 的结果，连测试都帮你做好。这些想法能不能做到，留到下一节讲。\n","categories":["李宏毅机器学习笔记"],"tags":["ML","note","Next-Step-of-ML"]},{"title":"Life Long Learning","url":"https://ch3nye.top/Life-Long-Learning/","content":" Life Long Learning (LLL)\n\n开始之前的说明，如果读者是学过transfer learning 的话，学这一节可能会轻松很多，LLL的思想在我看来是和transfer learning是很相似的。\n\n可以直观的翻译成终身学习，我们人类在学习过程中是一直在用同一个大脑在学习，但是我们之前讲的所有机器学习的方法都是为了解决一个专门的问题设计一个模型架构然后去学习的。所以，传统的机器学习的情景和人类的学习是很不一样的，现在我们就要考虑为什么不能用同一个模型学会所有的任务。\n\n也有人把Life Long Learning 称为Continuous Learning，Never Ending Learning，Incremental Learning，在不同的文献中可能有不同的叫法，我们只要知道这些方法都是再指终生学习就可。\n\n现在我们回归初心，我想大多数人在学习机器学习之前的是这样认为的，机器学习就是如上图所示，我们教机器学学会任务1，再教会它任务2，我们就不断地较它各种任务，学到最后它就成了天网🤣。但是实际上我们都知道，现在的机器学习是分开任务来学的，就算是这样很多任务还是得不到很好的结果。所以机器学习现在还是很初级的阶段，在很多任务上都无法胜任。\n我们今天分三个部分来叙述Life-Long Learning：\n\nKnowledge Retention 知识保留\n\nbut NOT Intransigence 但不顽固\n\n\nKnowledge Transfer 知识潜移\nModel Expansion 模型扩展\n\nbut Parameter Efficiency 但参数高效\n\n\n\n上述的几个部分的具体含义会在下面详细解释。\n Knowledge Retention\nbut NOT Intransigence\n知识保留，但不顽固\n\n知识保留但不顽固的精神是：我们希望模型在做完一个任务的学习之后，在学新的知识的时候，能够保留对原来任务能力，但是这种能力的保留又不能太过顽固以至于不能学会新的任务。\n Example - Image\n我们举一个栗子看看机器的脑洞有多大。这里是影像辨识的栗子，来看看在影像辨识任务中是否需要终身学习。\n\n如上图所示，我们有两个任务，都是在做手写数字辨识，但是两个的corpus 是不同的（corpus1 图片上存在一些噪声）。network 的架构是三层，每层都是50个neuron，然后让机器先学任务1，学完第一个任务以后在两个corpus 上进行测试，得到的结果如左边的柱状图（task2的结果更好一点其实是很直觉的，因为corpus2上没有noise，这可以理解为transfer learning）。然后我们在把这个模型用corpus2 进行一波训练，再在两个corpus上进行测试得到的结果如右侧柱状图，发现第一个任务有被遗忘的现象发生。\n这时候你可能会说，这个模型的架构太小了，他只有三层每层只有50个neuron，会发生遗忘的现象搞不好是因为它脑容量有限。但是我们实践过发现并不是模型架构太小。我们把两个corpus 混到一起用同样的模型架构train 一发，得到的结果如下图右下角：\n\n所以说，明明这个模型的架构可以把两个任务都学的很好，为什么先学一个在学另一个的话会忘掉第一个任务学到的东西呢。\n Example - Question Answering\n另一个栗子：问答系统，问答系统（如下图）要做的事情是训练一个Deep Network ，给这个模型看很多的文章和问题，然后你问它一个问题，他就会告诉你答案。具体怎么输入文章和问题，怎么给你答案，怎么设计网络，不是重点就不展开。\n\n\nModel From https://arxiv.org/pdf/1502.05698.pdf\n\n对于QA系统已经被玩烂的corpus 是bAbi 这个数据集，这里面有20种不同的题型，比如问where、what 等。我们一次让模型学习这20种题型，每次学习完成以后我们都用题型五做一次测试，也就是以题型五作为baseline，结果如下：\n\n我们可以看到只有在学完题型五的时候，再问机器题型五的问题，它可以给出很好的答案，但是在学完题型六以后它马上把题型五忘的一干二净了。\n这个现象在以其他的题型作为baseline 的时候同样出现了：\n\nps：有趣的是，在题型十作为baseline 的时候可能是由于题型6、9、17、18和题型10比较相似，所以在做完这些题型的QA任务的时候在题型10上也能得到比较好的结果。\n那你又会问了，是不是因为网络的架构不够大，机器的脑容量太小以至于学不起来。其实不是，当我们同时train这20种题型得到的结果是还不错的：\n\n所以机器的遗忘是和人类很不一样的，他不是因为脑容量不够而忘记的，不知道为什么它在学过一些新的任务以后就会较大程度的遗忘以前学到的东西，这个状况我们叫做Catastrophic Forgetting（灾难性遗忘）。之所以加个形容词是因为这种遗忘是不可接收，就像下面这张图学了就忘了，淦：\n\n Wait a minute…\n你可能会说这个灾难性遗忘的问题你上面不是已经有了一个很好的解决方法了吗，你只要把多个任务的corpus 放在一起train 就好了啊。但是，长远来说这一招是行不通的，因为我们很难一直维护所有使用过的训练数据，而且就算我们很好的保留了所有数据，在计算上也有问题，我们每次学新任务的时候就要重新训练所有的任务，这样的代价是不可接受的。\n另外，多任务同时train 这个方法其实可以作为LLL的上界。\n总之，LLL主要探讨的问题是，让机器在学习新的知识的时候能不要忘记过去学过的东西。\n那这个问题有什么样的解法呢，接下来就来介绍一个经典解法。\n Elastic Weight Consolidation (EWC)\n怎么翻译啊，弹性参数巩固?\n基本精神：网络中的部分参数对先前任务是比较有用的，我们在学新的任务的时候只改变不重要的参数。\n\n如上图所示， θb\\theta^bθb 是模型从先前的任务中学出来的参数。\n每个参数 θib\\theta^{b}_{i}θib​ 都有一个守卫 bib_ibi​ ，这个首位就会告诉我们这个参数有多重要，我们有多么不能更改这个参数。\n我们在做EWC 的时候（train 新的任务的时候）需要再原先的损失函数上加上一个regularization ，如上图所示，我们通过平方差的方式衡量新的参数 θi\\theta_iθi​ 和旧的参数 θib\\theta^{b}_{i}θib​ 的差距，让后用一个守卫值来告诉模型这个参数的重要性，当这个守卫 bib_ibi​ 等于零的时候就是说参数 θi\\theta_iθi​ 是没有约束的，可以根据当前任务随意更改，当守卫 bib_ibi​ 趋近于无穷大的时候，说明这个参数 θi\\theta_iθi​ 对先前的任务是非常重要的，希望模型不要变动这个参数。\n\n所以现在问题是， bib_ibi​ 如何决定。这个问题我们下面来讲，先来通过一个简单的栗子再理解一下EWC的思想：\n\n上图是这样的，假设我们的模型只有两个参数，这两个图是两个task 的error surface ，颜色越深error 越大。假如说我们让机器学task1的时候我们的参数从 θ0θ^0θ0 移动到 θbθ^bθb ，然后我们又让机器学task2，在这学这个任务的时候我们没有加任何约束，它学完之后参数移动到了 θ∗θ^*θ∗ ，这时候模型参数在task1的error surface 上就是一个不太好的点。 这就直观的解释了为什么会出现Catastrophic Forgetting 。\n用了EWC 的话看起来是这样的：\n\n当我们使用EWC 对模型的参数的变化做一个限制，就如上面说的，我们给每个参数加一个守卫 bib_ibi​ ，这个 bib_ibi​ 是这么来的呢？不同文章有不同的做法，这里有一个简单的做法就是算这个参数的二次微分（loss对θ的二次微分体现参数loss变化的剧烈程度，二次微分值越大，原函数图像在该点变化越剧烈），如上图所示。我们可以看出， θ1bθ^b_1θ1b​ 在二次微分曲线的平滑段其变化不会造成原函数图像的剧烈变化，我们要给它一个小的守卫 b1b_1b1​ ， 反之 θ2bθ^b_2θ2b​ 则在谷底其变化会造成二次微分值的增大，导致原函数的变化更剧烈，我们要给它一个大的守卫 b2b_2b2​ 。也就是说，θ1bθ^b_1θ1b​ 能动，θ2bθ^b_2θ2b​ 动不得。\n有了上述的constraint ，我们就能让模型参数尽量不要在 θ2θ_2θ2​ 方向上移动，可以在 θ1θ_1θ1​ 上移动，得到的效果可能就会是这样的：\n\n EWC - Experiment\n我们来看看EWC的原始paper中的实验结果：\n\n三个task其实就是对MNIST 数据集做不同的变换后做辨识任务。每行是模型对该行的task准确率的变化，从第一行可以看出，当我们用EWC的方法做完三个任务学习以后仍然能维持比较好的准确率。值得注意的是，在下面两行中，L2的方法在学习新的任务的时候发生了Intransigence（顽固）的现象，就是模型顽固的记住了以前的任务，而无法学习新的任务。\n EWC Variant\n有很多EWC 的变体，给几个参考：\n\n\n\nElastic Weight Consolidation (EWC)\n\n\nhttp://www.citeulike.org/group/15400/article/14311063\n\n\nSynaptic Intelligence (SI)\n\n\nhttps://arxiv.org/abs/1703.04200\n\n\nMemory Aware Synapses (MAS)\n\n\nSpecial part: Do not need labelled data\n\n\nhttps://arxiv.org/abs/1711.09601\n\n\n\n Generating Data\n上面我们说Mutli-task Learning 虽然好用，但是由于存储和计算的限制我们不能这么做，所以采取了EWC 等其他方法，而Mutli-task Learning 可以考虑为Life-Long Learning 的upper bound。 反过来我们不禁在想，虽然说要存储所有过去的资料很难，但是Multi-task Learning 确实那么好用，那我们能不能Learning 一个model，这个model 可以产生过去的资料，所以我们只要存一个model 而不用存所有训练数据，这样我们就做Multi-task 的learning。（这里暂时忽略算力限制，只讨论数据生成问题）\n\n这个过程是这样的，我们先用training data 1 训练得到解决task 1 的model，同时用这些数据生成train 一个能生成这些数据的generator ，存储这个generator 而不是存储training data ；当来了新的任务，我们就用这个generator 生成task 1的training data 和 task2 的training data 混在一起，用Multi-task Learning 的方法train 出能同时解决task1 和task2 的model，同时我们用混在一起的数据集train 出一个新的generator ，这个generator 能生成这个混合数据集；以此类推。这样我们就可以做Mutli-task Learning ，而不用存储大量数据。但是这个方法在实际中到底能不能做起来，还尚待研究，一个原因是实际上生成数据是没有那么容易的，比如说生成贴合实际的高清的影像对于机器来说就很难，关于generator model 的训练方法这里就不展开了，在生成模型的一节里有讲，GAN是一个解法。以下Generating Data 这种方法的参考：\n\nhttps://arxiv.org/abs/1705.08690\nhttps://arxiv.org/abs/1711.10563\n\n Adding New Classes\n在刚才的讨论种，我们都是假设解不同的任务用的是相同的网络架构，但是如果现在我们的task 是不同，需要我们更改网络架构的话要怎么办呢？比如说，两个分类任务的类别数量不同，我们就要修改network 的output layer 。这里就列一些参考给大家：\n\n\nLearning without forgetting (LwF)\nhttps://arxiv.org/abs/1606.09282\n\n\n\niCaRL: Incremental Classifier and Representation Learning\nhttps://arxiv.org/abs/1611.07725\n\n Knowledge Transfer\n怎么翻译？知识迁移？\n\n我们不仅希望机器可以可以记住以前学的knowledge ，我们还希望机器在学习新的knowledge 的时候能把以前学的知识做transfer。什么意思呢，我来解释一下：我们之前都是每个任务都训练一个单独的模型，这种方式会损失一个很重要的信息，就是解决不同问题之间的通用知识。形象点来说，比如你先学过线性代数和概率论，那你在学机器学习的时候就会应用先前学过的知识，学起来就会很顺利。Life-Long Learning 也是希望机器能够把不同任务之间的知识进行迁移，让以前学过的知识可以应用到解决新的任务上面。如下图，机器从一个憨憨一样的机器人学着学着就高端起来了：\n\n这就是为什么我们期望用同一个模型解决多个任务，而不是为每个任务单独训练一个模型。\n Life-Long v.s. Transfer\n讲了这么多transfer，你可能会说，这不就是在做transfer Learning 吗？\n\nLLL 确实有应用Transfer Learning 的思想，但是它比后者更进一步。怎么说呢，Transfer Learning 的精神是应用先前任务的模型到新的任务上，让模型可以解决或者说更好的解决新的任务，而不在乎此时模型是否还能解决先前的任务；但是LLL 就比Transfer Learning 更进一步，它会考虑到模型在学会新的任务的同时，还不能忘记以前的任务的解法。\n Evaluation\n讲到这里，我们来说一下如何衡量LLL 的好坏。其实，有很多不同的的衡量方法，这里简介一种。\n\n这里每一行是一个模型在每个任务上的测试结果，每一列是用一个任务对一个模型在做完某些任务的训练以后进行测试的结果。\nRi,jR_{i,j}Ri,j​ : 在训练完task i 后，模型在task j 上的performance 。\n如果 i &gt; j : 在学完task i 以后，模型在先前的task j 上的performance。\n如果 i &lt; j : 在学完task i 以后，模型在没学过的task j 上的performance，来说明前面学完的 i 个task 能不能transfer 到 task j 上。\n\nAccuracy 是指说机器在学玩所有T 个task 以后，在所有任务上的平均准确率，所以如上图红框，就把最后一行加起来取平均就是现在这个LLL model 的Accuracy ，形式化公式如上图所示。\nBackward Transfer 是指机器有多会做[Knowledge Retention](#Knowledge Retention)（知识保留），有多不会遗忘过去学过的任务。做法是针对每一个task 的测试集（每列），计算模型学完T 个task 以后的performance 减去模型刚学完对应该测试集的时候的performance ，求和取平均，形式化公式如上图所示。\nBackward Transfer 的思想就是把机器学到最后的表现减去机器刚学完那个任务还记忆犹新的表现，得到的差值通常都是负的，因为机器总是会遗忘的，它学到最后往往就一定程度的忘记以前学的任务，如果你做出来是正的，说明机器在学过新的知识以后对以前的任务有了触类旁通的效果，那就很强😮。\nForward Transfer 是指机器有多会做Knowledge Transfer （知识迁移），有多会把过去学到的知识应用到新的任务上。做法是对每个task 的测试集，计算模型学过task i 以后对task i+1 的performance 减去随机初始的模型在task i+1 的performance ，求和取平均。，形式化公式如下图所示。\n\n Gradient Episodic Memory (GEM)\n上述的Backward Transfer 让这个值是正的就说明，model 不仅没有遗忘过学过的知识，还在学了新的知识以后对以前的任务触类旁通，这件事是有研究的，比如GEM 。\n\n\nGEM: https://arxiv.org/abs/1706.08840\nA-GEM: https://arxiv.org/abs/1812.00420\n\nGEM 想做到的事情是，在新的task 上训练出来的gradient 在更新的参数的时候，要考虑一下过去的gradient ，使得参数更新的方向至少不能是以前梯度的方向（更新参数是要向梯度的反方向更新）。\n需要注意的是，这个方法需要我们保留少量的过去的数据，以便在train 新的task 的时候（每次更新参数的时候）可以计算出以前的梯度。\n形象点来说，以上图为例，左边，如果现在新的任务学出来的梯度是g ，那更新的时候不会对以前的梯度g1 g2 造成反向的影响；右边，如果现在新的情况是这样的，那梯度在更新的时候会影响到g1，g 和g1 的内积是负的，意味着梯度g 会把参数拉向g1 的反方向，因此会损害model 在task 1上的performance。所以我们取一个尽可能接近g 的g’ ，使得g’ 和两个过去任务数据算出来的梯度的内积都大于零。这样的话就不会损害到以前task 的performance ，搞不好还能让过去的task 的loss 变得更小。\n我们来看看GEM 的效果：\n\n很直观，就不解释了，总而言之就是GEM 很强这样子了啦🤣。\n Model Expansion\nbut parameter efficiency\n模型扩张，且参数高效\n\n上面讲的内容，我们都假设模型是足够大的，也就是说模型的参数够多，它是有能力把所有任务都做好，只不过因为某些原因它没有做到罢了。但是如果现在我们的模型已经学了很多任务了，所有参数都被充分利用了，他已经没有能力学新的任务了，那我们就要给模型进行扩张。同时，我们还要保证扩张不是任意的，而是有效率的扩张，如果每次学新的任务，模型都要进行一次扩张，那这样的话你最终就会无法存下你的模型，而且臃肿的模型中大概率很多参数都是没有用的。\n这个问题在2018年老师讲课的时候还没有很多文献可以参考，这里就流水账的记录一下老师讲的流水帐。\n Progressive Neural Networks\n\n\nhttps://arxiv.org/abs/1606.04671\n\n这个方法是这样的，我们在学task 1的时候就正常train，在学task 2的时候就搞一个新的network ，这个网路不仅会吃训练集数据，而且会把训练集数据input 到task 1的network中得到的每层输出吃进去，这时候是fix 住task 1 network，而调整task 2 network 。同理，当学task 3的时候，搞一个新的network ，这个网络不仅吃训练集数据，而且会把训练集数据丢入task 1 network 和 task 2 network ，将其每层输出吃进去，也是fix 住前两个network 只改动第三个network 。\n这是一个早期的想法，2016年就出现了，但是这个方法，终究还是不太能学很多任务。\n Expert Gate\n\n\nhttps://arxiv.org/abs/1611.06194\nAljundi, R., Chakravarty, P., Tuytelaars, T.: Expert gate: Lifelong learning with a network of\nexperts. In: CVPR (2017)\n\n思想是这样的：每一个task 训练一个network 。但是train 了另一个network ，这个network 会判断新的任务和原先的哪个任务最相似，加入现在新的任务和T1 最相似，那他就把network 1最为新任务的初始化network，希望以此做到知识迁移。但是这个方法还是每一个任务都会有一个新的network ，所以还是不太好。\n Net2Net\n如果我们在增加network 参数的时候直接增加神经元进去，可能会破坏这个模型原来做的准确率，那我们怎么增加参数才能保证不会损害模型在原来任务上的准确率呢？Net2Net 是一个解决方法：\n\n\nhttps://arxiv.org/abs/1511.05641\n用到了Net2Net：https://arxiv.org/abs/1811.07017\n\nNet2Net的具体做法是这样的，这里举一个栗子，如上图所示，当我么你要在中间增加一个neuron 时，我们把f 变为f/2 ，这样的话同样的输入在新旧两个模型中得到的输出就还是相同的，同时我们也增加了模型的参数。但是这样做出现一个问题，就是h[2] h[3] 两个神经元将会在后面更新参数的时候完全一样，这样的话就相当于没有扩张模型，所以我们要在这些参数上加上一个小小的noise ，让他们看起来还是有小小的不同，以便更新参数。\n图中第二个引用的文章就用了Net2Net，需要注意，不是来一个任务就扩张一次模型，而是当模型在新的任务的training data 上得不到好的Accuracy 的时候才用Net2Net 扩张模型。\n Curriculum Learning\n\n如上图所示，模型的效果是非常受任务训练顺序影响的。也就是说，会不会发生遗忘，能不能做到知识迁移，和训练任务的先后顺序是有很大关系的。假如说LLL 在未来变得非常热门，那怎么安排机器学习的任务顺序可能会是一个需要讨论的热点问题，这个问题叫做Curriculum Learning 。\n2018年就已经有一篇文献是在将这个问题：\n\n\nhttp://taskonomy.stanford.edu/#abstract CVPR 的best paper\n\n文章目的是找出任务间的先后次序，比如说先做3D-Edges 和 Normals 对 Point Matching 和Reshading 就很有帮助。\n","categories":["李宏毅机器学习笔记"],"tags":["ML","note","Next-Step-of-ML"]},{"title":"Anomaly Detection","url":"https://ch3nye.top/Anomaly-Detection/","content":" Anomaly Detection\n翻译成中文就是，异常探测。异常探测就是要让机器知道&quot;I don’t know&quot;，这件事。\n Problem Formulation\n异常探测问题通常形式化成下述表示形式：\n\n给出一个训练数据集 {x1,x2,...,xN}\\{x^1, x^2,..., x^N\\}{x1,x2,...,xN}\n我们想找出一个函数去探测输入 xxx 是否和训练数据相似\n\n也就是说异常探测的任务就是找一个function，这个function做的事情就是，我们给它一个和训练数据相似的输入，它就返回一个结果告诉我们输入是正常的，反之，当我们输入一个和训练数据差别非常大的样本，它就返回一个结果告诉我们输入是异常的。\n\n通常我们看到anomaly 这个词都是负面的，但是这里说的异常探测并不是负面意思，只是在说某个数据和训练数据集中的数据差别很大而已。Anomaly 有时也会用outlier, novelty, exceptions 替换，使用novelty的时候就是找一个新颖的东西，所以，总的来说我们就是要找出和训练集样本不一样的东西。\n至于怎么定义similarity，这就是Anomaly Detection需要探讨的问题，不同的方法我们要用不同的方式定义相似性。\n What is Anomaly?\n这里我要强调一下什么叫做异常，机器到底要看到什么才应该认定为Anomaly，这其实是取决你提供给机器什么样的训练数据。举个栗子：\n\n如果你提供了很多的雷丘作为训练数据，皮卡丘就是异常的。如果你提供了很多的皮卡丘作为训练数据，雷丘就是异常的。如果你提供很多的宝可梦作为训练数据，数码宝贝就是异常的。\n Applications\n异常探测有很多应用，比如可以应用到欺诈检测（Fraud Detection）\n Fraud Detection\nFraud Detection 应用于银行或保险等行业，Fraud 包括但不限于伪造支票，信用卡欺诈等行为。欺诈探测的目的是预测欺诈行为，防止欺诈交易的发生。\n这种任务的训练资料就是正常的刷卡机行为，收集很多的交易记录，这些交易记录是为正常的交易行为，现在新来一笔交易记录，我们就把它输入到模型中，判断这笔交易是否有异常，甚至可以做到预测和这笔交易有关的对象接下来是否有发生欺诈交易的可能，从而预防欺诈的发生。（直觉上可以简单的想象：如果正常的交易金额比较小，频率比较低，若短时间内有非常多的高额消费，这可能是异常行为）\n这种应用是有一些比赛的，下面是kaggle上的链接：\n\nRef: https://www.kaggle.com/ntnu-testimon/paysim1/home\nRef: https://www.kaggle.com/mlg-ulb/creditcardfraud/home\n实践篇：一个关于Fraud Detection的例子（一）\n\n Network Intrusion Detection\n异常检测还可以应用到网络系统的入侵检测，训练数据是正常的网络连接。现在来了一个新的连接，你希望用Anomaly Detection 让机器自动决定这个新的连接是否为攻击行为。\n\nRef: http://kdd.ics.uci.edu/databases/kddcup99/kddcup99.html\nKDD99老经典了😂\n\n Cancer Detection\n异常检测还可以应用到医疗（癌细胞的侦测），训练数据是正常细胞。现在给出一个新的细胞，让机器判断这个细胞是否为癌细胞。\n\nRef: https://www.kaggle.com/uciml/breast-cancer-wisconsin-data/home\n\n Binary Classification\n接下来就是具体怎么作Anomaly Detection 了，直觉的想法就是：现在我们可以收集正负两种训练数据：\n\n\n正常数据：{x1,x2,⋯,xN}\\{x^1,x^2,⋯,x^N\\}{x1,x2,⋯,xN}\n\n\n异常数据：{x~1,x~2,⋯,x~N}\\{\\widetilde{x}^1, \\widetilde{x}^2, ⋯, \\widetilde{x}^N\\}{x1,x2,⋯,xN}\n\n\n然后train一个二分类器就完事了。\n这个问题其实并没有那么简单，因为不太容易把异常检测看作一个binary classification的问题。为什么这样说呢？\n举个栗子，我们现在将Pokémon 视为正常数据，将其他所有事物视为异常数据，我们要让机器判断输入是不是一个Pokémon ：\n\n那问题来了，首先不是Pokémon 的事物太多了，数码宝贝不是，凉宫春日不是，水壶不是，所以我们很难将除正常数据以外的所以事物都归结为一类，很难收集所有的异常数据，很难将异常数据的特征都让机器记住。\n再者，很多时候你很难收集到异常数据，比如欺诈检测，在所有的交易中，异常交易的比例非常至少。\n Categories\n现在我们把异常检测任务分为两类：\n\n一类，是不只有训练数据 {x1,x2,⋯,xN}\\{x^1,x^2,⋯,x^N\\}{x1,x2,⋯,xN} ，同时这些数据还具有label {y^1,y^2,⋯,y^N}\\{\\hat{y}^1,\\hat{y}^2,⋯,\\hat{y}^N\\}{y^​1,y^​2,⋯,y^​N} 。 用这样的数据集可以train出一个classifier，让机器通过学习这些样本，以预测出新来的样本的label，但是我们希望分类器有能力知道新给样本不属于原本的训练数据的任何类别，它会给新样本贴上“unknown”的标签。训练classifier 可以用generative model、logistic regression、deep learning等方法，你可以从中挑一个自己喜欢的算法train 出一个classifier 。\n上述的这种类型的任务，train出的classifier 具有看到不知道的数据会标上这是未知物的能力，这算是异常检测的其中一种，又叫做Open-set Recognition。我们希望做分类的时候模型是open 的，它可以辨识它没看过的东西，没看过的东西它就贴一个“unknown”的标签。\n另一类，所有训练数据都是没有label 的，这时你只能根据现有资料的特征去判断，新给的样本跟原先的样本集是否相像。这种类型的数据又分成两种情况：\n\nClean：手上的样本是干净的（所有的训练样本都是正常的样本）\nPolluted：手上的样本已经被污染（训练样本已经被混杂了一些异常的样本，更常见）\n\n情况二是更常见的，对于刚才的诈欺检测的例子而言，银行收集了大量的交易记录，它把所有的交易记录都当做是正常的，然后告诉机器这是正常交易记录，然后希望机器可以借此检测出异常的交易。但所谓的正常的交易记录可能混杂了异常的交易，只是银行在收集资料的时候不知道这件事。所以我们更多遇到的是：手上有训练样本，但我没有办法保证所有的训练样本都是正常的，可能有非常少量的训练样本是异常的。\n Case 1: With Classifier\n我们来看看情况一，就是训练样本有label的情况，也就是说我们可以train出一个初始的model。举的栗子是检测输入是不是辛普森家族的人物。\n\n\n我们现在收集的辛普森家庭的人物都具有标注（霸子，丽莎，荷马，美枝），有了这些训练资料以后就可以训练出一个辛普森家庭成员的分类器。如下图所示，我们就可以给分类器看一张照片，它就可以判断这个照片中的人物是辛普森家庭里面的哪个人物。\n\n这个数据集来自kaggle上一个非常喜欢辛普森家庭的人，它collect并且label了数千张数据，以下是参考链接\n\nSource of model: https://www.kaggle.com/alexattia/the-simpsons-characters-dataset/\n\n然后，这个人训练出一个分类器，然后用这个分类器做了测试，结果有百分之九十六的正确率。如下图所示：\n\n现在我们想做的事情是基于这个分类器来进行异常检测，判断输入的人物是否来自辛普森家庭。\n Confidence score\n我们原本是使用分类器来进行分类，现在希望分类器不仅可以来自分类，还会输出一个数值，这个数值代表信心分数（Confidence score ），然后根据这个信心分数做异常检测。\n\n如上图所示，我们可以定义一个阈值 λ\\lambdaλ ，若信心分数大于 λ\\lambdaλ 就说明是来自于辛普森家庭。若信心分数小于 λ\\lambdaλ 就说明不是来自于辛普森家庭.\n那问题就来了，如何计算这个confidence score呢？我们希望这个信心分数应该有这样的能力，当我们将图片输入辛普森家庭的分类器中，若分类器非常的肯定这个图片到底是谁，输出的信心分数就会非常的高，反之则低。\n需要再提一嘴的是，当我们将图片输入分类器时，分类器的输出是一个几率分布（distribution），是这张图片属于各个类别的可能性的得分，如下图所示：\n\n从上图直观的观察来看，当分类器非常确定一个输入的类别的时候，它会给出很集中的得分，反之则是分散的。\n刚才讲的都是定性的分析，现在需要将定性分析的结果化为信心分数。一个非常直觉的方法就是将分类器输出的分布中最高数值作为信心分数，所以上面那张图输出的信心分数为0.97（霸子），下面那张图输出的信心分数为0.26（凉宫春日）。（这个方法就是后面我们做实验用的方法）如下图所示：\n\n根据信心分数来进行异常检测不是唯一的方法，因为分类器输出的是distribution，那么就可以计算交叉熵（cross entropy）。交叉熵越大就代表输出越平均，代表机器没有办法去肯定输出的图片是哪个类别，表示输出的信心分数是比较低。总之我们有不同的方法根据分类器决定它的信心分数.\n那我们实际来做一下，就用分布中最高数值作为信心分数的方法，结果如下图所示：\n\n现在我输入一张训练资料没有的图片（荷马），分类器输出荷马的信心分数是1.00；输入霸子的图片，分类器输出霸子的信心分数为0.81，输出郭董的信心分数为0.12；输入三玖的图片和李老师的图片结果就是相对很分散的。我们可以发现，如果输入的是辛普森家庭的人物，分类器输出比较高信心分数。如果输入不是辛普森家庭的人物，分类器输出的信心分数是比较低。但是输入凉宫春日的图片，分类器输出柯阿三的信心分数为0.99，所以这种方法还是有有一些瑕疵的。\n接下来我们来看看不同的数据集使用这种方法的统计结果：\n\n我们可以看到真正的来自辛普森家庭的人物的信心分数都集中在1.0附近，你仔细看上面的图标中有一些红色的数据（分类错误）比较分散的分布在坐标轴上，所以说分类错误的数据会得到比较低的信心分数，这个方法还是有一点瑕疵的。\n而非辛普森家庭的动漫人物的分布比较分散，90%的数据都是信心分数比较低的，分散在坐标轴上，但是还是有10%的数据信心分数分布在1.0附近。这10%样本不是辛普森家庭的人物，但给了比较高的分数，也说明这个方法是有瑕疵的。\n总的来说，如果你要做异常检测的问题，在你手上有一个分类器或者说能train出一个分类器的情况下，这应该是你第一个要尝试的baseline。虽然很简单，但实际上不见得结果表现会很差。\n Outlook: Network for Confidence Estimation\n\n\nTerrance DeVries, Graham W. Taylor, Learning Confidence for Out-of-Distribution Detection in Neural Networks, arXiv, 2018\n\n你训练一个neuron network时，可以直接让neuron network输出信心分数，这个问题我先不细节，在这里我先引用一个文献（2018年的paper）是有这样的技术的。\n More Detail\n接下来我们要讲一下关于上述在有分类模型的情况下根据信心分数进行异常检测的具体细节，包括threshold（阈值）如何定，如何判断异常判断结果的好坏等问题。\n首先我们先来复习一下model framework：\n\n我们有大量的训练资料，且训练资料具有标注（辛普森家庭的人物），因此我们可以训练一个分类器。不管你用什么算法，目标是可以从分类器中得到对应图片的信心分数。然后就根据信心分数建立异常检测的系统，若信心分数高于某个threshold（阀值）时就认为是正常，若低于某个threshold 时就认为是异常。\nDev Set(development set):\n\n用于训练过程中，指导调整超参数的样本集，使用起来类似于测试集；\n以前机器学习数据量少，超参数少的时候可能是没有这个样本集的，只有训练集（train set）和测试集（test set）。这时测试集作为验证集使用；\n现在数据量多了，可以单独分出一部分样本作为dev set，用于超参数调优，模型经过训练集训练，和验证集调优，然后交给测试集测试性能；\n作者：猛虎神威凯蒂猫\nRef：https://www.zhihu.com/question/53052465/answer/281962767\n来源：知乎\n\n在现在这个异常探测任务中，我们需要的development set 需要包含大量的images，这些images需要被标注上是否是来自辛普森家庭的人物。另外，需要强调的是在训练时所有的样本数据都是来自辛普森家庭的人物，标签是辛普森家庭的哪个人物。但是，我们在做Dev Set 时需要模拟测试数据集，也就是说这里面还需要包含非辛普森家庭的人物的图片。\n有了Dev Set 以后，我们就可以把我们异常探测系统应用在Dev Set ，然后计算异常探测系统在Dev Set上的得分是多少。等下会说明这个得分如何计算。你能够在Dev Set 衡量一个异常探测系统的表现以后，你就可以拿来调整threshold，以找出让最好的threshold 。\n决定hyperparameters（超参）以后，就有了一个异常探测的系统，你就可以让它上线。输入一张图片，系统就会决定是不是辛普森家庭的人物。整个Anomaly Detection System 构建就结束了。但是我们还遗留了一个问题：\n如何计算一个异常侦测系统的性能好坏？\n\n上图是100张辛普森家庭人物的图片（蓝色）和5张不是辛普森家庭人物的图片（红色）输入到Anomaly Detection 中得到的Confidence Score。你会发现上图左侧有一个辛普森家庭的图片的信心分数竟然是非常低的，在这里异常探测系统犯了一个错误，认为它不是辛普森家庭的人物。我们可以看到，五张非辛普森家庭人物的图片的信心得分其实还挺高的，尤其是魔法少女（0.998）。在实践的时候，很多人都发现这些异常图片会得到很高的分数，其实这不是特别大的阻碍，因为真正是辛普森家庭的人物的信心分数会非常集中到1.0，只要异常图片没有正常图片的信心分数那~么~高~，就还是可以得到比较好的异常探测效果的。\n\n我们怎么来评估一个异常检测系统的好坏呢？我们知道异常检测其实是一个二元分类（binary classification）的问题。在二元分类中我们都是用正确率来衡量一个系统的好坏，但是在异常检测中正确率并不是一个好的评估系统的指标。你可能会发现一个系统很可能有很高的正确率，但其实这个系统什么事都没有做。为什么这样呢？因为在异常检测的问题中正常的数据和异常的数据之间的比例是非常悬殊的。在这个例子里面，我们使用了正常的图片有一百张，异常的图片有五张。这会造成只用准确率衡量系统的好坏会得到非常奇怪的结果的。\n举个栗子，如上图所示，我们将threshold λλλ 设为0.3和0.5。 λλλ 以上认为是正常的， λλλ 以下认为是异常的。这时你会发现这个系统的正确率都是95.2%，所以异常侦测问题中不会用正确率来直接当做评估指标。\n首先我们要知道在异常检测中有两种错误：一种错误是异常的数据被判断为正常的数据，另外一种是正常的数据被判为异常的数据。假设我们将 λλλ 设为0.5（0.5以上认为是正常的数据，0.5以下认为是异常的数据），这时就可以计算机器在这两种错误上分别范了多少错误。如下图所示：\n\n我们可以看到机器判断出现了两类错误，miss了4个anomaly data，false alarm了1个normal data 。\n\n若我们将threshold 切在比0.8稍高的部分，如上图所示，这时会发现在五张异常的图片中，其中有两张认为是异常的图片，其余三种被判断为正常的图片；在一百张正确的图片中，其中有六张图片被认为是异常的图片，其余九十四张图片被判断为正常的图片。\n那一个系统比较好呢？其实你是很难回答这个问题。有人可能会很直觉的认为：当阀值为0.5时有五个错误，阀值为0.8时有九个错误，所以认为左边的系统好，右边的系统差。但其实一个系统是好还是坏，取决你觉得false alarm比较严重还是missing比较严重。\n\n所以你在做异常探测时，可能需要有Cost Table 告诉你每一种错误有多大的Cost 。如上图所示我们将两个Cost Table 分别计算一下threshold λλλ 为0.5和0.8的两个系统的cost ，发现取不同的的cost 衡量标准，就会有不同的判定结果。\n为什么要这样呢，举一个形象一点的栗子，如果说你今天要做癌症检测的异常检测，与误判比起来，我们显然更加不能容忍missing，如果一个其实有癌症的人被系统miss 了，耽误了最佳治疗时间，就是生命的代价。\n其实还有很多衡量异常检测系统的指标，这里就不细讲这些，有一个常用的指标为Area under ROC curve。若使用这种衡量的方式，你就不需要决定threshold ，而是看你将测试集的结果做一个排序（高分至低分），根据这个排序来决定这个系统好还是不好。\n Possible Issue\n\n如果我们直接用一个分类器来检测输入的数据是不是异常的，当然这并不是一种很弱的方法，但是有时候无法给你一个perfect 的结果，我们用上图来说明用classifier 做异常侦测时有可能会遇到的问题。假设现在做一个猫和狗的分类器，将属于猫🐱的一类放在一边，属于狗🐕的一类放在一边。若输入一笔资料即没有猫的特征也没有狗的特征（草泥马，马莱貘），机器不知道该放在哪一边，就可能放在这个boundary上，得到的信息分数就比较低，你就知道这些资料是异常的。\n你有可能会遇到这样的状况：有些资料会比猫更像猫（老虎），比狗还像狗（狼）。机器在判断猫和狗时是抓一些猫的特征跟狗的特征，也许老虎在猫的特征上会更强烈，狼在狗的特征上会更强烈。对于机器来说虽然有些资料在训练时没有看过（异常），但是它有非常强的特征会给分类器很大的信心看到某一种类别。\n\n再比如，刚才做辛普森家庭人物的异常检测的时候，黄色的魔法少女和小樱都给了比较高的信心分数，所以我们猜测分类器是看到黄色就给很高的分数，我们就把三玖的脸和头发涂黄，果然分数暴增，在老师的脸上涂黄，效果也是显著的。🤣所以这些都是异常检测的问题。\n To Learn More\n\n当然有些方法可以解这个问题，这里列一些文献给大家进行参考。\n\nKimin Lee, Honglak Lee, Kibok Lee, Jinwoo Shin, Training Confidence-calibrated Classifiers for Detecting Out-of-Distribution Samples, ICLR 2018\n\n其中的一个解决方法是：假设我们可以收集到一些异常的样本，我们可以教机器看到正常样本时不要只学会分类这件事情，要学会一边做分类一边看到正常的资料信心分数就高，看到异常的资料就要给出低的信心分数。\n\nMark Kliger, Shachar Fleishman, Novelty Detection with GAN, arXiv, 2018\n\n但是会遇到的问题是：很多时候不容易收集到异常的数据。有人就想出了一个神奇的解决方法就是：既然收集不到异常的数据，那我们就通过Generative Model 来生成异常的数据。这样你可能遇到的问题是：生成的数据太像正常的资料，那这样就不是我们所需要的。所以还要做一些特别的constraint ，让生成的资料有点像正常的资料，但是又跟正常的资料又没有很像。接下来就可以使用上面的方法来训练你的classifier 。\n Case 2：Without Labels\n在第二种情况下，我们没有classifier ，也就是说我们收集到的数据没有label 。\n Twitch Play Pokémon\n\n这是一个真实的Twitch Plays Pokémon例子，这个的例子是这样的：有人开了一个宝可梦的游戏，全世界的人都可以连接一起玩这个宝可梦的游戏。右边的框是每一个人都在输入指令同时操控这个游戏，这个游戏最多纪录大概有八万人同时玩这个游戏。当大家都在同时操作同一个角色时，玩起来其实是相当崩溃的。（Paperclip2019年做的解密活动中的一部分似乎是在模仿这个游戏模式）\n这个游戏是多年前进行的，不过现在twitch 上居然还有升级版的，直播地址：\n\nhttps://www.twitch.tv/twitchplayspokemon\n\n以下是我录制的一小段gif：\n\n\n人们玩的时候就非常的崩溃，发现根本玩不下去，那么崩溃的原因是什么呢？可能是因为有Troll（网络小白？）。这些人可能根本就不会玩，所以大家都没有办法继续玩下去；或者可能觉得很有趣就乱按按键；或者是不知名的恶意，不想让大家结束这个游戏（脚本小子）。人们相信有一些Troll 潜藏在人们当中，他们想要阻挠游戏的进行。\n假定多数玩家是想要通关的，那我们就可以用异常检测的技术，收集所有玩家的行为（训练数据），我们可以从多数玩家的行为知道正常玩家的行为是咋样的，然后侦测出异常的玩家（Troll）。\n Problem Formulation\n\n我们需要一些训练的样本： {x1,x2,...,xN}\\{x^1,x^2,...,x^N\\}{x1,x2,...,xN} ，每一个 xix^ixi 代表一个玩家，如果我们使用machine learning 的方法来求解这个问题，首先这个玩家要能够表示为feature vector 。举个例子：向量的第一维可以是玩家说垃圾话的频率，第二维是统计玩家无政府状态发言，可以想象有可能是这样的，一个玩家越是喜欢在无政府状态下发言，就越有可能是搞破坏的。\n这个数据集是真实存在的：\n\nhttps://github.com/ahaque/twitch-troll-detection (Albert Haque)\n\n我们现在只有大量的训练数据，但是没有label。我们刚才可以根据分类器的conference来判断是不是异常的资料，我们在没有classifier的情况下可以建立一个模型，这个模型是告诉我们P(x)的几率有多大（根据训练数据，找一个几率模型，这个模型可以告诉我们某一种行为发生的概率有多大）。当我们有了模型P ，我们就可以将玩家的特征向量输入模型，得到一个几率（就是现实情况中发生这名玩家的动作的概率），如果这个几率大于threshold ，就认为是正常玩家，反之则判定为异常玩家。综上所述我们的问题的解决方案就被形式化为下图：\n\n这个模型可以通过生成模型的方式来找，比如说Gaussian Distribution Model ，我们假定玩家数据是在特征空间中符合高斯分布，那我们可以通过现有数据计算出这个分布模型的 μμμ 和 ∑∑∑ ，就得到这个模型了，然后我们就可以按上述进行数据输入，做判定了。\n\n如上图所示，这是一个真实的资料，假设每一个玩家可以用二维的向量来描述（一个是说垃圾话的几率，一个是无政府状态发言的几率）。我们可以观察到，喜欢在无政府状态发言的玩家占多数，喜欢说辣鸡话的玩家占少数，综上，我们直观的理解是正常玩家应该是喜欢在无政府状态发言，而比较少说辣鸡话的。所以我们的模型 P(x)P(x)P(x) 的结果应该符合这样的描述，如上图右上角。\nps：民主状态：统计一段时间内的玩家指令数，选取数量最多的一个执行；无政府状态：一段时间内所有的指令中随机pick 一个执行。\n那为什么会出现这种情况呢，我们来简单分析一下。事实上很多人强烈支持无政府状态，强烈反对民主状态，所以这个游戏多数是在无政府状态下进行。假设一个玩家不考虑自己是要在什么状态下发言，大多数人有八成的几率是在无政府下进行发言，有人觉得多数Troll 是在民主状态下发言，因为进入了民主状态，只要Troll 多发言就能够让他的行为被选中的概率放大，所以小白会特别喜欢在民主状态下发言。\n从这个图上可以很直觉的看出（右上角的三个点）一个玩家落在说垃圾的话几率低，通常在无政府状态下发言，这个玩家有可能是一个正常的玩家， P(x)P(x)P(x) 大。假设有玩家落在有五成左右的几率说垃圾话，二成的几率在无政府状态下发言；或者落在有七成左右的几率说垃圾话，七成的几率在无政府状态下发言，显然这个玩家比较有可能是一个不正常的玩家， P(x)P(x)P(x) 小。\n我们直觉上明白这件事情，但是我们仍然希望用一个数值化的方法告诉我们玩家落在哪里会更加的异常。\n Maximum Likelihood\n这里所指数值化的方法我们需要用到Likelihood 的概念（ps：如果你学过李老师的ML的**这一节课**你应该已经知道这个概念）：\n\n\n\n假设我们的数据是从一个probability density function（概率密度函数） fθ(x)f_{\\theta}(x)fθ​(x)  中sample出来的\n\nθ 是概率密度函数的参数，是未知的，是我们要从数据中学习出来的\nθ 将决定我们概率密度函数的形状\n\n\n\n现在我们要做的事就是找出这个概率密度函数究竟长什么样子\n\n\n在找这个概率密度函数的过程中，我们需要一个衡量方法，来衡量当前找到的这概率密度函数的好坏，我们就是用Likelihood ，这个似然函数的概念和损失函数有点像，Maximum Likelihood 的想法和Minimize Loss 的想法是一样的。\nLikelihood 的意思是：计算概率密度函数 fθf_{\\theta}fθ​ 产生如图所示的所有数据概率有多大。\n若严格说的话， fθ(x)f_{\\theta}(x)fθ​(x) 输出并不是概率，它的输出是probability density ；输出的范围也并不是(0,1)，有可能大于1。如果你概率论学的比较好，那你应该能很好理解这个概率密度，不行的话这里我们就简单的将其理解成概率。 xix^ixi 在以 θ\\thetaθ 为参数的probability density function sample出来的概率是 fθ(xi)f_{\\theta}(x^i)fθ​(xi) ，所以所有的样本从这个 fθ(x)f_{\\theta}(x)fθ​(x) 中sample出来的概率就是各个样本的概率连乘。\n连乘得到的结果就是likelihood 如上图所示 L(θ)L(\\theta)L(θ) 。likelihood 的可能性显然是由 θ\\thetaθ 控制的，选择不同的 θ\\thetaθ 就有不同的probability density function，就可以算法不同的likelihood 。\n而我们现在并不知道这个 θ 是多少，我们要算一个 θ∗θ^*θ∗ 算出来的likelihood是最大的。\n\n那上述的概率密度函数长什么样子呢，我们要确定这个概率密度函数的架构。我们可以假设这个概率密度函数是Gaussian Distribution （高斯分布），高斯分布的函数式就是上图最上面的公式。\n在Gaussian Distribution 的函数式中，参数 θ\\thetaθ 应该是 {mean:μ,convariancematrix:Σ}\\{mean:\\mu, convariance matrix:\\Sigma\\}{mean:μ,convariancematrix:Σ}。假设如上图所示的数据是由左上角的 {μ,Σ}\\{\\mu,\\Sigma\\}{μ,Σ} 来生成的，数据点应该在左上角的蓝色圈圈中sample 出来，它的likelihood 是比较大；如果是右下角 {μ,Σ}\\{\\mu,\\Sigma\\}{μ,Σ} ，它远离高密度区域，这个函数sample 出的数据因该多数在右下角蓝色圈圈中，但这与实际不符，显然这样计算出来的likelihood 是比较低的。\nps：不要问为什么用高斯分布，如果我用其他的分布你也会问同样的问题。再者就是因为它该死的好用，现实中很多数据的分布都比较好的符合高斯分布。你永远可以根据你的数据选择合适的分布模型。另外这里再考虑一下，如果 fθ(x)f_{\\theta}(x)fθ​(x) 是一个很复杂的网络， θ\\thetaθ 是网络中的大量参数，这时候你就会有更多的自由选择一个model 来模拟生成数据点，这样就不会被限制住，在看起来就不像Gaussian 产生的数据却硬要说是Gaussian 产生的数据。因为我们这门课还没有讲到其它进阶的模型，所以现在用Gaussian Distribution 来当做我们资料是由Gaussian Distribution 所产生的。\nGuassian Distribution 中的参数是很好解的，μ∗\\mu^*μ∗ 等于所有training data 做平均， Σ∗\\Sigma^*Σ∗ 等于将x减去 μ∗\\mu^*μ∗ 乘以 xxx 减去 μ∗\\mu^*μ∗ 的转置，然后做平均。得到的结果如下图：\n\n好了，现在我们根据training data 找出了 {μ∗,Σ∗}\\{\\mu^*,\\Sigma^*\\}{μ∗,Σ∗} ，接下来就可以做异常检测了。如上图所示，将 {μ,Σ}\\{\\mu,\\Sigma\\}{μ,Σ} 代入probability density function ，若大于某一个threshold （阈值）就说明是正常的，若小于这个threshold 就说明是异常的。\n\n每一笔资料都可以代入probability density function 算出一个数值，结果如图所示。若落在颜色深的红色区域，就说明算出来的数值越大，越是一般的玩家，颜色浅的蓝色区域，就说明这个玩家的行为越异常。如图所示的两个点：右上角的一个玩家落在很喜欢说垃圾话，多数喜欢在无政府状态下发言的区域，就说明是一个正常的玩家。左下角的一个玩家落在很少说垃圾话，特别喜欢在民主时发言，就说明是一个异常的玩家。\n到这里我们对Twitch Play Pokémon 的异常玩家探测就结束了，我们来稍微总结一下，我们用了两种特征表示玩家，假定了玩家的在这两个特征维度上分布符合Gaussian Distribution ，然后根据数据学除了一个比较好的fit 这些数据的高斯分布模型，然后用阈值分类的方法完成了异常检测。\n上述方法中，我们只是用了两个特征，那如果加上更多的特征或许效果会更好：\n\n再加入一些特征，比如如果有人不停按start 键，那它可能就是一个异常玩家，还有和其他人一不一样，比较一样的可能就会是正常玩家，再有不停唱反调的人可能也是异常玩家。加上这些特征，实践一下得到的结果：\n\n这里取log 的原因是原本的函数算出来的输出太小了。我们可以看到第一个和第三个玩家除了第四个特征都一样，但是第一个玩家和大家的选择完全一样，第三个玩家和大家的选择在大多数情况下是相同的，这是第一个得到的分数反而低，是因为机器会觉得如果你和所有人完全一样这件事就是很异常的。\n Outlook: Auto-encoder\n上述是用Generative Model （生成模型） 来进行异常检测，我们也可以使用Auto-encoder 来做这个任务。\n\n我们把所有的训练资料训练一个Encoder ，Encoder 所做的事情是将输入的图片（比如辛普森）变为code（一个向量），Decoder 所做事情是将code 解回原来的图片。训练时Encoder 和Decoder 是同时训练，训练目标是希望输入和输出越接近越好。\n\n在测试的时候，我们就把一张图片放入这个Auto-Encoder model，得到一个输出图片。因为这个网络训练时使用的是辛普森家庭的图片，所以它对辛普森家庭的图片的还原应该是比较好的，而异常的图片的还原应该会是模糊的，基于此我们可以做Auto-Encoder based Anomaly Detection。\n More …\n\nMachine Learning 中也有其它做异常检测的方法，比如SVM的One-class SVM ，只需要正常的资料就可以训练SVM ，然后就可以区分正常的还是异常的数据。在Random Forset 的Isolated Forest ，它所做的事情跟One-class SVM 所做的事情很像（给出正常的训练进行训练，模型会告诉你异常的资料是什么样子）。\n\nOne-class SVM Ref: https://papers.nips.cc/paper/1723-support-vector-method-for-novelty-detection.pdf\nIsolated Forest Ref: https://cs.nju.edu.cn/zhouzh/zhouzh.files/publication/icdm08b.pdf\n\n","categories":["李宏毅机器学习笔记"],"tags":["ML","note","Next-Step-of-ML"]},{"title":"Explainable ML","url":"https://ch3nye.top/Explainable-ML/","content":" Explainable ML\n 基本概念\n举例来说，在图片识别任务中，我们给机器一张图片，它就会告诉我们图片中有什么事物，但是我们不只希望机器能告诉我们图片中有什么，我们还希望它能告诉我们它判断图片中存在该事物的理由。\n\n如上图所示，可解释的机器学习目前分为两种类型：\n\n\n局部解释\n为什么你认为图中有一只猫🐱？\n\n\n全局解释\n你认为猫🐱应该是什么样子的？\n\n\n 为什么我们需要Explainable ML\n\n举例来说：\n现在很多公司都会想要用机器来协助做履历的判读，我们希望用机器来做履历的判读的时候是公平的，那我怎么让人知道机器是不是公平的呢？也许我们就需要机器它一边判读，一边告诉我们他筛选履历的理由，这样我们就可以知道机器到底是真的在判读一个人的能力，还是仅仅凭借肤色和性别这样的数据进行判断。\n又或者，让机器协助判断一个罪犯是否可以被假释，我们需要知道机器是否真的根据具体的事证来判断这件事，还是仅仅凭借肤色和性别这样的数据进行判断。\n又或者，进行金融相关的决策需要提供判断理由，比如为什么不给某人贷款？\n还有，当我们可以做可解释的机器学习模型时，我们就能做模型诊断，就可以知道机器到底学到了什么，是否和我们的预期相同。（ps: 准确率不足以让我们精确调整模型，只有当我们知道why the answer is wrong, so i can fix it. ）\n\n My Point of View\n我（李宏毅老师）的观点\n可解释机器学习的目标 != 完全了解机器学习模型如何工作 可以说完全了解它是如何工作，这件事未必是必要的。\n\n很多人不信任机器学习模型，因为他们认为它是个黑盒\n但是人类本身也是个黑盒\n\n所以可解释机器学习的目标应该是 让人（你的客户、老板、你自己）舒服 😆\n也许最终在未来，可解释机器学习可以根据不同的人给出不同的解释。比如，给小学生和大学生解释机器为什么会学习这件事，给出不同的解释。\n Interpretable v.s. Powerful\n可解释性 v.s. 强大\n\n简单的模型似乎不会很强\n深度学习模型是复杂的，却是强大的\n\n这里是说，可解释的机器学习不是放弃复杂的模型，只使用简单的模型，而是去尝试解释复杂模型。\n\n那有没有即容易解释又强大的模型呢，比如决策树？\n\n\n但是，事实上只用一棵决策树效果并不好，通常我们都会使用很多决策树结合起来得到比较强的效果，比如随机森林，xgboost 等，这些强大的模型也是很难解释的：\n\nhttps://stats.stackexchange.com/questions/230581/decision-tree-too-large-to-interpret\n Local Explanation Explain the Decision\nQuestion: why do you think this image is a cat?\n我们想知道机器在图片上的那部分看到了猫🐱，或者说它把那部分认成了猫。\n 基本精神\n现在有一个object xxx ，其中有NNN个Component，现在希望知道哪些Component对机器做判定是重要的。\n\n\n\nComponent\n在图片辨识任务中，component可以是一个pixel，或者一个segment（如上图中分割的图片块）\n在NLP任务中，component可以是一个word。\n\n\n关键精神：拿掉某个component或者改动其数值，观察决策变化。如果这个改动使模型决策发生非常大的变化，那这个component就是比较重要的。\n 举栗来说：\n\nReference: Zeiler, M. D., &amp; Fergus, R. (2014). Visualizing and understanding convolutional networks. In Computer Vision–ECCV 2014 (pp. 818-833)\n\n我们使用一个灰色的方块盖住图片的某个位置，观察模型的判断，最终得到如下结果。蓝色的部分是灰色方块覆盖该位置时模型认为该图片大概率不是正确标签的，红色部分表示当灰色方块覆盖该位置时模型认为该图片大概率是正确标签。\n\n上图反映出，该模型确实学习到正确的事物。另外，覆盖图片的方块的颜色、大小都是需要人工调整的参数，这其实是至关重要的。\n\nKaren Simonyan, Andrea Vedaldi, Andrew Zisserman, “Deep Inside Convolutional Networks: Visualising Image Classification Models and Saliency Maps”, ICLR, 2014\n\n\n对于每一个输 xxx 将其某个维度加上一个小小的扰动，然后观察模型判断结果和原判断结果的差值，根据这个扰动造成的结果差值来了解机器对图片中哪些像素比较敏感。（其实这个扰动造成的影响，可以理解为偏微分∂yk∂xn\\frac{\\partial y_k}{\\partial x_n}∂xn​∂yk​​） 根据这个偏微分的值，我们画出了上图的结果，亮度越大表示偏微分值越大。可见，该模型确实学习到正确的事物。\n Case Study: Pokémon v.s. Digimon\n我们想看看Saliency Map能带来什么样的结果，用机器学习辨识输入进来的图片是pokemon还是digimon。\n\n我们看看结果：\n\n好像很厉害哦，但是我们使用saliency map看看机器到底学到了什么。\n\n\n我们看到，机器好像把关注点都放在了本体之外的部分上，原来pokemon和digimon数据集一个是png，一个是jpg，两者的背景一个是透明的一个是纯黑的🤣\n 更多…\n\n\nGrad-CAM (https://arxiv.org/abs/1610.02391)\n\n\nSmoothGrad (https://arxiv.org/abs/1706.03825)\n\n\nLayer-wise Relevance Propagation(https://arxiv.org/abs/1604.00825)\n\n\nGuided Backpropagation(https://arxiv.org/abs/1412.6806)\n\n\n Limitation of Gradient based Approaches\n基于梯度下降的方法的限制\n刚才我们用gradient的方法判断一个pixel或者一个segment是否重要，但是梯度有一个限制就是gradient saturation（梯度饱和）。\n\ngradient saturation\n\n\n举例来说，我们如果通过鼻子的长度来判断一个生物是不是大象，鼻子越长是大象的信心分数越高，但是当鼻子长到一定程度信心分数就不太会变化了，信心分数就封顶了。在上图标记的点，梯度已经很小了，这和我们想要的结果不太一样了，因为我们希望鼻子的长度越长，它是大象的信心分数就越大。回到先前的判断重要性的方法，也就是说当某个特征梯度饱和时，用gradient based的方法判断，这个特征对模型做判别变得不重要了。\n那怎么解决这个问题呢，如图中列出的参考方法。\n Attack Interpretation\n攻击机器学习的解释是可能的吗？（可能的）\n\nInterpretation of Neural Networks is Fragile\nAmirata Ghorbani, Abubakar Abid, James Zou\nhttps://arxiv.org/abs/1710.10547\n\n\n我们可以加一些神奇的扰动到图片上，人眼虽然辨识不出来，也不改变分类结果，但是模型聚焦的判定点却变了。\n Global Explanation: Explain the whole Model\nQuestion: What do you think a “cat” looks like?\n我们想知道为什么机器认为的猫🐱具体长什么样子。（也就是去做一个生成器Generator）\n Activation Maximization(review)\n这个技术之前讲深度学习的时候讲过，参考，是说找一个样本让某一个filter的output或者是某一个neuron的output最大化：\n\n举例来说，在手写数字辨识的任务中，我们想知道机器心里理想的1长什么样子，也就是说要让机器画出一个1。做法是我们照一张图片作为输入x∗x^*x∗让代表1的output的那一个维度的值最大化，即：\nx∗=argmax⁡xyix^* = arg \\max_{x} y_i\nx∗=argxmax​yi​\n画出来的结果如上图所示，很差😐。鉴于之前讲过的Adversarial Attack，这个结果大概也在我们的意料之中了，在Adversarial Attack中我们给图片加上一个微小的扰动，模型就会将其辨识成不同的类别，现在也是一样，我们给机器看一个杂讯，它可能就会认为这个杂讯图片就是1，就是2…所以，我们光是让机器找一张图片，maximize output的某一维度，不足以告诉我们机器心中认为的该output维度代表的数字的样子。\n我们需要加一些额外的限制：\n\n我们不仅要maximize output的某个维度，还要加一个限制函数R(x)R(x)R(x)让xxx尽可能像一个数字图片，这个函数输入是一张图片，输出是这张图片有多像是数字。\n那这个限制函数R(x)R(x)R(x)要怎么定义呢，定义方式有很多种，这里就简单的定义为所有pixel的和的数值，这个值越小说明黑色rgb(0,0,0)越多，这个图片就越像是一个数字，由于这里数字是用白色的笔写的，所以就加一个负号，整体去maximize yiy_iyi​与R(x)R(x)R(x)的和，形式化公式就变成了：\nx∗=argmax⁡x(yi+R(x))x^* = arg \\max_{x}( y_i + R(x) )\nx∗=argxmax​(yi​+R(x))\n得到的结果如上图所示，比加限制函数之前要好很多了，从每个图片中多少能看出一点数字的形状了，尤其是6那一张。\n在更复杂的模型上，比如要在ImgNet这个大语料库训练出的模型上做上述的事情，在不加限制函数的情况下将会得到同样杂乱无章的结果。而且在这种复杂模型中要想得到比较好的Global Explanation结果，往往要加上更多、更复杂、更精妙的限制函数。以下是研究者做出的比较厉害Global Explanation的结果：\n\n参考：https://arxiv.org/abs/1506.06579\n Constraint from Generator\n生成器的约束\n刚才是用人想象的方法（限制函数），告诉机器什么样的东西看起来像是一张正常的图片。现在我们让机器自己生成图片。只要我们收集一大堆图片给机器看，然后训出一个图片生成器。\n\n图片生成器会接收一个低维向量，输出一张图片。这个低维向量zzz可能是从gaussian distribution或者normal distribution中randomly sample出来的点，把这个输入zzz丢入生成器他就会输出一个图片。关于怎么根据这些训练资料训练出这个生成器，你可以使用GAN，VAE等方法，这里不需要展开。\n那怎么把这个图片生成器作为限制函数，限制我们让机器画出来的图片呢？如上图下半部分所示，我们将方法转变为：找一个zzz，将它输入到图片生成器中产生一个图片，再将这个图片输入原来的图片分类器，得到一个类别判断yyy，我们同样要maximize yiy_iyi​. 其实可以考虑为我们把图片生成器和图片分类器连接在一起，fix住两者的参数，通过梯度下降的方法不断改变zzz，最终找到一个合适的z∗z^*z∗可以maximize yiy_iyi​.之后我们只要将这个z∗z^*z∗丢入图片生成器就可以拿到图片x∗x^*x∗了。形式化表述为：\nz∗=argmax⁡xyiz^* = arg \\max_{x} y_i\nz∗=argxmax​yi​\n通过上述的方法（再加上亿点点trick），可以得到的结果如下图：\n\n参考：https://arxiv.org/abs/1612.00005\n Using A Model To Explain Another\n核心思想：Using interpretable model to mimic uninterpretable models.\n用一个可解释的模型，去解释一个复杂的不可解释的模型\n方法：用一个可解释的模型，去模拟一个不可解释的模型的行为。如下图所示：\n\n比如说，训练一个线性模型（蓝色）去模拟神经网络模型（黑色，黑盒）的行为，让两者的输出尽可能相同。但是问题是，由于线性模型太弱了，不足以模拟复杂的神经网络模型，难顶😑。那其实我们可以只模拟神经网络的一部分：\n Local Interpretable Model-Agnostic Explanations (LIME)\n不可知模型的局部解释\n\n如上图所示，黑盒模型是蓝色线条，我们可能无法用线性函数拟合这条曲线，但是我们可以尝试模拟图中标记点的部分，如红线所示。\nLIME的步骤：\n\n给出你想解释的数据点\n在这个点附近sample数据点\n用线性（或其他可解释模型）fit这些点\n解释线性模型\n\n通过上述步骤，我们就能近似的解释我们感兴趣的点的附近区域内，模型的行为。问题又来了，我们怎么定义“附近”这个概念呢，我们对附近的定义不同，我们得到的分析结果就不同，就像上面Local Explanation中覆盖图片的方块的大小和颜色不同会得到不同的结果。对于上面的图，如果我们把附近定的比较宽泛，就会变成这样：\n\n显然，得到的线性模型就很不一样了。所以结论就是，这个附近是需要你自己调整的hyperparameter.\nPS：LIME通常只能用作局部近似分析，因为可解释模型一般都很简单，简单到不能模拟复杂模型的行为。\n LIME - Image 实践\n接下来我们实际把LIME用作解释一个图片分类器：\n\nRef: https://medium.com/@kstseng/lime-local-interpretable-model-agnostic-explanation-技術介紹-a67b6c34c3f8\n\n\n\n\n首先现有一张要解释的图片，我们想知道模型为什么把它认作树蛙\n\n\n在这张图附近sample一些数据\n我们通常会用一些toolkit把图片做一下切割，然后随机的丢掉一些图块，得到的新图片作为原图片附近区域中的数据点。\n\n\n把上述这些图片输入原黑盒模型，得到黑盒的输出\n\n\n用线性模型（或者其他可解释模型）fit上述数据\n在上面的栗子中我们在做图片辨识任务，此时我们可能在将图片丢到线性模型之前先做一个特征抽取，如下图所示：\n\n\n\n​\t\t根据你做的任务不同有不同的trick。\n\n\n解释你的线性模型\n\n如上图所示，当线性模型种某个特征维度对应的weight：\n\n趋近于零，说明这个segment对模型判定树蛙不重要\n正值，说明这个segment对模型判定树蛙有作用\n负值，说明这个segment对模型判定树蛙有反作用\n\n\n\n李宏毅老师实做：\n\n机器认为图中25%的概率有和服，5%的概率有实验袍。用LIME分析一下，机器认为哪部分是和服，哪部分是实验袍。结果如上图所示。🤔\n Decision Tree\n上面使用线性模型解释黑盒模型，我们也可以使用其他的可解释的简单模型来解释，比如决策树。但是如果我们为了让decision tree模仿一个复杂模型的行为，而训练出一个很深的tree，那就是搬起石头砸自己的脚。也就是说，我们不希望训练出一个太深的decision tree，我们需要限制tree的深度。\n\n我们将模拟黑盒模型 θθθ 的决策树模型写作 TθT_θTθ​，我们希望有一个函数 O(Tθ)O(T_θ)O(Tθ​) 表示 TθT_θTθ​ 有多复杂，比如说这个函数可以用 TθT_θTθ​ 的平均深度表示。我们现在希望复杂模型找到一个决策树，同时希望这个决策树的复杂度小，怎么做这件事呢，看看下面这个work：\n\nhttps://arxiv.org/pdf/1711.06178.pdf\n\n\n想法是：我们来训练一个特别的神经网络，我们在训练这个神经网络时就考虑到它将来要被决策树分析，我们在训练它本身的参数时，同时要考虑到最小化后面用来分析它的决策时的复杂度。我们可以在损失函数种加入 O(Tθ)O(T_θ)O(Tθ​) 作为regularization，这个东西也可叫做tree的regularization。\n但是这个 O(Tθ)O(T_θ)O(Tθ​) 是不可微分的，怎么做梯度下降呢？这也是这篇文章神奇的地方，作者说他train了一个神奇的神经网络，我们可以给这个神经网络另一个神经网络的参数，它就可以输出一个数值，来表示输入的网络的复杂度。所以我们只要用这个神奇的网络，替换树正则项，就可以做梯度下降了。其实，文中描述的这个神奇的神经网络只是一个简单的前反馈神经网络而已。这个网络还蛮准确的预测到输入的网络如果用decision tree模拟时，decision tree的深度😮。\n这个神奇的网络怎么来呢？训练这个神奇的网络，可以通过我们自己random的生成一些NN，然后用DT模拟这些NN，再把NN和DT的深度做为样本，train出这个神奇的神经网络。\n细节可以参考上述文献。\n","categories":["李宏毅机器学习笔记"],"tags":["ML","note","Next-Step-of-ML"]},{"title":"Attack and Defense","url":"https://ch3nye.top/Attack-and-Defense/","content":" Attack and Defense\n这里的攻击和防御是指针对人工智能或者说机器学习的模型的攻击与防御。\n Motivation\n做攻击与防御的动机是什么？\n\n我们想把机器学习技术应用到实际生活中\n目前的机器学习模型在多数情况下对噪声的干扰抗性不足\n我希望机器学习的模型不仅能够对抗一般的噪声干扰，同时还能对抗来自人类的恶意攻击的噪声\n特别是对于用来做垃圾邮件分类、恶意软件检测、网络入侵检测等任务的模型，我们更加需要让这样的模型健壮起来。\n\n Attack\n其实，目前多数的模型都是比较容易被攻击，而往往防御攻击是比较困的。\n What do we want to do?\n我们到底要做的是怎么样的攻击呢，这里举一个栗子：\n\n这里有一个图片辨识的模型，你给它一张猫🐱片，他就告诉你这张图片是一只猫，如上图所示。现在我们要做的事是这样，我们在图片 xxx 上加上攻击者特制的噪声 ΔxΔxΔx ，使得机器认不出这张图片，或者机器会把这张图片认成另一种攻击者指定的类别。类似这样的做法就叫做对模型的对抗样本攻击(Adversarial Attack)。\n Loss Function for Attack\n我们通常在train这样的model的时候会minimize一个损失函数，我们希望模型的输出能和真正的标签尽可能相同：\n\n现在我们要对模型进行攻击，攻击分成两种：有目标的攻击和无目标的攻击。\n Non-targeted Attack\n无目标的攻击的概念是这样的：我们要找一个 x′x&#x27;x′ 使其输入到模型后产生的输出能和 ytruey^{true}ytrue 越远越好（可以使用各种衡量方法，比如交叉熵cross entropy）。\n\n这个过程也是需要训练的，不过和原来训练模型的方式不同，我们现在要fixed住模型的参数 θθθ ，去调整 xxx ，使得模型输出 y′y&#x27;y′ 和正确值 ytruey^{true}ytrue 越远越好，所以损失函数可以定义为：\nL(x′)=−C(y′,ytrue)L(x&#x27;) = -C(y&#x27;, y^{true})\nL(x′)=−C(y′,ytrue)\n Targeted-Attack\n有目标攻击的概念是这样的：我们要找一个 x′x&#x27;x′ 使其输入到模型后产生的输出能和攻击者指定的错误输出 yfalsey^{false}yfalse 越近越好（可以使用各种衡量方法，比如交叉熵cross entropy）。\n\n同上，有目标的攻击也是需要训练的，一样的要fixed住模型的参数 θθθ ，去调整 xxx ，使得模型输出 y′y&#x27;y′ 和正确值 ytruey^{true}ytrue 越远越好，同时 y′y&#x27;y′ 能和指定的错误输出 yfalsey^{false}yfalse 越近越好，所以损失函数可以定义为：\nL(x′)=−C(y′,ytrue)+C(y′+yfalse)L(x&#x27;) = -C(y&#x27;, y^{true}) + C(y&#x27; + y^{false})\nL(x′)=−C(y′,ytrue)+C(y′+yfalse)\n但是如果 x’x’x’ 和原来的图片差太多那就没什么意义了，人一眼就可以看出这张图被修改过，甚至都看不出它原来的样子了。所以，我们需要一个限制条件，让 x0x^0x0 和 x′x&#x27;x′ 尽可能相同，让人认为恶意样本和原始样本看起来相同，骗过人类😁。我们的想法是如果这两者之间的差距小到一个阈值，人就不会发现，所以Constraint可以形式化的写成：\nd(x0,x′)≤εd(x^0,x&#x27;) \\le ε\nd(x0,x′)≤ε\n Constraint\n这个约束的distance应该怎么做呢？这里举两个栗子：\n\n L2-norm\n二范数如上图所示，挺好算的，就是取两者之差的平方和。\n L-infinity\nL∞L_\\inftyL∞​ 范数可以理解为取所有差值中最大的一个。\n对于不同的模型攻击时使用的distance应该是不同的，要选择合适的方法去做（根据人类的感知，来判断某个计算distance的方法是否合适）。\n上图中四个颜色的方块，分别使用了两种不同的方法进行了修改，给每一个pixel一点点修改，给一个pixel比较大的修改。彩色的四个方块，所以其实 xxx 有12个分量，调整后的两个图片的二范数distance计算出来的值其实时一样的，但是就人类的感官来看，似乎右下角的图片能看出来绿色更深了。我们再用无穷范数算一下，发现右下角的distance是大的，右上角则较小。所以说，似乎二范数计算distance不能很好的反应人类在图片上的感知。\n How to Attack\n攻击的过程如上面所说的，就和训练神经网络的是相似的，只不过是fix住参数，根据梯度下降改变输入，找到一个 x∗x^*x∗ 满足我们的目的而已。这个过程可以用以下方程描述：\nx∗=argmin⁡d(x0,x′)≤εL(x′)x^* = arg\\min_{d(x^0,x&#x27;) \\le ε} L(x&#x27;)\nx∗=argd(x0,x′)≤εmin​L(x′)\n这里有个限制，似乎会感到困惑，因为一般的模型训练中是没有限制的。没关系这个很好处理的，如下所示：\n\n我们将原始的 x0x_0x0​ 作为初始值，用计算梯度，然后更新输入为 xtx^txt ，然后做个判断，看看 xtx^txt 和 x0x^0x0 之间的距离是否大过阈值，如果超过的话就修正一下输入，让输入符合constraint，否则就继续。那怎么修正呢，这个fix函数可以这样做：\n\nfit()：对于所有符合constraint的 xxx 我们返回一个最接近 xtx^txt 的就可以了。\n如上图所示，对于二范数衡量distance，就是在 x0x^0x0 为圆心，ε 为半径的圆内找一个最接近 xtx^txt 的输入替代 xtx^txt ，如果超出了就找圆心和超出的点所在直线与圆的交点替换就可以了；对于无穷范数衡量distance，就是在任意方向上都不可以超出 ε 远的距离，如果超出了就在各个维度上都拽到边界值上就可以了。图中是二维的，实际上就上述小方块来说是12维的，真实的模型通常是上万维的。\n Example-实践\n使用上述的方法，在ResNet-50这个模型上做Targeted-Attack实验，原本是一张Tiger cat的图片，yfalsey^{false}yfalse 定为Star Fish，update 50次参数的攻击效果如下：\n\n效果是针不戳。我们看看两张图片的差异，因为差异太小肉眼难以看见，所以看看x50倍的效果：\n\n上面我们叠加上去的噪声是我们精心构造的，那我们随机加一些噪声会怎么样呢？\n\n我们可以看到，这些噪声都是比较明显的，但是前两张都被比较正确的识别出来了，至少都还是cat🐱，当我们叠加的噪声十分夸张时机器才没办法辨识这个图片。\n What happend？\n总结一下，其实模型很容易就能被attack。但是这件事为什么会发生呢？我们可以这样来解释一下：\n\n我们可以这样解释，你可以想象 x0x^0x0 是在非常高维空间中的一个点，你把这个点随机的移动，你会发现多数情况下在这个点的附近很大一个范围内都是能正确辨识的空间，当你把这个点推到的很远的时候才会让机器识别成类似的事物，当你把它推的非常远时才会被辨识成不相关的事物。但是，有一些神奇维度，在这些神奇的维度上 x0x^0x0 的正确区域只有非常狭窄的一点点，我们只要将 x0x^0x0 推离原值一点点，就会让模型输出产生很大的变化。\n总结这个现象就是在某一些维度上正确域是很狭窄的，只要在这些维度上推离一点点就会让机器判断出错。上述说法只是在现象上解释，出现了什么样的情况，但是没能真正的弄清楚这个现象是怎么出现的，其背后的原理是什么。\n Attack Approaches\n可供参考的方法：\n\n\nFGSM (https://arxiv.org/abs/1412.6572)\n\n\nBasic iterative method (https://arxiv.org/abs/1607.02533)\n\n\nL-BFGS (https://arxiv.org/abs/1312.6199)\n\n\nDeepfool (https://arxiv.org/abs/1511.04599)\n\n\nJSMA (https://arxiv.org/abs/1511.07528)\n\n\nC&amp;W (https://arxiv.org/abs/1608.04644)\n\n\nElastic net attack (https://arxiv.org/abs/1709.04114)\n\n\nSpatially Transformed (https://arxiv.org/abs/1801.02612)\n\n\nOne Pixel Attack (https://arxiv.org/abs/1710.08864)\n\n\n…… only list a few\n\n\n有很多五花八门的不同的攻击方法，但是这些攻击方法往往不同之处就在于使用不同的distance测量方法作为constraint，还有使用不同的优化Loss的方法：\n\nFGSM（Fast Gradient Sign Method）可能不是一个非常好的方法，但是他确实是一个非常简单的方法。他的做法如上图蓝框框所示，我们只要把 x0x^0x0 减去一个 εΔxεΔxεΔx 就算完了。这个 ΔxΔxΔx 就是Loss对 xxx 的偏微分的正负。举例来说，如果 ∂L/∂x1\\partial L/\\partial x_1∂L/∂x1​ 是正的，那我们就在 x0x^0x0 的第一维减掉一个 ε ，反之则加上 ε 。这个方法就像一拳超人一样，只要攻击一次就达成目标。\n其实，多攻击几次在文献中的结果看来效果是更好的，这种攻击叫做 Iterative FGSM，这里不展开。\n我们来直观的解释一下FGSM：\n\n当我们更新输入的时候，我们只要找到梯度的方向，就把输入朝向梯度的反方向更新一个ε，举例来说，在上图二维平面上，如果梯度指向左下角，不管它指向左下角具体哪个角度，我们都会将参数更新到橙色的点。而如果用原来的方法我们就会更新到梯度方向的反方向上，比如点 x1x^1x1 ，很可能是不会超出constraint的方框的。你可以理解成FGSM是在使用一个巨大的learning rate，大到一次update就会直接跳出constraint，我们再使用fit函数将跑出边界的点拉回到边界的角角上。\n White Box v.s. Black Box\n上述的攻击方法，我们都是fix住神经网络的参数去找 x′x&#x27;x′ . 也就是说要想攻击模型，我们必须知道模型的所有参数 θθθ ，此类攻击我们称为白盒攻击。那如果我们不把模型泄露出去，这样就安全了吗？也不会百分百安全，因为我们还可以采取黑盒攻击。😐\n那黑盒攻击怎么做呢？\n Black Box Attack\n现在我们有一个图片辨识模型，完全不知道这个模型的参数和内部结构，但是，我们能拿到这个模型训练使用的corpus（数据集，语料库）。\n\n我们可以通过这个数据集自己训练一个模型，称为proxy network。然后，我们用上述的对抗样本的方法攻击这个代理模型，得到一个trigger（恶意样本），用这个恶意样本去攻击原模型。神奇的是，这种方法往往能够成功。\n\nhttps://arxiv.org/pdf/1611.02770.pdf\n\n我们来看看这个方法的实验结果，引用自上述论文：\n\n值描述的是模型辨识正确的概率，也就是攻击失败的概率。上述五种神经网络的架构是不一样的，但是我们可以看到即使是不同架构的模型攻击成功的概率也是非常高的，而相同的架构的模型攻击成功率则明显是更高的。\n Universal Adversarial Attack\n核心精神是找一个通用的攻击向量，将其叠加到任意样本上都会让模型辨识出错。\n\nhttps://arxiv.org/abs/1610.08401\n\n\n这件事做成以后，你可以想象，只要在做辨识任务的摄像机前面贴一张噪声照片，就可以让所有结果都出错😮。另外，这个通用攻击甚至也可以做上述的黑盒攻击。\n Adversarial Reprogramming\n对抗重编程攻击？不知道如何翻译合适，这个攻击的核心精神是：通过找一些噪声，让机器的行为发生改变，达到重编程实现其他功能的效果。举个栗子：\n\nGamaleldin F. Elsayed*, Ian Goodfellow,* Jascha Sohl-Dickstein, “Adversarial Reprogramming of Neural Networks”, ICLR, 2019\n\n\n本来模型在做图片辨识，我们通过攻击希望这个模型能做数方块这个任务，当我们把图片中间贴上上图中那种方块图，机器就会帮我们数出途中方块的个数，如果有一个方块会输出tench，有两个方块就输出goldfish… 这件事还挺神奇的，因为我们并没有改变机器的任何参数，我们只是用了和前述相同的方法，找了一个噪声图片，然后把要数方块的图贴在噪声上，输入模型就会让模型帮我们数出方块的个数，真实very amazing啊😃。具体方法细节参考引用文章。\n Attack in the Real World\n我们想知道上述的攻击方法是否能应用在现实生活中，上述的所有方法中加入的噪声其实都非常的小，在数字世界中这些噪声会对模型的判别造成很大影响似乎是很合理的，但是在真实的世界中，机器是通过一个小相机看世界的，这样的小噪声通过相机以后可能就没有了。那我们就来看看其他人做的实验：\n视频连接：https://www.youtube.com/watch?v=zQ_uMenoBCk&amp;feature=youtu.be\n从视频中的结果来看似乎在现实世界中攻击是非常可行的。\n视频中只是在打印出来的图片上加噪声，但是在人脸识别任务中，你总不可能打印出人脸再上加噪声吧，有人就提出在眼睛框上加噪声：\n\nhttps://www.cs.cmu.edu/~sbhagava/papers/face-rec-ccs16.pdf\n\n\n左侧图是在整个人脸上加噪声，但是这种方法只能在实验室做，而右侧是让一个女性带上特制的noise眼镜，就会被是被成一个男性。这个work的作者在做演讲的时候是实际做了现场实验的。总之这种攻击在三次元世界中是切实可行的，但是要做到这件事也是要做非常多的努力的，列出几个难点：\n\n攻击者需要找到超越单个图像的扰动。（人是会动的，所以要保证所有角度都能攻击成功）\n极度不同的相邻像素之间的差异不太可能被相机准确捕捉到。（因相机解析度，人物运动等因素，要保证噪声以较大的色块的方式呈现，且不能出现相邻色块的极端差异，才能被相机正确捕捉）\n保证颜色是设备能够辨认的。（受限于设备工艺，尽量避免超出常见颜色阈值的色彩）\n\n另外一个栗子：\n对路边交通标志进行攻击，希望下列所有表示都能被是被成限速15的标志。他们也考虑了物理世界的种种显示，考虑到不能产生非常奇怪的图案引起怀疑，考虑到不同远近和角度，所以这个攻击确实会在未来自动驾驶道路上造成阻碍。\n\n Attack Beyond Image\n攻击也不知出现在图片辨识上，其他领域也是存在的，具体不展开了，只给出reference。\n\nYou can attack audio\n\nhttps://nicholas.carlini.com/code/audio_adversarial_examples/\nhttps://adversarial-attacks.net\n\n\nYou can attack text\n\nhttps://arxiv.org/pdf/1707.07328.pdf\n\n\n\n Defense\n有人觉得这种攻击能够成功，就是因为它对训练数据过拟合了，但这并不完全正确，因为就算你对model作regularization，作dropout，作model的ensemble，你也不能抗住攻击。可能你会觉得作ensemble尤其可能抵御住攻击，你攻击成功一个模型，我还有其他模型，但是这种想法是不行的，因为上面我们看到了attack是可以跨模型的，只要攻击成功ensemble中的一个模型，对其他模型也往往效果显著。\n现在的防御手段主要分成两大类：\n\nPassive defense：在不修改model的情况下，识别出被污染的输入。类似于在model外面加一层护盾\nProactive defense：在训练模型时考虑对抗攻击的防御，让模型变得robust起来\n\n Passive Defense\n\n可参考：https://evademl.org/\n\n怎么做被动攻击呢？我们来举个几个栗子：\n Filter\n\n我们在模型前加一个Filter，让有噪声的数据输入到Filter中做一个变换（比如，Smoothing），以尽可能消除噪声的影响。一张被加了对抗噪声的猫🐱片被识别为键盘，我们希望通过filter的处理以后能够减轻noise的伤害，而其他的正常图片经过filter处理后对辨识结果的影响尽可能小。\n那什么样的filter可以做到这件事呢，其实不需要太复杂的filter就可以做到这件事，比如作平滑化smoothing。来看看实验结果：\n\n为什么这件事会成功抵御攻击呢，我们可以想象攻击噪声只有在某些特殊的维度上才能造成显著的攻击效果，我们现在一旦加上filter，把那些特殊维度的值改变了，攻击效果就显著下降了，而这样的行为对正常的样本影响并不大。\n基于这样的思想我们再看一种方法，特征压缩。\n Feature Squeeze\n\nhttps://evademl.org/docs/featuresqueezing.pdf\nhttps://github.com/uvasrg/FeatureSqueezing\n\n\n核心思想是对输入样本的特征通过不同方式进行压缩简化，输入到model中得出预测结果和原样本的预测结果进行对别，如果差别较大，就判定为样本是恶意的对抗样本。更多细节可以阅读上述论文。\n Randomization at Inference Phase\n\nhttps://arxiv.org/abs/1711.01991\n\n\n如上图所示，我们将土图片做一些小小的缩放，在旁边加上小小的padding，将padded image输入模型，这样的方法再防御上也是效果显著的。\n上述的三个方法都是在模型前面加一层护盾的形式，类似这种方法的弱点就是，当防御方法被泄露的时候，攻击者就能比较容易的攻破护盾。比如，加Filter的方法，攻击者可以将这个filter想象成模型的一个layer，用传统的攻击方法进行攻击就可以了，从而设计出加了filter护盾的模型的恶意对抗样本；又或者说加上random缩放这个方法，如果你random作padding的方法被泄露出去，攻击有可能是成功的，可能可以做到universal的attack，也就是说针对所有的random padding都通用的attack的noise。\n Proactive Denfense\n核心精神：找出漏洞，补漏洞\n过程如下：\n\n我们训练network的方法就是给出训练数据，然后用这个数据集train出你的model。接下来就是对模型进行修补：\n你要再训练T次迭代，每次迭代过程中，要用对每个样本 xnx^nxn 根据攻击算法找出其可以attack的对抗样本，记作 x~n\\widetilde{x}^nxn ，将这些对抗样本加入数据集重训练model。\n这个增加训练数据的方法有点像Data Augmentation。为什么要作T次迭代呢，因为每次我们重训练以后model的参数就变了，就可能产生新的漏洞所以我们要不断的迭代多次，尽可能修补所有漏洞，实际上是不可能修补所有漏洞的。\n还有一件事，我们要注意在这个过程中找漏洞的algorithm，比如我们使用不同的算法，最后得到的是抗不同算法攻击的model，所以或许我么需要用不同算法，做多次这种防御攻击的重训练。\n To Learn More\n\nReference\n\nhttps://adversarial-ml-tutorial.org/ (Zico Kolter and Aleksander Madry)\n\n\nAdversarial Attack Toolbox:\n\nhttps://github.com/bethgelab/foolbox\nhttps://github.com/IBM/adversarial-robustness-toolbox\nhttps://github.com/tensorflow/cleverhans\n\n\n\n★推荐综述：人工智能系统安全与隐私风险\n\n更多的参考：\nAdversarial Attack Reference:\nIntriguing properties of neural networks\nhttps://arxiv.org/pdf/1312.6199\nExplaining and Harnessing Adversarial Examples\nhttps://arxiv.org/abs/1412.6572\nTowards Evaluating the Robustness of Neural Networks\nhttps://arxiv.org/pdf/1608.04644.pdf\nADVERSARIAL REPROGRAMMING OF NEURAL NETWORKS\nhttps://arxiv.org/pdf/1810.00069.pdf\nAudio Adversarial Examples: Targeted Attacks on Speech-to-Text\nhttps://arxiv.org/pdf/1801.01944.pdf\nTowards Deep Learning Models Resistant to Adversarial Attacks\nhttps://arxiv.org/abs/1706.06083\nObfuscated Gradients Give a False Sense of Security: Circumventing Defenses to Adversarial Examples\nhttps://arxiv.org/pdf/1802.00420.pdf\nOne pixel attack for fooling deep neural networks\n\nSlide :https://media.neurips.cc/Conferences/NIPS2018/Slides/adversarial_ml_slides_parts_1_4.pdf\nTwo hour tutorial: https://www.youtube.com/watch?v=TwP-gKBQyic (good one, with web page)\nGood fellow\nhttps://www.youtube.com/watch?v=CIfsB_EYsVI\nMore videos (I am not sure about the quality)\nhttps://www.youtube.com/watch?v=r8cW1p8VBOU\nhttps://www.youtube.com/watch?v=sh6OS6Lssv4\nTutorial which is too long: http://pralab.diee.unica.it/en/wild-patterns\n科普\nVideo: https://www.youtube.com/watch?v=oZYgaD004Dw\nVideo: https://www.youtube.com/watch?v=SA4YEAWVpbk\n\nhttps://zhuanlan.zhihu.com/p/37922148\nSlide :https://media.neurips.cc/Conferences/NIPS2018/Slides/adversarial_ml_slides_parts_1_4.pdf\nTwo hour tutorial: https://www.youtube.com/watch?v=TwP-gKBQyic (good one, with web page)\nGood fellow\nhttps://www.youtube.com/watch?v=CIfsB_EYsVI\nMore videos (I am not sure about the quality)\nhttps://www.youtube.com/watch?v=r8cW1p8VBOU\nhttps://www.youtube.com/watch?v=sh6OS6Lssv4\nTutorial which is too long: http://pralab.diee.unica.it/en/wild-patterns\n科普\nVideo: https://www.youtube.com/watch?v=oZYgaD004Dw\nVideo: https://www.youtube.com/watch?v=SA4YEAWVpbk\n\n","categories":["李宏毅机器学习笔记"],"tags":["ML","note","Next-Step-of-ML","Adversarial-Attack"]},{"title":"The Next Step for Machine Learning","url":"https://ch3nye.top/The-Next-Step-for-Machine-Learning/","content":" The Next Step for Machine Learning\n现在我们要讲一些新的技术，这些技术目前（2019年）被看作是机器学习的下一步，也就是说要让机器学习的技术真正落地到现实生活还需要哪些技术，还有哪些难题和障碍是需要我们解决的。\n 0x1 机器能不能知道“我不知道” Anomaly Detection\n这个问题听起来有点哲学哦，为什么要研究这个问题呢，举个栗子：你可能训练一个识别动物的model放到网络上供大家使用，但是你以为网友只会给你上传动物的图片吗，搞不好他就给你传个凉宫春日的图片😂：\n\n这种情况下，机器需要有能力说出：这个是我不知道的东西。机器会回答这是我不知道的，这样的技术就是Anomaly Detection。\n 0x2 说出为什么“我知道” Explanation ML\n今天我们看到各式各样机器学习非常强大的力量，感觉机器好像非常的聪明。但这有可能是这样一个故事，过去有一只马叫做汉斯，它非常的聪明，聪明到甚至可以做数学。举例来说：你跟它讲 9\\sqrt{9}9​ 是多少，它就会敲它的马蹄三次，大家欢呼道，这是一只会算数学的马。可以解决根号的问题，大家都觉得非常的惊叹。后面就有人怀疑说：难道汉斯真的这么聪明吗？在没有任何的观众的情况下，让汉斯自己去解决一个数学都是题目，这时候它就会一直踏它的马蹄，一直的不停。这是为什么呢？因为它之前学会了观察旁观人的反应，它知道什么时候该停下来。它可能不知道自己在干什么，它也不知道数学是什么，但是踏对了正确的题目就有萝卜吃，它只是看了旁边人的反应得到了正确的答案。\n今天我们看到种种机器学习的成果，难道机器真的有那么的聪明吗？会不会它会汉斯一样用了奇怪的方法来得到答案的。\n\n这件事其实是有可能发生的，举例来说，有人做了一个马的辨识器，两个model的辨识率都很高。然后分析，机器是根据什么来标识马的。\n\nhttp://iphome.hhi.de/samek/pdf/MonDSP18.pdf\n\n\n\n第一个DNN模型是看到图上黄红部分正常分析出有马，看上去挺正常的；第二个FV模型是看到下面红色部分，辨识出有马，它只是看到左下角的英文，标识出马，并没有学到马真正的样子，其实这些图片都是来自一个网站都有相同的水印。\n我们不知道AI有没有那么聪明，我们需要一些技术，让AI不只是做出决定，还要让它告诉我们说它为什么做出这样的决定。\n 0x3 机器的错觉\n我们知道说，人是有错觉的，比如下面两个圈圈，你觉得哪个圈圈颜色比较深？\n\n你可能会觉得左边比较深，但你大概猜到我在搞你，会说右边更深。\n但是其实：\n\n确实左边的圈圈颜色比较深，这是一个计中计🤣\n机器跟人一样，也很容易被骗，我们可以加一些噪声，让机器本来以为是的后来判断为不是。如本来判断出来是熊喵，加了噪声，就判断错误了。这种就叫做 Adversarial Attack。你甚至可以想象相同的技术应用在自动驾驶领域，有人在交通标识牌上贴一个贴纸，你的自动驾驶汽车就加速撞车，这显然是不可接受的，所以我们要想办法防御这种攻击，具体的后面Attack and Defense一节中会讲到。\n\n 0x4 终身学习 Life-long Learning\n我们希望机器能终生学习。人就是终生学习的，上学期修了线性代数，这学期学机器学习，学好线性代数，机器学习学得更容易。机器能不能跟人一样也做终生学习呢？现在我们一般只让一个模型学习一个任务，比如Alpha Go就只学习下围棋，Alpha star就是玩星际2，它们并不是同一个模型。\n\n今天我们只让一个模型学习一个人任务，显然会存在如下问题\n\n模型的数量无限增长\n之前学到的技能对之后的学习没有帮助\n\n为什么我们今天不让机器去终生学习呢？比如我们先让机器学下围棋，然后再让它学玩星际2，实际上，当机器学完星际2之后它就不会下围棋了。这个现象叫做Catastrophic Forgetting（灾难性忘记？）。如果想让机器做终身学习，还尚在解决的问题。\n 0x5 学习如何学习 Meta-learning (Learn to Learn)\n以前我们设计一个机器学习算法，让机器能够学习；现在，我们能不能写出一个算法实现一个模型，让它能自己设计算法编写模型程序实现具有学习能力的模型。\n\n 0x6 一定要有很多训练数据吗\n在现实生活中有一些任务或者某些情境下只能获取少量的样本，甚至没有样本，比如受限于资金限制等情况，这时候我们要么把机器学习厚重的教科书砸到boss脸上辞职，要么试试下面的方法：\n\nFew-shot learning 让机器看少量的资料\nZero-shot learning 不给机器任何资料，只告诉机器物品的特征描述，然后机器根据描述进行判断\n\n 0x7 增强学习 Reinforcement Learning\nReinforcement Learning 真的有这么强吗？当你用Reinforcement Learning 去玩一些的游戏，Reinforcement Learning也许确实可以跟人做到差不多，但是需要很长时间才能达到，如下图机器需要900多个小时才能达到人类2个小时能达到的效果。机器感觉就是一个天资不佳却勤奋不解的笨小孩，他需要非常久的时间非常多的练习才能和人达到相同的水平，那Reinforcement Learning为什么学得这么慢，有没有办法让它快一点，就是我们要考虑的问题。\n\n\n图片来自：http://web.stanford.edu/class/psych209/Readings/LakeEtAlBBS.pdf\n\n 0x8 神经网络压缩 Network Compression\n如果我们要把机器学习的模型应用到现实生活中，由于设备的运算和存储能力有限，带来的问题就是我们能不能把Network 的架构缩小，但让它维持同样的能力。\n\n\n把一个大的神经网络缩小，减掉多余的神经元\n\n\n\n参数二值化\n都变成“+1”和“-1”。如果是连续数值，就需要大量的运算和内存，如果把所有参数进行二值化，那么运算起来就快，内存也占用少。\n\n\n 0x9 机器学习的谎言\n今天我们在训练的时候，假设训练和测试的数据分布是一样的或者至少非常相似，但是实际上在真实应用里面，这就是一个谎言。\n\n如果我们做手写数字辨识，在实验室情况下，训练资料和测试资料非常相似，可能会很容易达到99.5%。但是实际中，可能图片有背景，正确率变成57.5%，直接烂掉了。\n\n那么怎么解决机器在训练资料和测试资料不同的场景呢？现有可以参考的技术有Unsupervised Domain Adaptation，不在展开。\n","categories":["李宏毅机器学习笔记"],"tags":["ML","note","Next-Step-of-ML"]}]